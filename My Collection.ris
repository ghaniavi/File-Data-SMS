TY  - JOUR
T1  - Minimizing postprandial hypoglycemia in Type 1 diabetes patients using multiple insulin injections and capillary blood glucose self-monitoring with machine learning techniques
A1  - Oviedo, Silvia
A1  - Contreras, Ivan
A1  - Bertachi, Arthur
A1  - Quirós, Carmen
A1  - Giménez, Marga
A1  - Conget, Ignacio
A1  - Vehi, Josep
Y1  - 2019///
KW  -  Bolus reduction
KW  -  Hypoglycemia prediction
KW  -  Machine learning
KW  -  Postprandial hypoglycemia
KW  -  Type 1 diabetes
KW  - Blood glucose
JF  - Computer Methods and Programs in Biomedicine
VL  - 178
SP  - 175
EP  - 180
DO  - https://doi.org/10.1016/j.cmpb.2019.06.025
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719304900
N2  - Background: Diabetic patients treated with intensive insulin therapies require a tight glycemic control and may benefit from advanced tools to predict blood glucose (BG) concentration levels and hypo/hyperglycemia events. Prediction systems using machine learning techniques have mainly focused on applications for sensor augmented pump (SAP) therapy. In contrast, insulin bolus calculators that rely on BG prediction for multiple daily insulin (MDI) injections for patients under self-monitoring blood glucose (SMBG) are scarce because of insufficient data sources and limited prediction capability of forecasting models. Methods: We trained individualized models that can predict postprandial hypoglycemia via different machine learning algorithms using retrospective data from 10 real patients. In addition, we designed and tested a hypoglycemia reduction strategy for a similar in silico population. The system generates a bolus reduction suggestion as the scaled weighted sum of the predictions. We evaluated the general and postprandial glycemic outcomes of the in silico population to assess the systems capability of avoiding hypoglycemias. Results: The median [IQR] sensitivity and specificity for hypoglycemia cases where the BG level was below 70 mg/dL were 0.49 [0.2–0.5] and 0.74 [0.7–0.9], respectively. For hypoglycemia cases where the BG level was below 54 mg/dL, the median [IQR] sensitivity and specificity were 0.51 [0.4–0.6] and 0.74 [0.7–0.8], respectively. Conclusions: The results indicated a decrease of 37% in the median number of postprandial hypoglycemias median decrease of 44% for hypoglycemias of 70 mg/dL and 54 mg/dL, respectively. This dramatic reduction makes this method a good candidate to be integrated into any Decision Support System for diabetes management.
ER  - 
TY  - JOUR
T1  - A comparison of machine learning algorithms for diabetes prediction
A1  - Khanam, Jobeda Jamal
A1  - Foo, Simon Y
Y1  - 2021///
KW  -  Accuracy
KW  -  Data Mining
KW  -  K-fold Cross Validation
KW  -  Neural Network
KW  - Machine learning
JF  - ICT Express
VL  - 7
IS  - 4
SP  - 432
EP  - 439
DO  - https://doi.org/10.1016/j.icte.2021.02.004
UR  - https://www.sciencedirect.com/science/article/pii/S2405959521000205
N2  - Diabetes is a disease that has no permanent cure; hence early detection is required. Data mining, machine learning (ML) algorithms, and Neural Network (NN) methods are used in diabetes prediction in our research. We used the Pima Indian Diabetes (PID) dataset for our research, collected from the UCI Machine Learning Repository. The dataset contains information about 768 patients and their corresponding nine unique attributes. We used seven ML algorithms on the dataset to predict diabetes. We found that the model with Logistic Regression (LR) and Support Vector Machine (SVM) works well on diabetes prediction. We built the NN model with a different hidden layer with various epochs and observed the NN with two hidden layers provided 88.6% accuracy.
ER  - 
TY  - JOUR
T1  - Diabetes detection using deep learning techniques with oversampling and feature augmentation
A1  - García-Ordás, M T
A1  - Benavides, C
A1  - Benítez-Andrades, J A
A1  - Alaiz-Moretón, H
A1  - García-Rodríguez, I
Y1  - 2021///
KW  -  Computer
KW  -  Data augmentation; Data preprocessing; Diabetes detection; Early diagnosis; Health status; Learning techniques; Over sampling; State of the art
KW  -  Databases
KW  -  Deep learning
KW  -  Factual; Deep Learning; Diabetes Mellitus; Humans; Neural Networks
KW  -  age; Article; blood pressure; convolutional neural network; data accuracy; decision tree; deep learning; diabetes mellitus; feature extraction; glucose level; human; Indian; insulin level; k nearest neighbor; multilayer perceptron; oversampling; prediction; pregnancy; random forest; sparse autoencoder; support vector machine; variational autoencoder; factual database
KW  -  glucose; insulin
KW  - Blood pressure; Classification (of information); Convolutional neural networks; Diagnosis; Learning systems; Pathology; Pipelines
PB  - Elsevier Ireland Ltd
JF  - Computer Methods and Programs in Biomedicine
VL  - 202
DO  - 10.1016/j.cmpb.2021.105968
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101087885&doi=10.1016%2Fj.cmpb.2021.105968&partnerID=40&md5=b891ab2bcafbd954404e907c2ca88e20
N1  - cited By 17
N2  - Background and objective: Diabetes is a chronic pathology which is affecting more and more people over the years. It gives rise to a large number of deaths each year. Furthermore, many people living with the disease do not realize the seriousness of their health status early enough. Late diagnosis brings about numerous health problems and a large number of deaths each year so the development of methods for the early diagnosis of this pathology is essential. Methods: In this paper, a pipeline based on deep learning techniques is proposed to predict diabetic people. It includes data augmentation using a variational autoencoder (VAE), feature augmentation using an sparse autoencoder (SAE) and a convolutional neural network for classification. Pima Indians Diabetes Database, which takes into account information on the patients such as the number of pregnancies, glucose or insulin level, blood pressure or age, has been evaluated. Results: A 92.31% of accuracy was obtained when CNN classifier is trained jointly the SAE for featuring augmentation over a well balanced dataset. This means an increment of 3.17% of accuracy with respect the state-of-the-art. Conclusions: Using a full deep learning pipeline for data preprocessing and classification has demonstrate to be very promising in the diabetes detection field outperforming the state-of-the-art proposals. © 2021 Elsevier B.V.
ER  - 
TY  - JOUR
T1  - Type 2: Diabetes mellitus prediction using Deep Neural Networks classifier
A1  - P, Bala Manoj Kumar
A1  - R, Srinivasa Perumal
A1  - K, Nadesh R
A1  - K, Arivuselvan
Y1  - 2020///
KW  -  Classification
KW  -  Diabetes mellitus
KW  -  Feature selection
KW  -  Prediction
KW  - Deep learning
JF  - International Journal of Cognitive Computing in Engineering
VL  - 1
SP  - 55
EP  - 61
DO  - https://doi.org/10.1016/j.ijcce.2020.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S2666307420300073
N2  - Diabetes is a severe disease, most of the people are not aware of the risk associated with the disease because of that people die due to diabetic nephropathy, cardiac stroke and some other disorders. Therefore, early identification of diabetes helps to maintain sound health and life. Deep Learning approaches are used to predict diabetes accurately as humans do. In this paper, Deep Neural Network (DNN) classifier, an unsupervised learning approach is used for accurate prediction on Pima Indian Diabetes dataset and Feature Importance model that is bagged with Extra Trees and Random Forest is used for feature selection. The Pima Indian Diabetes dataset (PID) was acquired from the repository of UCI. The existing dataset has experimented with different formats of train test splits. The performance of the model was evaluated through accuracy, specificity, sensitivity, recall and precision. The model acheived 98.16% accuracy with random train-test split and it is observed that, the model obtained better performance than other state-of-art methods.
ER  - 
TY  - JOUR
T1  - Deep learning for diabetic retinopathy detection and classification based on fundus images: A review
A1  - Tsiknakis, Nikos
A1  - Theodoropoulos, Dimitris
A1  - Manikis, Georgios
A1  - Ktistakis, Emmanouil
A1  - Boutsora, Ourania
A1  - Berto, Alexa
A1  - Scarpa, Fabio
A1  - Scarpa, Alberto
A1  - Fotiadis, Dimitrios I
A1  - Marias, Kostas
Y1  - 2021///
KW  -  Classification
KW  -  Deep learning
KW  -  Detection
KW  -  Diabetic retinopathy
KW  -  Fundus
KW  -  Retina
KW  -  Review
KW  -  Segmentation
KW  - Artificial intelligence
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104599
EP  - 104599
DO  - https://doi.org/10.1016/j.compbiomed.2021.104599
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521003930
N2  - Diabetic Retinopathy is a retina disease caused by diabetes mellitus and it is the leading cause of blindness globally. Early detection and treatment are necessary in order to delay or avoid vision deterioration and vision loss. To that end, many artificial-intelligence-powered methods have been proposed by the research community for the detection and classification of diabetic retinopathy on fundus retina images. This review article provides a thorough analysis of the use of deep learning methods at the various steps of the diabetic retinopathy detection pipeline based on fundus images. We discuss several aspects of that pipeline, ranging from the datasets that are widely used by the research community, the preprocessing techniques employed and how these accelerate and improve the models' performance, to the development of such deep learning models for the diagnosis and grading of the disease as well as the localization of the disease's lesions. We also discuss certain models that have been applied in real clinical settings. Finally, we conclude with some important insights and provide future research directions.
ER  - 
TY  - JOUR
T1  - Detection and classification of retinal lesions for grading of diabetic retinopathy
A1  - Usman Akram, M
A1  - Khalid, Shehzad
A1  - Tariq, Anam
A1  - Khan, Shoab A
A1  - Azam, Farooque
Y1  - 2014///
KW  -  -Mediods
KW  -  Classification
KW  -  Cotton wool spots
KW  -  Haemorrhage
KW  -  Hard exudates
KW  -  Microaneurysms
KW  -  NPDR
KW  - Diabetic retinopathy
JF  - Computers in Biology and Medicine
VL  - 45
SP  - 161
EP  - 171
DO  - https://doi.org/10.1016/j.compbiomed.2013.11.014
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513003430
N2  - Diabetic Retinopathy (DR) is an eye abnormality in which the human retina is affected due to an increasing amount of insulin in blood. The early detection and diagnosis of DR is vital to save the vision of diabetes patients. The early signs of DR which appear on the surface of the retina are microaneurysms, haemorrhages, and exudates. In this paper, we propose a system consisting of a novel hybrid classifier for the detection of retinal lesions. The proposed system consists of preprocessing, extraction of candidate lesions, feature set formulation, and classification. In preprocessing, the system eliminates background pixels and extracts the blood vessels and optic disc from the digital retinal image. The candidate lesion detection phase extracts, using filter banks, all regions which may possibly have any type of lesion. A feature set based on different descriptors, such as shape, intensity, and statistics, is formulated for each possible candidate region: this further helps in classifying that region. This paper presents an extension of the m-Mediods based modeling approach, and combines it with a Gaussian Mixture Model in an ensemble to form a hybrid classifier to improve the accuracy of the classification. The proposed system is assessed using standard fundus image databases with the help of performance parameters, such as, sensitivity, specificity, accuracy, and the Receiver Operating Characteristics curves for statistical analysis.
ER  - 
TY  - JOUR
T1  - Rule extraction by support vector machine: a case study of type II diabetes mellitus diagnosis
A1  - Yang, Chien-Hsin
A1  - Yeh, Tsu-Ming
A1  - Liao, Mou-Yuan
Y1  - 2009///
JF  - IFAC Proceedings Volumes
VL  - 42
IS  - 4
SP  - 1685
EP  - 1688
DO  - https://doi.org/10.3182/20090603-3-RU-2001.0143
UR  - https://www.sciencedirect.com/science/article/pii/S147466701634040X
N1  - 13th IFAC Symposium on Information Control Problems in Manufacturing
N2  - Diabetes Mellitus (DM) is a typical metabolic syndrome in medicine. Many of the risk factors related to DM, including biochemical and anthropometric measurements have been discovered from a succession of studies. Although the risk factors have been found, their usability, i.e. their predictability or explainability, is lower in practical diagnoses. In other words, it is short of being comprehensive for physicians if the diagnosis modeling contains no more than the selected features. Compared to the anterior method, rule extraction possesses sufficient comprehensibility in medicine. For this reason, a support vector machine (SVM) is employed to DM diagnosis in this study. An SVM with prototype method was used to extract diagnosis rules. Another method, decision tree is as a benchmark. The results showed that this method had a better performance than decision tree approach. Both these two approaches have the advantage of being reliably comprehensible. In addition, the suggestions are helpful in prediction medicine.
ER  - 
TY  - JOUR
T1  - Predicting progression patterns of type 2 diabetes using multi-sensor measurements
A1  - Ramazi, Ramin
A1  - Perndorfer, Christine
A1  - Soriano, Emily C
A1  - Laurenceau, Jean-Philippe
A1  - Beheshti, Rahmatollah
Y1  - 2021///
KW  -  Continuous glucose monitoring
KW  -  Deep learning
KW  -  Predictive modeling
KW  -  Type 2 diabetes
KW  - Multi-modal data
JF  - Smart Health
VL  - 21
SP  - 100206
EP  - 100206
DO  - https://doi.org/10.1016/j.smhl.2021.100206
UR  - https://www.sciencedirect.com/science/article/pii/S2352648321000283
N2  - Type 2 diabetes – a prevalent chronic disease worldwide – increases the risk for serious health consequences including heart and kidney disease. Forecasting diabetes progression can inform disease management strategies, thereby potentially reducing the likelihood or severity of its consequences. We use continuous glucose monitoring and actigraphy data from 54 individuals with type 2 diabetes to predict their future hemoglobin A1c, HDL cholesterol, LDL cholesterol, and triglyceride levels one year later. Using a combination of convolutional and recurrent neural networks, we develop a deep neural network architecture that can learn the dynamic patterns in different sensors’ data and combine those patterns with additional demographic and lab measurment data. To further demonstrate the generalizability of our model, we also evaluate its performance using an independent public dataset of individuals with type 1 diabetes. In addition to diabetes, our approach could be useful for other serious and chronic physical illness, where dynamic (e.g., from multiple sensors) and static (e.g., demographic) data are used for creating predictive models.
ER  - 
TY  - JOUR
T1  - Hybrid approach using fuzzy sets and extreme learning machine for classifying clinical datasets
A1  - Nahato, Kindie Biredagn
A1  - Nehemiah, Khanna H
A1  - Kannan, A
Y1  - 2016///
KW  -  Classification
KW  -  Euclidean distance
KW  -  Fuzzification
KW  -  Fuzzy set
KW  -  Membership function
KW  - Extreme learning machine
JF  - Informatics in Medicine Unlocked
VL  - 2
SP  - 1
EP  - 11
DO  - https://doi.org/10.1016/j.imu.2016.01.001
UR  - https://www.sciencedirect.com/science/article/pii/S2352914816000022
N2  - Data mining techniques play a major role in developing computer aided diagnosis systems and expert systems that will aid a physician in clinical decision making. In this work, a classifier that combines the relative merits of fuzzy sets and extreme learning machine (FELM) for clinical datasets is proposed. The three major subsystems in the FELM framework are preprocessing subsystem, fuzzification subsystem and classification subsystem. Missing value imputation and outlier elimination are handled by the preprocessing subsystem. The fuzzification subsystem maps each feature to a fuzzy set and the classification subsystem uses extreme learning machine for classification. Cleveland heart disease (CHD), Statlog heart disease (SHD) and Pima Indian diabetes (PID) datasets from the University of California Irvine (UCI) machine learning repository have been used for experimentation. The CHD and SHD datasets have been experimented with two class labels one indicating the absence and the other indicating the presence of heart disease. The CHD dataset has also been experimented with five class labels, one class label indicating the absence of heart disease and the other four class labels indicating the severity of heart disease namely low risk, medium risk, high risk and serious. The PID data set has been experimented with two class labels one indicating the absence and the other indicating the presence of gestational diabetes. The classifier has achieved an accuracy of 93.55% for CHD data set with two class labels; 73.77% for CHD data set with five class labels; 94.44% for SHD data set and 92.54% for PID dataset.
ER  - 
TY  - JOUR
T1  - Text preprocessing for improving hypoglycemia detection from clinical notes – A case study of patients with diabetes
A1  - Zhou, Lina
A1  - Siddiqui, Tariq
A1  - Seliger, Stephen L
A1  - Blumenthal, Jacob B
A1  - Kang, Yin
A1  - Doerfler, Rebecca
A1  - Fink, Jeffrey C
Y1  - 2019///
KW  -  Adverse safety events detection
KW  -  Clinical notes
KW  -  Diabetes
KW  -  Text preprocessing
KW  - Hypoglycemia
JF  - International Journal of Medical Informatics
VL  - 129
SP  - 374
EP  - 380
DO  - https://doi.org/10.1016/j.ijmedinf.2019.06.020
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618314461
N2  - Background and objective
Hypoglycemia is a common safety event when attempting to optimize glycemic control in diabetes (DM). While electronic medical records provide a natural ground for detecting and analyzing hypoglycemia, ICD codes used in the databases may be invalid, insensitive or non-specific in detecting new hypoglycemic events. We developed text preprocessing methods to improve automatic detection of hypoglycemia from analysis of clinical encounter text notes.
Methods
We set out to improve hypoglycemia detection from clinical notes by introducing three preprocessing methods: stop word filtering, medication signaling, and ICD narrative enrichment. To test the proposed methods, we selected clinical notes from VA Maryland Healthcare System, based on various combinations of three criteria that are suggestive of hypoglycemia, including ICD-9 code of diabetes and hypoglycemia, laboratory glucose values < 70 md/dL, and text reference to a proximate hypoglycemia event. In addition, we constructed one dataset of 395 clinical notes from year 2009 and another of 460 notes from year 2014 to test the generality of the proposed methods. For each of the datasets, two physician judges manually reviewed individual clinical notes to determine whether hypoglycemia was present or absent. A third physician judge served as a final adjudicator for disagreements.
Results
Each of the proposed preprocessing methods contributed to the performance of hypoglycemia detection by significantly increasing the F1 score in the range of 5.3∼7.4% on one dataset (p < .01). Among the methods, stop word filtering contributed most to the performance improvement (7.4%). Combining all the preprocessing methods led to greater performance gain (p < .001) compared with using each method individually. Similar patterns were observed for the other dataset with the F1 score being increased in the range of 7.7%∼9.4% by individual methods (p < .001). Nevertheless, combining the three methods did not yield additional performance gain.
Conclusion
The proposed text preprocessing methods improved the performance of hypoglycemia detection from clinical text notes. Stop word filtering achieved the most performance improvement. ICD narrative enrichment boosted the recall of detection. Combining the three preprocessing methods led to additional performance gains.
ER  - 
TY  - JOUR
T1  - Baroreflex sensitivity with different lags and random forests for staging cardiovascular autonomic neuropathy in subjects with diabetes
A1  - Petry, Daiana
A1  - Mirian de Godoy Marques, Claudia
A1  - Brum Marques, Jefferson Luiz
Y1  - 2020///
KW  -  Baroreflex sensitivity
KW  -  Diabetes mellitus
KW  -  Heart rate variability
KW  -  Machine learning
KW  -  Random forest
KW  -  Subclinical CAN
KW  - Cardiovascular autonomic neuropathy
JF  - Computers in Biology and Medicine
VL  - 127
SP  - 104098
EP  - 104098
DO  - https://doi.org/10.1016/j.compbiomed.2020.104098
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520304297
N2  - Impaired baroreflex sensitivity (BRS) may indicate cardiovascular autonomic neuropathy (CAN), which often remains undiagnosed during the initial course of diabetes mellitus. The baroreflex mechanism can be considered negative feedback because of baroreflex delay, the time delay between a change in blood pressure and the counteracting heart rate response. This work sought to analyze BRS considering lags from 1 to 10 RR intervals. We hypothesized that diabetic patients with subclinical CAN (SCAN) have a detectable delay in autonomic nervous system activity and that this would differ from patients without CAN (NCAN) and with established CAN (ECAN). In the first stage, 30 patients were included in an exploratory analysis using the Principal Component Analysis. Six indexes related to the BRS delay were proposed and considered significant for staging diabetic patients. Three indexes allowed for the differentiating of patients with and without CAN, and three indexes distinguished subjects with SCAN from subjects with NCAN or ECAN. Then, in the second stage, a random forest model was developed with 72 subjects, using the variables selected in the first stage. It was possible to detect SCAN, and to point out those subjects with the potential to change the CAN stage, allowing for the tracking of CAN progression. The model achieved a sensitivity of 96% and specificity of 100% to detect SCAN. Thus, the BRS analysis considering delayed reaction in the dynamics of heart rate variability may contribute to an accurate screening tool to staging CAN, in addition to indicating patients with most insidious disease progress.
ER  - 
TY  - JOUR
T1  - Data preprocessing for heart disease classification: A systematic literature review
A1  - Benhar, H
A1  - Idri, A
A1  - Fernández-Alemán, J L
Y1  - 2020///
KW  -  Cardiac datasets
KW  -  Cardiology
KW  -  Data preprocessing
KW  -  Literature review
KW  - Datamining
JF  - Computer Methods and Programs in Biomedicine
VL  - 195
SP  - 105635
EP  - 105635
DO  - https://doi.org/10.1016/j.cmpb.2020.105635
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314681
N2  - Context
Early detection of heart disease is an important challenge since 17.3 million people yearly lose their lives due to heart diseases. Besides, any error in diagnosis of cardiac disease can be dangerous and risks an individual's life. Accurate diagnosis is therefore critical in cardiology. Data Mining (DM) classification techniques have been used to diagnosis heart diseases but still limited by some challenges of data quality such as inconsistencies, noise, missing data, outliers, high dimensionality and imbalanced data. Data preprocessing (DP) techniques were therefore used to prepare data with the goal of improving the performance of heart disease DM based prediction systems.
Objective
The purpose of this study is to review and summarize the current evidence on the use of preprocessing techniques in heart disease classification as regards: (1) the DP tasks and techniques most frequently used, (2) the impact of DP tasks and techniques on the performance of classification in cardiology, (3) the overall performance of classifiers when using DP techniques, and (4) comparisons of different combinations classifier-preprocessing in terms of accuracy rate.
Method
A systematic literature review is carried out, by identifying and analyzing empirical studies on the application of data preprocessing in heart disease classification published in the period between January 2000 and June 2019. A total of 49 studies were therefore selected and analyzed according to the aforementioned criteria.
Results
The review results show that data reduction is the most used preprocessing task in cardiology, followed by data cleaning. In general, preprocessing either maintained or improved the performance of heart disease classifiers. Some combinations such as (ANN + PCA), (ANN + CHI) and (SVM + PCA) are promising terms of accuracy. However the deployment of these models in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of interpretation.
ER  - 
TY  - JOUR
T1  - An improved support vector machine-based diabetic readmission prediction
A1  - Cui, Shaoze
A1  - Wang, Dujuan
A1  - Wang, Yanzhang
A1  - Yu, Pay-Wen
A1  - Jin, Yaochu
Y1  - 2018///
KW  -  Diabetes
KW  -  Feature selection
KW  -  Support vector machine
KW  -  Synthetic minority over-sampling
KW  - Hospital readmission
JF  - Computer Methods and Programs in Biomedicine
VL  - 166
SP  - 123
EP  - 135
DO  - https://doi.org/10.1016/j.cmpb.2018.10.012
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718308083
N2  - Background and objective
In healthcare systems, the cost of unplanned readmission accounts for a large proportion of total hospital payment. Hospital-specific readmission rate becomes a critical issue around the world. Quantification and early identification of unplanned readmission risks will improve the quality of care during hospitalization and reduce the occurrence of readmission. In clinical practice, medical workers generally use LACE score method to evaluate patient readmission risks, but this method usually performs poorly. With this in mind, this study presents a novel method combining support vector machine and genetic algorithm to build the risk prediction model, which simultaneously involves feature selection and the processing of imbalanced data. This model aims to provide decision support for clinicians during the discharge management of patients with diabetes.
Method
The experiments were conducted from a set of 8756 medical records with 50 different features about diabetic readmission. After preprocessing the data, an effective SMOTE-based method was proposed to solve the imbalance data problem. Further, in order to improve prediction performance, a hybrid feature selection mechanism was devised to select the important features. Subsequently, an improved support vector machine-based (SVM-based) method was developed and the genetic algorithm was used to tune the sensitive parameter of the algorithm. Finally, the five-fold cross-validation method was applied to compare the performance of proposed method with other methods (LACE score, logistic regression, naïve bayes, decision tree and feed forward neural networks).
Results
Experimental results indicate that the proposed SVM-based method achieves an accuracy of 81.02%, a sensitivity of 82.89%, a specificity of 79.23%, and outperforms other popular algorithms in identifying diabetic patients who may be readmitted.
Conclusions
Our research can improve the performance of clinic decision support systems for diabetic readmission, by which the readmission possibility as well as the waste of medical resources can be reduced.
ER  - 
TY  - JOUR
T1  - Prediction of blood glucose concentration for type 1 diabetes based on echo state networks embedded with incremental learning
A1  - Li, Ning
A1  - Tuo, Jianyong
A1  - Wang, Youqing
A1  - Wang, Menghui
Y1  - 2020///
KW  -  Continuous glucose monitoring systems (CGMS)
KW  -  Echo state networks (ESN)
KW  -  Feedback network
KW  -  Incremental learning
KW  - Blood glucose prediction
JF  - Neurocomputing
VL  - 378
SP  - 248
EP  - 259
DO  - https://doi.org/10.1016/j.neucom.2019.10.003
UR  - https://www.sciencedirect.com/science/article/pii/S092523121931375X
N2  - Valid prediction of blood glucose concentration can help people to manage diabetes mellitus, alert hypoglycemia/hyperglycemia, exploit artificial pancreas, and plan a treatment program. Along the development of continuous glucose monitoring system (CGMS), the massive historical data require a new modeling framework based on a data-driven perspective. Studies indicate that the glucose time series (i.e., CGMS readings) involve chaotic properties; therefore, echo state networks (ESN) and its improved variants are proposed to establish subject-specific prediction models owing to their superiority in processing chaotic systems. This study mainly has two innovations: (1) a novel combination of incremental learning and ESN is developed to obtain a suitable network structure through partial optimization of parameters; (2) a feedback ESN is proposed to excavate the relationship of different predictions. These methods are assessed on ten patients with diabetes mellitus. Experimental results substantiate that the proposed methods achieve superior prediction performance in terms of four evaluation metrics compared with three conventional methods.
ER  - 
TY  - JOUR
T1  - Predictive Glucose Monitoring for Type 1 Diabetes Using Latent Variable-based Multivariate Statistical Analysis
A1  - Zhao, Chunhui
A1  - Dassau, Eyal
A1  - Harvey, Rebecca A
A1  - Seborg, Dale E
A1  - Doyle, Francis J
Y1  - 2011///
KW  -  continuous glucose monitoring (CGM)
KW  -  glucose prediction
KW  -  latent variables (LVs)
KW  -  multivariate statistical analysis
KW  - Type 1 diabetes mellitus (T1DM)
JF  - IFAC Proceedings Volumes
VL  - 44
IS  - 1
SP  - 7102
EP  - 7107
DO  - https://doi.org/10.3182/20110828-6-IT-1002.02330
UR  - https://www.sciencedirect.com/science/article/pii/S1474667016447452
N1  - 18th IFAC World Congress
N2  - Abstract
Accurate prediction of future glucose concentration for type 1 diabetes mellitus is needed to improve glycemic control, which can produce early and proactive glycemia management before glucose concentrations drift to undesirable levels. This paper assesses the feasibility of data-driven latent variable (LV) based statistical analysis methods to characterize the glycemic variability and serve as the forecasting engine. It illustrates the capability of LV-based multivariate statistical analysis to (1) model the correlation relationship among glucose and exogenous inputs, (2) make reliable predictions of future glucose concentrations, and (3) capture the underlying dynamics of prediction errors for on-line reliability evaluation. The new approach provides an automatic predictive monitoring strategy for glucose management in type 1 diabetes. Its feasibility is successfully assessed using data collected from the Food and Drug Administration (FDA)-accepted University of Virginia (UVA)\University of Padova metabolic simulator.
ER  - 
TY  - JOUR
T1  - Predicting hypoglycemic drugs of type 2 diabetes based on weighted rank support vector machine
A1  - Wang, Xinye
A1  - Yang, Yi
A1  - Xu, Yitian
A1  - Chen, Qian
A1  - Wang, Hongmei
A1  - Gao, Huafang
Y1  - 2020///
KW  -  Hypoglycemic drugs
KW  -  Therapeutic schemes
KW  -  Type 2 diabetes
KW  -  Weighted rank support vector machine
KW  - Multi-label learning
JF  - Knowledge-Based Systems
VL  - 197
SP  - 105868
EP  - 105868
DO  - https://doi.org/10.1016/j.knosys.2020.105868
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120302276
N2  - Diabetes has become a disease that seriously endangers people’s health, then how to control the content of glycemic is an important issue. Since the treatment scheme of patient is usually a combination of multiple hypoglycemic drugs, multi-label learning is an effective method to solve this problem. By analyzing the type 2 diabetes data set including 2443 diabetics provided by the Chinese People’s Liberation Army General Hospital, we find that the defined daily dose system (DDDs) of drugs is an imbalanced problem, traditional multi-label methods easily leads to poor prediction results. In order to overcome the shortcoming, a weighted rank support vector machine (WRank-SVM) is proposed in this paper. We firstly define the weight of each label and then give each sample different weight according to relevant–irrelevant label pair. This method ensures that the prediction results on drugs with higher DDDs are as accurate as possible. Compared with the other six popular multi-label methods, our WRank-SVM can effectively predict the schemes for hypoglycemic drugs of type 2 diabetes. Meanwhile, receiver operating characteristic (ROC) curve is employed to statistically show the effectiveness of the model. Finally, the correlation between labels and features is further analyzed, and 13 important features are selected to improve the average precision of our proposed algorithm.
ER  - 
TY  - JOUR
T1  - A Review On digital image processing techniques for in-Vivo confocal images of the cornea
A1  - Herrera-Pereda, Raidel
A1  - Taboada Crispi, Alberto
A1  - Babin, Danilo
A1  - Philips, Wilfried
A1  - Holsbach Costa, Márcio
Y1  - 2021///
KW  -  Confocal microscopy
KW  -  Digital image processing
KW  -  Review
KW  - Cornea
JF  - Medical Image Analysis
VL  - 73
SP  - 102188
EP  - 102188
DO  - https://doi.org/10.1016/j.media.2021.102188
UR  - https://www.sciencedirect.com/science/article/pii/S1361841521002346
N2  - This work reviews the scientific literature regarding digital image processing for in vivo confocal microscopy images of the cornea. We present and discuss a selection of prominent techniques designed for semi- and automatic analysis of four areas of the cornea (epithelium, sub-basal nerve plexus, stroma and endothelium). The main context is image enhancement, detection of structures of interest, and quantification of clinical information. We have found that the preprocessing stage lacks of quantitative studies regarding the quality of the enhanced image, or its effects in subsequent steps of the image processing. Threshold values are widely used in the reviewed methods, although generally, they are selected empirically and manually. The image processing results are evaluated in many cases through comparison with gold standards not widely accepted. It is necessary to standardize values to be quantified in terms of sensitivity and specificity of methods. Most of the reviewed studies do not show an estimation of the computational cost of the image processing. We conclude that reliable, automatic, computer-assisted image analysis of the cornea is still an open issue, constituting an interesting and worthwhile area of research.
ER  - 
TY  - JOUR
T1  - Glucose dynamics in Type I diabetes: Insights from the classic and linear minimal models
A1  - Fernandez, Margarita
A1  - Villasana, Minaya
A1  - Streja, Dan
Y1  - 2007///
KW  -  Classic minimal model
KW  -  Glucose effectiveness index
KW  -  Insulin sensitivity index
KW  -  Linear minimal model
KW  - Type I diabetes
JF  - Computers in Biology and Medicine
VL  - 37
IS  - 5
SP  - 611
EP  - 627
DO  - https://doi.org/10.1016/j.compbiomed.2006.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S0010482506000825
N2  - This study demonstrates that the classic minimal model (MM) and the linear minimal model (LMM) are able to follow the dynamics of glucose in Type I diabetes. LMM precision is better than the MM with systematic lower mean values for the coefficient of variation (CV) in all characteristic model parameters. LMM SIL=7.40 is not significantly different from MM SI=10.71 (units 1/min per μU/ml, α=0.001) with a strong correlation (Rs = 0.83, α=0.01). LMM SGL=0.0407 appears to be significantly different to SG=0.0266 (units 1/min, α=0.001) but correlates very well (Rs=0.91,α=0.01). Since residuals appear to be heteroscedastic, further work is required to address the effect of modeling and signal processing on them. For the data under study, the models are not able to fit two-thirds of the data windows available. This is because none of the models are able to follow complex situations such as the presence of several bolus injections, the absence of insulin supply or inappropriate insulin dosage. A synthesis of the patterns found in these windows is presented which would be useful for the development of new models for fitting these data.
ER  - 
TY  - JOUR
T1  - Clustering and visualization of a high-dimensional diabetes dataset
A1  - Lasek, Piotr
A1  - Mei, Zhen
Y1  - 2019///
KW  -  clustering
KW  -  data visualization
KW  -  diabetes
KW  -  high-dimensional clustering
KW  -  interactive data visualization
KW  -  visualization
KW  - data mining
JF  - Procedia Computer Science
VL  - 159
SP  - 2179
EP  - 2188
DO  - https://doi.org/10.1016/j.procs.2019.09.392
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919315959
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - Data clustering algorithms have proved to be important and widely used methods of artificial intelligence and data mining for discovering unknown yet important patterns in datasets. Nevertheless, one of the additional aspects of data clustering is proper interpretation of the clustering results. In this paper we aim to investigate possibilities of using both data clustering and visualization methods to analyze a sample diabetes dataset. In the first part, we focus on how to cluster a highly-dimensional sample dataset and then, we concentrate on how to properly visually present the clustering results in the most meaningful way to uncover potentially interesting behavioral patterns or features of diabetes patients. In this work we examine two clustering algorithms (DBSCAN, k-Means) along with several different distance measures. We also present sample visualizations of clustering results generated by an application which we have developed and discuss if the proposed way of clustering results visualization can be helpful in understanding the analyzed dataset and lead a viewer to drawing valuable conclusions about it.
ER  - 
TY  - JOUR
T1  - Stark Assessment of Lifestyle Based Human Disorders Using Data Mining Based Learning Techniques
A1  - Sharma, M
A1  - Singh, G
A1  - Singh, R
Y1  - 2017///
KW  -  Diabetes
KW  -  Heart disease
KW  -  Lifestyle
KW  -  Ophthalmology
KW  -  Oral and digestive disorder
KW  - Data mining
JF  - IRBM
VL  - 38
IS  - 6
SP  - 305
EP  - 324
DO  - https://doi.org/10.1016/j.irbm.2017.09.002
UR  - https://www.sciencedirect.com/science/article/pii/S1959031817301112
N2  - Background: Medical informatics has observed an unrestrained growth in the database. Latest advancements in the field of medical sciences have wiped out lots of critical diseases. Nowadays, the medical industry is affluent in data sources. These data sources are of use only if these are effectively analyzed on time. Methods: Data mining techniques are artificially intelligent and used to investigate known and unknown patterns available in the medical databases. Nowadays, data mining techniques are chronically used to mine abundant data sources of medical science. This paper explores the practice of diverse data mining techniques, the role of dataset used, effect of preprocessing, and the performances of different data mining techniques in diagnosis of different lifestyle based diseases. The venture of this paper is to fetch out stark assessments of different data mining techniques used in medical sciences. Results: By far, surveillance discloses that significant effort has been made for mining the data allied to the Cardiology and Diabetes. As per Google Scholar, in last seven years, the percentage of articles published related to cardio, diabetes, digestive, dentistry and ophthalmology disease diagnosis using data mining are 42%, 26%, 18%, 10% and 4% respectively. So, a little attention has been paid to develop predictive model for the diseases viz. ophthalmology, dentistry and digestive disorders. In addition, the rate of usage of preprocessing in diagnosis of different disorders related to cardio, diabetes, digestive, dentistry and ophthalmology lies between 10.65%–17.75%, 8.48%–14.80%, 4.58–8.93%, 2.96%–7.73% and 5.83%–12.93% respectively. Conclusion: An attention is obligatory to develop smart diagnostic system to aware and save human masses from wide critical spectrum of diseases related to ophthalmology, oral and digestive systems.
ER  - 
TY  - JOUR
T1  - Pre-diabetes diagnosis based on ATR-FTIR spectroscopy combined with CART and XGBoots
A1  - Yang, Xinhao
A1  - Fang, Tao
A1  - Li, Yuanpeng
A1  - Guo, Liu
A1  - Li, Fucui
A1  - Huang, Furong
A1  - Li, Li
Y1  - 2019///
KW  -  CART
KW  -  Pre-diabetes diagnosis
KW  -  XGBoots
KW  - ATR-FTIR
JF  - Optik
VL  - 180
SP  - 189
EP  - 198
DO  - https://doi.org/10.1016/j.ijleo.2018.11.059
UR  - https://www.sciencedirect.com/science/article/pii/S0030402618318126
N2  - A rapid diagnosis model of pre-diabetes was established by using Attenuated total reflectance (ATR)-Fourier transform infrared spectroscopy (FTIR) combined with Classification and Regression Trees (CART) and eXtreme Gradient Boosting (XGBoost) ensemble learning algorithm. First, we collected peripheral blood samples of 112 volunteers and then measured the fasting blood glucose and 2h blood glucose levels of the oral glucose tolerance test, and determined the control group and the disease group according to the WHO diagnostic criteria (including impaired fasting glucose (IFG). Impaired glucose tolerance (IGT).In addition, ATR-FTIR spectra of peripheral blood samples were collected at the same time. The whole spectrum mid-infrared region (4000–600 cm−1) was used as the research object of diagnosis model of pre-diabetes. Second, preprocessing step, Savitzky-Golay (SG) smoothing pretreatment was performed and used PCA to extract spectral features. Finally, a rapid diagnosis model of pre-diabetes was established by using CART and XGBoost. CART model results were: Specificity: 80.00% (20/25), Sensitivity: 95.00% (19/20), Accuracy: 86.67% (39/45); XGBoost model results: Specificity: 100.00% (25/25), Sensitivity were: 85.00% (17/20)), Accuracy: 93.33% (42/45).The results show that the rapid diagnosis model of pre-diabetes was established by SG-PCA-XGBoost which has the best effect. Hence, the model established by ATR-FTIR spectra combined with SG-PCA-XGBoost in the experiment can more effectively diagnose pre-diabetes. This model needs no sample preprocessing; it is characterized by simple operation and time efficiency. Furthermore, the established model provides a fast accurate method for pre-diabetes diagnosis.
ER  - 
TY  - JOUR
T1  - Interpretable filter based convolutional neural network (IF-CNN) for glucose prediction and classification using PD-SS algorithm
A1  - Kamalraj, R
A1  - Neelakandan, S
A1  - Ranjith Kumar, M
A1  - Chandra Shekhar Rao, V
A1  - Anand, Rohit
A1  - Singh, Harinder
Y1  - 2021///
KW  -  
KW  -  IF-CNN
KW  -  Pet Dog-Smell Sensing (PD-SS)
KW  -  SVM
JF  - Measurement
VL  - 183
SP  - 109804
EP  - 109804
DO  - https://doi.org/10.1016/j.measurement.2021.109804
UR  - https://www.sciencedirect.com/science/article/pii/S0263224121007569
N2  - Diabetes mellitus is a disease commonly called Diabetes. Diabetes is among the most frequent diseases globally. This disease affects internationally with different ailments and complications in majority of peoples. Diabetes is a chronic disease with the ability to create a global medical care crisis. In compliance with International Diabetes Federation 382 million people are reportedly living with diabetes throughout the entire world. The diabetes disorder can't be cured but it can control, diagnosis and prediction of diabetes is vital to restrain the death rate as a result of its seriousness globally. Many In recent years, machine learning (ML) algorithms have been used in the prediction of diabetes. A clever predictive model utilizing deep modelling is commonly advised with the aid of conditional data collection to forecast the severity and particular risk factor of diabetics. In this article, to resolve this problem we employed the Interpretable Filter based Convolutional Neural Network (IF-CNN) prediction model and Pet Dog-Smell Sensing (PD-SS) algorithm that can automatically predict the diabetes from PIMA Indian diabetes datasets. This may enhance the general strategy of disease prediction in patients database that may solve the issues faced by traditional algorithms employing the Deep Neural Network (DNN) methods. The automated extraction, selection and classification of attributes, disease forecast is the hard task with aggressive performance for your PIMA information which may be implemented with the projected Deep Learning version economically. This may enhance the general plan of disease prediction from patients database that may solve the issues faced by traditional algorithms employing the Interpretable Filter established Deep Learning version prediction model to diagnose and predict the diabetes disease in multi-level databases. The intention of this research is to create a method that could carry out early diabetes predictions for a more reliable patient by including findings of SVM and CNN-LSTM(Long Short-Term Memory) machine learning methods also IF-CNN achieved 96.26% accuracy.
ER  - 
TY  - JOUR
T1  - Data mining for the diagnosis of type II diabetes from three-dimensional body surface anthropometrical scanning data
A1  - Su, Chad-Ton
A1  - Yang, Chien-Hsin
A1  - Hsu, Kuang-Hung
A1  - Chiu, Wen-Ko
Y1  - 2006///
KW  -  Backpropagation neural network
KW  -  Diagnosis
KW  -  Type II diabetes
KW  - Data mining
JF  - Computers & Mathematics with Applications
VL  - 51
IS  - 6
SP  - 1075
EP  - 1092
DO  - https://doi.org/10.1016/j.camwa.2005.08.034
UR  - https://www.sciencedirect.com/science/article/pii/S0898122106000381
N2  - Diabetes mellitus has become a general chronic disease as a result of changes in customary diets. Impaired fasting glucose (IFG) and fasting plasma glucose (FPG) levels are two of the indices which physicians use to diagnose diabetes mellitus. Although this is a fairly accurate approach, the tests are expensive and time consuming. This study attempts to construct a prediction model for Type II diabetes using anthropometrical body surface scanning data. Four data mining approaches, including backpropagation neural network, decision tree, logistic regression, and rough set, were used to select the relevant features from the data to predict diabetes. Accuracy of classification was evaluated for these approaches. The result showed that volume of trunk, left thigh circumference, right thigh circumference, waist circumference, volume of right leg, and subjects' age were associated with the condition of diabetes. The accuracy of the classification of decision tree and rough set was found to be superior to that of logistic regression and backpropagation neural network. Several rules were then extracted based on the anthropometrical data using decision tree. The result of implementing this method is not only useful for the physician as a tool for diagnosing diabetes, but it is sophisticated enough to be used in the practice of preventive medicine.
ER  - 
TY  - JOUR
T1  - Classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomography
A1  - Perdomo, Oscar
A1  - Rios, Hernán
A1  - Rodríguez, Francisco J
A1  - Otálora, Sebastián
A1  - Meriaudeau, Fabrice
A1  - Müller, Henning
A1  - González, Fabio A
Y1  - 2019///
KW  -  Deep learning models
KW  -  Interpretability
KW  -  Medical findings
KW  -  Retinal diseases
KW  - Optical coherence tomography
JF  - Computer Methods and Programs in Biomedicine
VL  - 178
SP  - 181
EP  - 189
DO  - https://doi.org/10.1016/j.cmpb.2019.06.016
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718318686
N2  - Background and objectives: Spectral Domain Optical Coherence Tomography (SD-OCT) is a volumetric imaging technique that allows measuring patterns between layers such as small amounts of fluid. Since 2012, automatic medical image analysis performance has steadily increased through the use of deep learning models that automatically learn relevant features for specific tasks, instead of designing visual features manually. Nevertheless, providing insights and interpretation of the predictions made by the model is still a challenge. This paper describes a deep learning model able to detect medically interpretable information in relevant images from a volume to classify diabetes-related retinal diseases. Methods: This article presents a new deep learning model, OCT-NET, which is a customized convolutional neural network for processing scans extracted from optical coherence tomography volumes. OCT-NET is applied to the classification of three conditions seen in SD-OCT volumes. Additionally, the proposed model includes a feedback stage that highlights the areas of the scans to support the interpretation of the results. This information is potentially useful for a medical specialist while assessing the prediction produced by the model. Results: The proposed model was tested on the public SERI-CUHK and A2A SD-OCT data sets containing healthy, diabetic retinopathy, diabetic macular edema and age-related macular degeneration. The experimental evaluation shows that the proposed method outperforms conventional convolutional deep learning models from the state of the art reported on the SERI+CUHK and A2A SD-OCT data sets with a precision of 93% and an area under the ROC curve (AUC) of 0.99 respectively. Conclusions: The proposed method is able to classify the three studied retinal diseases with high accuracy. One advantage of the method is its ability to produce interpretable clinical information in the form of highlighting the regions of the image that most contribute to the classifier decision.
ER  - 
TY  - JOUR
T1  - A comparative analysis on diagnosis of diabetes mellitus using different approaches – A survey
A1  - Anwar, Fareeha
A1  - Qurat-Ul-Ain
A1  - Ejaz, Muhammad Yasir
A1  - Mosavi, Amir
Y1  - 2020///
KW  -  Accuracy
KW  -  Deep learning
KW  -  Machine learning
KW  - Diabetes mellitus
JF  - Informatics in Medicine Unlocked
VL  - 21
SP  - 100482
EP  - 100482
DO  - https://doi.org/10.1016/j.imu.2020.100482
UR  - https://www.sciencedirect.com/science/article/pii/S235291482030633X
N2  - Diabetes Mellitus is commonly known as diabetes. It is one of the most chronic diseases as the World Health Organization (WHO) report shows that the number of diabetes patients has risen from 108 million to 422 million in 2014. Early diagnosis of diabetes is important because it can cause different diseases that include kidney failure, stroke, blindness, heart attacks, and lower limb amputation. Different diabetes diagnosis models are found in literature, but there is still a need to perform a survey to analyze which model is best. This paper performs a literature review for diabetes diagnosis approaches using Artificial Intelligence (neural networks, machine learning, deep learning, hybrid methods, and/or stacked-integrated use of different machine learning algorithms). More than thirty-five papers have been shortlisted that focus on diabetes diagnosis approaches. Different datasets are available online for the diagnosis of diabetes. Pima Indian Diabetes Dataset (PIDD) is the most commonly used for diabetes prediction. In contrast with other datasets, it has key factors which play an important role in diabetes diagnosis. This survey also throws light on the weaknesses of the existing approaches that make them less appropriate for a diabetes diagnosis. In artificial intelligence techniques, deep learning is widespread and in medical research, heart rate is getting more attention. Deep learning combined with other algorithms can give better results in diabetes diagnosis and heart rate should be used for other cardiac disease diagnoses.
ER  - 
TY  - JOUR
T1  - Computer-aided diagnosis of liver lesions using CT images: A systematic review
A1  - Nayantara, P Vaidehi
A1  - Kamath, Surekha
A1  - Manjunath, K N
A1  - Rajagopal, K V
Y1  - 2020///
KW  -  Classification
KW  -  Deep learning
KW  -  Feature extraction
KW  -  Hemangioma
KW  -  Hepatocellular carcinoma
KW  -  Liver diseases
KW  -  Liver/lesion segmentation
KW  - Computer-aided detection/diagnosis
JF  - Computers in Biology and Medicine
VL  - 127
SP  - 104035
EP  - 104035
DO  - https://doi.org/10.1016/j.compbiomed.2020.104035
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520303668
N2  - Background
Medical image processing has a strong footprint in radio diagnosis for the detection of diseases from the images. Several computer-aided systems were researched in the recent past to assist the radiologist in diagnosing liver diseases and reducing the interpretation time. The aim of this paper is to provide an overview of the state-of-the-art techniques in computer-assisted diagnosis systems to predict benign and malignant lesions using computed tomography images.
Methods
The research articles published between 1998 and 2020 obtained from various standard databases were considered for preparing the review. The research papers include both conventional as well as deep learning-based systems for liver lesion diagnosis. The paper initially discusses the various hepatic lesions that are identifiable on computed tomography images, then the computer-aided diagnosis systems and their workflow. The conventional and deep learning-based systems are presented in stages wherein the various methods used for preprocessing, liver and lesion segmentation, radiological feature extraction and classification are discussed.
Conclusion
The review suggests the scope for future, work as efficient and effective segmentation methods that work well with diverse images have not been developed. Furthermore, unsupervised and semi-supervised deep learning models were not investigated for liver disease diagnosis in the reviewed papers. Other areas to be explored include image fusion and inclusion of essential clinical features along with the radiological features for better classification accuracy.
ER  - 
TY  - JOUR
T1  - Automatic microvolt T-wave alternans identification in relation to ECG interferences surviving preprocessing
A1  - Burattini, Laura
A1  - Bini, Silvia
A1  - Burattini, Roberto
Y1  - 2011///
KW  -  Adaptive-match-filter method
KW  -  Complex-demodulation method
KW  -  Fast-Fourier-transform spectral method
KW  -  Laplacian-likelihood-ratio method
KW  -  Modified-moving-average method
KW  - T-wave alternans
JF  - Medical Engineering & Physics
VL  - 33
IS  - 1
SP  - 17
EP  - 30
DO  - https://doi.org/10.1016/j.medengphy.2010.08.014
UR  - https://www.sciencedirect.com/science/article/pii/S1350453310001839
N2  - The aim was to investigate the effect of interferences surviving preprocessing (residual noise, baseline wanderings, respiration modulation, replaced beats, missed beats and T-waves misalignment) on automatic identification of T-wave alternans (TWA), an ECG index of risk for sudden cardiac death. The procedures denominated fast-Fourier-transform spectral method (FFTSM), complex-demodulation method (CDM), modified-moving-average method (MMAM), Laplacian-likelihood-ratio method (LLRM), and adaptive-match-filter method (AMFM) were applied to interferences-corrupted synthetic ECG tracings and Holter ECG recordings from control-healthy subjects (CH-group; n=25) and acute-myocardial-infarction patients (AMI group; n=25). The presence of interferences in simulated data caused detection of false-positive TWA by all techniques but the FFTSM and AMFM. Clinical applications evidenced a discrepancy in that the FFTSM and LLRM detected no more than one TWA case in each population, whereas the CDM, MMAM, and AMFM detected TWA in all CH-subjects and AMI-patients, with significantly lower TWA amplitude in the former group. Because the AMFM is not prone to false-positive TWA detections, the latter finding suggests TWA as a phenomenon having continuously changing amplitude from physiological to pathological conditions. Only occasional detection of TWA by the FFTSM and LLRM in clinics can be ascribed to their limited ability in identifying TWA in the presence of interferences surviving preprocessing.
ER  - 
TY  - JOUR
T1  - Design of intelligent diabetes mellitus detection system using hybrid feature selection based XGBoost classifier
A1  - Prabha, Anju
A1  - Yadav, Jyoti
A1  - Rani, Asha
A1  - Singh, Vijander
Y1  - 2021///
KW  -  Feature selection
KW  -  MFCC
KW  -  PPG
KW  -  XGBoost
KW  - Diabetes detection
JF  - Computers in Biology and Medicine
VL  - 136
SP  - 104664
EP  - 104664
DO  - https://doi.org/10.1016/j.compbiomed.2021.104664
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521004583
N2  - In this work, a non-invasive diabetes mellitus detection system is proposed based on the wristband photoplethysmography (PPG) signal and basic physiological parameters (PhyP) to enable easy detection of diabetes mellitus (DM). A dataset of 217 participants with diabetes, prediabetes and normal conditions is used to develop the system. The Mel frequency cepstral coefficients (MFCC) extracted from 5s PPG signal segments and the PhyP are used as input for the machine learning algorithms. The K-nearest neighbors, support vector machine, random forest and extreme gradient boost (XGBoost) classifiers are used for classification. In addition, a hybrid feature selection method (Hybrid FS) is proposed to reduce the size of the input data. The Hybrid FS-based XGBoost system achieves a high accuracy of 99.93 % for non-invasive diabetes detection with fewer features and less computational effort. The analysis suggests that the PPG signal from a wearable sensor is a good alternative for simple non-invasive blood glucose measurements in routine applications.
ER  - 
TY  - JOUR
T1  - Reinforcement learning application in diabetes blood glucose control: A systematic review
A1  - Tejedor, Miguel
A1  - Woldaregay, Ashenafi Zebene
A1  - Godtliebsen, Fred
Y1  - 2020///
KW  -  Artificial pancreas
KW  -  Blood glucose control
KW  -  Closed-loop
KW  -  Insulin infusion
KW  - Reinforcement learning
JF  - Artificial Intelligence in Medicine
VL  - 104
SP  - 101836
EP  - 101836
DO  - https://doi.org/10.1016/j.artmed.2020.101836
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718304548
N2  - Background
Reinforcement learning (RL) is a computational approach to understanding and automating goal-directed learning and decision-making. It is designed for problems which include a learning agent interacting with its environment to achieve a goal. For example, blood glucose (BG) control in diabetes mellitus (DM), where the learning agent and its environment are the controller and the body of the patient respectively. RL algorithms could be used to design a fully closed-loop controller, providing a truly personalized insulin dosage regimen based exclusively on the patient’s own data.
Objective
In this review we aim to evaluate state-of-the-art RL approaches to designing BG control algorithms in DM patients, reporting successfully implemented RL algorithms in closed-loop, insulin infusion, decision support and personalized feedback in the context of DM.
Methods
An exhaustive literature search was performed using different online databases, analyzing the literature from 1990 to 2019. In a first stage, a set of selection criteria were established in order to select the most relevant papers according to the title, keywords and abstract. Research questions were established and answered in a second stage, using the information extracted from the articles selected during the preliminary selection.
Results
The initial search using title, keywords, and abstracts resulted in a total of 404 articles. After removal of duplicates from the record, 347 articles remained. An independent analysis and screening of the records against our inclusion and exclusion criteria defined in Methods section resulted in removal of 296 articles, leaving 51 relevant articles. A full-text assessment was conducted on the remaining relevant articles, which resulted in 29 relevant articles that were critically analyzed. The inter-rater agreement was measured using Cohen Kappa test, and disagreements were resolved through discussion.
Conclusions
The advances in health technologies and mobile devices have facilitated the implementation of RL algorithms for optimal glycemic regulation in diabetes. However, there exists few articles in the literature focused on the application of these algorithms to the BG regulation problem. Moreover, such algorithms are designed for control tasks as BG adjustment and their use have increased recently in the diabetes research area, therefore we foresee RL algorithms will be used more frequently for BG control in the coming years. Furthermore, in the literature there is a lack of focus on aspects that influence BG level such as meal intakes and physical activity (PA), which should be included in the control problem. Finally, there exists a need to perform clinical validation of the algorithms.
ER  - 
TY  - JOUR
T1  - Comparing convolutional neural networks and preprocessing techniques for HEp-2 cell classification in immunofluorescence images
A1  - Rodrigues, Larissa Ferreira
A1  - Naldi, Murilo Coelho
A1  - Mari, João Fernando
Y1  - 2020///
KW  -  Data augmentation
KW  -  Fine-tuning
KW  -  HEp-2 cells
KW  -  Hyperparameters
KW  -  Preprocessing
KW  -  Staining pattern classification
KW  - Convolutional neural networks
JF  - Computers in Biology and Medicine
VL  - 116
SP  - 103542
EP  - 103542
DO  - https://doi.org/10.1016/j.compbiomed.2019.103542
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519303993
N2  - Autoimmune diseases are the third highest cause of mortality in the world, and the identification of an anti-nuclear antibody via an immunofluorescence test for HEp-2 cells is a standard procedure to support diagnosis. In this work, we assess the performance of six preprocessing strategies and five state-of-the-art convolutional neural network architectures for the classification of HEp-2 cells. We also evaluate enhancement methods such as hyperparameter optimization, data augmentation, and fine-tuning training strategies. All experiments were validated using a five-fold cross-validation procedure over the training and test sets. In terms of accuracy, the best result was achieved by training the Inception-V3 model from scratch, without preprocessing and using data augmentation (98.28%). The results suggest the conclusions that most CNNs perform better on non-preprocessed images when trained from scratch on the analyzed dataset, and that data augmentation can improve the results from all models. Although fine-tuning training did not improve the accuracy compared to training the CNNs from scratch, it successfully reduced the training time.
ER  - 
TY  - JOUR
T1  - Classification of glucose records from patients at diabetes risk using a combined permutation entropy algorithm
A1  - Cuesta–Frau, D
A1  - Miró–Martínez, P
A1  - Oltra–Crespo, S
A1  - Jordán–Núñez, J
A1  - Vargas, B
A1  - Vigil, L
Y1  - 2018///
KW  -  Continuous glucose monitoring
KW  -  Diabetes
KW  -  Signal classification
KW  - Permutation entropy
JF  - Computer Methods and Programs in Biomedicine
VL  - 165
SP  - 197
EP  - 204
DO  - https://doi.org/10.1016/j.cmpb.2018.08.018
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718302062
N2  - Background and objectives: The adoption in clinical practice of electronic portable blood or interstitial glucose monitors has enabled the collection, storage, and sharing of massive amounts of glucose level readings. This availability of data opened the door to the application of a multitude of mathematical methods to extract clinical information not discernible with conventional visual inspection. The objective of this study is to assess the capability of Permutation Entropy (PE) to find differences between glucose records of healthy and potentially diabetic subjects. Methods: PE is a mathematical method based on the relative frequency analysis of ordinal patterns in time series that has gained a lot of attention in the last years due to its simplicity, robustness, and performance. We study in this paper the applicability of this method to glucose records of subjects at risk of diabetes in order to assess the predictability value of this metric in this context. Results: PE, along with some of its derivatives, was able to find significant differences between diabetic and non–diabetic patients from records acquired up to 3 years before the diagnosis. The quantitative results for PE were 3.5878 ± 0.3916 for the nondiabetic class, and 3.1564 ± 0.4166 for the diabetic class. With a classification accuracy higher than 70%, and by means of a Cox regression model, PE demonstrated that it is a very promising candidate as a risk stratification tool for continuous glucose monitoring. Conclusion: PE can be considered as a prospective tool for the early diagnosis of the glucoregulatory system.
ER  - 
TY  - JOUR
T1  - Early temporal prediction of Type 2 Diabetes Risk Condition from a General Practitioner Electronic Health Record: A Multiple Instance Boosting Approach
A1  - Bernardini, Michele
A1  - Morettini, Micaela
A1  - Romeo, Luca
A1  - Frontoni, Emanuele
A1  - Burattini, Laura
Y1  - 2020///
KW  -  Clinical Decision Support System
KW  -  Electronic Health Record
KW  -  Machine Learning
KW  -  Predictive Medicine
KW  -  Temporal Analysis
KW  - Type 2 Diabetes
JF  - Artificial Intelligence in Medicine
VL  - 105
SP  - 101847
EP  - 101847
DO  - https://doi.org/10.1016/j.artmed.2020.101847
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719306384
N2  - Early prediction of target patients at high risk of developing Type 2 diabetes (T2D) plays a significant role in preventing the onset of overt disease and its associated comorbidities. Although fundamental in early phases of T2D natural history, insulin resistance is not usually quantified by General Practitioners (GPs). Triglyceride-glucose (TyG) index has been proven useful in clinical studies for quantifying insulin resistance and for the early identification of individuals at T2D risk but still not applied by GPs for diagnostic purposes. The aim of this study is to propose a multiple instance learning boosting algorithm (MIL-Boost) for creating a predictive model capable of early prediction of worsening insulin resistance (low vs high T2D risk) in terms of TyG index. The MIL-Boost is applied to past electronic health record (EHR) patients’ information stored by a single GP. The proposed MIL-Boost algorithm proved to be effective in dealing with this task, by performing better than the other state-of-the-art ML competitors (Recall from 0.70 and up to 0.83). The proposed MIL-based approach is able to extract hidden patterns from past EHR temporal data, even not directly exploiting triglycerides and glucose measurements. The major advantages of our method can be found in its ability to model the temporal evolution of longitudinal EHR data while dealing with small sample size and variability in the observations (e.g., a small variable number of prescriptions for non-hospitalized patients). The proposed algorithm may represent the main core of a clinical decision support system.
ER  - 
TY  - JOUR
T1  - Type 2 diabetes mellitus prediction model based on data mining
A1  - Wu, Han
A1  - Yang, Shengqi
A1  - Huang, Zhangqin
A1  - He, Jian
A1  - Wang, Xiaoyi
Y1  - 2018///
KW  -  Data mining
KW  -  Diabetes mellitus
KW  - Hybrid prediction model
JF  - Informatics in Medicine Unlocked
VL  - 10
SP  - 100
EP  - 107
DO  - https://doi.org/10.1016/j.imu.2017.12.006
UR  - https://www.sciencedirect.com/science/article/pii/S2352914817301405
N2  - Due to its continuously increasing occurrence, more and more families are influenced by diabetes mellitus. Most diabetics know little about their health quality or the risk factors they face prior to diagnosis. In this study, we have proposed a novel model based on data mining techniques for predicting type 2 diabetes mellitus (T2DM). The main problems that we are trying to solve are to improve the accuracy of the prediction model, and to make the model adaptive to more than one dataset. Based on a series of preprocessing procedures, the model is comprised of two parts, the improved K-means algorithm and the logistic regression algorithm. The Pima Indians Diabetes Dataset and the Waikato Environment for Knowledge Analysis toolkit were utilized to compare our results with the results from other researchers. The conclusion shows that the model attained a 3.04% higher accuracy of prediction than those of other researchers. Moreover, our model ensures that the dataset quality is sufficient. To further evaluate the performance of our model, we applied it to two other diabetes datasets. Both experiments' results show good performance. As a result, the model is shown to be useful for the realistic health management of diabetes.
ER  - 
TY  - JOUR
T1  - Classifier ensemble methods in feature selection
A1  - Kiziloz, Hakan Ezgi
Y1  - 2021///
KW  -  Classifier ensemble
KW  -  Machine learning
KW  -  Multiobjective optimization
KW  - Feature selection
JF  - Neurocomputing
VL  - 419
SP  - 97
EP  - 107
DO  - https://doi.org/10.1016/j.neucom.2020.07.113
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220313151
N2  - Feature selection has become an indispensable preprocessing step in an expert system. Improving the feature selection performance could guide such a system to make better decisions. Classifier ensembles are known to improve performance when compared to the use of a single classifier. In this study, we aim to perform a formal comparison of different classifier ensemble methods on the feature selection domain. For this purpose, we compare the performances of six classifier ensemble methods: a greedy approach, two average-based approaches, two majority voting approaches, and a meta-classifier approach. In our study, the classifier ensemble involves five machine learning techniques: Logistic Regression, Support Vector Machines, Extreme Learning Machine, Naïve Bayes, and Decision Tree. Experiments are carried on 12 well-known datasets, and results with statistical tests are provided. The results indicate that ensemble methods perform better than single classifiers, yet, they require a longer execution time. Moreover, they can minimize the number of features better than existing ensemble algorithms, namely Random Forest, AdaBoost, and Gradient Boosting, in a less amount of time. Among ensemble methods, the greedy based method performs well in terms of both classification accuracy and execution time.
ER  - 
TY  - JOUR
T1  - A novel hybrid intelligent system with missing value imputation for diabetes diagnosis
A1  - Ramezani, Rohollah
A1  - Maadi, Mansoureh
A1  - Khatami, Seyedeh Malihe
Y1  - 2018///
KW  -  ANFIS
KW  -  Intelligent system
KW  -  Logistic regression
KW  -  Missing value
KW  - Diabetes
JF  - Alexandria Engineering Journal
VL  - 57
IS  - 3
SP  - 1883
EP  - 1891
DO  - https://doi.org/10.1016/j.aej.2017.03.043
UR  - https://www.sciencedirect.com/science/article/pii/S1110016817301369
N2  - Recently, diabetes becomes the widespread and major disease in the world. In this paper, we propose a novel hybrid classifier for diabetic diseases. The proposed hybrid classifier named Logistic Adaptive Network-based Fuzzy Inference System (LANFIS) is a combination of Logistic regression and Adaptive Network-based Fuzzy Inference System. Our proposed intelligent system does not use classifiers to continuous output, does not delete samples with missing values, and does not use insignificant attributes which reduces number of tests required during data acquisition. The diagnosis performance of the LANFIS intelligent system is calculated using sensitivity, specificity, accuracy and confusion matrix. Our findings show that the classification accuracy of LANFIS intelligent system is about 88.05%. Indeed, 3–5% increase in accuracy is obtained by the proposed intelligent system and it is better than fuzzy classifiers in the available literature by deleting all samples to missing values and applying traditional classifiers to different sets of features.
ER  - 
TY  - JOUR
T1  - Feature selection using grasshopper optimization algorithm in diagnosis of diabetes disease
A1  - Kamel, Seyed Reza
A1  - Yaghoubzadeh, Reyhaneh
Y1  - 2021///
KW  -  Data mining
KW  -  Feature selection
KW  -  Grasshopper optimization algorithm (GOA)
KW  -  Support vector machine (SVM)
KW  - Diabetes
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100707
EP  - 100707
DO  - https://doi.org/10.1016/j.imu.2021.100707
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821001908
N2  - Diabetes signifies different types of metabolic diseases that cause high blood glucose, either because of insufficient insulin production or because the body cells fail to respond to insulin the body makes normally. Early detection of diabetes remains necessary in today's healthcare industry in order to reduce the mortality rate caused by kidney failure and loss of vision that both often lead to death. Until now, the diagnosis of diabetes has been examined through different techniques, including machine learning and data mining methods. However, due to the complexity of calculations or the time-consuming processes, their given accuracy is not acceptable. The present research proposes a feature selection method based on a grasshopper optimization algorithm (GOA) to increase the accurate results of diabetes type II testing and employs different machine learning techniques to find an enhanced classifier. In doing so, a 10-fold cross-validation method is used to gain the reliability of the obtained responses. The feature selection technique is used in this study to identify the important features in the dataset. This approach is applied on the Prima Indian Dataset, using MATLAB software. The study result has shown promising accuracy of 97% achieved by the Support-Vector Machine (SVM) algorithm. A comparison is also made between the most recent AI algorithms and that of the present study to show the superiority of the grasshopper algorithm in selecting features and increasing the accuracy of diabetes testing.
ER  - 
TY  - JOUR
T1  - Development of non-invasive diabetes risk prediction models as decision support tools designed for application in the dental clinical environment
A1  - Hegde, Harshad
A1  - Shimpi, Neel
A1  - Panny, Aloksagar
A1  - Glurich, Ingrid
A1  - Christie, Pamela
A1  - Acharya, Amit
Y1  - 2019///
KW  -  Decision-support systems
KW  -  Electronic health records
KW  -  Evidence-based practice
KW  -  Machine leaning
KW  -  Modeling healthcare services
KW  - Dental informatics
JF  - Informatics in Medicine Unlocked
VL  - 17
SP  - 100254
EP  - 100254
DO  - https://doi.org/10.1016/j.imu.2019.100254
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819302758
N2  - The objective was to develop a predictive model using medical-dental data from an integrated electronic health record (iEHR) to identify individuals with undiagnosed diabetes mellitus (DM) in dental settings. Retrospective data retrieved from Marshfield Clinic Health System's data-warehouse was pre-processed prior to conducting analysis. A subset was extracted from the preprocessed dataset for external evaluation (Nvalidation) of derived predictive models. Further, subsets of 30%–70%, 40%–60% and 50%–50% case-to-control ratios were created for training/testing. Feature selection was performed on all datasets. Four machine learning (ML) classifiers were evaluated: logistic regression (LR), multilayer perceptron (MLP), support vector machines (SVM) and random forests (RF). Model performance was evaluated on Nvalidation. We retrieved a total of 5319 cases and 36,224 controls. From the initial 116 medical and dental features, 107 were used after performing feature selection. RF applied to the 50%–50% case-control ratio outperformed other predictive models over Nvalidation achieving a total accuracy (94.14%), sensitivity (0.941), specificity (0.943), F-measure (0.941), Mathews-correlation-coefficient (0.885) and area under the receiver operating curve (0.972). Future directions include incorporation of this predictive model into iEHR as a clinical decision support tool to screen and detect patients at risk for DM triggering follow-ups and referrals for integrated care delivery between dentists and physicians.
ER  - 
TY  - JOUR
T1  - A health informatics transformation model based on intelligent cloud computing – exemplified by type 2 diabetes mellitus with related cardiovascular diseases
A1  - Lin, Hsueh-Chun
A1  - Kuo, Yu-Chen
A1  - Liu, Meng-Yu
Y1  - 2020///
KW  -  Cloud computing
KW  -  MIMIC-III
KW  -  Machine learning
KW  -  Open data
KW  - Health informatics transformation
JF  - Computer Methods and Programs in Biomedicine
VL  - 191
SP  - 105409
EP  - 105409
DO  - https://doi.org/10.1016/j.cmpb.2020.105409
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719317158
N2  - Background and Objective
Many studies regarding health analysis request structured datasets but the legacy resources provide scattered data. This study aims to establish a health informatics transformation model (HITM) based upon intelligent cloud computing with the self-developed analytics modules by open source technique. The model was exemplified by the open data of type 2 diabetes mellitus (DM2) with related cardiovascular diseases.
Methods
The Apache-SPARK framework was employed to generate the infrastructure of the HITM, which enables the machine learning (ML) algorithms including random forest, multi-layer perceptron classifier, support vector machine, and naïve Bayes classifier as well as the regression analysis for intelligent cloud computing. The modeling applied the MIMIC-III open database as an example to design the health informatics data warehouse, which embeds the PL/SQL-based modules to extract the analytical data for the training processes. A coupling analysis flow can drive the ML modules to train the sample data and validate the results.
Results
The four modes of cloud computation were compared to evaluate the feasibility of the cloud platform in accordance with its system performance for more than 11,500 datasets. Then, the modeling adaptability was validated by simulating the featured datasets of obesity and cardiovascular-related diseases for patients with DM2 and its complications. The results showed that the run-time efficiency of the platform performed in around one minute and the prediction accuracy of the featured datasets reached 90%.
Conclusions
This study helped contribute the modeling for efficient transformation of health informatics. The HITM can be customized for the actual clinical database, which provides big data for training, with the proper ML modules for a predictable process in the cloud platform. The feedback of intelligent computing can be referred to risk assessment in health promotion.
ER  - 
TY  - JOUR
T1  - A Simplified CNNs Visual Perception Learning Network Algorithm for Foods Recognition
A1  - Xiao, Limei
A1  - Lan, Tian
A1  - Xu, Dayou
A1  - Gao, Weizhe
A1  - Li, Ce
Y1  - 2021///
KW  -  CNNs
KW  -  Jumping convolution
KW  -  Recognition
KW  - Food image
JF  - Computers & Electrical Engineering
VL  - 92
SP  - 107152
EP  - 107152
DO  - https://doi.org/10.1016/j.compeleceng.2021.107152
UR  - https://www.sciencedirect.com/science/article/pii/S0045790621001555
N2  - With improvements in human living standard, people’s demands on food quality are getting higher and higher. Effective food recognition algorithms are needed to obtain more useful food information. In order to solve the problem of low accuracy and slow speed of food recognition algorithms, a new food recognition algorithm based on CNN algorithm is proposed. First, the proposed algorithm preprocess the food images which are collected from the internet. And then use the traditional convolution extract the features from food images. The jumping convolution which is designed in this paper to extract food features jumping and combines the features from traditional convolutions. This algorithm cannot only solve the food recognition problem effectively, but also reduce the calculation parameters. Compared with the experimental results of other deep learning networks, the proposed algorithm has a good effect, and can recognize the food quickly and reduce the training time.
ER  - 
TY  - JOUR
T1  - Analysis for warning factors of type 2 diabetes mellitus complications with Markov blanket based on a Bayesian network model
A1  - Liu, Siying
A1  - Zhang, Runtong
A1  - Shang, Xiaopu
A1  - Li, Weizi
Y1  - 2020///
KW  -  Prevention
KW  -  Self-management
KW  -  Type 2 diabetes mellitus complications
KW  -  Warning factors
KW  - Bayesian network
JF  - Computer Methods and Programs in Biomedicine
VL  - 188
SP  - 105302
EP  - 105302
DO  - https://doi.org/10.1016/j.cmpb.2019.105302
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719306339
N2  - Background and objective
Type 2 diabetes mellitus (T2DM) complications seriously affect the quality of life and could not be cured completely. Actions should be taken for prevention and self-management. Analysis of warning factors is beneficial for patients, on which some previous studies focused. They generally used the professional medical test factors or complete factors to predict and prevent, but it was inconvenient and impractical for patients to self-manage. With this in mind, this study built a Bayesian network (BN) model, from the perspective of diabetic patients’ self-management and prevention, to predict six complications of T2DM using the selected warning factors which patients could have access from medical examination. Furthermore, the model was analyzed to explore the relationships between physiological variables and T2DM complications, as well as the complications themselves. The model aims to help patients with T2DM self-manage and prevent themselves from complications.
Methods
The dataset was collected from a well-known data center called the National Health Clinical Center between 1st January 2009 and 31st December 2009. After preprocess and impute the data, a BN model merging expert knowledge was built with Bootstrap and Tabu search algorithm. Markov Blanket (MB) was used to select the warning factors and predict T2DM complications. Moreover, a Bayesian network without prior information (BN-wopi) model learned using 10-fold cross-validation both in structure and in parameters was added to compare with other classifiers learned using 10-fold cross-validation fairly. The warning factors were selected according the structure learned in each fold and were used to predict. Finally, the performance of two BN models using warning features were compared with Naïve Bayes model, Random Forest model, and C5.0 Decision Tree model, which used all features to predict. Besides, the validation parameters of the proposed model were also compared with those in existing studies using some other variables in clinical data or biomedical data to predict T2DM complications.
Results
Experimental results indicated that the BN models using warning factors performed statistically better than their counterparts using all other variables in predicting T2DM complications. In addition, the proposed BN model were effective and significant in predicting diabetic nephropathy (DN) (AUC: 0.831), diabetic foot (DF) (AUC: 0.905), diabetic macrovascular complications (DMV) (AUC: 0.753) and diabetic ketoacidosis (DK) (AUC: 0.877) with the selected warning factors compared with other experiments.
Conclusions
The warning factors of DN, DF, DMV, and DK selected by MB in this research might be able to help predict certain T2DM complications effectively, and the proposed BN model might be used as a general tool for prevention, monitoring, and self-management.
ER  - 
TY  - JOUR
T1  - Rule extraction using Recursive-Rule extraction algorithm with J48graft combined with sampling selection techniques for the diagnosis of type 2 diabetes mellitus in the Pima Indian dataset
A1  - Hayashi, Yoichi
A1  - Yukita, Shonosuke
Y1  - 2016///
KW  -  Data mining
KW  -  Pima Indian diabetes
KW  -  Re-RX algorithm
KW  -  Sampling selection
KW  -  Type 2 diabetes mellitus
KW  - Rule extraction
JF  - Informatics in Medicine Unlocked
VL  - 2
SP  - 92
EP  - 104
DO  - https://doi.org/10.1016/j.imu.2016.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S2352914816300016
N2  - Diabetes is a complex disease that is increasing in prevalence around the world. Type 2 diabetes mellitus (T2DM) accounts for about 90–95% of all diagnosed adult cases of diabetes. Most present diagnostic methods for T2DM are black-box models, which are unable to provide the reasons underlying diagnosis to physicians; therefore, algorithms that can provide further insight are needed. Rule extraction can provide such explanations; however, in the medical setting, extracted rules must be not only highly accurate, but also simple and easy to understand. The Recursive-Rule eXtraction (Re-RX) algorithm is a “white-box” model that provides highly accurate classification. However, due to its recursive nature, it tends to generate more rules than other algorithms. Therefore, in this study, we propose the use of a rule extraction algorithm, Re-RX with J48graft, combined with sampling selection techniques (sampling Re-RX with J48graft) to achieve highly accurate, concise, and interpretable classification rules for the Pima Indian Diabetes (PID) dataset, which comprises 768 samples with two classes (diabetes or non-diabetes) and eight continuous attributes. The use of this algorithm resulted in an average accuracy of 83.83% after 10 runs of 10-fold cross validation. Sampling Re-RX with J48 graft achieved substantially better accuracy and provided a considerably fewer average number of rules and antecedents than the original Re-RX algorithm. These results suggest that sampling Re-RX with J48graft provides more accurate, concise, and interpretable extracted rules than previous algorithms, and is therefore more suitable for medical decision making, including the diagnosis of T2DM.
ER  - 
TY  - JOUR
T1  - A model for early prediction of diabetes
A1  - Mahboob Alam, Talha
A1  - Iqbal, Muhammad Atif
A1  - Ali, Yasir
A1  - Wahab, Abdul
A1  - Ijaz, Safdar
A1  - Imtiaz Baig, Talha
A1  - Hussain, Ayaz
A1  - Malik, Muhammad Awais
A1  - Raza, Muhammad Mehdi
A1  - Ibrar, Salman
A1  - Abbas, Zunish
Y1  - 2019///
KW  -  Artificial neural network (ANN)
KW  -  Data mining
KW  -  Diabetes
KW  -  K-means clustering
KW  -  Random forest
KW  - Association rule mining
JF  - Informatics in Medicine Unlocked
VL  - 16
SP  - 100204
EP  - 100204
DO  - https://doi.org/10.1016/j.imu.2019.100204
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819300176
N2  - Diabetes is a common, chronic disease. Prediction of diabetes at an early stage can lead to improved treatment. Data mining techniques are widely used for prediction of disease at an early stage. In this research paper, diabetes is predicted using significant attributes, and the relationship of the differing attributes is also characterized. Various tools are used to determine significant attribute selection, and for clustering, prediction, and association rule mining for diabetes. Significant attributes selection was done via the principal component analysis method. Our findings indicate a strong association of diabetes with body mass index (BMI) and with glucose level, which was extracted via the Apriori method. Artificial neural network (ANN), random forest (RF) and K-means clustering techniques were implemented for the prediction of diabetes. The ANN technique provided a best accuracy of 75.7%, and may be useful to assist medical professionals with treatment decisions.
ER  - 
TY  - JOUR
T1  - A method to assist in the diagnosis of early diabetic retinopathy: Image processing applied to detection of microaneurysms in fundus images
A1  - Rosas-Romero, Roberto
A1  - Martínez-Carballido, Jorge
A1  - Hernández-Capistrán, Jonathan
A1  - Uribe-Valencia, Laura J
Y1  - 2015///
KW  -  Eye fundus image
KW  -  Principal component analysis
KW  -  Radon transform
KW  - Morphological processing
JF  - Computerized Medical Imaging and Graphics
VL  - 44
SP  - 41
EP  - 53
DO  - https://doi.org/10.1016/j.compmedimag.2015.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S089561111500097X
N2  - Diabetes increases the risk of developing any deterioration in the blood vessels that supply the retina, an ailment known as Diabetic Retinopathy (DR). Since this disease is asymptomatic, it can only be diagnosed by an ophthalmologist. However, the growth of the number of ophthalmologists is lower than the growth of the population with diabetes so that preventive and early diagnosis is difficult due to the lack of opportunity in terms of time and cost. Preliminary, affordable and accessible ophthalmological diagnosis will give the opportunity to perform routine preventive examinations, indicating the need to consult an ophthalmologist during a stage of non proliferation. During this stage, there is a lesion on the retina known as microaneurysm (MA), which is one of the first clinically observable lesions that indicate the disease. In recent years, different image processing algorithms, which allow the detection of the DR, have been developed; however, the issue is still open since acceptable levels of sensitivity and specificity have not yet been reached, preventing its use as a pre-diagnostic tool. Consequently, this work proposes a new approach for MA detection based on (1) reduction of non-uniform illumination; (2) normalization of image grayscale content to improve dependence of images from different contexts; (3) application of the bottom-hat transform to leave reddish regions intact while suppressing bright objects; (4) binarization of the image of interest with the result that objects corresponding to MAs, blood vessels, and other reddish objects (Regions of Interest—ROIs) are completely separated from the background; (5) application of the hit-or-miss Transformation on the binary image to remove blood vessels from the ROIs; (6) two features are extracted from a candidate to distinguish real MAs from FPs, where one feature discriminates round shaped candidates (MAs) from elongated shaped ones (vessels) through application of Principal Component Analysis (PCA); (7) the second feature is a count of the number of times that the radon transform of the candidate ROI, evaluated at the set of discrete angle values {0°, 1°, 2°, …, 180°}, is characterized by a valley between two peaks. The proposed approach is tested on the public databases DiaretDB1 and Retinopathy Online Challenge (ROC) competition. The proposed MA detection method achieves sensitivity, specificity and precision of 92.32%, 93.87% and 95.93% for the diaretDB1 database and 88.06%, 97.47% and 92.19% for the ROC database. Theory, results, challenges and performance related to the proposed MA detecting method are presented.
ER  - 
TY  - JOUR
T1  - Deep learning in pervasive health monitoring, design goals, applications, and architectures: An overview and a brief synthesis
A1  - Boulemtafes, Amine
A1  - Khemissa, Hamza
A1  - Derki, Mohamed Saddek
A1  - Amira, Abdelouahab
A1  - Djedjig, Nabil
Y1  - 2021///
JF  - Smart Health
VL  - 22
SP  - 100221
EP  - 100221
DO  - https://doi.org/10.1016/j.smhl.2021.100221
UR  - https://www.sciencedirect.com/science/article/pii/S235264832100043X
N2  - The continuous growth of an aging population in some countries, and patients with chronic conditions needs the development of efficient solutions for healthcare. Pervasive Health Monitoring (PHM) is an important pervasive computing application that has the potential to provide patients with a high-quality medical service and enable quick-response alerting of critical conditions. To that end, PHM enables continuous and ubiquitous monitoring of patients' health and wellbeing using Internet of Things (IoT) technologies, such as wearables and ambient sensors. In recent years, deep learning (DL) has attracted a growing interest from the research community to improve PHM applications. In this paper, we discuss the state-of-the-art of DL-based PHM, through identifying, (1) the main PHM applications where DL is successful, (2) design goals and objectives of using DL in PHM, and (3) design notes including DL architectures and data preprocessing. Finally, main advantages, limitations and challenges of the adoption of DL in PHM are discussed.
ER  - 
TY  - JOUR
T1  - Blockchain-Enabled healthcare system for detection of diabetes
A1  - Chen, Mengji
A1  - Malook, Taj
A1  - Rehman, Ateeq Ur
A1  - Muhammad, Yar
A1  - Alshehri, Mohammad Dahman
A1  - Akbar, Aamir
A1  - Bilal, Muhammad
A1  - Khan, Muazzam A
Y1  - 2021///
KW  -  Classification algorithms
KW  -  Diabetes disease
KW  -  Healthcare
KW  -  Secure systems
KW  - Blockchain
JF  - Journal of Information Security and Applications
VL  - 58
SP  - 102771
EP  - 102771
DO  - https://doi.org/10.1016/j.jisa.2021.102771
UR  - https://www.sciencedirect.com/science/article/pii/S221421262100020X
N2  - Blockchain has penetrated numerous domains such as, industries, government agencies, online voting, and healthcare, etc. Among these domains, healthcare is one of the trending and most important one, which consists of a control system and an Electronic Health Records (EHRs). Diabetes is one of the most rapidly growing chronic diseases that increases the death ratio across the globe. This paper presents a Blockchain-enabled diabetes disease detection framework that provides an earlier detection of this disease by using various machine learning classification algorithms and maintains the EHRs of the patients in a secure manner. Our EHRs sharing framework combines symptom-based disease prediction, Blockchain, and interplanetary file system (IPFS) in which the patient's health information are collected via wearable sensor devices. This information is then sent to EHRs manager, where an ML model is executed for further processing to collect the desired results. The results along with the physiological parameters are then stored in the Blockchain with the approval of concerned patient and his/her practitioner. It is anticipated that our proposed system will help the healthcare society in order to store, process, and share the patient health information in a secure manner.
ER  - 
TY  - JOUR
T1  - Towards Insulin Monitoring: Infrequent Kalman Filter Estimates for Diabetes Management ⁎⁎This work was made possible with support from The Leona M. and Harry B. Helmsley Charitable Trust (Grant no. 2018PG-TID061).
A1  - Wolkowicz, Kelilah L
A1  - Deshpande, Sunil
A1  - Doyle Iii, Francis J
A1  - Dassau, Eyal
Y1  - 2020///
KW  -  Biomedical system modeling
KW  -  Kalman filters
KW  -  Measurement noise
KW  -  Sampling intervals
KW  -  State estimation
KW  -  diabetes
KW  -  simulation
KW  -  visualization
KW  - Chronic care
JF  - IFAC-PapersOnLine
VL  - 53
IS  - 2
SP  - 15877
EP  - 15883
DO  - https://doi.org/10.1016/j.ifacol.2020.12.279
UR  - https://www.sciencedirect.com/science/article/pii/S2405896320305577
N1  - 21st IFAC World Congress
N2  - We propose a Kalman filter-based observer utilizing noisy remote compartment insulin measurements to estimate plasma insulin concentration. The design considers plant-model mismatch, sensor noise, as well as both uniform sampling intervals, mimicking infrequent continuous measurements, and non-uniform sampling intervals, mimicking infrequent on-demand measurements. The performance of the observer is demonstrated on ten in-silico subjects from the UVA/Padova simulator using real-life scenarios, including variability in sensor noise and variability in insulin pharmacokinetics. The proposed observer provides insight into the future use of insulin measurements for diabetes management.
ER  - 
TY  - JOUR
T1  - Establishment of noninvasive diabetes risk prediction model based on tongue features and machine learning techniques
A1  - Li, Jun
A1  - Chen, Qingguang
A1  - Hu, Xiaojuan
A1  - Yuan, Pei
A1  - Cui, Longtao
A1  - Tu, Liping
A1  - Cui, Ji
A1  - Huang, Jingbin
A1  - Jiang, Tao
A1  - Ma, Xuxiang
A1  - Yao, Xinghua
A1  - Zhou, Changle
A1  - Lu, Hao
A1  - Xu, Jiatuo
Y1  - 2021///
KW  -  Critical blood glucose
KW  -  Diabetes
KW  -  High blood glucose
KW  -  Machine learning
KW  -  Non-invasive
KW  -  Risk prediction
KW  -  Tongue diagnosis
KW  - Chinese medicine
JF  - International Journal of Medical Informatics
VL  - 149
SP  - 104429
EP  - 104429
DO  - https://doi.org/10.1016/j.ijmedinf.2021.104429
UR  - https://www.sciencedirect.com/science/article/pii/S1386505621000551
N2  - Background
Diabetes is a chronic noncommunicable disease with high incidence rate. Diabetics without early diagnosis or standard treatment may contribute to serious multisystem complications, which can be life threatening. Timely detection and intervention of prediabetes is very important to prevent diabetes, because it is inevitable in the development and progress of the disease.
Objective
Our objective was to establish the predictive model that can be applied to evaluate people with blood glucose in high and critical state.
Methods
We established the diabetes risk prediction model formed by a combined TCM tongue diagnosis with machine learning techniques. 1512 subjects were recruited from the hospital. After data preprocessing, we got the dataset 1 and dataset 2. Dataset 1 was used to train classical machine learning model, while dataset 2 was used to train deep learning model. To evaluate the performance of the prediction model, we used Classification Accuracy(CA), Precision, Recall, F1-score, Precision-Recall curve(P-R curve), Area Under the Precision-Recall curve(AUPRC), Receiver Operating Characteristic curve(ROC curve), Area Under the Receiver Operating Characteristic curve(AUROC), then selected the best diabetes risk prediction model.
Results
On the test set of dataset 1, the CA of non-invasive Stacking model was 71 %, micro average AUROC was 0.87, macro average AUROC was 0.84, and micro average AUPRC was 0.77. In the critical blood glucose group, the AUROC was 0.84, AUPRC was 0.67. In the high blood glucose group, AUROC was 0.87, AUPRC was 0.83. On the validation set of dataset 2, the CA of ResNet50 model was 69 %, micro average AUROC was 0.84, macro average AUROC was 0.83, and micro average AUPRC was 0.73. In the critical blood glucose group, AUROC was 0.88, AUPRC was 0.71. In the high blood glucose group, AUROC was 0.80, AUPRC was 0.76. On the test set of dataset 2, the CA of ResNet50 model was 65 %, micro average AUROC was 0.83, macro average AUROC was 0.82, and micro average AUPRC was 0.71. In the critical blood glucose group, the prediction of AUROC was 0.84, AUPRC was 0.60. In the high blood glucose group, AUROC was 0.87, AUPRC was 0.71.
Conclusions
Tongue features can improve the prediction accuracy of the diabetes risk prediction model formed by classical machine learning model significantly. In addition to the excellent performance, Stacking model and ResNet50 model which were recommended had non-invasive operation and were easy to use. Stacking model and ResNet50 model had high precision, low false positive rate and low misdiagnosis rate on detecting hyperglycemia. While on detecting blood glucose value in critical state, Stacking model and ResNet50 model had a high sensitivity, a low false negative rate and a low missed diagnosis rate. The study had proved that the differential changes of tongue features reflected the abnormal glucose metabolism, thus the diabetes risk prediction model formed by a combined TCM tongue diagnosis and machine learning technique was feasible.
ER  - 
TY  - JOUR
T1  - A personalized blood glucose level prediction model with a fine-tuning strategy: A proof-of-concept study
A1  - Seo, W
A1  - Park, S.-W.
A1  - Kim, N
A1  - Jin, S.-M.
A1  - Park, S.-M.
Y1  - 2021///
KW  -  Blood Glucose; Blood Glucose Self-Monitoring; Diet; Forecasting; Neural Networks
KW  -  Blood glucose level; Blood glucose management; Continuous glucose monitoring; Convolutional neural network; Data-driven approach; Fine tuning; Personalized model; Prediction modelling; Proof of concept; Tuning strategy
KW  -  C peptide; carbohydrate; glucose; hemoglobin A1c
KW  -  Computer
KW  -  Deep neural networks
KW  -  adult; Article; blood glucose monitoring; comparative study; controlled study; convolutional neural network; female; glucose blood level; human; insulin dependent diabetes mellitus; insulin release; major clinical study; male; middle aged; non insulin dependent diabetes mellitus; predictive value; pregnancy diabetes mellitus; proof of concept; random forest; blood glucose monitoring; diet; forecasting
KW  - Blood; Convolutional neural networks; Decision trees; Errors; Forecasting; Glucose; Information management; Large dataset; Mean square error
PB  - Elsevier Ireland Ltd
JF  - Computer Methods and Programs in Biomedicine
VL  - 211
DO  - 10.1016/j.cmpb.2021.106424
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116030368&doi=10.1016%2Fj.cmpb.2021.106424&partnerID=40&md5=1cafaea0107d866acdd8b5fbcb453b4b
N1  - cited By 1
N2  - Background: The accurate prediction of blood glucose (BG) level is still a challenge for diabetes management. This is due to various factors such as diet, personal physiological characteristics, stress, and activities influence changes in BG level. To develop an accurate BG level predictive model, we propose a personalized model based on a convolutional neural network (CNN) with a fine-tuning strategy. Methods: We utilized continuous glucose monitoring (CGM) datasets from 1052 professional CGM sessions and split them into three groups according to type 1, type 2, and gestational diabetes mellitus (T1DM, T2DM, and GDM, respectively). During the preprocessing, only CGM data points were utilized, and future BG levels of four different prediction horizons (PHs, 15, 30, 45, and 60 min) were used as output. In training, we trained a general CNN and a multi-output random forest regressor using a hold-out method for each group. Next, we developed two personalized models: (1) by fine-tuning the general CNN on partial sample points of each CGM dataset, and (2) by learning a CNN from scratch on the points. Results: For all groups, the fine-tuned CNN showed the lowest average root mean squared error, average mean absolute percentage error, highest average time gain (PH = 15 and 60 min in T1DM) and highest percentage in region A of Clarke error grid analysis at all PHs. In the performance comparison between the fine-tuned CNN and other models, we found that the fine-tuned CNN improved the performance of the general CNN in most cases and outperformed the scratch CNN at all PHs in all groups, making the fine-tuning strategy was useful for accurate BG level prediction. We analyzed all cases of four predictive patterns in each group, and found that the input BG level trend and the BG level at the time of prediction were related to the future BG level trend. Conclusions: We demonstrated the efficacy of the fine-tuning method in a large number of CGM datasets and analyzed the four predictive patterns. Therefore, we believe that the proposed method will significantly contribute to the development of an accurate personalized model and the analysis for its predictions. © 2021
ER  - 
TY  - JOUR
T1  - Drawing clinical pictures of heart failure with high mortality risk
A1  - Mohammadi, Tanya
A1  - Mohammadi, Babak
Y1  - 2021///
KW  -  Association rules
KW  -  Clinical picture
KW  -  Cohort
KW  -  Diabetes
KW  -  Mortality risk
KW  - Heart failure
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100752
EP  - 100752
DO  - https://doi.org/10.1016/j.imu.2021.100752
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821002276
N2  - Background
Risk stratification allows guiding treatment for patients with heart diseases. We aimed to provide clinical rules for identifying patients at high risk for mortality owing to heart failure.
Methods
The research was a secondary analysis of data from a community cohort study that had been conducted from October 2007 to December 2011 in the United States. In total, 425 adult patients (180 women) with heart failure had been included. The mean (SD) age was 73·5 (13·1) years, and participants had been followed for 2 years. The outcome was mortality and the predictors were a variety of demographic, clinical, and laboratory variables.
Results
We recognized two clusters of patients in the sample with different survival probabilities (P < 0·001) and defined clinical rules for identifying patients at high risk for mortality. Association rule mining showed that diabetes was a strong solitary risk for mortality in heart failure patients (support = 38·4%, confidence = 100%, lift = 1·9). Also, combinations of categories in age, sex, systolic blood pressure, estimated glomerular filtration rate, left ventricular ejection fraction, serum sodium, blood urea nitrogen, the cause of heart failure (ischemic vs. non-ischemic), diabetes mellitus, and increase in natriuretic peptide were able to identify patients at risk for death.
Conclusion
Different approaches to risk stratification provide distinct sets of clinical pictures. We presented the results as sets of clinical pictures accessible to all healthcare professionals. The clinical pictures support recognizing patients with heart failure who are at high risk for mortality.
ER  - 
TY  - JOUR
T1  - Evolutionary correlated gravitational search algorithm (ECGS) with genetic optimized Hopfield neural network (GHNN) – A hybrid expert system for diagnosis of diabetes
A1  - Jayashree, J
A1  - Ananda Kumar, S
Y1  - 2019///
KW  -  Evolutionary correlated gravitational search algorithm (ECGS)
KW  -  Genetic optimized Hopfield neural network (GHNN)
KW  -  Pima Indian Diabetic Dataset
KW  - Diabetics
JF  - Measurement
VL  - 145
SP  - 551
EP  - 558
DO  - https://doi.org/10.1016/j.measurement.2018.12.083
UR  - https://www.sciencedirect.com/science/article/pii/S0263224118312284
N2  - In worldwide 415 million of peoples are affected by diabetics in the year of 2015, that is increased from the year of 2012. Based on the survey, it clearly shows the diabetics are one of the dangerous diseases because it leads to create several risk of early death. Due to the seriousness of the diabetic, it has been detected in early stage by creating expert system. During this process, the expert system has several issues such as accuracy of prediction due to the huge dimension of the diabetic feature that reduce the entire efficiency of the system. So, in this paper introduced the evolutionary correlated gravitational search algorithm (ECGS) for selecting the optimized features. The introduced method analyzes each diabetic feature according to the correlation and mutual information is selected with minimum computation time and cost. The selected features are processed by genetic optimized Hopfield neural network (GHNN) for predicting the diabetic related features effectively. Then the efficiency of the system is implemented using MATLAB tool that utilizes the Pima Indian Diabetic Dataset for analyzing the efficiency of introduced diabetic expert system. The efficiency of the system is evaluated in terms of using mean square error rate, F-measurer, accuracy, confusion matrix and ROC curve.
ER  - 
TY  - JOUR
T1  - Similarity classifier using similarity measure derived from Yu's norms in classification of medical data sets
A1  - Luukka, Pasi
Y1  - 2007///
KW  -  Classification
KW  -  Entropy minimization
KW  -  Preprocessing
KW  -  Yu's norms
KW  - Similarity
JF  - Computers in Biology and Medicine
VL  - 37
IS  - 8
SP  - 1133
EP  - 1140
DO  - https://doi.org/10.1016/j.compbiomed.2006.10.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482506001880
N2  - A new approach using a similarity measure based on Yu's norms is presented for the detection of erythemato-squamous diseases, diabetes, breast cancer, lung cancer and lymphography. The domain contains records of patients with known diagnoses. The results are very promising with all data sets and (in conclusion, can be drawn that) a similarity model derived from Yu's norms could be used for the diagnosis of patients taking into consideration the error rate. A similarity classifier derived from Yu's norms was used to detect the six erythemato-squamous diseases when 34 features defining six disease indications were used as inputs. The results confirmed that the proposed model has potential in detecting the erythemato-squamous diseases. The similarity model derived from Yu's norms achieved an accuracy rate (97.8%) which was higher than that of the stand-alone neural network model or the ANFIS model suggested in Übeyli and Güler [Automatic detection of erythemato-squamous diseases using adaptive neuro-fuzzy inference systems, Comput. Biol. Med. 35 (2005) 421–433] or the similarity model based on Łukasiewicz similarity [Luukka and Leppälampi, Similarity classifier with generalized mean applied to medical data, Comput. Biol. Med. 36 (2006) 1026–1040]. With PIMA Indian diabetes, the detection model has an error rate of about 24% which is much better than the overall rate of 33% for diabetes. Also, a classifier was applied to the lung cancer data set and the results were to my knowledge better than before. When the lung cancer data were preprocessed with an entropy minimization technique and the classifier with similarity based on Yu's norm was applied, 99.99% accuracy was achieved. The use of this preprocessing method enhanced the results over 30%. In lymphography, entropy minimization also enhanced the results remarkably and 86.2% accuracy was achieved.
ER  - 
TY  - JOUR
T1  - Improving microaneurysm detection in color fundus images by using context-aware approaches
A1  - Antal, Bálint
A1  - Hajdu, András
Y1  - 2013///
KW  -  Context-aware weighting
KW  -  Ensemble learning
KW  -  Retinal image processing
KW  - Microaneurysm detection
JF  - Computerized Medical Imaging and Graphics
VL  - 37
IS  - 5
SP  - 403
EP  - 408
DO  - https://doi.org/10.1016/j.compmedimag.2013.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S0895611113000967
N1  - Retinal Image Analysis
N2  - In this paper, we present two approaches to improve microaneurysm detector ensembles. First, we provide an approach to select a set of preprocessing methods for a microaneurysm candidate extractor to enhance its detection performance in color fundus images. The performance of the candidate extractor with each preprocessing method is measured in six microaneurysm categories. The best performing preprocessing method for each category is selected and organized into an ensemble-based method. We tested our approach on the publicly available DiaretDB1 database, where the proposed approach led to an improvement regarding the individual approaches. Second, an adaptive weighting approach for microaneurysm detector ensembles is presented.The basis of the adaptive weighting approach is the spatial location and contrast of the detected microaneurysm. During training, the performance of ensemble members is measured with respect to these contextual information, which serves as a basis for the optimal weights assigned to the detectors. We have tested this approach on two publicly available datasets, where it showed its competitiveness compared without previously published ensemble-based approach for microaneurysm detection. Moreover, the proposed approach outperformed all the investigated individual detectors.
ER  - 
TY  - JOUR
T1  - Art networks with geometrical distances
A1  - Dagher, Issam
Y1  - 2006///
KW  -  Category choice
KW  -  Fuzzy ART
KW  -  Fuzzy ARTMAP
KW  -  Neural networks
KW  - norm
JF  - Journal of Discrete Algorithms
VL  - 4
IS  - 4
SP  - 538
EP  - 553
DO  - https://doi.org/10.1016/j.jda.2005.06.007
UR  - https://www.sciencedirect.com/science/article/pii/S1570866705000493
N2  - In this paper, ART networks (Fuzzy ART and Fuzzy ARTMAP) with geometrical norms are presented. The category choice of these networks is based on the Lp norm. Geometrical properties of these architectures are presented. Comparisons between this category choice and the category choice of the ART networks are illustrated. And simulation results on the databases taken from the UCI repository are performed. It will be shown that using the Lp norm is geometrically more attractive. It will operate directly on the input patterns without the need for doing any preprocessing. It should be noted that the ART architecture requires two preprocessing steps: normalization and complement coding. Simulation results on different databases show the good generalization performance of the Fuzzy ARTMAP with Lp norm compared to the performance of a typical Fuzzy ARTMAP.
ER  - 
TY  - JOUR
T1  - Application of dual tree complex wavelet transform in tandem mass spectrometry
A1  - Murugesan, Selvaraaju
A1  - Tay, David B H
A1  - Cooke, Ira
A1  - Faou, Pierre
Y1  - 2015///
KW  -  Dual tree complex wavelet transform
KW  -  Peptides detection
KW  -  Proteomic data processing
KW  -  Signal denoising
KW  - Tandem mass spectrometry
JF  - Computers in Biology and Medicine
VL  - 63
SP  - 36
EP  - 41
DO  - https://doi.org/10.1016/j.compbiomed.2015.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515001699
N2  - Mass Spectrometry (MS) is a widely used technique in molecular biology for high throughput identification and sequencing of peptides (and proteins). Tandem mass spectrometry (MS/MS) is a specialised mass spectrometry technique whereby the sequence of peptides can be determined. Preprocessing of the MS/MS data is indispensable before performing any statistical analysis on the data. In this work, preprocessing of MS/MS data is proposed based on the Dual Tree Complex Wavelet Transform (DTCWT) using almost symmetric Hilbert pair of wavelets. After the preprocessing step, the identification of peptides is done using the database search approach. The performance of the proposed preprocessing technique is evaluated by comparing its performance against Discrete Wavelet Transform (DWT) and Stationary Wavelet Transform (SWT). The preprocessing performed using DTCWT identified more peptides compared to DWT and SWT.
ER  - 
TY  - JOUR
T1  - Computerized Extraction of Information on the Quality of Diabetes Care from Free Text in Electronic Patient Records of General Practitioners
A1  - Voorham, Jaco
A1  - Denig, Petra
Y1  - 2007///
JF  - Journal of the American Medical Informatics Association
VL  - 14
IS  - 3
SP  - 349
EP  - 354
DO  - https://doi.org/10.1197/jamia.M2128
UR  - https://www.sciencedirect.com/science/article/pii/S1067502707000540
N2  - Objective
This study evaluated a computerized method for extracting numeric clinical measurements related to diabetes care from free text in electronic patient records (EPR) of general practitioners.
Design and Measurements
Accuracy of this number-oriented approach was compared to manual chart abstraction. Audits measured performance in clinical practice for two commonly used electronic record systems.
Results
Numeric measurements embedded within free text of the EPRs constituted 80% of relevant measurements. For 11 of 13 clinical measurements, the study extraction method was 94%–100% sensitive with a positive predictive value (PPV) of 85%–100%. Post-processing increased sensitivity several points and improved PPV to 100%. Application in clinical practice involved processing times averaging 7.8 minutes per 100 patients to extract all relevant data.
Conclusion
The study method converted numeric clinical information to structured data with high accuracy, and enabled research and quality of care assessments for practices lacking structured data entry.
ER  - 
TY  - JOUR
T1  - PSO-FCM based data mining model to predict diabetic disease
A1  - Raja, J Beschi
A1  - Pandian, S Chenthur
Y1  - 2020///
KW  -  Fuzzy Clustering Means (FCM)
KW  -  Particle Swarm Optimization (PSO)
KW  -  Sensitivity
KW  -  Specificity and accuracy
KW  -  Type 2 diabetes mellitus (T2DM)
KW  - Data mining
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105659
EP  - 105659
DO  - https://doi.org/10.1016/j.cmpb.2020.105659
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314929
N2  - Background and Objective
Diabetic disease is typically composed because of higher than normal blood sugar levels. Instead the production of insulin may be regarded insufficient. It has been noted in recent days that the percentage of diabetes-affected patients have grown to a larger extent throughout the world. Evidently, this problem must be taken more seriously in the coming days to ensure that the average percentages of diabetes-affected individuals are reduced. Recently, several research teams conducted detailed research on the data mining platform to determine the precision of each other. Data mining can be used by parametric modeling from the health data, including diabetic patient data sets, to synthesize expertise in the field.
Methods
In this study, a new model is proposed for forecasting type 2 diabetes mellitus (T2DM) based on data mining strategies. The combined Particle Swarm Optimization (PSO) and Fuzzy Clustering Means (FCM) (PSO-FCM) are used to evaluate a set of medical data relating to a diabetes diagnosis challenge.
Results
Experiments are performed on the Pima Indians Diabetes Database. The sensitivity, specificity and accuracy metrics widely used in medical studies have been used to assess the effectiveness of the proposed system reliability. It was found that the prototype has achieved 8.26 percent more accuracy than the other methods.
Conclusion
The conclusion produced by using the method shows that, as compared with other models, the proposed PSO-FCM method delivers greater performance.
ER  - 
TY  - JOUR
T1  - Prediction of postprandial glucose excursions in type 1 diabetes using control-oriented process models
A1  - Adelberger, D
A1  - Reiterer, F
A1  - Schrangl, P
A1  - Ringemann, Ch.
A1  - Huschto, T
A1  - del Re, L
Y1  - 2021///
KW  -  Grey box modeling
KW  -  Identification
KW  -  Prediction
KW  -  Validation
KW  - Diabetes
JF  - IFAC-PapersOnLine
VL  - 54
IS  - 15
SP  - 466
EP  - 471
DO  - https://doi.org/10.1016/j.ifacol.2021.10.300
UR  - https://www.sciencedirect.com/science/article/pii/S2405896321017055
N1  - 11th IFAC Symposium on Biological and Medical Systems BMS 2021
N2  - Reliable prediction of future blood glucose (BG) values is of high relevance for diabetes patients, since it enables the use of predictive glucose alarms (warning the patient about impending situations with dangerously low or high BG), as well as of model-based algorithms for smart glucose control. Control-oriented graybox process models have proven very suitable for such tasks, especially when identified on data from clinical trials under well-defined conditions. The current paper analyzes how such models can also be reliably parametrized using outpatient data of patients on multiple daily injection (MDI) therapy. A dedicated preprocessing algorithm is presented to look for suitable (i.e. complete and sensible) data segments that allow for a reliable system identification. The focus of the current paper is on the prediction of postprandial glucose trajectories, more specifically on predictions made exactly at the time of meal ingestion. This corresponds to a particularly challenging task, but one with high importance for the model-based optimization of insulin doses. It is demonstrated that the identified process models are a suitable choice for predicting such postprandial glucose excursions.
ER  - 
TY  - JOUR
T1  - Detection of Hard Exudates in Color Fundus Images of the Human Retina
A1  - JayaKumari, C
A1  - Maruthi, R
Y1  - 2012///
KW  -  Contextual Clustering
KW  -  Echo State Neural Network
KW  - Hard Exudates
JF  - Procedia Engineering
VL  - 30
SP  - 297
EP  - 302
DO  - https://doi.org/10.1016/j.proeng.2012.01.864
UR  - https://www.sciencedirect.com/science/article/pii/S1877705812008740
N1  - International Conference on Communication Technology and System Design 2011
N2  - Diabetic Retinopathy is a progressive ocular disease. The disease may advance from mild to severe non-proliferative diabetic retinopathy and it is one of the most significant factors contributing to blindness. Therefore, it is necessary everyone with diabetes should get a comprehensive dilated eye exam at least once in a year. In this study, a state-of-art image processing techniques to automatically detect the presence of hard exudates in the fundus images are presented. After the contrast adaptive histogram equalization as preprocessing stage, contextual clustering algorithms have been applied to segment the exudates. The key features are like the standard deviation, mean, intensity, edge strength and compactness of the segmented regions are extracted and fed as inputs into Echo State Neural Network (ESNN) to discriminate between the normal and pathological image. A total of 50 images have been used to find the exudates out of which 35 images consisting of both normal and abnormal are used to train the ESSN and the remaining 15 images are used to test the neural network. Furthermore, it confirms 93.0% sensitivity and 100% specificity in terms of exudates based classification.
ER  - 
TY  - JOUR
T1  - An extensive analysis of various texture feature extractors to detect Diabetes Mellitus using facial specific regions
A1  - Shu, Ting
A1  - Zhang, Bob
A1  - Yan Tang, Yuan
Y1  - 2017///
KW  -  Diabetes Mellitus detection
KW  -  Facial key block analysis
KW  -  Image gray-scale histogram
KW  -  Medical biometrics
KW  - Texture feature analysis
JF  - Computers in Biology and Medicine
VL  - 83
SP  - 69
EP  - 83
DO  - https://doi.org/10.1016/j.compbiomed.2017.02.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517300422
N2  - Introduction: Researchers have recently discovered that Diabetes Mellitus can be detected through non-invasive computerized method. However, the focus has been on facial block color features. In this paper, we extensively study the effects of texture features extracted from facial specific regions at detecting Diabetes Mellitus using eight texture extractors. Materials and methods: The eight methods are from four texture feature families: (1) statistical texture feature family: Image Gray-scale Histogram, Gray-level Co-occurance Matrix, and Local Binary Pattern, (2) structural texture feature family: Voronoi Tessellation, (3) signal processing based texture feature family: Gaussian, Steerable, and Gabor filters, and (4) model based texture feature family: Markov Random Field. In order to determine the most appropriate extractor with optimal parameter(s), various parameter(s) of each extractor are experimented. For each extractor, the same dataset (284 Diabetes Mellitus and 231 Healthy samples), classifiers (k-Nearest Neighbors and Support Vector Machines), and validation method (10-fold cross validation) are used. Results: According to the experiments, the first and third families achieved a better outcome at detecting Diabetes Mellitus than the other two. Conclusions: The best texture feature extractor for Diabetes Mellitus detection is the Image Gray-scale Histogram with bin number=256, obtaining an accuracy of 99.02%, a sensitivity of 99.64%, and a specificity of 98.26% by using SVM.
ER  - 
TY  - JOUR
T1  - Integration of clinical criteria into the training of deep models: Application to glucose prediction for diabetic people
A1  - De Bois, Maxime
A1  - El-Yacoubi, Mounîm A
A1  - Ammi, Mehdi
Y1  - 2021///
KW  -  Clinical acceptability
KW  -  Diabetes
KW  -  Glucose prediction
KW  -  Neural network
KW  - Deep learning
JF  - Smart Health
VL  - 21
SP  - 100193
EP  - 100193
DO  - https://doi.org/10.1016/j.smhl.2021.100193
UR  - https://www.sciencedirect.com/science/article/pii/S2352648321000155
N2  - The standard way to train neural-network-based solutions in healthcare does not consider clinical criteria, leading to models that are not necessarily clinically acceptable. In this study, we look at this problem from the perspective of the forecasting of future glucose values of people with diabetes. We propose a new training methodology that achieves the best possible tradeoff between accuracy and medical requirements set by health authorities. Starting from a solution maximizing the prediction accuracy, we progressively relax the accuracy constraints to focus more on the medical ones. This is achieved by considering a new loss function specifically designed for glucose prediction. We evaluate the proposed approach on both people with type-1 and type-2 diabetes. We show that it improves the clinical acceptability of the predictions. Moreover, for given clinical criteria, we are able to find the optimal solution that maximizes the accuracy while at the same time meeting clinical the criteria.
ER  - 
TY  - JOUR
T1  - Augmenting aer2vec: Enriching distributed representations of adverse event report data with orthographic and lexical information
A1  - Ding, Xiruo
A1  - Mower, Justin
A1  - Subramanian, Devika
A1  - Cohen, Trevor
Y1  - 2021///
KW  -  Natural language processing
KW  -  Post-marketing surveillance
KW  -  Retrofitting
KW  -  Subword embeddings
KW  -  Word embeddings
KW  - Pharmacovigilance
JF  - Journal of Biomedical Informatics
VL  - 119
SP  - 103833
EP  - 103833
DO  - https://doi.org/10.1016/j.jbi.2021.103833
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001611
N2  - Adverse Drug Events (ADEs) are prevalent, costly, and sometimes preventable. Post-marketing drug surveillance aims to monitor ADEs that occur after a drug is released to market. Reports of such ADEs are aggregated by reporting systems, such as the Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS). In this paper, we consider the topic of how best to represent data derived from reports in FAERS for the purpose of detecting post-marketing surveillance signals, in order to inform regulatory decision making. In our previous work, we developed aer2vec, a method for deriving distributed representations (concept embeddings) of drugs and side effects from ADE reports, establishing the utility of distributional information for pharmacovigilance signal detection. In this paper, we advance this line of research further by evaluating the utility of encoding orthographic and lexical information. We do so by adapting two Natural Language Processing methods, subword embedding and vector retrofitting, which were developed to encode such information into word embeddings. Models were compared for their ability to distinguish between positive and negative examples in a set of manually curated drug/ADE relationships, with both aer2vec enhancements offering advantages in performances over baseline models, and best performance obtained when retrofitting and subword embeddings were applied in concert. In addition, this work demonstrates that models leveraging distributed representations do not require extensive manual preprocessing to perform well on this pharmacovigilance signal detection task, and may even benefit from information that would otherwise be lost during the normalization and standardization process.
ER  - 
TY  - JOUR
T1  - Graph Convolutional Network Enabled Two-Stream Learning Architecture for Diabetes Classification based on Flash Glucose Monitoring Data
A1  - Liu, Yicun
A1  - Liu, Wei
A1  - Chen, Haorui
A1  - Cai, Xiaoling
A1  - Zhang, Rui
A1  - An, Zhe
A1  - Shi, Dawei
A1  - Ji, Linong
Y1  - 2021///
KW  -  Attention mechanism
KW  -  Data processing
KW  -  Graph convolutional network
KW  -  LSTM
KW  - Diabetes classification
JF  - Biomedical Signal Processing and Control
VL  - 69
SP  - 102896
EP  - 102896
DO  - https://doi.org/10.1016/j.bspc.2021.102896
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421004936
N2  - The classification of type 1 and type 2 diabetes is currently performed based on biochemical indicators and clinical experience. However, considering the unsatisfactory efficiency and accuracy of the experience-based diabetes type classification, we aim to propose a data-driven diabetes classification model through exploiting features contained in flash glucose monitoring (FGM) data. In particular, we propose a novel data reorganization and topologization method to reasonably extract the features of glycemic variability influence. Furthermore, a graph convolutional network is adopted to learn the inter-day influence feature and a Long Short-Term Memory network to characterize intra-day glycemic variability, which enables simultaneous characterization of slow and fast dynamics in FGM data. Finally, to visualize the effectiveness of our model, a t-distributed stochastic neighbor embedding method is implemented. The effectiveness of the proposed model is evaluated through a cross-validation approach using a dataset containing FGM records of 113 diabetic subjects. Compared with classical machine learning algorithms and neural networks, the proposed model achieved the highest specificity value (0.9943) in diabetes type classification, F-Measure (0.8824) and Matthews correlation coefficient score (0.8250). The obtained results indicate the feasibility of achieving diabetes classification by learning the patterns hidden in continuous glucose monitoring data.
ER  - 
TY  - JOUR
T1  - Probabilistic graphical models for finding optimal multipurpose multicomponent therapy
A1  - Pavlovskii, Vladislav V
A1  - Derevitskii, Ilia V
A1  - Savitskaya, Daria A
Y1  - 2021///
KW  -  bayes network
KW  -  diabetes mellitus
KW  -  predictive modelling
KW  -  probabilistic graphical models
KW  - optimal therapy
JF  - Procedia Computer Science
VL  - 193
SP  - 382
EP  - 392
DO  - https://doi.org/10.1016/j.procs.2021.10.039
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921020809
N1  - 10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021
N2  - This paper suggests a probabilistic graphical models’ approach for finding optimal therapy. This approach is based on creating a network of dependencies using statistics of patient treatment. We used Bayesian networks for describing diabetes mellitus treatment. 4 networks were created, one of them with expert knowledge, and the other was created using different algorithms. Treatment outcomes include a set of treatment-goal values and a combination of drugs. Networks were trained and validated by the treatment dataset. Results of validation showed that this approach was high-quality for cases that had a wide representation of using medication. Most of the predictions were equal with the expert’s opinion, therefore models could be used as part of Decision Support Systems for medical experts who work with patients suffering from T2DM (Type 2 Diabetes Mellitus).
ER  - 
TY  - JOUR
T1  - A robust signal preprocessing framework for wrist pulse analysis
A1  - Wang, Dimin
A1  - Zhang, David
A1  - Lu, Guangming
Y1  - 2016///
KW  -  Intra-class distance
KW  -  Period segmentation
KW  -  Wavelet-based decomposition
KW  - Wrist pulse preprocessing
JF  - Biomedical Signal Processing and Control
VL  - 23
SP  - 62
EP  - 75
DO  - https://doi.org/10.1016/j.bspc.2015.08.002
UR  - https://www.sciencedirect.com/science/article/pii/S1746809415001366
N2  - Wrist pulse has been a physical health indicator in Traditional Chinese Medicine (TCM) for a long history. With the development of sensor technology and bioinformatics, quantifying pulse diagnosis by using signal processing technology is attracting increasing attentions in recent years. Since wrist pulse signals collected by the sensors are often corrupted by artifacts in real situations, many approaches on the wrist pulse preprocessing including pulse de-noising and baseline wander removal are introduced for more accurate wrist pulse analysis. However, these scattered methods are incomplete with some limitations when used to preprocess our special pulse data for the clinical applications. This paper presents a robust signal preprocessing framework for wrist pulse analysis. The cascade filter based on frequency-dependent analysis (FDA) is first introduced to remove the high frequency noises and to select the significant pulse intervals. Then the curve fitting method is developed to adjust the direction and the baseline drift with minimum signal distortion. Last, the period segmentation and pulse normalization is applied for the feature extraction. The effectiveness of the proposed pulse preprocessing is validated through experiments on actual pulse records with biochemical markers. In contrast with the traditional methods, the proposed preprocessing framework is effective in extracting more accurate pulse features. And the highest classification rate 91.6% is obtained on diabetes diagnosis. The results demonstrate that our method is superior to the former pulse preprocessing researches and practical for wrist pulse analysis.
ER  - 
TY  - JOUR
T1  - Research on TCM Diabetes Assisted Diagnosis and Treatment Plan Integrating Association Mining and Quantitative Calculation
A1  - Zhang, Yujie
A1  - Bai, Rujiang
A1  - Han, Jin
A1  - Chen, Qiming
A1  - Gao, Xu
Y1  - 2021///
KW  -  diabetes treatment
KW  -  intelligent decision
KW  -  sentiment analysis
KW  - TCM data mining
JF  - Procedia Computer Science
VL  - 188
SP  - 52
EP  - 60
DO  - https://doi.org/10.1016/j.procs.2021.05.052
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921011376
N1  - CQVIP Conference on Data Driven Intelligence and Innovation
N2  - This paper attempts to explore more precise and intelligent assisted methodological references for the diagnosis and treatment of diabetes in Traditional Chinese Medicine (TCM) by using medical record data from TCM diabetes, and to construct a TCM diabetes assisted diagnosis and treatment framework of “association mining-quantifying calculation-intelligent assisted diagnosis and treatment” what can be described as cluster analyzing and match mining for symptom and drug by association mining, as quantitatively calculating and evaluating for the condition of diabetes; as disease diagnosis, drug recommendation and decision making by intelligent assisted diagnosis and treatment. This paper synthetically employs the technical methodology of association rules, topic clustering, emotion recognition and content recommendation, to realize the knowledge mining, quantify the evaluation and assist decision-making of TCM diabetes, and to propose a reference for the process of consultation and treatment. Overall, the study contributes to the academic and practical value in diabetes by elaborating a new technical methodology and providing a brief empirical test for doctors and researchers.
ER  - 
TY  - JOUR
T1  - A systematic map of medical data preprocessing in knowledge discovery
A1  - Idri, A
A1  - Benhar, H
A1  - Fernández-Alemán, J L
A1  - Kadi, I
Y1  - 2018///
KW  -  Clinical data
KW  -  Data preprocessing
KW  -  Electronic heath records
KW  -  Mapping study
KW  - Medical datamining
JF  - Computer Methods and Programs in Biomedicine
VL  - 162
SP  - 69
EP  - 85
DO  - https://doi.org/10.1016/j.cmpb.2018.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717313706
N2  - Background and objective
Datamining (DM) has, over the last decade, received increased attention in the medical domain and has been widely used to analyze medical datasets in order to extract useful knowledge and previously unknown patterns. However, historical medical data can often comprise inconsistent, noisy, imbalanced, missing and high dimensional data. These challenges lead to a serious bias in predictive modeling and reduce the performance of DM techniques. Data preprocessing is, therefore, an essential step in knowledge discovery as regards improving the quality of data and making it appropriate and suitable for DM techniques. The objective of this paper is to review the use of preprocessing techniques in clinical datasets.
Methods
We performed a systematic map of studies regarding the application of data preprocessing to healthcare and published between January 2000 and December 2017. A search string was determined on the basis of the mapping questions and the PICO categories. The search string was then applied in digital databases covering the fields of computer science and medical informatics in order to identify relevant studies. The studies were initially selected by reading their titles, abstracts and keywords. Those that were selected at that stage were then reviewed using a set of inclusion and exclusion criteria in order to eliminate any that were not relevant. This process resulted in 126 primary studies.
Results
Selected studies were analyzed and classified according to their publication years and channels, research type, empirical type and contribution type. The findings of this mapping study revealed that researchers have paid a considerable amount of attention to preprocessing in medical DM in last decade. A significant number of the selected studies used data reduction and cleaning preprocessing tasks. Moreover, the disciplines in which preprocessing have received most attention are: cardiology, endocrinology and oncology.
Conclusions
Researchers should develop and implement standards for an effective integration of multiple medical data types. Moreover, we identified the need to perform literature reviews.
ER  - 
TY  - JOUR
T1  - Data-driven modeling and prediction of blood glucose dynamics: Machine learning applications in type 1 diabetes
A1  - Woldaregay, Ashenafi Zebene
A1  - Årsand, Eirik
A1  - Walderhaug, Ståle
A1  - Albers, David
A1  - Mamykina, Lena
A1  - Botsis, Taxiarchis
A1  - Hartvigsen, Gunnar
Y1  - 2019///
KW  -  Blood glucose dynamics
KW  -  Blood glucose level prediction
KW  -  Machine learning
KW  - Type 1 diabetes
JF  - Artificial Intelligence in Medicine
VL  - 98
SP  - 109
EP  - 134
DO  - https://doi.org/10.1016/j.artmed.2019.07.007
UR  - https://www.sciencedirect.com/science/article/pii/S0933365717306218
N2  - Background
Diabetes mellitus (DM) is a metabolic disorder that causes abnormal blood glucose (BG) regulation that might result in short and long-term health complications and even death if not properly managed. Currently, there is no cure for diabetes. However, self-management of the disease, especially keeping BG in the recommended range, is central to the treatment. This includes actively tracking BG levels and managing physical activity, diet, and insulin intake. The recent advancements in diabetes technologies and self-management applications have made it easier for patients to have more access to relevant data. In this regard, the development of an artificial pancreas (a closed-loop system), personalized decision systems, and BG event alarms are becoming more apparent than ever. Techniques such as predicting BG (modeling of a personalized profile), and modeling BG dynamics are central to the development of these diabetes management technologies. The increased availability of sufficient patient historical data has paved the way for the introduction of machine learning and its application for intelligent and improved systems for diabetes management. The capability of machine learning to solve complex tasks with dynamic environment and knowledge has contributed to its success in diabetes research.
Motivation
Recently, machine learning and data mining have become popular, with their expanding application in diabetes research and within BG prediction services in particular. Despite the increasing and expanding popularity of machine learning applications in BG prediction services, updated reviews that map and materialize the current trends in modeling options and strategies are lacking within the context of BG prediction (modeling of personalized profile) in type 1 diabetes.
Objective
The objective of this review is to develop a compact guide regarding modeling options and strategies of machine learning and a hybrid system focusing on the prediction of BG dynamics in type 1 diabetes. The review covers machine learning approaches pertinent to the controller of an artificial pancreas (closed-loop systems), modeling of personalized profiles, personalized decision support systems, and BG alarm event applications. Generally, the review will identify, assess, analyze, and discuss the current trends of machine learning applications within these contexts.
Method
A rigorous literature review was conducted between August 2017 and February 2018 through various online databases, including Google Scholar, PubMed, ScienceDirect, and others. Additionally, peer-reviewed journals and articles were considered. Relevant studies were first identified by reviewing the title, keywords, and abstracts as preliminary filters with our selection criteria, and then we reviewed the full texts of the articles that were found relevant. Information from the selected literature was extracted based on predefined categories, which were based on previous research and further elaborated through brainstorming among the authors.
Results
The initial search was done by analyzing the title, abstract, and keywords. A total of 624 papers were retrieved from DBLP Computer Science (25), Diabetes Technology and Therapeutics (31), Google Scholar (193), IEEE (267), Journal of Diabetes Science and Technology (31), PubMed/Medline (27), and ScienceDirect (50). After removing duplicates from the list, 417 records remained. Then, we independently assessed and screened the articles based on the inclusion and exclusion criteria, which eliminated another 204 papers, leaving 213 relevant papers. After a full-text assessment, 55 articles were left, which were critically analyzed. The inter-rater agreement was measured using a Cohen Kappa test, and disagreements were resolved through discussion.
Conclusion
Due to the complexity of BG dynamics, it remains difficult to achieve a universal model that produces an accurate prediction in every circumstance (i.e., hypo/eu/hyperglycemia events). Recently, machine learning techniques have received wider attention and increased popularity in diabetes research in general and BG prediction in particular, coupled with the ever-growing availability of a self-collected health data. The state-of-the-art demonstrates that various machine learning techniques have been tested to predict BG, such as recurrent neural networks, feed-forward neural networks, support vector machines, self-organizing maps, the Gaussian process, genetic algorithm and programs, deep neural networks, and others, using various group of input parameters and training algorithms. The main limitation of the current approaches is the lack of a well-defined approach to estimate carbohydrate intake, which is mainly done manually by individual users and is prone to an error that can severely affect the predictive performance. Moreover, a universal approach has not been established to estimate and quantify the approximate effect of physical activities, stress, and infections on the BG level. No researchers have assessed model predictive performance during stress and infection incidences in a free-living condition, which should be considered in future studies. Furthermore, a little has been done regarding model portability that can capture inter- and intra-variability among patients. It seems that the effect of time lags between the CGM readings and the actual BG levels is not well covered. However, in general, we foresee that these developments might foster the advancement of next-generation BG prediction algorithms, which will make a great contribution in the effort to develop the long–awaited, so-called artificial pancreas (a closed-loop system).
ER  - 
TY  - JOUR
T1  - A Dynamic Bayesian Network model for long-term simulation of clinical complications in type 1 diabetes
A1  - Marini, Simone
A1  - Trifoglio, Emanuele
A1  - Barbarini, Nicola
A1  - Sambo, Francesco
A1  - Di Camillo, Barbara
A1  - Malovini, Alberto
A1  - Manfrini, Marco
A1  - Cobelli, Claudio
A1  - Bellazzi, Riccardo
Y1  - 2015///
KW  -  CVD
KW  -  Nephropaty
KW  -  Simulation
KW  -  Tabu search
KW  -  Type 1 diabetes
KW  - Dynamic Bayesian Network
JF  - Journal of Biomedical Informatics
VL  - 57
SP  - 369
EP  - 376
DO  - https://doi.org/10.1016/j.jbi.2015.08.021
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001896
N2  - The increasing prevalence of diabetes and its related complications is raising the need for effective methods to predict patient evolution and for stratifying cohorts in terms of risk of developing diabetes-related complications. In this paper, we present a novel approach to the simulation of a type 1 diabetes population, based on Dynamic Bayesian Networks, which combines literature knowledge with data mining of a rich longitudinal cohort of type 1 diabetes patients, the DCCT/EDIC study. In particular, in our approach we simulate the patient health state and complications through discretized variables. Two types of models are presented, one entirely learned from the data and the other partially driven by literature derived knowledge. The whole cohort is simulated for fifteen years, and the simulation error (i.e. for each variable, the percentage of patients predicted in the wrong state) is calculated every year on independent test data. For each variable, the population predicted in the wrong state is below 10% on both models over time. Furthermore, the distributions of real vs. simulated patients greatly overlap. Thus, the proposed models are viable tools to support decision making in type 1 diabetes.
ER  - 
TY  - JOUR
T1  - Automatic detection of microaneurysms in retinal fundus images
A1  - Wu, Bo
A1  - Zhu, Weifang
A1  - Shi, Fei
A1  - Zhu, Shuxia
A1  - Chen, Xinjian
Y1  - 2017///
KW  -  Classifier
KW  -  Diabetic retinopathy (DR)
KW  -  Eye fundus images
KW  -  Local features
KW  -  Profile features
KW  - Microaneurysms (MAs)
JF  - Computerized Medical Imaging and Graphics
VL  - 55
SP  - 106
EP  - 112
DO  - https://doi.org/10.1016/j.compmedimag.2016.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S0895611116300787
N1  - Special Issue on Ophthalmic Medical Image Analysis
N2  - Diabetic retinopathy (DR) is one of the leading causes of new cases of blindness. Early and accurate detection of microaneurysms (MAs) is important for diagnosis and grading of diabetic retinopathy. In this paper, a new method for the automatic detection of MAs in eye fundus images is proposed. The proposed method consists of four main steps: preprocessing, candidate extraction, feature extraction and classification. A total of 27 characteristic features which contain local features and profile features are extracted for KNN classifier to distinguish true MAs from spurious candidates. The proposed method has been evaluated on two public database: ROC and e-optha. The experimental result demonstrates the efficiency and effectiveness of the proposed method, and it has the potential to be used to diagnose DR clinically.
ER  - 
TY  - JOUR
T1  - Efficient semi-supervised feature selection with noise insensitive trace ratio criterion
A1  - Liu, Yun
A1  - Nie, Feiping
A1  - Wu, Jigang
A1  - Chen, Lihui
Y1  - 2013///
KW  -  Graph based learning
KW  -  Re-scale preprocessing
KW  -  Semi-supervised
KW  -  Trace ratio
KW  - Feature selection
JF  - Neurocomputing
VL  - 105
SP  - 12
EP  - 18
DO  - https://doi.org/10.1016/j.neucom.2012.05.031
UR  - https://www.sciencedirect.com/science/article/pii/S0925231212007151
N1  - Learning for Scalable Multimedia Representation
N2  - Feature selection is an effective method to deal with high-dimensional data. While in many applications such as multimedia and web mining, the data are often high-dimensional and very large scale, but the labeled data are often very limited. On these kind of applications, it is important that the feature selection algorithm is efficient and can explore labeled data and unlabeled data simultaneously. In this paper, we target on this problem and propose an efficient semi-supervised feature selection algorithm to select relevant features using both labeled and unlabeled data. First, we analyze a popular trace ratio criterion in the dimensionality reduction, and point out that the trace ratio criterion tends to select features with very small variance. To solve this problem, we propose a noise insensitive trace ratio criterion for feature selection with a re-scale preprocessing. Interestingly, the feature selection with the noise insensitive trace ratio criterion can be much more efficiently solved. Based on the noise insensitive trace ratio criterion, we propose a new semi-supervised feature selection algorithm. The algorithm fully explores the distribution of the labeled and unlabeled data with a special label propagation method. Experimental results verify the effectiveness of the proposed algorithm, and show improvement over traditional supervised feature selection algorithms.
ER  - 
TY  - JOUR
T1  - A meta-analysis of comorbidities in COVID-19: Which diseases increase the susceptibility of SARS-CoV-2 infection?
A1  - Singh, Manoj Kumar
A1  - Mobeen, Ahmed
A1  - Chandra, Amit
A1  - Joshi, Sweta
A1  - Ramachandran, Srinivasan
Y1  - 2021///
KW  -  Cancer
KW  -  Comorbidity
KW  -  Leukemia
KW  -  NAFLD
KW  -  Psoriasis
KW  -  SARS-CoV-2
KW  -  Type II diabetes
KW  - COVID-19
JF  - Computers in Biology and Medicine
VL  - 130
SP  - 104219
EP  - 104219
DO  - https://doi.org/10.1016/j.compbiomed.2021.104219
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521000135
N2  - Comorbidities in COVID-19 patients often lead to more severe outcomes. The disease-specific molecular events, which may induce susceptibility to Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection, are being investigated. To assess this, we retrieved array-based gene expression datasets from patients of 30 frequently occurring acute, chronic, or infectious diseases. Comparative analyses of the datasets were performed after quantile normalization and log2 transformation. Among the 78 host genes prominently implicated in COVID-19 infection, ACE2 (receptor for SARS-CoV-2) was positively regulated in several cases, namely, leukemia, psoriasis, lung cancer, non-alcoholic fatty liver disease (NAFLD), breast cancer, and pulmonary arterial hypertension (PAH). FURIN was positively regulated in some cases, such as leukemia, psoriasis, NAFLD, lung cancer, and type II diabetes (T2D), while TMPRSS2 was positively regulated in only 3 cases, namely, leukemia, lung cancer, and T2D. Genes encoding various interferons, cytokines, chemokines, and mediators of JAK-STAT pathway were positively regulated in leukemia, NAFLD, and T2D cases. Among the 161 genes that are positively regulated in the lungs of COVID-19 patients, 99–111 genes in leukemia (including various studied subtypes), 77 genes in NAFLD, and 48 genes in psoriasis were also positively regulated. Because of the high similarity in gene expression patterns, the patients of leukemia, NAFLD, T2D, psoriasis, and PAH may need additional preventive care against acquiring SARS-CoV-2 infections. Further, two genes CARBONIC ANHYDRASE 11 (CA11) and CLUSTERIN (CLU) were positively regulated in the lungs of patients infected with either SARS-CoV-2, or SARS-CoV or Middle East Respiratory Syndrome Coronavirus (MERS-CoV).
ER  - 
TY  - JOUR
T1  - New label noise injection methods for the evaluation of noise filters
A1  - Garcia, Luís P F
A1  - Lehmann, Jens
A1  - de Carvalho, André C.P.L.F.
A1  - Lorena, Ana C
Y1  - 2019///
KW  -  Borderline noise
KW  -  Noise filters
KW  -  Noise injection
KW  - Label noise
JF  - Knowledge-Based Systems
VL  - 163
SP  - 693
EP  - 704
DO  - https://doi.org/10.1016/j.knosys.2018.09.031
UR  - https://www.sciencedirect.com/science/article/pii/S0950705118304829
N2  - Noise is often present in real datasets used for training Machine Learning classifiers. Their disruptive effects in the learning process may include: increasing the complexity of the induced models, a higher processing time and a reduced predictive power in the classification of new examples. Therefore, treating noisy data in a preprocessing step is crucial for improving data quality and to reduce their harmful effects in the learning process. There are various filters using different concepts for identifying noisy examples in a dataset. Their ability in noise preprocessing is usually assessed in the identification of artificial noise injected into one or more datasets. This is performed to overcome the limitation that only a domain expert can guarantee whether a real example is indeed noisy. The most frequently used label noise injection method is the noise at random method, in which a percentage of the training examples have their labels randomly exchanged. This is carried out regardless of the characteristics and example space positions of the selected examples. This paper proposes two novel methods to inject label noise in classification datasets. These methods, based on complexity measures, can produce more challenging and realistic noisy datasets by the disturbance of the labels of critical examples situated close to the decision borders and can improve the noise filtering evaluation. An extensive experimental evaluation of different noise filters is performed using public datasets with imputed label noise and the influence of the noise injection methods are compared in both data preprocessing and classification steps.
ER  - 
TY  - JOUR
T1  - Efficient treatment of outliers and class imbalance for diabetes prediction
A1  - Nnamoko, Nonso
A1  - Korkontzelos, Ioannis
Y1  - 2020///
KW  -  Data preprocessing
KW  -  Imbalanced data
KW  -  Machine learning
KW  -  Oversampling
KW  -  SMOTE
KW  - Outlier detection
JF  - Artificial Intelligence in Medicine
VL  - 104
SP  - 101815
EP  - 101815
DO  - https://doi.org/10.1016/j.artmed.2020.101815
UR  - https://www.sciencedirect.com/science/article/pii/S093336571830681X
N2  - Learning from outliers and imbalanced data remains one of the major difficulties for machine learning classifiers. Among the numerous techniques dedicated to tackle this problem, data preprocessing solutions are known to be efficient and easy to implement. In this paper, we propose a selective data preprocessing approach that embeds knowledge of the outlier instances into artificially generated subset to achieve an even distribution. The Synthetic Minority Oversampling TEchnique (SMOTE) was used to balance the training data by introducing artificial minority instances. However, this was not before the outliers were identified and oversampled (irrespective of class). The aim is to balance the training dataset while controlling the effect of outliers. The experiments prove that such selective oversampling empowers SMOTE, ultimately leading to improved classification performance.
ER  - 
TY  - JOUR
T1  - Accurate disease detection quantification of iris based retinal images using random implication image classifier technique
A1  - Wang, Yu
A1  - Shan, Shan
Y1  - 2021///
KW  -  Discrete Image Clustering Technique (DICT)
KW  -  Median filter
KW  -  Preprocessing
KW  -  Segmentation
KW  - Random Implication Image Classifier Technique (RIICT)
JF  - Microprocessors and Microsystems
VL  - 80
SP  - 103350
EP  - 103350
DO  - https://doi.org/10.1016/j.micpro.2020.103350
UR  - https://www.sciencedirect.com/science/article/pii/S0141933120305093
N2  - In a recent analysis, eye disease is complicated to predict the affected area by using fundus imaging. The eye disease like lesion, this lesion affected retinal image is due to unevenness illumination, contrast is low, and blurring image is analyzed, when this image is analysis not found the affected region of lesion affected retinal image in the conventional system. The conventional system is having some loss like accuracy less, and it takes more time to analyze the image, so the proposed technique is developed to diagnose eye retinal lesion disease to improve the image quality and increase the accuracy. The proposed Random Implication Image Classifier Technique (RIICT) method is developed in this system to classify the image in various color spaces can achieve excellent accuracy in different image analyzing. Preprocessing of the retinal image uses a median filter for noise removal and enhancement; this preprocessing process is covert the color image to grayscale image. In segmentation, the Discrete Image Clustering Technique (DICT) algorithms that are selected and arranged to produce seeds are used to separate the image foreground and background of the input image to find the affected region based on the similarity easily. The proposed Random Implication Image Classifier Technique (RIICT) algorithm is used to classifying the lesion results of this system. This proposed system detects the disease like cotton wool spot, lesion quickly, and classifies the development region in various iris images to handle in this system. Finally, the RIICT system is given a better accuracy result is 96.7%.
ER  - 
TY  - JOUR
T1  - Point process analysis in brain networks of patients with diabetes
A1  - Li, Wei
A1  - Li, Yapeng
A1  - Hu, Chunhong
A1  - Chen, Xi
A1  - Dai, Hui
Y1  - 2014///
KW  -  Functional brain networks
KW  -  Point process
KW  - Resting-state fMRI
JF  - Neurocomputing
VL  - 145
SP  - 182
EP  - 189
DO  - https://doi.org/10.1016/j.neucom.2014.05.045
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214006948
N2  - Noise and individual differences arise from disturbances in the effective use of resting-state functional magnetic resonance image (fMRI) datasets. In this study, the point process is used to treat fMRI datasets of healthy controls and patients with diabetes; then, functional brain networks of subjects are established using two sets of BOLD signals. The results illustrate that differences between the healthy controls and the patients were more obvious in point process signals than nonpoint process signals. Our results also suggest that there is a higher recognition accuracy of the signals by preprocessing with the point process. These findings may suggest that the point process approach can reduce BOLD signals noise, providing a new method for functional magnetic resonance data preprocessing, and may provide a promising method for early data preprocessing in computer-aided disease diagnostics.
ER  - 
TY  - JOUR
T1  - Personalized predictive models for symptomatic COVID-19 patients using basic preconditions: Hospitalizations, mortality, and the need for an ICU or ventilator
A1  - Wollenstein-Betech, Salomón
A1  - Cassandras, Christos G
A1  - Paschalidis, Ioannis Ch.
Y1  - 2020///
KW  -  COVID-19
KW  -  Coronavirus
KW  -  Electronic health records (EHRs)
KW  -  Hospitalization
KW  -  ICU
KW  -  Mortality
KW  -  SARS-CoV-2
KW  -  Ventilator
KW  - Predictive models
JF  - International Journal of Medical Informatics
VL  - 142
SP  - 104258
EP  - 104258
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104258
UR  - https://www.sciencedirect.com/science/article/pii/S138650562030616X
N2  - Background
The rapid global spread of the SARS-CoV-2 virus has provoked a spike in demand for hospital care. Hospital systems across the world have been over-extended, including in Northern Italy, Ecuador, and New York City, and many other systems face similar challenges. As a result, decisions on how to best allocate very limited medical resources and design targeted policies for vulnerable subgroups have come to the forefront. Specifically, under consideration are decisions on who to test, who to admit into hospitals, who to treat in an Intensive Care Unit (ICU), and who to support with a ventilator. Given today’s ability to gather, share, analyze and process data, personalized predictive models based on demographics and information regarding prior conditions can be used to (1) help decision-makers allocate limited resources, when needed, (2) advise individuals how to better protect themselves given their risk profile, (3) differentiate social distancing guidelines based on risk, and (4) prioritize vaccinations once a vaccine becomes available.
Objective
To develop personalized models that predict the following events: (1) hospitalization, (2) mortality, (3) need for ICU, and (4) need for a ventilator. To predict hospitalization, it is assumed that one has access to a patient’s basic preconditions, which can be easily gathered without the need to be at a hospital and hence serve citizens and policy makers to assess individual risk during a pandemic. For the remaining models, different versions developed include different sets of a patient’s features, with some including information on how the disease is progressing (e.g., diagnosis of pneumonia).
Materials and Methods
National data from a publicly available repository, updated daily, containing information from approximately 91,000 patients in Mexico were used. The data for each patient include demographics, prior medical conditions, SARS-CoV-2 test results, hospitalization, mortality and whether a patient has developed pneumonia or not. Several classification methods were applied and compared, including robust versions of logistic regression, and support vector machines, as well as random forests and gradient boosted decision trees.
Results
Interpretable methods (logistic regression and support vector machines) perform just as well as more complex models in terms of accuracy and detection rates, with the additional benefit of elucidating variables on which the predictions are based. Classification accuracies reached 72 %, 79 %, 89 %, and 90 % for predicting hospitalization, mortality, need for ICU and need for a ventilator, respectively. The analysis reveals the most important preconditions for making the predictions. For the four models derived, these are: (1) for hospitalization:age, pregnancy, diabetes, gender, chronic renal insufficiency, and immunosuppression; (2) for mortality: age, immunosuppression, chronic renal insufficiency, obesity and diabetes; (3) for ICU need: development of pneumonia (if available), age, obesity, diabetes and hypertension; and (4) for ventilator need: ICU and pneumonia (if available), age, obesity, and hypertension.
ER  - 
TY  - JOUR
T1  - Short-term prediction of future continuous glucose monitoring readings in type 1 diabetes: Development and validation of a neural network regression model
A1  - Cichosz, Simon Lebech
A1  - Jensen, Morten Hasselstrøm
A1  - Hejlesen, Ole
Y1  - 2021///
KW  -  CGM
KW  -  Continuous glucose monitoring
KW  -  Glucose
KW  -  Prediction
KW  -  Type 1 diabetes
KW  - Neural network
JF  - International Journal of Medical Informatics
VL  - 151
SP  - 104472
EP  - 104472
DO  - https://doi.org/10.1016/j.ijmedinf.2021.104472
UR  - https://www.sciencedirect.com/science/article/pii/S1386505621000988
N2  - Background and objective
CGM systems are still subject to a time-delay, which especially during rapid changes causes clinically significant difference between the CGM and the actual BG level. This study had the aim of exploring the potential of developing and validating a model for prediction of future CGM measurements in order to overcome the time-delay.
Methods
An artificial neural network regression (NN) approach were used to predict CGM values with a lead-time of 15 min. The NN were trained and internally validated on 23 million minutes of CGM and externally validated on 2 million minutes of CGM. The validation included data from 278 type 1 diabetes patients using three different CGM sensors. The NN performance were compared with three alternative methods, linear extrapolation, spline extrapolation and last observation carried forward.
Results
The internal validation yielded a RMSE of 9.1 mg/dL, a MARD of 4.2 % and 99.9 % of predictions were in the A + B zone of the consensus error grid. The external validation yielded a RMSE of 5.9–11.3 mg/dL, a MARD of 3.2–5.4 % and 99.9–100 % of predictions were in the A + B zone of the consensus error grid. The NN performed better on all parameters compared to the two alternative methods.
Conclusions
We proposed and validated a NN glucose prediction model that is potential simple to use and implement. The model only needs input from a CGM system in order to facilitate glucose prediction with a lead time of 15 min. The approach yielded good results for both internal and external validation.
ER  - 
TY  - JOUR
T1  - A fully automatic nerve segmentation and morphometric parameter quantification system for early diagnosis of diabetic neuropathy in corneal images
A1  - Al-Fahdawi, Shumoos
A1  - Qahwaji, Rami
A1  - Al-Waisy, Alaa S
A1  - Ipson, Stanley
A1  - Malik, Rayaz A
A1  - Brahma, Arun
A1  - Chen, Xin
Y1  - 2016///
KW  -  Anisotropic diffusion filtering
KW  -  Automatic nerve segmentation
KW  -  Corneal confocal microscopy
KW  -  Corneal subbasal epithelium
KW  -  Diabetic peripheral neuropathy
KW  - Diabetes
JF  - Computer Methods and Programs in Biomedicine
VL  - 135
SP  - 151
EP  - 166
DO  - https://doi.org/10.1016/j.cmpb.2016.07.032
UR  - https://www.sciencedirect.com/science/article/pii/S0169260716301754
N2  - Diabetic Peripheral Neuropathy (DPN) is one of the most common types of diabetes that can affect the cornea. An accurate analysis of the nerve structures can assist the early diagnosis of this disease. This paper proposes a robust, fast and fully automatic nerve segmentation and morphometric parameter quantification system for corneal confocal microscope images. The segmentation part consists of three main steps. First, a preprocessing step is applied to enhance the visibility of the nerves and remove noise using anisotropic diffusion filtering, specifically a Coherence filter followed by Gaussian filtering. Second, morphological operations are applied to remove unwanted objects in the input image such as epithelial cells and small nerve segments. Finally, an edge detection step is applied to detect all the nerves in the input image. In this step, an efficient algorithm for connecting discontinuous nerves is proposed. In the morphometric parameters quantification part, a number of features are extracted, including thickness, tortuosity and length of nerve, which may be used for the early diagnosis of diabetic polyneuropathy and when planning Laser-Assisted in situ Keratomileusis (LASIK) or Photorefractive keratectomy (PRK). The performance of the proposed segmentation system is evaluated against manually traced ground-truth images based on a database consisting of 498 corneal sub-basal nerve images (238 are normal and 260 are abnormal). In addition, the robustness and efficiency of the proposed system in extracting morphometric features with clinical utility was evaluated in 919 images taken from healthy subjects and diabetic patients with and without neuropathy. We demonstrate rapid (13 seconds/image), robust and effective automated corneal nerve quantification. The proposed system will be deployed as a useful clinical tool to support the expertise of ophthalmologists and save the clinician time in a busy clinical setting.
ER  - 
TY  - JOUR
T1  - Comparison of Classifiers for the Risk of Diabetes Prediction
A1  - Nai-arun, Nongyao
A1  - Moungmai, Rungruttikarn
Y1  - 2015///
KW  -  artificial neural networks
KW  -  bagging
KW  -  boosting
KW  -  decision tree
KW  -  logistic regression
KW  -  naive bayes
KW  -  random forest
KW  - diabetes
JF  - Procedia Computer Science
VL  - 69
SP  - 132
EP  - 142
DO  - https://doi.org/10.1016/j.procs.2015.10.014
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915031786
N1  - The 7th International Conference on Advances in Information Technology
N2  - This paper applied a use of algorithms to classify the risk of diabetes mellitus. Four well known classification models that are Decision Tree, Artificial Neural Networks, Logistic Regression and Naive Bayes were first examined. Then, Bagging and Boosting techniques were investigated for improving the robustness of such models. Additionally, Random Forest was not ignored to evaluate in the study. Findings suggest that the best performance of disease risk classification is Random Forest algorithm. Therefore, its model was used to create a web application for predicting a class of the diabetes risk.
ER  - 
TY  - JOUR
T1  - Evaluating warfarin dosing models on multiple datasets with a novel software framework and evolutionary optimisation
A1  - Truda, Gianluca
A1  - Marais, Patrick
Y1  - 2021///
KW  -  Anticoagulant
KW  -  Genetic programming
KW  -  Machine learning
KW  -  Pharmacogenetics
KW  -  Python
KW  -  Software
KW  -  Supervised learning
KW  - Warfarin
JF  - Journal of Biomedical Informatics
VL  - 113
SP  - 103634
EP  - 103634
DO  - https://doi.org/10.1016/j.jbi.2020.103634
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420302628
N2  - Warfarin is an effective preventative treatment for arterial and venous thromboembolism, but requires individualised dosing due to its narrow therapeutic range and high individual variation. Many machine learning techniques have been demonstrated in this domain. This study evaluated the accuracy of the most promising algorithms on the International Warfarin Pharmacogenetics Consortium dataset and a novel clinical dataset of South African patients. Support vectors and linear regression were amongst the top performers in both datasets and performed comparably to recent stacked ensemble approaches, whilst neural networks were one of the worst performers in both datasets. We also introduced genetic programming to automatically optimise model architectures and hyperparameters without human guidance. Remarkably, the generated models were found to match the performance of the best models hand-crafted by human experts. Finally, we present a novel software framework (Warfit-learn) for warfarin dosing research. It leverages the most successful techniques in preprocessing, imputation, and parallel evaluation, with the goal of accelerating research and making results in this domain more reproducible.
ER  - 
TY  - JOUR
T1  - A tongue features fusion approach to predicting prediabetes and diabetes with machine learning
A1  - Li, Jun
A1  - Yuan, Pei
A1  - Hu, Xiaojuan
A1  - Huang, Jingbin
A1  - Cui, Longtao
A1  - Cui, Ji
A1  - Ma, Xuxiang
A1  - Jiang, Tao
A1  - Yao, Xinghua
A1  - Li, Jiacai
A1  - Shi, Yulin
A1  - Bi, Zijuan
A1  - Wang, Yu
A1  - Fu, Hongyuan
A1  - Wang, Jue
A1  - Lin, Yenting
A1  - Pai, ChingHsuan
A1  - Guo, Xiaojing
A1  - Zhou, Changle
A1  - Tu, Liping
A1  - Xu, Jiatuo
Y1  - 2021///
KW  -  Deep learning
KW  -  Diabetics
KW  -  Features fusion
KW  -  Machine learning
KW  -  Noninvasive
KW  -  Prediabetics
KW  -  Risk prediction model
KW  -  Tongue diagnosis
KW  - Traditional Chinese medicine
JF  - Journal of Biomedical Informatics
VL  - 115
SP  - 103693
EP  - 103693
DO  - https://doi.org/10.1016/j.jbi.2021.103693
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000228
N2  - Background
Diabetics has become a serious public health burden in China. Multiple complications appear with the progression of diabetics pose a serious threat to the quality of human life and health. We can prevent the progression of prediabetics to diabetics and delay the progression to diabetics by early identification of diabetics and prediabetics and timely intervention, which have positive significance for improving public health.
Objective
Using machine learning techniques, we establish the noninvasive diabetics risk prediction model based on tongue features fusion and predict the risk of prediabetics and diabetics.
Methods
Applying the type TFDA-1 Tongue Diagnosis Instrument, we collect tongue images, extract tongue features including color and texture features using TDAS, and extract the advanced tongue features with ResNet-50, achieve the fusion of the two features with GA_XGBT, finally establish the noninvasive diabetics risk prediction model and evaluate the performance of testing effectiveness.
Results
Cross-validation suggests the best performance of GA_XGBT model with fusion features, whose average CA is 0.821, the average AUROC is 0.924, the average AUPRC is 0.856, the average Precision is 0.834, the average Recall is 0.822, the average F1-score is 0.813. Test set suggests the best testing performance of GA_XGBT model, whose average CA is 0.81, the average AUROC is 0.918, the average AUPRC is 0.839, the average Precision is 0.821, the average Recall is 0.81, the average F1-score is 0.796. When we test prediabetics with GA_XGBT model, we find that the AUROC is 0.914, the Precision is 0.69, the Recall is 0.952, the F1-score is 0.8. When we test diabetics with GA_XGBT model, we find that the AUROC is 0.984, the Precision is 0.929, the Recall is 0.951, the F1-score is 0.94.
Conclusions
Based on tongue features, the study uses classical machine learning algorithm and deep learning algorithm to maximum the respective advantages. We combine the prior knowledge and potential features together, establish the noninvasive diabetics risk prediction model with features fusion algorithm, and detect prediabetics and diabetics noninvasively. Our study presents a feasible method for establishing the association between diabetics and the tongue image information and prove that tongue image information is a potential marker which facilitates effective early diagnosis of prediabetics and diabetics.
ER  - 
TY  - JOUR
T1  - An efficient Map Reduce-Based Hybrid NBC-TFIDF algorithm to mine the public sentiment on diabetes mellitus – A big data approach
A1  - Ramsingh, J
A1  - Bhuvaneswari, V
Y1  - 2021///
KW  -  Diabetes
KW  -  Map reduce
KW  -  Opinion
KW  -  Social media
KW  - Big-Data
JF  - Journal of King Saud University - Computer and Information Sciences
VL  - 33
IS  - 8
SP  - 1018
EP  - 1029
DO  - https://doi.org/10.1016/j.jksuci.2018.06.011
UR  - https://www.sciencedirect.com/science/article/pii/S1319157818302593
N2  - The increase in the usage of internet and social media has enabled people exchange views, opinions and thoughts as never before. This exchange of data has paved the way for sentiment analysis. The basic task of sentiment analysis is to classify the data into positive, negative and neutral. In this paper an effective MapReduce-Based Hybrid NBC-TFIDF (Naive Bayes Classifier -Term Frequency Inverse Document Frequency) algorithm is proposed to mine people sentiment. A Map Reduce-Based Hybrid NBC is employed to classify the data based on the polarity score of each sentence in social media data. The polarity score is calculated using the emotion corpus and the Diabetic corpus is created using food Glycemic Index and physical activity index. This study analyses the correlation of food habits, physical activity and diabetic risk factors among Indian population using social network data. Around two million data has been identified for the study and the study is restricted to India. The experimental result shows that MapReduce-Based Hybrid NBC–TFIDF performs efficiently in multimode cluster. The results reveal that no individual factor is associated with diabetic risk and also a group of common factors contribute to diabetes mellitus. It is found that 60% of the social media data had positive polarity about the food items that are high in Glycemic Index which is the main root cause for type – 2 Diabetes. This Big-Data analysis reveals that the young generations of India are unaware of risk factors of Diabetes mellitus.
ER  - 
TY  - JOUR
T1  - Extracting causal relations from the literature with word vector mapping
A1  - An, Ning
A1  - Xiao, Yongbo
A1  - Yuan, Jing
A1  - Yang, Jiaoyun
A1  - Alterovitz, Gil
Y1  - 2019///
KW  -  Causal extraction
KW  -  Causal graph
KW  -  Literature analysis
KW  -  Word vector
KW  - Causality
JF  - Computers in Biology and Medicine
VL  - 115
SP  - 103524
EP  - 103524
DO  - https://doi.org/10.1016/j.compbiomed.2019.103524
UR  - https://www.sciencedirect.com/science/article/pii/S001048251930383X
N2  - Causal graphs play an essential role in the determination of causalities and have been applied in many domains including biology and medicine. Traditional causal graph construction methods are usually data-driven and may not deliver the desired accuracy of a graph. Considering the vast number of publications with causality knowledge, extracting causal relations from the literature to help to establish causal graphs becomes possible. Current supervised-learning-based causality extraction methods requires sufficient labeled data to train a model, and rule-based causality extraction methods are limited by the predefined patterns. This paper proposes a causality extraction framework by integrating rule-based methods and unsupervised learning models to overcome these limitations. The proposed method consists of three modules, including data preprocessing, syntactic pattern matching, and causality determination. In data preprocessing, abstracts are crawled based on attribute names before sentences are extracted and simplified. In syntactic pattern matching, these simplified sentences are parsed to obtain the part-of-speech tags, and triples are achieved based on these tags by matching the two designed syntactic patterns. In causality determination, four verb seed sets are initialized, and word vectors are constructed for the verbs in both the seed sets and the triples by applying an unsupervised machine learning model. Causal relations are identified by comparing the similarity between the verbs in each triple and that in each seed set to overcome the limitation of the seed sets. Causality extraction results on the attributes from the risk factors for Alzheimer’s disease show that our method outperforms Bui’s method and Alashri’s method in terms of precision, recall, specificity, accuracy and F-score, with increases in the F-score of 8.29% and 5.37%, respectively.
ER  - 
TY  - JOUR
T1  - Preprocessing RR interval time series for heart rate variability analysis and estimates of standard deviation of RR intervals
A1  - Thuraisingham, R A
Y1  - 2006///
KW  -  Artifact removal
KW  -  Standard deviation of RR intervals
KW  - Detrending
JF  - Computer Methods and Programs in Biomedicine
VL  - 83
IS  - 1
SP  - 78
EP  - 82
DO  - https://doi.org/10.1016/j.cmpb.2006.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169260706001040
N2  - Heart rate variability is concerned with the analysis of the fluctuations in the interval between heart beats known as RR intervals. The long time RR time series obtained suffer from non-stationarity and the presence of ectopic beats, which prevents extraction of useful statistical information. The paper describes a wavelet-based technique for trend removal and a nonlinear filter to remove ectopic beats. This attempts to correct the limitations observed in a recent advanced heart rate toolkit [J. Niskanen, M.P. Tarvainen, P.O. Ranta-aho P.A. Karjalainen, Software for advanced HRV analysis, Comput. Meth. Prog. Biomed.,76 (2004) 73–81] when preprocessing. The results are encouraging. The preprocessed data are then used to obtain the standard deviation of RR interval time series (SDRR) of 15 healthy patients and 15 patients suffering from congestive heart failure. The results demonstrate the importance of preprocessing. The analysis show that the SDRR values of congestive heart failure patients are depressed compared to the healthy group.
ER  - 
TY  - JOUR
T1  - Machine Learning-Based Predictive Modeling of Complications of Chronic Diabetes.
A1  - Derevitskii, Ilia V
A1  - Kovalchuk, Sergey V
Y1  - 2020///
KW  -  Machine Learning
KW  -  Type Two Diabetes
KW  - Diabetes Complications
JF  - Procedia Computer Science
VL  - 178
SP  - 274
EP  - 283
DO  - https://doi.org/10.1016/j.procs.2020.11.029
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920324030
N1  - 9th International Young Scientists Conference in Computational Science, YSC2020, 05-12 September 2020
N2  - Chronic diabetes is one of the most common chronic diseases in the world. For health care, this type of diabetes is one of the highest priority problems. This disease is related to many concomitant diseases, that lead to early disability and increased cardiovascular risk. T2DM patients have an increased risk of various complications. For such patients, medical specialists needed practical tools for calculating the future risks of complications. In this study, we propose a method for calculating the set of risks of the most common T2DM complications. These complications include nephropathy, neuropathy, chronic heart failure, and atrial fibrillation. This method includes a set of models for calculating the risk of a specific complication during a six-month follow-up. These models are based on Machine Learning methods. The method has a high quality of the prediction for different complications. Therefore, medical specialists can use this model as a practical tool. This tool can be used as part of the Support and Decision System for working with T2DM patients.
ER  - 
TY  - JOUR
T1  - Application of irregular and unbalanced data to predict diabetic nephropathy using visualization and feature selection methods
A1  - Cho, Baek Hwan
A1  - Yu, Hwanjo
A1  - Kim, Kwang-Won
A1  - Kim, Tae Hyun
A1  - Kim, In Young
A1  - Kim, Sun I
Y1  - 2008///
KW  -  Diabetic nephropathy
KW  -  Feature selection
KW  -  Risk factor analysis
KW  -  Support vector machines
KW  -  Visualization
KW  - Decision support systems
JF  - Artificial Intelligence in Medicine
VL  - 42
IS  - 1
SP  - 37
EP  - 53
DO  - https://doi.org/10.1016/j.artmed.2007.09.005
UR  - https://www.sciencedirect.com/science/article/pii/S0933365707001157
N2  - Summary
Objective
Diabetic nephropathy is damage to the kidney caused by diabetes mellitus. It is a common complication and a leading cause of death in people with diabetes. However, the decline in kidney function varies considerably between patients and the determinants of diabetic nephropathy have not been clearly identified. Therefore, it is very difficult to predict the onset of diabetic nephropathy accurately with simple statistical approaches such as t-test or χ2-test. To accurately predict the onset of diabetic nephropathy, we applied various machine learning techniques to irregular and unbalanced diabetes dataset, such as support vector machine (SVM) classification and feature selection methods. Visualization of the risk factors was another important objective to give physicians intuitive information on each patient's clinical pattern.
Methods and materials
We collected medical data from 292 patients with diabetes and performed preprocessing to extract 184 features from the irregular data. To predict the onset of diabetic nephropathy, we compared several classification methods such as logistic regression, SVM, and SVM with a cost sensitive learning method. We also applied several feature selection methods to remove redundant features and improve the classification performance. For risk factor analysis with SVM classifiers, we have developed a new visualization system which uses a nomogram approach.
Results
Linear SVM classifiers combined with wrapper or embedded feature selection methods showed the best results. Among the 184 features, the classifiers selected the same 39 features and gave 0.969 of the area under the curve by receiver operating characteristics analysis. The visualization tool was able to present the effect of each feature on the decision via graphical output.
Conclusions
Our proposed method can predict the onset of diabetic nephropathy about 2–3 months before the actual diagnosis with high prediction performance from an irregular and unbalanced dataset, which statistical methods such as t-test and logistic regression could not achieve. Additionally, the visualization system provides physicians with intuitive information for risk factor analysis. Therefore, physicians can benefit from the automatic early warning of each patient and visualize risk factors, which facilitate planning of effective and proper treatment strategies.
ER  - 
TY  - JOUR
T1  - Compositional Data Analysis of Glucose Profiles of Type 1 Diabetes Patients ⁎⁎This project has been partially supported by the Spanish Government MINECO through Grant DPI-2016-78831-C2-2-R and the National Council of Technological and Scientific Development, CNPq Brazil through Grants 202050/2015-7 and 207688/2014-1
A1  - Biagi, Lyvia
A1  - Bertachi, Arthur
A1  - Martín-Fernández, Josep Antoni
A1  - Vehí, Josep
Y1  - 2019///
KW  -  Biomedical Systems
KW  -  Continuous Glucose Monitoring
KW  -  Decision Support System
KW  -  Type 1 Diabetes
KW  - Compositional Data Analysis
JF  - IFAC-PapersOnLine
VL  - 52
IS  - 1
SP  - 1006
EP  - 1011
DO  - https://doi.org/10.1016/j.ifacol.2019.06.194
UR  - https://www.sciencedirect.com/science/article/pii/S2405896319302824
N1  - 12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019
N2  - Time spent in different glucose ranges indicate the occurrence of adverse events and measure the quality of glucose control in type one diabetes (T1D) patients. This work proposes a Compositional Data (CoDa) approach applied to glucose profiles obtained from six T1D patients using continuous glucose monitor (CGM). Glucose profiles limited to 6-h duration were analyzed at four different times of the day These glucose profiles were distributed into time spent in five glucose ranges, which determine the composition. The log-ratio coordinates of the compositions were categorized through a clustering algorithm, which later made possible the obtainment of a linear model that should be used to predict the category of a 6-h period in different times of day. Leave-one-out cross-validation was performed, achieving an average above 90% of correct classification. A probabilistic model of transition between the category of the past 6-h of glucose to the category of the future 6-h period was obtained. Results show that the CoDa approach not only works as new analysis tool and is suitable for the categorization of glucose profiles, but also is a complementary tool for the prediction of different categories of glucose control. This prediction could assist patients to take correction measures in advance to adverse situations.
ER  - 
TY  - JOUR
T1  - COVID-19 detection in X-ray images using convolutional neural networks
A1  - Arias-Garzón, Daniel
A1  - Alzate-Grisales, Jesús Alejandro
A1  - Orozco-Arias, Simon
A1  - Arteaga-Arteaga, Harold Brayan
A1  - Bravo-Ortiz, Mario Alejandro
A1  - Mora-Rubio, Alejandro
A1  - Saborit-Torres, Jose Manuel
A1  - Serrano, Joaquim Ángel Montell
A1  - de la Iglesia Vayá, Maria
A1  - Cardona-Morales, Oscar
A1  - Tabares-Soto, Reinel
Y1  - 2021///
KW  -  Deep learning
KW  -  Segmentation
KW  -  Transfer learning
KW  -  X-ray
KW  - COVID-19
JF  - Machine Learning with Applications
VL  - 6
SP  - 100138
EP  - 100138
DO  - https://doi.org/10.1016/j.mlwa.2021.100138
UR  - https://www.sciencedirect.com/science/article/pii/S2666827021000694
N2  - COVID-19 global pandemic affects health care and lifestyle worldwide, and its early detection is critical to control cases’ spreading and mortality. The actual leader diagnosis test is the Reverse transcription Polymerase chain reaction (RT-PCR), result times and cost of these tests are high, so other fast and accessible diagnostic tools are needed. Inspired by recent research that correlates the presence of COVID-19 to findings in Chest X-ray images, this papers’ approach uses existing deep learning models (VGG19 and U-Net) to process these images and classify them as positive or negative for COVID-19. The proposed system involves a preprocessing stage with lung segmentation, removing the surroundings which does not offer relevant information for the task and may produce biased results; after this initial stage comes the classification model trained under the transfer learning scheme; and finally, results analysis and interpretation via heat maps visualization. The best models achieved a detection accuracy of COVID-19 around 97%.
ER  - 
TY  - JOUR
T1  - Towards adequate prediction of prediabetes using spatiotemporal ECG and EEG feature analysis and weight-based multi-model approach
A1  - Tobore, Igbe
A1  - Kandwal, Abhishek
A1  - Li, Jingzhen
A1  - Yan, Yan
A1  - Omisore, Olatunji Mumini
A1  - Enitan, Efetobore
A1  - Sinan, Li
A1  - Yuhang, Liu
A1  - Wang, Lei
A1  - Nie, Zedong
Y1  - 2020///
KW  -  Artificial intelligence
KW  -  Electrocardiogram
KW  -  Electroencephalogram
KW  -  Multi-model
KW  -  Multi-sensor
KW  - Prediabetes
JF  - Knowledge-Based Systems
VL  - 209
SP  - 106464
EP  - 106464
DO  - https://doi.org/10.1016/j.knosys.2020.106464
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120305931
N2  - Prediabetes is a metabolic condition before the occurrence of diabetes. The diagnosis of prediabetes can slow down or eliminate the growing cases of diabetes around the world. This paper presents a novel approach to identifying some vital physiological features for prediabetes prediction to stem the growing trend of type-2 diabetes. A standard OGTT experiment was conducted using BIOPAC 150MP, g-SAHARA and Mindray physiological device to capture continuous electrocardiogram (ECG) rhythm and electroencephalogram (EEG) of 40 human subjects while measuring blood glucose value at a regular interval. Features from the captured physiological signals were analyzed using an integrated space–time principal component analysis, independent component analysis, least absolute shrinkage and selector operator, and piecewise aggregate approximation techniques. The results from feature analysis show that certain features, namely HRV, QT, and ST from ECG; alpha, beta, and theta from the right parental hemisphere, along with alpha and delta from the left occipital hemisphere from EEG show significant correlation with change in the blood glucose. Furthermore, a weight-based multi-model was proposed by combining five (5) classification methods. The selected ECG and EEG features were applied for training the proposed multi-model classification, which is used to predict prediabetes. The evaluation of the multi-model performance produced accuracy, precision, and F1-measure of 92.0%, 88.8%, and 82.7% respectively, which is higher than the individual methods. The experimental results show that the coupling of multi-model electrophysiological data acquired with wearable multi-sensor devices can be utilized to diagnose diabetes early.
ER  - 
TY  - JOUR
T1  - An intelligent healthcare monitoring framework using wearable sensors and social networking data
A1  - Ali, Farman
A1  - El-Sappagh, Shaker
A1  - Islam, S M Riazul
A1  - Ali, Amjad
A1  - Attique, Muhammad
A1  - Imran, Muhammad
A1  - Kwak, Kyung-Sup
Y1  - 2021///
KW  -  Big data analysis
KW  -  Healthcare monitoring system
KW  -  Semantic knowledge
KW  -  Social network analysis
KW  -  Wearable sensors
KW  - Machine learning
JF  - Future Generation Computer Systems
VL  - 114
SP  - 23
EP  - 43
DO  - https://doi.org/10.1016/j.future.2020.07.047
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X1931605X
N2  - Wearable sensors and social networking platforms play a key role in providing a new method to collect patient data for efficient healthcare monitoring. However, continuous patient monitoring using wearable sensors generates a large amount of healthcare data. In addition, the user-generated healthcare data on social networking sites come in large volumes and are unstructured. The existing healthcare monitoring systems are not efficient at extracting valuable information from sensors and social networking data, and they have difficulty analyzing it effectively. On top of that, the traditional machine learning approaches are not enough to process healthcare big data for abnormality prediction. Therefore, a novel healthcare monitoring framework based on the cloud environment and a big data analytics engine is proposed to precisely store and analyze healthcare data, and to improve the classification accuracy. The proposed big data analytics engine is based on data mining techniques, ontologies, and bidirectional long short-term memory (Bi-LSTM). Data mining techniques efficiently preprocess the healthcare data and reduce the dimensionality of the data. The proposed ontologies provide semantic knowledge about entities and aspects, and their relations in the domains of diabetes and blood pressure (BP). Bi-LSTM correctly classifies the healthcare data to predict drug side effects and abnormal conditions in patients. Also, the proposed system classifies the patients’ health condition using their healthcare data related to diabetes, BP, mental health, and drug reviews. This framework is developed employing the Protégé Web Ontology Language tool with Java. The results show that the proposed model precisely handles heterogeneous data and improves the accuracy of health condition classification and drug side effect predictions.
ER  - 
TY  - JOUR
T1  - A deep learning approach using effective preprocessing techniques to detect COVID-19 from chest CT-scan and X-ray images
A1  - Ahamed, Khabir Uddin
A1  - Islam, Manowarul
A1  - Uddin, Ashraf
A1  - Akhter, Arnisha
A1  - Paul, Bikash Kumar
A1  - Yousuf, Mohammad Abu
A1  - Uddin, Shahadat
A1  - Quinn, Julian M W
A1  - Moni, Mohammad Ali
Y1  - 2021///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Pneumonia
KW  -  Rediology
KW  -  Respiratory diseases
KW  - Coronavirus
JF  - Computers in Biology and Medicine
VL  - 139
SP  - 105014
EP  - 105014
DO  - https://doi.org/10.1016/j.compbiomed.2021.105014
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521008088
N2  - Coronavirus disease-19 (COVID-19) is a severe respiratory viral disease first reported in late 2019 that has spread worldwide. Although some wealthy countries have made significant progress in detecting and containing this disease, most underdeveloped countries are still struggling to identify COVID-19 cases in large populations. With the rising number of COVID-19 cases, there are often insufficient COVID-19 diagnostic kits and related resources in such countries. However, other basic diagnostic resources often do exist, which motivated us to develop Deep Learning models to assist clinicians and radiologists to provide prompt diagnostic support to the patients. In this study, we have developed a deep learning-based COVID-19 case detection model trained with a dataset consisting of chest CT scans and X-ray images. A modified ResNet50V2 architecture was employed as deep learning architecture in the proposed model. The dataset utilized to train the model was collected from various publicly available sources and included four class labels: confirmed COVID-19, normal controls and confirmed viral and bacterial pneumonia cases. The aggregated dataset was preprocessed through a sharpening filter before feeding the dataset into the proposed model. This model attained an accuracy of 96.452% for four-class cases (COVID-19/Normal/Bacterial pneumonia/Viral pneumonia), 97.242% for three-class cases (COVID-19/Normal/Bacterial pneumonia) and 98.954% for two-class cases (COVID-19/Viral pneumonia) using chest X-ray images. The model acquired a comprehensive accuracy of 99.012% for three-class cases (COVID-19/Normal/Community-acquired pneumonia) and 99.99% for two-class cases (Normal/COVID-19) using CT-scan images of the chest. This high accuracy presents a new and potentially important resource to enable radiologists to identify and rapidly diagnose COVID-19 cases with only basic but widely available equipment.
ER  - 
TY  - JOUR
T1  - Improved logistic regression model for diabetes prediction by integrating PCA and K-means techniques
A1  - Zhu, Changsheng
A1  - Idemudia, Christian Uwa
A1  - Feng, Wenfang
Y1  - 2019///
KW  -  Data mining
KW  -  Diabetes
KW  -  K-means
KW  -  Logistic regression
KW  - PCA
JF  - Informatics in Medicine Unlocked
VL  - 17
SP  - 100179
EP  - 100179
DO  - https://doi.org/10.1016/j.imu.2019.100179
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819300139
N2  - Diabetes causes a large number of deaths each year and a large number of people living with the disease do not realize their health condition early enough. In this study, we propose a data mining based model for early diagnosis and prediction of diabetes using the Pima Indians Diabetes dataset. Although K-means is simple and can be used for a wide variety of data types, it is quite sensitive to initial positions of cluster centers which determine the final cluster result, which either provides a sufficient and efficiently clustered dataset for the logistic regression model, or gives a lesser amount of data as a result of incorrect clustering of the original dataset, thereby limiting the performance of the logistic regression model. Our main goal was to determine ways of improving the k-means clustering and logistic regression accuracy result. Our model comprises of PCA (principal component analysis), k-means and logistic regression algorithm. Experimental results show that PCA enhanced the k-means clustering algorithm and logistic regression classifier accuracy versus the result of other published studies, with a k-means output of 25 more correctly classified data, and a logistic regression accuracy of 1.98% higher. As such, the model is shown to be useful for automatically predicting diabetes using patient electronic health records data. A further experiment with a new dataset showed the applicability of our model for the predication of diabetes.
ER  - 
TY  - JOUR
T1  - Performance comparison of publicly available retinal blood vessel segmentation methods
A1  - Vostatek, Pavel
A1  - Claridge, Ela
A1  - Uusitalo, Hannu
A1  - Hauta-Kasari, Markku
A1  - Fält, Pauli
A1  - Lensu, Lasse
Y1  - 2017///
KW  -  Retinal imaging
KW  -  Vessel segmentation
KW  - Fundus
JF  - Computerized Medical Imaging and Graphics
VL  - 55
SP  - 2
EP  - 12
DO  - https://doi.org/10.1016/j.compmedimag.2016.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S0895611116300702
N1  - Special Issue on Ophthalmic Medical Image Analysis
N2  - Retinal blood vessel structure is an important indicator of many retinal and systemic diseases, which has motivated the development of various image segmentation methods for the blood vessels. In this study, two supervised and three unsupervised segmentation methods with a publicly available implementation are reviewed and quantitatively compared with each other on five public databases with ground truth segmentation of the vessels. Each method is tested under consistent conditions with two types of preprocessing, and the parameters of the methods are optimized for each database. Additionally, possibility to predict the parameters of the methods by the linear regression model is tested for each database. Resolution of the input images and amount of the vessel pixels in the ground truth are used as predictors. The results show the positive influence of preprocessing on the performance of the unsupervised methods. The methods show similar performance for segmentation accuracy, with the best performance achieved by the method by Azzopardi et al. (Acc 94.0) on ARIADB, the method by Soares et al. (Acc 94.6, 94.7) on CHASEDB1 and DRIVE, and the method by Nguyen et al. (Acc 95.8, 95.5) on HRF and STARE. The method by Soares et al. performed better with regard to the area under the ROC curve. Qualitative differences between the methods are discussed. Finally, it was possible to predict the parameter settings that give performance close to the optimized performance of each method.
ER  - 
TY  - JOUR
T1  - Pattern Recognition of White Matter Lesions Associated With Diabetes Mellitus Type 2
A1  - Luna, Jocellyn
A1  - Peláez, Enrique
A1  - Loayza, Francis R
A1  - Alvarado, Ronald
A1  - Pastor, Maria A
Y1  - 2021///
KW  -  WMH brain lesions
KW  -  classification
KW  -  machine learning
KW  -  segmentation
KW  - Diabetes
JF  - IFAC-PapersOnLine
VL  - 54
IS  - 15
SP  - 370
EP  - 375
DO  - https://doi.org/10.1016/j.ifacol.2021.10.284
UR  - https://www.sciencedirect.com/science/article/pii/S240589632101689X
N1  - 11th IFAC Symposium on Biological and Medical Systems BMS 2021
N2  - The White Matter Hyperintensities (WMHs) are usually associated with diabetes which is relevant in medical research to understand the long-term affection of diabetes. However, there is not enough evidence to distinguish whether the WMHs observed in diabetes subjects are structurally different from those observed in healthy subjects. This work aims to recognize the patterns associated with diabetes using the WMHs features of diabetic patients. We used Machine Learning models, such as Logistic Regression (LR), Support Vector Machines (SVM), Random Forest (RF), and a Multilayer perceptron (MLP) Neural Network to classify the features extracted from the WMH segments from T1 and FLAIR sequences of Magnetic Resonance Images (MRI) obtained from diabetic patients. Four classification models were evaluated and compared in their performance and Logistic Regression showed the best results, with an accuracy of 88%, as belonging or not to a diabetic class. Our results showed that diabetic patients have WMH patterns that are structurally different from controls, which may be useful for patients follow up.
ER  - 
TY  - JOUR
T1  - Machine learning based diabetes prediction and development of smart web application
A1  - Ahmed, N
A1  - Ahammed, R
A1  - Islam, M M
A1  - Uddin, M A
A1  - Akhter, A
A1  - Talukder, M.A.-A.
A1  - Paul, B K
Y1  - 2021///
PB  - KeAi Communications Co.
JF  - International Journal of Cognitive Computing in Engineering
VL  - 2
SP  - 229
EP  - 241
DO  - 10.1016/j.ijcce.2021.12.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125878637&doi=10.1016%2Fj.ijcce.2021.12.001&partnerID=40&md5=4b3e471adb50755aeee15988e3c8aeb2
N1  - cited By 6
N2  - Diabetes is a very common disease affecting individuals worldwide. Diabetes increases the risk of long-term complications including heart disease, and kidney failure among others. People might live longer and lead healthier lives if this disease is detected early. Different supervised machine learning models trained with appropriate datasets can aid in diagnosing the diabetes at the primary stage. The goal of this work is to find effective machine-learning-based classifier models for detecting diabetes in individuals utilizing clinical data. The machine learning algorithms to be trained with several datasets in this article include Decision tree (DT), Naive Bayes (NB), k-nearest neighbor (KNN), Random Forest (RF), Gradient Boosting (GB), Logistic Regression (LR) and Support Vector Machine (SVM). We have applied efficient pre-processing techniques including label-encoding and normalization that improve the accuracy of the models. Further, using various feature selection approaches, we have identified and prioritized a number of risk factors. Extensive experiments have been conducted to analyze the performance of the model using two different datasets. Our model is compared with some recent study and the results show that the proposed model can provide better accuracy of 2.71% to 13.13% depending on the dataset and the adopted ML algorithm. Finally, a machine learning algorithm showing the highest accuracy is selected for further development. We integrate this model in a web application using python flask web development framework. The results of this study suggest that an appropriate preprocessing pipeline on clinical data and applying ML-based classification may predict diabetes accurately and efficiently. © 2021
ER  - 
TY  - JOUR
T1  - Development and use of a clinical decision support system for the diagnosis of social anxiety disorder
A1  - Fathi, Sina
A1  - Ahmadi, Maryam
A1  - Birashk, Behrouz
A1  - Dehnad, Afsaneh
Y1  - 2020///
KW  -  Adaptive neuro-fuzzy inference system
KW  -  Artificial intelligence
KW  -  Clinical decision support system
KW  -  Neuro-fuzzy
KW  -  Social phobia
KW  - Social anxiety disorder
JF  - Computer Methods and Programs in Biomedicine
VL  - 190
SP  - 105354
EP  - 105354
DO  - https://doi.org/10.1016/j.cmpb.2020.105354
UR  - https://www.sciencedirect.com/science/article/pii/S016926071931507X
N2  - Background
Mental disorders, according to the definition of World Health Organization, consist of a wide range of signs, which are generally specified by a combination of unusual thoughts, feelings, behavior, and relationships with others. Social anxiety disorder (SAD) is one of the most prevalent mental disorders, described as permanent and severe fear or feeling of embarrassment in social situations. Considering the imprecise nature of SAD symptoms, the main objective of this study was to generate an intelligent decision support system for SAD diagnosis, using Adaptive neuro-fuzzy inference system (ANFIS) technique and to conduct an evaluation method, using sensitivity, specificity and accuracy metrics.
Method
In this study, a real-world dataset with the sample size of 214 was selected and used to generate the model. The method comprised a multi-stage procedure named preprocessing, classification, and evaluation. The preprocessing stage, itself, consists of three steps called normalization, feature selection, and anomaly detection, using the Self-Organizing Map (SOM) clustering method. The ANFIS technique with 5-fold cross-validation was used for the classification of social anxiety disorder.
Results and conclusion
The preprocessed dataset with seven input features were used to train the ANFIS model. The hybrid optimization learning algorithm and 41 epochs were used as optimal learning parameters. The accuracy, sensitivity, and specificity metrics were reported 98.67%, 97.14%, and 100%, respectively. The results revealed that the proposed model was quite appropriate for SAD diagnosis and in line with findings of other studies. Further research study addressing the design of a decision support system for diagnosing the severity of SAD is recommended.
ER  - 
TY  - JOUR
T1  - An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm
A1  - Ramsingh, J
A1  - Bhuvaneswari, V
Y1  - 2021///
KW  -  Diabetes mellitus
KW  -  Hadoop
KW  -  MDBSCAN
KW  -  MapReduce
KW  - Fuzzy classifier
JF  - Applied Soft Computing
VL  - 108
SP  - 107423
EP  - 107423
DO  - https://doi.org/10.1016/j.asoc.2021.107423
UR  - https://www.sciencedirect.com/science/article/pii/S156849462100346X
N2  - In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.
ER  - 
TY  - JOUR
T1  - Activity detection and classification from wristband accelerometer data collected on people with type 1 diabetes in free-living conditions
A1  - Cescon, Marzia
A1  - Choudhary, Divya
A1  - Pinsker, Jordan E
A1  - Dadlani, Vikash
A1  - Church, Mei Mei
A1  - Kudva, Yogish C
A1  - Doyle III, Francis J
A1  - Dassau, Eyal
Y1  - 2021///
KW  -  Artificial pancreas
KW  -  Automated insulin delivery
KW  -  Free-living conditions
KW  -  Physical activity
KW  -  Supervised learning
KW  -  Wearable devices
KW  -  Wrist-worn accelerometer
KW  - Type 1 diabetes mellitus
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104633
EP  - 104633
DO  - https://doi.org/10.1016/j.compbiomed.2021.104633
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521004273
N2  - This paper introduces methods to estimate aspects of physical activity and sedentary behavior from three-axis accelerometer data collected with a wrist-worn device at a sampling rate of 32 [Hz] on adults with type 1 diabetes (T1D) in free-living conditions. In particular, we present two methods able to detect and grade activity based on its intensity and individual fitness as sedentary, mild, moderate or vigorous, and a method that performs activity classification in a supervised learning framework to predict specific user behaviors. Population results for activity level grading show multi-class average accuracy of 99.99%, precision of 98.0 ± 2.2%, recall of 97.9 ± 3.5% and F1 score of 0.9 ± 0.0. As for the specific behavior prediction, our best performing classifier, gave population multi-class average accuracy of 92.43 ± 10.32%, precision of 92.94 ± 9.80%, recall of 92.20 ± 10.16% and F1 score of 92.56 ± 9.94%. Our investigation showed that physical activity and sedentary behavior can be detected, graded and classified with good accuracy and precision from three-axial accelerometer data collected in free-living conditions on people with T1D. This is particularly significant in the context of automated glucose control systems for diabetes, in that the methods we propose have the potential to inform changes in treatment parameters in response to the intensity of physical activity, allowing patients to meet their glycemic targets.
ER  - 
TY  - JOUR
T1  - Closed-loop control with unannounced exercise for adults with type 1 diabetes using the Ensemble Model Predictive Control
A1  - Garcia-Tirado, Jose
A1  - Corbett, John P
A1  - Boiroux, Dimitri
A1  - Jørgensen, John Bagterp
A1  - Breton, Marc D
Y1  - 2019///
KW  -  Exercise
KW  -  Hypoglycemia
KW  -  Model Predictive Control
KW  -  Type 1 diabetes
KW  - Artificial pancreas
JF  - Journal of Process Control
VL  - 80
SP  - 202
EP  - 210
DO  - https://doi.org/10.1016/j.jprocont.2019.05.017
UR  - https://www.sciencedirect.com/science/article/pii/S0959152419303610
N2  - This paper presents an individualized Ensemble Model Predictive Control (EnMPC) algorithm for blood glucose (BG) stabilization and hypoglycemia prevention in people with type 1 diabetes (T1D) who exercise regularly. The EnMPC formulation can be regarded as a simplified multi-stage MPC allowing for the consideration of Nen scenarios gathered from the patient's recent behavior. The patient's physical activity behavior is characterized by an exercise-specific input signal derived from the deconvolution of the patient's continuous glucose monitor (CGM), accounting for known inputs such as meal, and insulin pump records. The EnMPC controller was tested in a cohort of in silico patients with representative inter-subject and intra-subject variability from the FDA-accepted UVA/Padova simulation platform. Results show a significant improvement on hypoglycemia prevention after 30 min of mild to moderate exercise in comparison to a similarly tuned baseline controller (rMPC); with a reduction in hypoglycemia occurrences (<70 mg/dL), from 3.08 % ±3.55 with rMPC to 0.78 % ±2.04 with EnMPC (P < 0.05).
ER  - 
TY  - JOUR
T1  - Model-fusion-based online glucose concentration predictions in people with type 1 diabetes
A1  - Yu, Xia
A1  - Turksoy, Kamuran
A1  - Rashid, Mudassir
A1  - Feng, Jianyuan
A1  - Hobbs, Nicole
A1  - Hajizadeh, Iman
A1  - Samadi, Sediqeh
A1  - Sevil, Mert
A1  - Lazaro, Caterina
A1  - Maloney, Zacharie
A1  - Littlejohn, Elizabeth
A1  - Quinn, Laurie
A1  - Cinar, Ali
Y1  - 2018///
KW  -  Model fusion strategy
KW  -  Online glucose prediction
KW  -  Type 1 diabetes
KW  - Adaptive filtering algorithms
JF  - Control Engineering Practice
VL  - 71
SP  - 129
EP  - 141
DO  - https://doi.org/10.1016/j.conengprac.2017.10.013
UR  - https://www.sciencedirect.com/science/article/pii/S0967066117302460
N2  - Accurate predictions of glucose concentrations are necessary to develop an artificial pancreas (AP) system for people with type 1 diabetes (T1D). In this work, a novel glucose forecasting paradigm based on a model fusion strategy is developed to accurately characterize the variability and transient dynamics of glycemic measurements. To this end, four different adaptive filters and a fusion mechanism are proposed for use in the online prediction of future glucose trajectories. The filter fusion mechanism is developed based on various prediction performance indexes to guide the overall output of the forecasting paradigm. The efficiency of the proposed model fusion based forecasting method is evaluated using simulated and clinical datasets, and the results demonstrate the capability and prediction accuracy of the data-based fusion filters, especially in the case of limited data availability. The model fusion framework may be used in the development of an AP system for glucose regulation in patients with T1D.
ER  - 
TY  - JOUR
T1  - Kidney disease detection and segmentation using artificial neural network and multi-kernel k-means clustering for ultrasound images
A1  - Nithya, A
A1  - Appathurai, Ahilan
A1  - Venkatadri, N
A1  - Ramji, D R
A1  - Anna Palagan, C
Y1  - 2020///
KW  -  Bilateral filter
KW  -  Classification
KW  -  GLCM features
KW  -  Multi-kernel k-means clustering
KW  -  Neural network
KW  -  Segmentation
KW  - Ultrasound image
JF  - Measurement
VL  - 149
SP  - 106952
EP  - 106952
DO  - https://doi.org/10.1016/j.measurement.2019.106952
UR  - https://www.sciencedirect.com/science/article/pii/S0263224119308188
N2  - The main aim of this paper is to design and develop an approach for kidney disease detection and segmentation using a combination of clustering and classification approach. Nowadays, kidney stone detection and segmentation is one of the crucial procedures in surgical and treatment planning for ultrasound images. However, at present, kidney stone segmentation in ultrasound images is mostly performed manually in clinical practice. Apart from being time-consuming, manual stone delineation is difficult and depends on the individual operator. Therefore, in this work, we proposed a kidney stone detection using artificial neural network and segmentation using multi-kernel k-means clustering algorithm. Normally, the system comprises of four modules like (i) preprocessing, (ii) feature extraction, (iii) classification and (iv) segmentation. Primarily, we eliminate the noise present in the input image using median filter. Then, we extract the important GLCM features from the image. After that, we classify the image as normal or abnormal using neural network classifier. Finally, the abnormal images are given to the segmentation stage to segment the stone and tumor part separately using multi. Kernel K-means clustering algorithm. The experimentation results show that the proposed system as linear + quadratic based segmentation achieves the maximum accuracy of 99.61%, compare with all other methods.
ER  - 
TY  - JOUR
T1  - A new machine learning technique for an accurate diagnosis of coronary artery disease
A1  - Abdar, Moloud
A1  - Książek, Wojciech
A1  - Acharya, U Rajendra
A1  - Tan, Ru-San
A1  - Makarenkov, Vladimir
A1  - Pławiak, Paweł
Y1  - 2019///
KW  -  Classification
KW  -  Feature selection
KW  -  Genetic algorithm
KW  -  Machine learning
KW  -  Normalization
KW  -  Particle swarm optimization
KW  - Coronary artery disease (CAD)
JF  - Computer Methods and Programs in Biomedicine
VL  - 179
SP  - 104992
EP  - 104992
DO  - https://doi.org/10.1016/j.cmpb.2019.104992
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718314585
N2  - Background and objective
Coronary artery disease (CAD) is one of the commonest diseases around the world. An early and accurate diagnosis of CAD allows a timely administration of appropriate treatment and helps to reduce the mortality. Herein, we describe an innovative machine learning methodology that enables an accurate detection of CAD and apply it to data collected from Iranian patients.
Methods
We first tested ten traditional machine learning algorithms, and then the three-best performing algorithms (three types of SVM) were used in the rest of the study. To improve the performance of these algorithms, a data preprocessing with normalization was carried out. Moreover, a genetic algorithm and particle swarm optimization, coupled with stratified 10-fold cross-validation, were used twice: for optimization of classifier parameters and for parallel selection of features.
Results
The presented approach enhanced the performance of all traditional machine learning algorithms used in this study. We also introduced a new optimization technique called N2Genetic optimizer (a new genetic training). Our experiments demonstrated that N2Genetic-nuSVM provided the accuracy of 93.08% and F1-score of 91.51% when predicting CAD outcomes among the patients included in a well-known Z-Alizadeh Sani dataset. These results are competitive and comparable to the best results in the field.
Conclusions
We showed that machine-learning techniques optimized by the proposed approach, can lead to highly accurate models intended for both clinical and research use.
ER  - 
TY  - JOUR
T1  - Deep learning based joint segmentation and characterization of multi-class retinal fluid lesions on OCT scans for clinical use in anti-VEGF therapy
A1  - Hassan, Bilal
A1  - Qin, Shiyin
A1  - Ahmed, Ramsha
A1  - Hassan, Taimur
A1  - Taguri, Abdel Hakeem
A1  - Hashmi, Shahrukh
A1  - Werghi, Naoufel
Y1  - 2021///
KW  -  Deep learning
KW  -  Lesions detection
KW  -  Medical image analysis
KW  -  Optical coherence tomography (OCT)
KW  -  Retinal fluids segmentation
KW  - Radiomics
JF  - Computers in Biology and Medicine
VL  - 136
SP  - 104727
EP  - 104727
DO  - https://doi.org/10.1016/j.compbiomed.2021.104727
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521005217
N2  - Background
In anti-vascular endothelial growth factor (anti-VEGF) therapy, an accurate estimation of multi-class retinal fluid (MRF) is required for the activity prescription and intravitreal dose. This study proposes an end-to-end deep learning-based retinal fluids segmentation network (RFS-Net) to segment and recognize three MRF lesion manifestations, namely, intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED), from multi-vendor optical coherence tomography (OCT) imagery. The proposed image analysis tool will optimize anti-VEGF therapy and contribute to reducing the inter- and intra-observer variability.
Method
The proposed RFS-Net architecture integrates the atrous spatial pyramid pooling (ASPP), residual, and inception modules in the encoder path to learn better features and conserve more global information for precise segmentation and characterization of MRF lesions. The RFS-Net model is trained and validated using OCT scans from multiple vendors (Topcon, Cirrus, Spectralis), collected from three publicly available datasets. The first dataset consisted of OCT volumes obtained from 112 subjects (a total of 11,334 B-scans) is used for both training and evaluation purposes. Moreover, the remaining two datasets are only used for evaluation purposes to check the trained RFS-Net's generalizability on unseen OCT scans. The two evaluation datasets contain a total of 1572 OCT B-scans from 1255 subjects. The performance of the proposed RFS-Net model is assessed through various evaluation metrics.
Results
The proposed RFS-Net model achieved the mean F1 scores of 0.762, 0.796, and 0.805 for segmenting IRF, SRF, and PED. Moreover, with the automated segmentation of the three retinal manifestations, the RFS-Net brings a considerable gain in efficiency compared to the tedious and demanding manual segmentation procedure of the MRF.
Conclusions
Our proposed RFS-Net is a potential diagnostic tool for the automatic segmentation of MRF (IRF, SRF, and PED) lesions. It is expected to strengthen the inter-observer agreement, and standardization of dosimetry is envisaged as a result.
ER  - 
TY  - JOUR
T1  - Multi-channel Convolutions Neural Network Based Diabetic Retinopathy Detection from Fundus Images
A1  - Butt, M Mohsin
A1  - Latif, Ghazanfar
A1  - Iskandar, D N F Awang
A1  - Alghazo, Jaafar
A1  - Khan, Adil H
Y1  - 2019///
KW  -  Convolutional Neural Networks (CNN)
KW  -  Fundus Images
KW  -  Multi-Channel DR Detection
KW  - Diabetic Retinopathy Detection
JF  - Procedia Computer Science
VL  - 163
SP  - 283
EP  - 291
DO  - https://doi.org/10.1016/j.procs.2019.12.110
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919321490
N1  - 16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence
N2  - Diabetic Retinopathy (DR) is an eye medical condition usually found in patients suffering from diabetes. The initial stages of DR can either show no symptoms or cause mild vision problems but advanced stages of the disease can cause blindness. The main cause of DR is high blood sugar in diabetic patients that affect the blood vessels supplying blood to the retina and blocks them. The body tries to grow new vessels to supply the retina, however, they usually don’t develop properly and can easily leak. DR Detection is an extremely challenging task due to the variation of retina change throughout the disease stages. In this paper, a multi- channel Convolutional Neural Network (CNN) is proposed for DR detection from fundus images of the eyes. The proposed system is tested on a DR Dataset consisting of 35,126 images provided by EyePACS. Experimental results indicate that the accuracy of 97.08% is achieved through the model that outperforms those achieved through other methods in recent studies.
ER  - 
TY  - JOUR
T1  - Convexity shape constraints for retinal blood vessel segmentation and foveal avascular zone detection
A1  - Escorcia-Gutierrez, José
A1  - Torrents-Barrena, Jordina
A1  - Gamarra, Margarita
A1  - Romero-Aroca, Pedro
A1  - Valls, Aida
A1  - Puig, Domenec
Y1  - 2020///
KW  -  Blood vessel segmentation
KW  -  Convexity shape prior
KW  -  Foveal avascular zone detection
KW  - Diabetic retinopathy
JF  - Computers in Biology and Medicine
VL  - 127
SP  - 104049
EP  - 104049
DO  - https://doi.org/10.1016/j.compbiomed.2020.104049
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520303802
N2  - Diabetic retinopathy (DR) has become a major worldwide health problem due to the increase in blindness among diabetics at early ages. The detection of DR pathologies such as microaneurysms, hemorrhages and exudates through advanced computational techniques is of utmost importance in patient health care. New computer vision techniques are needed to improve upon traditional screening of color fundus images. The segmentation of the entire anatomical structure of the retina is a crucial phase in detecting these pathologies. This work proposes a novel framework for fast and fully automatic blood vessel segmentation and fovea detection. The preprocessing method involved both contrast limited adaptive histogram equalization and the brightness preserving dynamic fuzzy histogram equalization algorithms to enhance image contrast and eliminate noise artifacts. Afterwards, the color spaces and their intrinsic components were examined to identify the most suitable color model to reveal the foreground pixels against the entire background. Several samples were then collected and used by the renowned convexity shape prior segmentation algorithm. The proposed methodology achieved an average vasculature segmentation accuracy exceeding 96%, 95%, 98% and 94% for the DRIVE, STARE, HRF and Messidor publicly available datasets, respectively. An additional validation step reached an average accuracy of 94.30% using an in-house dataset provided by the Hospital Sant Joan of Reus (Spain). Moreover, an outstanding detection accuracy of over 98% was achieved for the foveal avascular zone. An extensive state-of-the-art comparison was also conducted. The proposed approach can thus be integrated into daily clinical practice to assist medical experts in the diagnosis of DR.
ER  - 
TY  - JOUR
T1  - A research framework for pharmacovigilance in health social media: Identification and evaluation of patient adverse drug event reports
A1  - Liu, Xiao
A1  - Chen, Hsinchun
Y1  - 2015///
KW  -  Adverse drug event extraction
KW  -  Health social media analytics
KW  -  Information search and retrieval
KW  -  Pharmacovigilance
KW  -  Text mining
KW  - Knowledge acquisition
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - 268
EP  - 279
DO  - https://doi.org/10.1016/j.jbi.2015.10.011
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415002336
N2  - Social media offer insights of patients’ medical problems such as drug side effects and treatment failures. Patient reports of adverse drug events from social media have great potential to improve current practice of pharmacovigilance. However, extracting patient adverse drug event reports from social media continues to be an important challenge for health informatics research. In this study, we develop a research framework with advanced natural language processing techniques for integrated and high-performance patient reported adverse drug event extraction. The framework consists of medical entity extraction for recognizing patient discussions of drug and events, adverse drug event extraction with shortest dependency path kernel based statistical learning method and semantic filtering with information from medical knowledge bases, and report source classification to tease out noise. To evaluate the proposed framework, a series of experiments were conducted on a test bed encompassing about postings from major diabetes and heart disease forums in the United States. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Our framework significantly outperforms prior work.
ER  - 
TY  - JOUR
T1  - Systemic health evaluation of RF generators using Gaussian mixture models
A1  - Bowen, Ryan M
A1  - Sahin, Ferat
A1  - Radomski, Aaron
Y1  - 2016///
KW  -  Gaussian mixtures
KW  -  Mixture models
KW  -  One-class classification
KW  -  RF power generators
KW  - Health monitoring
JF  - Computers & Electrical Engineering
VL  - 53
SP  - 13
EP  - 28
DO  - https://doi.org/10.1016/j.compeleceng.2016.04.020
UR  - https://www.sciencedirect.com/science/article/pii/S0045790616301033
N2  - We propose an application of specific machine learning techniques capable of evaluating systemic health of a Radio Frequency (RF) power generator. System signatures or fingerprints are collected from multivariate time-series data samples of sensor values under typical operational loads. These fingerprints are transformed into feature vectors using standard scaling/translation methods and the Fast Fourier Transform (FFT). The number of features per fingerprint are reduced by banding neighboring features and Principal Component Analysis (PCA). The reduced feature vectors are used with the Expectation Maximization (EM) algorithm to learn parameters for a Gaussian Mixture Model (GMM) to represent normal operation. One-class classification of normal fingerprints is achieved by thresholding the likelihood of a fingerprint feature vectors. Fingerprints were collected from normal operational conditions and seeded non-normal conditions. Preprocessing methods and algorithmic parameters have been selected using an iterative grid search. Average robust true positive rate achieved was 94.76% and best specificity reported is 86.56%.
ER  - 
TY  - JOUR
T1  - Comparison of texture-based classification and deep learning for plantar soft tissue histology segmentation
A1  - Brady, Lynda
A1  - Wang, Yak-Nam
A1  - Rombokas, Eric
A1  - Ledoux, William R
Y1  - 2021///
KW  -  Diabetes
KW  -  Neural network
KW  -  Texture classification
KW  -  Tissue segmentation
KW  -  Whole slide image
KW  - Histology
JF  - Computers in Biology and Medicine
VL  - 134
SP  - 104491
EP  - 104491
DO  - https://doi.org/10.1016/j.compbiomed.2021.104491
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521002857
N2  - Histomorphological measurements can be used to identify microstructural changes related to disease pathomechanics, in particular, plantar soft tissue changes with diabetes. However, these measurements are time-consuming and susceptible to sampling and human measurement error. We investigated two approaches to automate segmentation of plantar soft tissue stained with modified Hart's stain for elastin with the eventual goal of subsequent morphological analysis. The first approach used multiple texture- and color-based features with tile-wise classification. The second approach used a convolutional neural network modified from the U-Net architecture with fewer channel dimensions and additional downsampling steps. A hybrid color and texture feature, Fourier reduced histogram of uniform improved opponent color local binary patterns (f-IOCLBP), yielded the best feature-based segmentation, but still performed 3.6% worse on average than the modified U-Net. The texture-based method was sensitive to changes in illumination and stain intensity, and segmentation errors were often in large regions of single tissues or at tissue boundaries. The U-Net was able to segment small, few-pixel tissue boundaries, and errors were often trivial to clean up with post-processing. A U-Net approach outperforms hand-crafted features for segmentation of plantar soft tissue stained with modified Hart's stain for elastin.
ER  - 
TY  - JOUR
T1  - Predictive Modeling for Decision Support in the Tasks of Selecting the Drug for Obesity Treatment
A1  - Zhukova, Irina V
A1  - Derevitskii, Ilia V
A1  - Matveev, Georgy A
A1  - Golikova, Tatiana I
A1  - Babenko, Alina Yu.
Y1  - 2021///
KW  -  DSS
KW  -  Drug Therapy
KW  -  Machine Learning
KW  -  Weight-Reducing-Therapy
KW  - Obesity
JF  - Procedia Computer Science
VL  - 193
SP  - 371
EP  - 381
DO  - https://doi.org/10.1016/j.procs.2021.10.038
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921020792
N1  - 10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021
N2  - Obesity is a serious and dangerous problem in the modern world. Since 1975, the number of obese people worldwide has tripled. Obesity is a major risk factor for noncommunicable diseases such as cardiovascular diseases, type 2 diabetes, musculoskeletal disorders (especially osteoarthritis), some cancers and the risk for these diseases increases, with increases in BMI. One of the methods of treating obesity is drug therapy, however, many anti-obesity drugs are expensive, so doctors and patients need to understand in advance whether the drug will have a positive effect. This information will help optimize the use of hospital and patient time and financial resources. In this article, we propose the method of using predictive machine learning regression models for selecting drug treatment for obese patients as part of the creating the Decision Support System (DSS). The proposed models have peaty high accuracy, which allows to use it by medical experts in practical problems of choosing a therapy for obese patients. Moreover, we evaluated the efficiency of our method by conducting a virtual experiment, which is described below. The algorithm selected the more effective drug for 33 out of 41 patients. The average effectiveness was 2.1% of body weight. Validation result of the best model: Root Mean Squared Error (RMSE) is 1.69, Mean Absolute Error (MAE) is 1.34.
ER  - 
TY  - JOUR
T1  - An effective feature engineering for DNN using hybrid PCA-GWO for intrusion detection in IoMT architecture
A1  - R.M., Swarna Priya
A1  - Maddikunta, Praveen Kumar Reddy
A1  - M., Parimala
A1  - Koppu, Srinivas
A1  - Gadekallu, Thippa Reddy
A1  - Chowdhary, Chiranji Lal
A1  - Alazab, Mamoun
Y1  - 2020///
KW  -  Deep neural networks
KW  -  Grey-Wolf Optimization algorithm
KW  -  Healthcare
KW  -  Intrusion Detection System (IDS)
KW  -  Machine learning
KW  -  Principal Component Analysis (PCA)
KW  - Internet of Medical Things (IoMT)
JF  - Computer Communications
VL  - 160
SP  - 139
EP  - 149
DO  - https://doi.org/10.1016/j.comcom.2020.05.048
UR  - https://www.sciencedirect.com/science/article/pii/S014036642030298X
N2  - The entire computing paradigm is changed due to the technological advancements in Information and Communication Technology (ICT). Due to these advancements, various new communication channels are being introduced, out of which the Internet of Things (IoT) plays a significant role. The Internet of Medical Things (IoMT) is a special category of IoT in which the medical devices communicate with each other for sharing sensitive data. These advancements help the healthcare industry to have better contact and care towards their patients. But they too have certain drawbacks since there are so many security and privacy issues like replay, man-in-the-middle, impersonation, privileged-insider, remote hijacking, password guessing, denial of service (DoS) attacks and malware attacks. When the sensitive data is being attacked by any of these attacks, there is a chance of losing the authorized data to the attacker or getting altered due to which the data is not available for the authorized users and customers. Machine learning algorithms are widely used in the Intrusion Detection System (IDS) for detecting and classifying the attacks at the network and host level in a dynamic manner. Many supervised and unsupervised algorithms have been designed by researchers from the area of machine learning and data mining to identify the reliable detection of an anomaly. However, the main challenge in the IDS models are changed in dynamic and random behavior of malicious attacks and designing a scalable solution that can handle this behavior. The rapid change in network behavior and the fast evolution of various attacks paved the way for evaluating various datasets that are generated over the years and to design different dynamic approaches. In this paper, a deep neural network (DNN) is used to develop effective and efficient IDS in the IoMT environment to classify and predict unforeseen cyberattacks. The network parameter are preprocessed, optimized and tuned by hyperparameter selection methods. A comprehensive analysis of experiments in DNN with other machine learning algorithms are compared on the benchmark intrusion detection dataset. Through rigorous testing, it has proved that the proposed DNN model performs better than the existing machine learning approaches with an increase in accuracy by 15% and decreases in time complexity by 32%, which helps in faster alerts to avoid post effects of intrusion in sensitive cloud data storage.
ER  - 
TY  - JOUR
T1  - Near-infrared determination of polyphenols using linear and nonlinear regression algorithms
A1  - Huang, Yue
A1  - Du, Guorong
A1  - Ma, Yanjun
A1  - Zhou, Jun
Y1  - 2015///
KW  -  Near-infrared spectroscopy
KW  -  Nonlinear regression modeling
KW  -  Partial least square regression
KW  - Polyphenols
JF  - Optik - International Journal for Light and Electron Optics
VL  - 126
IS  - 19
SP  - 2030
EP  - 2034
DO  - https://doi.org/10.1016/j.ijleo.2015.05.064
UR  - https://www.sciencedirect.com/science/article/pii/S0030402615003708
N2  - In the present study, the possibility of using Fourier transform near-infrared spectroscopy (FT-NIR) to measure the concentration of polyphenols in Yunnan tobacco was investigated. Selected samples representing a wide range of varieties and regions were analyzed by high performance liquid chromatography (HPLC) for the concentrations of polyphenols in tobacco. Results showed that positive correlations existed between NIR spectra and concentration of objective compound upon the established linear and nonlinear regression models. The optimal model was obtained by comparing different modeling processes. It was demonstrated that the PLS regression covering the range of 5450–4250cm−1 could lead to a good linear relationship between spectra and polyphenols with the R2 of 0.9170. Optimal model generated the RMSEP of 0.254, RSEP of 0.0554, and RPD of 3.47, revealing that the linear model was able to predict the content of polyphenols in tobacco. Support vector regression (SVR) preprocessed by SNV obtained the predictable results with the R2 of 0.8461, RMSEP of 0.374, and RPD of 2.36, which was inferior to PLS modeling.
ER  - 
TY  - JOUR
T1  - Optimized clinical segmentation of retinal blood vessels by using combination of adaptive filtering, fuzzy entropy and skeletonization
A1  - Rezaee, Khosro
A1  - Haddadnia, Javad
A1  - Tashk, Ashkan
Y1  - 2017///
KW  -  Adaptive filter
KW  -  Fuzzy entropy and skeleton method
KW  -  Image processing
KW  -  Wiener filter
KW  - Retinas vessels
JF  - Applied Soft Computing
VL  - 52
SP  - 937
EP  - 951
DO  - https://doi.org/10.1016/j.asoc.2016.09.033
UR  - https://www.sciencedirect.com/science/article/pii/S1568494616304926
N2  - The analysis of retina blood vessels in clinics indices is one of the most efficient methods employed for diagnosing diseases such as diabetes, hypertension and arthrosclerosis. In this paper, an efficient algorithm is proposed that introduces a higher ability of segmentation by employing Skeletonization and a threshold selection based on Fuzzy Entropy. In the first step, the blurring noises caused by hand shakings during ophthalmoscopy and color photography imageries are removed by a designed Wiener’s filter. Then, in the second step, a basic extraction of the blood vessels from the retina based on an adaptive filtering is obtained. At the last step of the proposed method, an optimal threshold for discriminating main vessels of the retina from other parts of the tissue is achieved by employing fuzzy entropy. Finally, an assessment procedure based on four different measurement techniques in the terms of retinal fundus colors is established and applied to DRIVE and STARE database images. Due to the evaluation comparative results, the proposed extraction of retina blood vessels enables specialists to determine the progression stage of potential diseases, more accurate and in real-time mode.
ER  - 
TY  - JOUR
T1  - A complementary method for automated detection of microaneurysms in fluorescein angiography fundus images to assess diabetic retinopathy
A1  - Tavakoli, Meysam
A1  - Shahri, Reza Pourreza
A1  - Pourreza, Hamidreza
A1  - Mehdizadeh, Alireza
A1  - Banaee, Touka
A1  - Bahreini Toosi, Mohammad Hosein
Y1  - 2013///
KW  -  Diabetic retinopathy
KW  -  Fluorescein angiography
KW  -  Fundus images
KW  -  Microaneurysms
KW  -  Radon transform
KW  - Computer aided diagnosis
JF  - Pattern Recognition
VL  - 46
IS  - 10
SP  - 2740
EP  - 2753
DO  - https://doi.org/10.1016/j.patcog.2013.03.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313001404
N2  - Early detection of microaneurysms (MAs), the first sign of Diabetic Retinopathy (DR), is an essential first step in automated detection of DR to prevent vision loss and blindness. This study presents a novel and different algorithm for automatic detection of MAs in fluorescein angiography (FA) fundus images, based on Radon transform (RT) and multi-overlapping windows. This project addresses a novel method, in detection of retinal land marks and lesions to diagnose the DR. At the first step, optic nerve head (ONH) was detected and masked. In preprocessing stage, top-hat transformation and averaging filter were applied to remove the background. In main processing section, firstly, we divided the whole preprocessed image into sub-images and then segmented and masked the vascular tree by applying RT in each sub-image. After detecting and masking retinal vessels and ONH, MAs were detected and numbered by using RT and appropriated thresholding. The results of the proposed method were evaluated on three different retinal images databases, the Mashhad Database with 120 FA fundus images, Second Local Database from Tehran with 50 FA retinal images and a part of Retinopathy Online Challenge (ROC) database with 22 images. Automated DR detection demonstrated a sensitivity and specificity of 94% and 75% for Mashhad database and 100% and 70% for the Second Local Database respectively.
ER  - 
TY  - JOUR
T1  - Identification of palm print using dermatoglyphics analysis and detection system
A1  - Qiao, Yiaohua
A1  - Li, Zhen
A1  - Wang, Qing
A1  - Zeng, Yanjun
A1  - Liang, Ke
Y1  - 2005///
KW  -  Automatic identification
KW  -  Image processing
KW  -  Palm print
KW  - Dermatoglyphics
JF  - Medical Engineering & Physics
VL  - 27
IS  - 3
SP  - 229
EP  - 235
DO  - https://doi.org/10.1016/j.medengphy.2004.10.012
UR  - https://www.sciencedirect.com/science/article/pii/S1350453304002073
N2  - This paper investigated the biological characteristics of palm print. The preprocessing plays an important role in identifying the ridge characteristics of palm due to the complexity and poor quality of the images. As a functional plug-in of the dermatoglyphics analysis and detection system, the template-based image preprocessing was proposed in this study, including histogram redistribution, ridge orientation, and skeletonization. Using this system, the automatic identification of ridges and triradiuses was accomplished with an effective result. The study demonstrated the feasibility of the method and the potential of the system for being applied as an auxiliary diagnostic tool for heredity diseases (e.g. mammary cancer and neurofibromatosis).
ER  - 
TY  - JOUR
T1  - Microaneurysm detection using fully convolutional neural networks
A1  - Chudzik, Piotr
A1  - Majumdar, Somshubra
A1  - Calivá, Francesco
A1  - Al-Diri, Bashir
A1  - Hunter, Andrew
Y1  - 2018///
KW  -  Convolutional neural networks
KW  -  Microaneurysm detection
KW  -  Retinal fundus images
KW  - Medical image analysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 158
SP  - 185
EP  - 192
DO  - https://doi.org/10.1016/j.cmpb.2018.02.016
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717308544
N2  - Backround and Objectives
Diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough. Microaneurysms are the earliest clinical signs of diabetic retinopathy. This paper presents an automatic method for detecting microaneurysms in fundus photographies.
Methods
A novel patch-based fully convolutional neural network with batch normalization layers and Dice loss function is proposed. Compared to other methods that require up to five processing stages, it requires only three. Furthermore, to the best of the authors’ knowledge, this is the first paper that shows how to successfully transfer knowledge between datasets in the microaneurysm detection domain.
Results
The proposed method was evaluated using three publicly available and widely used datasets: E-Ophtha, DIARETDB1, and ROC. It achieved better results than state-of-the-art methods using the FROC metric. The proposed algorithm accomplished highest sensitivities for low false positive rates, which is particularly important for screening purposes.
Conclusions
Performance, simplicity, and robustness of the proposed method demonstrates its suitability for diabetic retinopathy screening applications.
ER  - 
TY  - JOUR
T1  - A novel machine learning approach for early detection of hepatocellular carcinoma patients
A1  - Książek, Wojciech
A1  - Abdar, Moloud
A1  - Acharya, U Rajendra
A1  - Pławiak, Paweł
Y1  - 2019///
KW  -  Data mining
KW  -  Feature selection
KW  -  Genetic algorithm
KW  -  Hepatocellular carcinoma (HCC)
KW  -  Normalization
KW  - Machine learning
JF  - Cognitive Systems Research
VL  - 54
SP  - 116
EP  - 127
DO  - https://doi.org/10.1016/j.cogsys.2018.12.001
UR  - https://www.sciencedirect.com/science/article/pii/S1389041718308714
N2  - Liver cancer is quite common type of cancer among individuals worldwide. Hepatocellular carcinoma (HCC) is the malignancy of liver cancer. It has high impact on individual’s life and investigating it early can decline the number of annual deaths. This study proposes a new machine learning approach to detect HCC using 165 patients. Ten well-known machine learning algorithms are employed. In the preprocessing step, the normalization approach is used. The genetic algorithm coupled with stratified 5-fold cross-validation method is applied twice, first for parameter optimization and then for feature selection. In this work, support vector machine (SVM) (type C-SVC) with new 2level genetic optimizer (genetic training) and feature selection yielded the highest accuracy and F1-Score of 0.8849 and 0.8762 respectively. Our proposed model can be used to test the performance with huge database and aid the clinicians.
ER  - 
TY  - JOUR
T1  - Cluster Based Outlier Detection Algorithm for Healthcare Data
A1  - Christy, A
A1  - Gandhi, G Meera
A1  - Vaithyasubramanian, S
Y1  - 2015///
KW  -  F-Measure
KW  -  Likelihood Ratio
KW  -  Outlier Detection
KW  -  Outlier Score
KW  -  etc.
KW  - Cluster-Based
JF  - Procedia Computer Science
VL  - 50
SP  - 209
EP  - 215
DO  - https://doi.org/10.1016/j.procs.2015.04.058
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915005591
N1  - Big Data, Cloud and Computing Challenges
N2  - Outliers has been studied in a variety of domains including Big Data, High dimensional data, Uncertain data, Time Series data, Biological data, etc. In majority of the sample datasets available in the repository, atleast 10% of the data may be erroneous, missing or not available. In this paper, we utilize the concept of data preprocessing for outlier reduction. We propose two algorithms namely Distance-Based outlier detection and Cluster-Based outlier algorithm for detecting and removing outliers using a outlier score. By cleaning the dataset and clustering based on similarity, we can remove outliers on the key attribute subset rather than on the full dimensional attributes of dataset. Experiments were conducted using 3 built-in Health care dataset available in R package and the results show that the cluster-based outlier detection algorithm providing better accuracy than distance based outlier detection algorithm.
ER  - 
TY  - JOUR
T1  - Coarse-to-fine classification for diabetic retinopathy grading using convolutional neural network
A1  - Wu, Zhan
A1  - Shi, Gonglei
A1  - Chen, Yang
A1  - Shi, Fei
A1  - Chen, Xinjian
A1  - Coatrieux, Gouenou
A1  - Yang, Jian
A1  - Luo, Limin
A1  - Li, Shuo
Y1  - 2020///
KW  -  Coarse-to-fine classification
KW  -  Convolutional neural networks
KW  -  Fundus images
KW  - Diabetic retinopathy grading
JF  - Artificial Intelligence in Medicine
VL  - 108
SP  - 101936
EP  - 101936
DO  - https://doi.org/10.1016/j.artmed.2020.101936
UR  - https://www.sciencedirect.com/science/article/pii/S0933365720301354
N2  - Diabetic retinopathy (DR) is the most common eye complication of diabetes and one of the leading causes of blindness and vision impairment. Automated and accurate DR grading is of great significance for the timely and effective treatment of fundus diseases. Current clinical methods remain subject to potential time-consumption and high-risk. In this paper, a hierarchically Coarse-to-fine network (CF-DRNet) is proposed as an automatic clinical tool to classify five stages of DR severity grades using convolutional neural networks (CNNs). The CF-DRNet conforms to the hierarchical characteristic of DR grading and effectively improves the classification performance of five-class DR grading, which consists of the following: (1) The Coarse Network performs two-class classification including No DR and DR, where the attention gate module highlights the salient lesion features and suppresses irrelevant background information. (2) The Fine Network is proposed to classify four stages of DR severity grades of the grade DR from the Coarse Network including mild, moderate, severe non-proliferative DR (NPDR) and proliferative DR (PDR). Experimental results show that proposed CF-DRNet outperforms some state-of-art methods in the publicly available IDRiD and Kaggle fundus image datasets. These results indicate our method enables an efficient and reliable DR grading diagnosis in clinic.
ER  - 
TY  - JOUR
T1  - Machine learning to promote health management through lifestyle changes for hypertension patients
A1  - Islam, Md. Mazharul
A1  - Shamsuddin, Rittika
Y1  - 2021///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Health management
KW  -  Perturbation-based simulation
KW  -  Prediction
KW  - Hypertension
JF  - Array
VL  - 12
SP  - 100090
EP  - 100090
DO  - https://doi.org/10.1016/j.array.2021.100090
UR  - https://www.sciencedirect.com/science/article/pii/S2590005621000370
N2  - The purpose of this paper is to investigate the use of machine learning models to develop a diagnostic system for hypertension patients so that people can modify their daily lifestyle to manage their condition. We propose this system by adopting the concepts of saliency maps for image data to non-image, lifestyle data with a data perturbation simulation technique. We trained the proposed system on a new lifestyle dataset that we extracted from a survey on Asian sub-population. The proposed system consists of a convolution neural network (CNN) as the diagnostic model, and is combined with simulation techniques to explain the concepts/insights learnt by the CNN. We compared classification performance of the CNN model with other baseline models fitted with other types of hypertension data including neural network, decision tree and other CNN model from literature. The CNN achieved a 68–70% accuracy on training and testing datasets. Comparing with other baseline models, our CNN model provided more consistent performance in terms of accuracy, sensitivity, specificity and area under receiver operating characteristic (ROC) curve. Using the simulations, we learnt that CNN captured not only direct correlation between the variables and the target, but also learnt group-based interactions. Our study reveals that age, gender, diabetes status, body mass index, smoking, occupation and education are some important lifestyle factors affecting hypertension. Avoiding smoking, maintaining a balanced diet to prevent unnecessary weight gaining, regular monitoring of blood sugar level for diabetic care, and stress relief exercise can reduce hypertension risk.
ER  - 
TY  - JOUR
T1  - Non-invasive detection of diabetic complications via pattern analysis of temporal facial colour variations
A1  - Majtner, Tomáš
A1  - Nadimi, Esmaeil S
A1  - Yderstræde, Knud B
A1  - Blanes-Vidal, Victoria
Y1  - 2020///
KW  -  Feature extraction
KW  -  Machine learning
KW  -  Skin patch analysis
KW  -  Skin redness
KW  - Pattern recognition
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105619
EP  - 105619
DO  - https://doi.org/10.1016/j.cmpb.2020.105619
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314528
N2  - Background and Objective: Diabetes mellitus is a common disorder amounting to 400 million patients worldwide. It is often accompanied by a number of complications, including neuropathy, nephropathy, and cardiovascular diseases. For example, peripheral neuropathy is present among 20–30% of diabetics before the diagnosis is substantiated. For this reason, a reliable detection method for diabetic complications is crucial and attracts a lot of research attention. Methods: In this paper, we introduce a non-invasive detection framework for patients with diabetic complications that only requires short video recordings of faces from a standard commercial camera. We employed multiple image processing and pattern recognition techniques to process video frames, extract relevant information, and predict the health status. To evaluate our framework, we collected a dataset of 114 video files from diabetic patients, who were diagnosed with diabetes for years and 60 video files from the control group. Extracted features from videos were tested using two conceptually different classifiers. Results: We found that our proposed framework correctly identifies patients with diabetic complications with 92.86% accuracy, 100% sensitivity, and 80% specificity. Conclusions: Our study brings a novel perspective on diagnosis procedures in this field. We used multiple techniques from image processing, pattern recognition, and machine learning to robustly process video frames and predict the health status of our subjects with high efficiency.
ER  - 
TY  - JOUR
T1  - JTSA: An open source framework for time series abstractions
A1  - Sacchi, Lucia
A1  - Capozzi, Davide
A1  - Bellazzi, Riccardo
A1  - Larizza, Cristiana
Y1  - 2015///
KW  -  Biomedical data mining software tool
KW  -  Data analysis workflow
KW  -  Temporal pattern discovery
KW  -  Time series analysis
KW  - Temporal abstractions
JF  - Computer Methods and Programs in Biomedicine
VL  - 121
IS  - 3
SP  - 175
EP  - 188
DO  - https://doi.org/10.1016/j.cmpb.2015.05.006
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715001492
N2  - Background and objective
The evaluation of the clinical status of a patient is frequently based on the temporal evolution of some parameters, making the detection of temporal patterns a priority in data analysis. Temporal abstraction (TA) is a methodology widely used in medical reasoning for summarizing and abstracting longitudinal data.
Methods
This paper describes JTSA (Java Time Series Abstractor), a framework including a library of algorithms for time series preprocessing and abstraction and an engine to execute a workflow for temporal data processing. The JTSA framework is grounded on a comprehensive ontology that models temporal data processing both from the data storage and the abstraction computation perspective. The JTSA framework is designed to allow users to build their own analysis workflows by combining different algorithms. Thanks to the modular structure of a workflow, simple to highly complex patterns can be detected. The JTSA framework has been developed in Java 1.7 and is distributed under GPL as a jar file.
Results
JTSA provides: a collection of algorithms to perform temporal abstraction and preprocessing of time series, a framework for defining and executing data analysis workflows based on these algorithms, and a GUI for workflow prototyping and testing. The whole JTSA project relies on a formal model of the data types and of the algorithms included in the library. This model is the basis for the design and implementation of the software application. Taking into account this formalized structure, the user can easily extend the JTSA framework by adding new algorithms. Results are shown in the context of the EU project MOSAIC to extract relevant patterns from data coming related to the long term monitoring of diabetic patients.
Conclusions
The proof that JTSA is a versatile tool to be adapted to different needs is given by its possible uses, both as a standalone tool for data summarization and as a module to be embedded into other architectures to select specific phenotypes based on TAs in a large dataset.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy detection through deep learning techniques: A review
A1  - Alyoubi, Wejdan L
A1  - Shalash, Wafaa M
A1  - Abulkhair, Maysoon F
Y1  - 2020///
JF  - Informatics in Medicine Unlocked
VL  - 20
SP  - 100377
EP  - 100377
DO  - https://doi.org/10.1016/j.imu.2020.100377
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820302069
N2  - Diabetic Retinopathy (DR) is a common complication of diabetes mellitus, which causes lesions on the retina that effect vision. If it is not detected early, it can lead to blindness. Unfortunately, DR is not a reversible process, and treatment only sustains vision. DR early detection and treatment can significantly reduce the risk of vision loss. The manual diagnosis process of DR retina fundus images by ophthalmologists is time-, effort-, and cost-consuming and prone to misdiagnosis unlike computer-aided diagnosis systems. Recently, deep learning has become one of the most common techniques that has achieved better performance in many areas, especially in medical image analysis and classification. Convolutional neural networks are more widely used as a deep learning method in medical image analysis and they are highly effective. For this article, the recent state-of-the-art methods of DR color fundus images detection and classification using deep learning techniques have been reviewed and analyzed. Furthermore, the DR available datasets for the color fundus retina have been reviewed. Difference challenging issues that require more investigation are also discussed.
ER  - 
TY  - JOUR
T1  - Unsupervised sorting of retinal vessels using locally consistent Gaussian mixtures
A1  - Relan, D
A1  - Relan, R
Y1  - 2021///
KW  -  Blood vessels
KW  -  Classification
KW  -  Homomorphic filtering
KW  -  Locally consistent Gaussian mixture model
KW  -  Multiscale line operator
KW  - Retinal imaging
JF  - Computer Methods and Programs in Biomedicine
VL  - 199
SP  - 105894
EP  - 105894
DO  - https://doi.org/10.1016/j.cmpb.2020.105894
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720317272
N2  - Background and Objectives: Retinal blood vessels classification into arterioles and venules is a major task for biomarker identification. Especially, clustering of retinal blood vessels is a challenging task due to factors affecting the images such as contrast variability, non-uniform illumination etc. Hence, a high performance automatic retinal vessel classification system is highly prized. In this paper, we propose a novel unsupervised methodology to classify retinal vessels extracted from fundus camera images into arterioles and venules. Methods: The proposed method utilises the homomorphic filtering (HF) to preprocess the input image for non-uniform illumination and denoising. In the next step, an unsupervised multiscale line operator segmentation technique is used to segment the retinal vasculature before extracting the discriminating features. Finally, the Locally Consistent Gaussian Mixture Model (LCGMM) is utilised for unsupervised sorting of retinal vessels. Results: The performance of the proposed unsupervised method was assessed using three publicly accessible databases: INSPIRE-AVR, VICAVR, and MESSIDOR. The proposed framework achieved 90.14%, 90.3% and 93.8% classification rate in zone B for the three datasets respectively. Conclusions: The proposed clustering framework provided high classification rate as compared to conventional Gaussian mixture model using Expectation-Maximisation (GMM-EM) approach, thus have a great capability to enhance computer assisted diagnosis and research in field of biomarker discovery.
ER  - 
TY  - JOUR
T1  - Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiability
A1  - Berg, Jeremias
A1  - Järvisalo, Matti
Y1  - 2017///
KW  -  Boolean satisfiability
KW  -  Constrained clustering
KW  -  Correlation clustering
KW  -  Cost-optimal clustering
KW  -  Maximum satisfiability
KW  - Boolean optimization
JF  - Artificial Intelligence
VL  - 244
SP  - 110
EP  - 142
DO  - https://doi.org/10.1016/j.artint.2015.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S0004370215001022
N1  - Combining Constraint Solving with Mining and Learning
N2  - Integration of the fields of constraint solving and data mining and machine learning has recently been identified within the AI community as an important research direction with high potential. This work contributes to this direction by providing a first study on the applicability of state-of-the-art Boolean optimization procedures to cost-optimal correlation clustering under constraints in a general similarity-based setting. We develop exact formulations of the correlation clustering task as Maximum Satisfiability (MaxSAT), the optimization version of the Boolean satisfiability (SAT) problem. For obtaining cost-optimal clusterings, we apply a state-of-the-art MaxSAT solver for solving the resulting MaxSAT instances optimally, resulting in cost-optimal clusterings. We experimentally evaluate the MaxSAT-based approaches to cost-optimal correlation clustering, both on the scalability of our method and the quality of the clusterings obtained. Furthermore, we show how the approach extends to constrained correlation clustering, where additional user knowledge is imposed as constraints on the optimal clusterings of interest. We show experimentally that added user knowledge allows clustering larger datasets, and at the same time tends to decrease the running time of our approach. We also investigate the effects of MaxSAT-level preprocessing, symmetry breaking, and the choice of the MaxSAT solver on the efficiency of the approach.
ER  - 
TY  - JOUR
T1  - Recognition of hard exudates using Deep Learning
A1  - Auccahuasi, Wilver
A1  - Flores, Edward
A1  - Sernaque, Fernando
A1  - Cueva, Juanita
A1  - Diaz, Monica
A1  - Oré, Elizabeth
Y1  - 2020///
KW  -  features
KW  -  images
KW  -  processing
KW  -  retinography
KW  -  segmentation
KW  - Diabetes Mellitus
JF  - Procedia Computer Science
VL  - 167
SP  - 2343
EP  - 2353
DO  - https://doi.org/10.1016/j.procs.2020.03.287
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920307535
N1  - International Conference on Computational Intelligence and Data Science
N2  - Diabetes Mellitus is a metabolic disease characterized by the presence of elevated blood glucose levels. Diabetes itself causes other chronic complications, including an eye disease known as diabetic retinopathy. Nowadays, diabetic retinopathy is the most frequent cause of blindness among the active population of developed countries. The principles that produce this disease are not completely known and can not yet be prevented. However, there are effective treatments that delay their evolution as long as it is diagnosed with sufficient anticipation. The problem of diabetic retinopathy is that it is an asymptomatic disease and only defects appear in the vision at an advanced stage of the disease. So in the early stages of diabetic retinopathy is usually imperceptible, diabetic patients do not realize that they have the disease and do not undergo an eye examination. Sometimes the patient is examined when it is too late for proper treatment, due to the presence of severe damage to the retina, occurring only the diagnosis of Diabetes. Currently, technology is becoming more important in the field of health, due to this, a series of systems have been designed to help decision making that helps in the early detection of diabetic retinopathy through the images of Eye, in the present work we present a methodology to be able to recognize the hard exudates that is the first manifestation of diabetic retinopathy, by presenting coloration similar to the other anatomical forms of the eye, its automatic recognition is complicated, the methodology that is presented consists of the use of a database of fundus images with positive and negative symptoms of diabetic retinopathy, from this database a set of images is created that correspond to the hard exudates and images that do not correspond to the hard exudates, with this set of images creates a convolutional network, in order to improve the recognition, obtaining sultados that can satisfy in the clinical practice.
ER  - 
TY  - JOUR
T1  - Novel methods for high-resolution assessment of cardiac action potential repolarization
A1  - Meo, Marianna
A1  - Meste, Olivier
A1  - Signore, Sergio
A1  - Rota, Marcello
Y1  - 2019///
KW  -  Action potential
KW  -  Cardiac repolarization
KW  -  Murine model
KW  -  Singular value decomposition
KW  - Diabetes
JF  - Biomedical Signal Processing and Control
VL  - 51
SP  - 30
EP  - 41
DO  - https://doi.org/10.1016/j.bspc.2019.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419300394
N2  - The profile of the action potential (AP) of cardiomyocytes contributes to the modality of ventricular repolarization of the heart. Experimentally, the examination of the AP in isolated cardiomyocytes provides information on their electrical properties, adaptations to physiological and pathological conditions, and putative ionic mechanisms involved in the process. Currently, there are no available platforms for automated assessment of AP properties and standard methodologies restrict the examination of the AP repolarization to discrete, user-defined ranges, neglecting significant intervals of the electrical recovery. This study proposes two automatic methods to assess AP profile throughout the entire repolarization phase. One method is based on AP data inversion and direct extraction of patterns describing beat-to-beat dynamics. The second method is based on evolutive singular value decomposition (ESVD), which identifies common patterns in a series of consecutive APs. The two methodologies were employed to analyze electrical signals collected from cardiomyocytes obtained from healthy mice and animals with diabetes, a condition associated with alterations of AP properties in cardiac cells. Our methodologies revealed that the duration of the early repolarization phase of the AP tended to become progressively longer during a stimulation train, whereas the late repolarization progressively shortened. Although this behavior was comparable in the two groups of cells, alterations in AP dynamics occurred at distinct repolarization levels, a feature highlighted by the ESVD approach. In conclusion, the proposed methodologies allow detailed, automatic analysis of the AP repolarization and identification of critical alterations occurring in the electrical behavior of myocytes under pathological conditions.
ER  - 
TY  - JOUR
T1  - Rule-based Medical Treatment Graph for the Modeling of Hypo-and Hyperglycemia at Onset
A1  - Deja, Rafal
A1  - Froelich, Wojciech
A1  - Deja, Grazyna
Y1  - 2021///
KW  -  diabetes mellitus
KW  -  modeling clinical pathways
KW  - decision support systems
JF  - Procedia Computer Science
VL  - 192
SP  - 1393
EP  - 1400
DO  - https://doi.org/10.1016/j.procs.2021.08.142
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921016318
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 25th International Conference KES2021
N2  - This paper proposes a rule-based medical treatment graph (RB-MTG), a decision support tool that assists physicians in establishing insulin therapy. The RB-MTG models clinical pathways, i.e. the sequences of blood glucose measurements and insulin injections. It provides visualization of alternative clinical pathways, especially those that lead to dangerous states of the patient’s health. By interpreting the RB-MTG, the physician assesses the patient’s condition and plans their insulin therapy. At each phase of the treatment, the RB-MTG suggests the insulin dosage that leads to normoglycemia - the blood glucose level that is the norm for a healthy person. This way, it is possible to avoid the course of the disease that leads to hypo- or hyperglycemia. Physicians have verified the usefulness of our approach.
ER  - 
TY  - JOUR
T1  - Design regulatory interaction network for anxiety disorders using R: A bioinformatics approach
A1  - Habib, Nahida
A1  - Paul, Bikash Kumar
A1  - Islam, Md. Shadidul
A1  - Ahmed, Kawsar
Y1  - 2018///
KW  -  Angina
KW  -  Asthma
KW  -  Diabetes
KW  -  Heart attack
KW  -  High blood pressure
KW  -  Regulatory interaction network R
KW  - Bio-informatics
JF  - Beni-Suef University Journal of Basic and Applied Sciences
VL  - 7
IS  - 3
SP  - 326
EP  - 335
DO  - https://doi.org/10.1016/j.bjbas.2018.03.014
UR  - https://www.sciencedirect.com/science/article/pii/S2314853518300313
N2  - Bio-informatics research fields are progressively growing enriched with the incremental use of R tools. Anxiety Disorders can be defined as one of the most common mental disorders. They are tangible psychiatric disorders or a group of mental illnesses thought to be caused by either genetic vulnerability or environmental susceptibility factors. Several studies have revealed that anxiety is associated with an increased risk of Angina, Asthma, Diabetes, Heart Attack and High Blood Pressure. Thus, these five anxiety disorders must have some genetic association among them. Finding the genetic relationship among the above diseases will help to understand the gene linkage and connection association among them which leads to the way to common drug design. Here, after investigating the genetic association of Angina, Asthma, Diabetes, Heart Attack and High Blood Pressure a Regulatory Interaction Network is designed using R. The investigated diseases genes are collected, preprocessed, processed, mined and optimized using R. This research aimed to create a new dimension in the field of drug design.
ER  - 
TY  - JOUR
T1  - Integrating semantic annotation and information visualization for the analysis of multichannel fluorescence micrographs from pancreatic tissue
A1  - Herold, Julia
A1  - Zhou, Luxian
A1  - Abouna, Sylvie
A1  - Pelengaris, Stella
A1  - Epstein, David
A1  - Khan, Michael
A1  - Nattkemper, Tim W
Y1  - 2010///
KW  -  - and -cell counting
KW  -  Bioimage informatics
KW  -  Exploratory data analysis (EDA)
KW  -  Image processing
KW  -  Information visualization
KW  -  Machine vision
KW  -  Pattern recognition
KW  - Fluorescence microscopy
JF  - Computerized Medical Imaging and Graphics
VL  - 34
IS  - 6
SP  - 446
EP  - 452
DO  - https://doi.org/10.1016/j.compmedimag.2009.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S0895611109001323
N1  - Biomedical Image Technologies and Methods - BIBE 2008
N2  - The challenging problem of computational bioimage analysis receives growing attention from life sciences. Fluorescence microscopy is capable of simultaneously visualizing multiple molecules by staining with different fluorescent dyes. In the analysis of the result multichannel images, segmentation of ROIs resembles only a first step which must be followed by a second step towards the analysis of the ROI’s signals in the different channels. In this paper we present a system that combines image segmentation and information visualization principles for an integrated analysis of fluorescence micrographs of tissue samples. The analysis aims at the detection and annotation of cells of the Islets of Langerhans and the whole pancreas, which is of great importance in diabetes studies and in the search for new anti-diabetes treatments. The system operates with two modules. The automatic annotation module applies supervised machine learning for cell detection and segmentation. The second information visualization module can be used for an interactive classification and visualization of cell types following the link-and-brush principle for filtering. We can compare the results obtained with our system with results obtained manually by an expert, who evaluated a set of example images three times to account for his intra-observer variance. The comparison shows that using our system the images can be evaluated with high accuracy which allows a considerable speed up of the time-consuming evaluation process.
ER  - 
TY  - JOUR
T1  - Video-based eye tracking performance for computer-assisted diagnostic support of diabetic neuropathy
A1  - Avendaño-Valencia, Luis David
A1  - Yderstræde, Knud B
A1  - Nadimi, Esmaeil S
A1  - Blanes-Vidal, Victoria
Y1  - 2021///
KW  -  Computer-assisted diagnosis
KW  -  Diabetic neuropathy
KW  -  Heteroscedastic ARX (H-ARX) models
KW  - Video-based eye tracking
JF  - Artificial Intelligence in Medicine
VL  - 114
SP  - 102050
EP  - 102050
DO  - https://doi.org/10.1016/j.artmed.2021.102050
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721000439
N2  - Diabetes is currently one of the major public health threats. The essential components for effective treatment of diabetes include early diagnosis and regular monitoring. However, health-care providers are often short of human resources to closely monitor populations at risk. In this work, a video-based eye-tracking method is proposed as a low-cost alternative for detection of diabetic neuropathy. The method is based on the tracking of the eye-trajectories recorded on videos while the subject follows a target on a screen, forcing saccadic movements. Upon extraction of the eye trajectories, representation of the obtained time-series is made with the help of heteroscedastic ARX (H-ARX) models, which capture the dynamics and latency on the subject's response, while features based on the H-ARX model's predictive ability are subsequently used for classification. The methodology is evaluated on a population constituted by 11 control and 20 insulin-treated diabetic individuals suffering from diverse diabetic complications including neuropathy and retinopathy. Results show significant differences on latency and eye movement precision between the populations of control subjects and diabetics, while simultaneously demonstrating that both groups can be classified with an accuracy of 95%. Although this study is limited by the small sample size, the results align with other findings in the literature and encourage further research.
ER  - 
TY  - JOUR
T1  - Automated Detection System for Diabetic Retinopathy Using Two Field Fundus Photography
A1  - Kumar, P N Sharath
A1  - Deepak, R U
A1  - Sathar, Anuja
A1  - Sahasranamam, V
A1  - Kumar, R Rajesh
Y1  - 2016///
KW  -  automatic detection
KW  -  cotton-wool spots
KW  -  exudates
KW  -  fundus images
KW  -  hemorrhages
KW  -  microaneurysms
KW  -  red lesions
KW  -  white lesions
KW  - diabetic retinopathy
JF  - Procedia Computer Science
VL  - 93
SP  - 486
EP  - 494
DO  - https://doi.org/10.1016/j.procs.2016.07.237
UR  - https://www.sciencedirect.com/science/article/pii/S187705091631479X
N1  - Proceedings of the 6th International Conference on Advances in Computing and Communications
N2  - Diabetic retinopathy (DR) is a leading cause of vision loss, caused by damage to the retina from complications of diabetes. Analysis of the retinal photographs for key characteristics of DR can result in early diagnosis and better management of DR. This paper presents a method for automated analysis and classification of the retina as DR or non-DR using two-field mydriatic fundus photography. The optic disc region is located by multi-level wavelet decomposition and recursive region growing from an automatically identified seed point. Blood vessels are extracted by applying histogram analysis on the two median filtered images. Red lesions are detected using three stage intensity transformation and white lesions from multi-level histogram analysis. The final classification of the retina as DR or non-DR is based on an aggregate of the lesions extracted from each image. The proposed method has been validated against diagnosis by a panel of expert ophthalmologists on images from 368 patients. The observed sensitivity and specificity were 80% and 50% respectively. The results show that automated screening based on two-field photography can be applied in routine screening.
ER  - 
TY  - JOUR
T1  - BigFCM: Fast, precise and scalable FCM on hadoop
A1  - Ghadiri, Nasser
A1  - Ghaffari, Meysam
A1  - Nikbakht, Mohammad Amin
Y1  - 2017///
KW  -  Big data
KW  -  Clustering
KW  -  Data mining
KW  -  Unsupervised learning and clustering
KW  -  Vagueness and fuzzy logic
KW  - MapReduce algorithms
JF  - Future Generation Computer Systems
VL  - 77
SP  - 29
EP  - 39
DO  - https://doi.org/10.1016/j.future.2017.06.010
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X17312359
N2  - Clustering plays an important role in mining big data both as a modeling technique and a preprocessing step in many data mining process implementations. Fuzzy clustering provides more flexibility than non-fuzzy methods by allowing each data record to belong to more than one cluster to some degree. However, a serious challenge in fuzzy clustering is the lack of scalability. Massive datasets in emerging fields such as geosciences, biology, and networking do require parallel and distributed computations with high performance to solve real-world problems. Although some clustering methods are already improved to execute on big data platforms, their execution time is highly increased for gigantic datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering method named BigFCM is proposed and designed for the Hadoop distributed data platform. Based on the MapReduce programming model, the proposed algorithm exploits several mechanisms including an efficient caching design to achieve several orders of magnitude reduction in execution time. The BigFCM performance compared with Apache Mahout K-Means and Fuzzy K-Means through an evaluation framework developed in this research. Extensive evaluation using over multi-gigabyte datasets including SUSY and HIGGS shows that BigFCM is scalable while it preserves the quality of clustering.
ER  - 
TY  - JOUR
T1  - A data analytics approach to building a clinical decision support system for diabetic retinopathy: Developing and deploying a model ensemble
A1  - Piri, Saeed
A1  - Delen, Dursun
A1  - Liu, Tieming
A1  - Zolbanin, Hamed M
Y1  - 2017///
KW  -  Clinical decision support systems
KW  -  Data analytics
KW  -  Model ensembles
KW  -  Predictive modeling
KW  -  Variable importance
KW  - Diabetic retinopathy
JF  - Decision Support Systems
VL  - 101
SP  - 12
EP  - 27
DO  - https://doi.org/10.1016/j.dss.2017.05.012
UR  - https://www.sciencedirect.com/science/article/pii/S0167923617300908
N2  - Diabetes is a common chronic disease that may lead to several complications. Diabetic retinopathy (DR), one of the most serious of these complications, is the most common cause of vision loss among diabetic patients. In this paper, we analyzed data from more than 1.4 million diabetics and developed a clinical decision support system (CDSS) for predicting DR. While the existing diagnostic approach requires access to ophthalmologists and expensive equipment, our CDSS only uses demographic and lab data to detect patients' susceptibility to retinopathy with a high accuracy. We illustrate how a combination of multiple data preparation and modeling steps helped us improve the performance of our CDSS. From the data preprocessing aspect, we aggregated the data at the patient level and incorporated comorbidity information into our models. From the modeling perspective, we built several predictive models and developed a novel “confidence margin” ensemble technique that outperformed the existing ensemble models. Our results suggest that diabetic neuropathy, creatinine serum, blood urea nitrogen, glucose serum plasma, and hematocrit are the most important variables in detecting DR. Our CDSS provides several important practical implications, including identifying the DR risk factors, facilitating the early diagnosis of DR, and solving the problem of low compliance with annual retinopathy screenings.
ER  - 
TY  - JOUR
T1  - Deep convolutional neural networks for diabetic retinopathy detection by image classification
A1  - Wan, Shaohua
A1  - Liang, Yan
A1  - Zhang, Yin
Y1  - 2018///
KW  -  Convolutional neural networks
KW  -  Fundus images classification
KW  -  Transfer learning
KW  - Diabetic retinopathy
JF  - Computers & Electrical Engineering
VL  - 72
SP  - 274
EP  - 282
DO  - https://doi.org/10.1016/j.compeleceng.2018.07.042
UR  - https://www.sciencedirect.com/science/article/pii/S0045790618302556
N2  - Diabetic retinopathy (DR) is a common complication of diabetes and one of the major causes of blindness in the active population. Many of the complications of DR can be prevented by blood glucose control and timely treatment. Since the varieties and the complexities of DR, it is really difficult for DR detection in the time-consuming manual diagnosis. This paper is to attempt towards finding an automatic way to classify a given set of fundus images. We bring convolutional neural networks (CNNs) power to DR detection, which includes 3 major difficult challenges: classification, segmentation and detection. Coupled with transfer learning and hyper-parameter tuning, we adopt AlexNet, VggNet, GoogleNet, ResNet, and analyze how well these models do with the DR image classification. We employ publicly available Kaggle platform for training these models. The best classification accuracy is 95.68% and the results have demonstrated the better accuracy of CNNs and transfer learning on DR image classification.
ER  - 
TY  - JOUR
T1  - Computer based prognosis model with dimensionality reduction and validation of attributes for prolonged survival prediction
A1  - Raji, C G
A1  - Anand, H S
A1  - Vinod Chandra, S S
Y1  - 2017///
KW  -  Association rule mining algorithms
KW  -  Data mining
KW  -  Data preprocessing
KW  -  Principal component analysis
KW  -  Ranking
KW  -  Survival prediction model
KW  - Liver transplantation
JF  - Informatics in Medicine Unlocked
VL  - 9
SP  - 93
EP  - 106
DO  - https://doi.org/10.1016/j.imu.2017.07.002
UR  - https://www.sciencedirect.com/science/article/pii/S2352914816300326
N2  - Medical databases contain large volume of data about patients and their clinical information. For extracting the features and their relationships from a huge database, various data mining techniques need to be employed. As Liver transplantation is the curative surgical procedure for the patients suffering from end stage liver disease, predicting the survival rate after Liver transplantation has a big impact. Appropriate selection of attributes and methods are necessary for the survival prediction. Liver transplantation data with 256 attributes were collected from 389 attributes of the United Nations Organ Sharing registry for the survival prediction. Initially 59 attributes were filtered manually, and then Principal Component Analysis (PCA) was applied for reducing the dimensionality of the data. After performing PCA, 197 attributes were obtained and they were ranked into 27 strong/relevant attributes. Using association rule mining techniques, the association between the selected attributes was identified and verified. Comparison of rules generated by various association rules mining algorithm before and after PCA was also carried out for affirming the results. The various rule mining algorithms used were Apriori, Treap mining and Tertius algorithms. Among these algorithms, Treap mining algorithm generated the rules with high accuracy. A Multilayer Perceptron model was built for predicting the long term survival of patients after Liver transplantation which produced high accuracy prediction result. The model performance was compared with Radial Basis Function model to prove the accuracy of survival of liver patients'. The top ranked attributes obtained from rule mining were fed to the models for effective training. This ensures that Treap mining generated associations of high impact attributes which in-turn made the survival prediction flawless.
ER  - 
TY  - JOUR
T1  - Diabetic Retinopathy: Present and Past
A1  - Gupta, Ankita
A1  - Chhikara, Rita
Y1  - 2018///
KW  -  Blood vessels segmentation
KW  -  Digital fundus images
KW  -  Lesions
KW  - Diabetic retinopathy
JF  - Procedia Computer Science
VL  - 132
SP  - 1432
EP  - 1440
DO  - https://doi.org/10.1016/j.procs.2018.05.074
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918308068
N1  - International Conference on Computational Intelligence and Data Science
N2  - Diabetes, a chronic disease affects various organs of human body including the retina. Diabetic Retinopathy (DR) results from the Diabetes Mellitus (DM). In literature various machine learning algorithms have been applied in detection of DR. This involves two steps; Feature extraction and Classification. This paper reviews the various techniques used for detecting DR based on the features like blood vessels, microaneurysms, haemorrhages etc. In most of the experiments retinal fundus images were used in which images of retina were captured by fundus camera. This review bifurcates the detection of DR into two approaches; Blood vessels segmentation and Identification of lesions. This paper compares the experimental results of various machine learning techniques based on parameters like sensitivity, specificity, Area Under Curve (AUC), Accuracy. The results are also compared with the deep neural networks and analysis of best technique has been provided.
ER  - 
TY  - JOUR
T1  - A new hybrid model based on secondary decomposition, reinforcement learning and SRU network for wind turbine gearbox oil temperature forecasting
A1  - Liu, Hui
A1  - Yu, Chengqing
A1  - Yu, Chengming
Y1  - 2021///
KW  -  Reinforcement Learning
KW  -  Secondary decomposition method
KW  -  Simple Recurrent Unit
KW  - Oil temperature forecasting
JF  - Measurement
VL  - 178
SP  - 109347
EP  - 109347
DO  - https://doi.org/10.1016/j.measurement.2021.109347
UR  - https://www.sciencedirect.com/science/article/pii/S0263224121003432
N2  - Oil temperature forecasting technology can realize real-time detection of the gearbox status of wind turbines. To make the oil temperature forecasting more accurate, a new hybrid model is presented in this study. The main modeling process of the presented method consists of three main steps. In step I, the proposed secondary decomposition method is utilized to preprocess the raw oil temperature data. In step II, the feature selection algorithm based on reinforcement learning selects the features of each sub-series. In step III, the simple recurrent unit network establishes forecasting models for each sub-series after feature selection and obtains the final forecasting results. By analyzing the forecasting results of multiple experiments, it can be concluded that: (1) the presented hybrid model can obtain satisfying forecasting results. Its RMSE values are 0.1101 °C, 0.1683 °C, and 0.1784 °C in three cases. (2) The presented hybrid model can get higher forecasting accuracy than the seventeen alternative models and six existing models in all cases. It improves the performance of traditional neural networks by over 90 percent.
ER  - 
TY  - JOUR
T1  - A novel retinal vessel detection approach based on multiple deep convolution neural networks
A1  - Guo, Yanhui
A1  - Budak, Ümit
A1  - Şengür, Abdulkadir
Y1  - 2018///
KW  -  Image segmentation
KW  -  Multiple deep convolution neural network
KW  - Retinal vessels segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 167
SP  - 43
EP  - 48
DO  - https://doi.org/10.1016/j.cmpb.2018.10.021
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718307818
N2  - Background and objective
Computer aided detection (CAD) offers an efficient way to assist doctors to interpret fundus images. In a CAD system, retinal vessel (RV) detection is a crucial step to identify the retinal disease regions. However, RV detection is still a challenging problem due to variations in morphology of the vessels on noisy and low contrast fundus images.
Methods
In this paper, we formulate the detection task as a classification problem and solve it using a multiple classifier framework based on deep convolutional neural networks. The multiple deep convolutional neural network (MDCNN) is constructed and trained on fundus images with limited image quantity. The MDCNN is trained using an incremental learning strategy to improve the networks’ performance. The final classification results are obtained from the voting procedure on the results of MDCNN.
Results
The MDCNN achieves better performance and significantly outperforms the state-of-the-art for automatic retinal vessel segmentation on the DRIVE dataset with 95.97% and 96.13% accuracy and 0.9726 and 0.9737 AUC (area below the operator receiver character curve) score on training and testing sets, respectively. Another public dataset, STARE, is also used to evaluate the proposed network. The experimental results demonstrate that the proposed MDCNN network achieves 95.39% accuracy and 0.9539 AUC score in STARE dataset. We further compare our result with several state-of-the-art methods based on AUC values. The comparison is shown that our proposal yields the third best AUC value.
Conclusions
Our method yields the better performance in the compared the state of the art methods. In addition, our proposal has no preprocessing stage, and the input color fundus images are fed into the CNN directly.
ER  - 
TY  - JOUR
T1  - Modified Alexnet architecture for classification of diabetic retinopathy images
A1  - Shanthi, T
A1  - Sabeenian, R S
Y1  - 2019///
KW  -  Accuracy
KW  -  Classification
KW  -  Convolutional neural network
KW  -  Messidor database
KW  - Diabetic retinopathy
JF  - Computers & Electrical Engineering
VL  - 76
SP  - 56
EP  - 64
DO  - https://doi.org/10.1016/j.compeleceng.2019.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S0045790618334190
N2  - Diabetic retinopathy (DR) is an illness occurring in the eye due to increase in blood glucose level. Among people in the age group of 70, 50% of deaths are attributed to diabetes. Early identification and appropriate treatment can reduce the loss of sight in many DR patients. Once the symptoms of DR are recognized, the severity of the disease should be evaluated for administering the right medication. This paper focuses on the classification of DR fundus images according to the severity of the disease using convolutional neural network with the application of suitable Pooling, Softmax and Rectified Linear Activation Unit (ReLU) layers to obtain a high level of accuracy. The performance of the proposed algorithm has been validated using Messidor database. In the case of healthy images, images of stage1, stage 2 and stage 3 of diabetic retinopathy, classification accuracies of 96.6% and 96.2%, 95.6% and 96.6% have been achieved.
ER  - 
TY  - JOUR
T1  - Mining heart disease risk factors in clinical text with named entity recognition and distributional semantic models
A1  - Urbain, Jay
Y1  - 2015///
KW  -  Clinical informatics
KW  -  Diabetes
KW  -  Distributional semantic models
KW  -  Heart disease risk factors
KW  -  Named entity recognition
KW  -  Natural language processing
KW  -  Translational research
KW  - Biomedical text mining
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - S143
EP  - S149
DO  - https://doi.org/10.1016/j.jbi.2015.08.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001768
N1  - Supplement: Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data
N2  - We present the design, and analyze the performance of a multi-stage natural language processing system employing named entity recognition, Bayesian statistics, and rule logic to identify and characterize heart disease risk factor events in diabetic patients over time. The system was originally developed for the 2014 i2b2 Challenges in Natural Language in Clinical Data. The system’s strengths included a high level of accuracy for identifying named entities associated with heart disease risk factor events. The system’s primary weakness was due to inaccuracies when characterizing the attributes of some events. For example, determining the relative time of an event with respect to the record date, whether an event is attributable to the patient’s history or the patient’s family history, and differentiating between current and prior smoking status. We believe these inaccuracies were due in large part to the lack of an effective approach for integrating context into our event detection model. To address these inaccuracies, we explore the addition of a distributional semantic model for characterizing contextual evidence of heart disease risk factor events. Using this semantic model, we raise our initial 2014 i2b2 Challenges in Natural Language of Clinical data F1 score of 0.838 to 0.890 and increased precision by 10.3% without use of any lexicons that might bias our results.
ER  - 
TY  - JOUR
T1  - Predicting healthcare trajectories from medical records: A deep learning approach
A1  - Pham, Trang
A1  - Tran, Truyen
A1  - Phung, Dinh
A1  - Venkatesh, Svetha
Y1  - 2017///
KW  -  Healthcare processes
KW  -  Irregular timing
KW  -  Long-Short Term Memory
KW  -  Predictive medicine
KW  - Electronic medical records
JF  - Journal of Biomedical Informatics
VL  - 69
SP  - 218
EP  - 229
DO  - https://doi.org/10.1016/j.jbi.2017.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417300710
N2  - Personalized predictive medicine necessitates the modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, stored in electronic medical records are episodic and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural network that reads medical records, stores previous illness history, infers current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors and models patient health state trajectories by the memory of historical records. Built on Long Short-Term Memory (LSTM), DeepCare introduces methods to handle irregularly timed events by moderating the forgetting and consolidation of memory. DeepCare also explicitly models medical interventions that change the course of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling, intervention recommendation, and future risk prediction. On two important cohorts with heavy social and economic burden – diabetes and mental health – the results show improved prediction accuracy.
ER  - 
TY  - JOUR
T1  - Multiple models for artificial pancreas predictions identified from free-living condition data: A proof of concept study
A1  - Toffanin, C
A1  - Aiello, E M
A1  - Del Favero, S
A1  - Cobelli, C
A1  - Magni, L
Y1  - 2019///
KW  -  Artificial pancreas
KW  -  Clinical trial
KW  -  MPC
KW  -  Type 1 diabetes
KW  -  Validation
KW  - Identification
JF  - Journal of Process Control
VL  - 77
SP  - 29
EP  - 37
DO  - https://doi.org/10.1016/j.jprocont.2019.03.007
UR  - https://www.sciencedirect.com/science/article/pii/S0959152419301660
N2  - The artificial pancreas (AP) is a closed-loop system to automatically regulate the glucose concentration in patients with type 1 diabetes (T1D). Model predictive control (MPC) revealed to be one of the most promising approaches for this control problem. Several MPC algorithms have been tested in clinical trials with satisfactorily results. However, the inter-patient variability characterising T1D patients limits the performance of MPC algorithms synthesized on average models and calls for patient-tailored models. The availability of experimental data on long outpatient trials motivated the study of identification techniques applicable to free-living patient data. Moreover, a detailed data analysis can be used to improve model identification. Considered that the postprandial (PP) glucose control is one of the most critical aspects of glucose regulation, the analysis was focused on the PP period. The intra-day variability is investigated via ANOVA test that highlighted a correlation between PP glucose profiles and different day periods (DPs). A data-driven multiple-model predictor (MMP) based on real-data analysis is proposed in this work. It exploits different identified models on the basis of the knowledge acquired through the data analysis. In particular, the MMP uses three basic models specific of each DP. These models have been identified through the impulse-response technique that achieved promising results in model identification from real-data. The prediction capabilities of the MMP are compared to the performance of a predictor built using a single model identified on a daily subset, showing an improvement in terms of predictions capabilities in the breakfast DP.
ER  - 
TY  - JOUR
T1  - Automatic identification of diabetic retinopathy stages by using fundus images and visibility graph method
A1  - Mohammadpoory, Zeynab
A1  - Nasrolahzadeh, Mahda
A1  - Mahmoodian, Naghmeh
A1  - Haddadnia, Javad
Y1  - 2019///
KW  -  Error-correcting output codes
KW  -  Fundus image
KW  -  Radon transform
KW  -  Visibility graph
KW  - Diabetic retinopathy
JF  - Measurement
VL  - 140
SP  - 133
EP  - 141
DO  - https://doi.org/10.1016/j.measurement.2019.02.089
UR  - https://www.sciencedirect.com/science/article/pii/S0263224119302064
N2  - Diabetic retinopathy (DR) is one of the problems caused due to the diabetes disease in which the retina is damaged because fluid leaks into the retina from the blood vessels. In extreme cases, the patient may loss vision. Therefore, determination of DR grades has an important role in the treatment process of the disease and preventing vision loss. Different image processing based methods have been proposed to detect the different stages of DR automatically. In this paper, a method based on Radon transform (RT) and visibility graph (VG) was proposed to automatically discriminate grades 0 (normal), 1, 2 and 3 of the DR from fundus images. The proposed method is summarized in two stages: feature extraction and classification. In this study, for the first time, the VG method was employed in the image processing field for feature extraction. Then, these features were given to error-correcting output codes (ECOC) method for classification purposes. The proposed method was easy enjoying an accuracy of 97.92%, a sensitivity of 95.83% and a specificity of 98.61%. The VG based method can be a very easy, cheap, and effective test for the automatic grading of DR stages and it can apply in other image processing application.
ER  - 
TY  - JOUR
T1  - Cloud based intelligent system for delivering health care as a service
A1  - Kaur, Pankaj Deep
A1  - Chana, Inderveer
Y1  - 2014///
KW  -  Application behavior
KW  -  Diabetes detection
KW  -  Quality of service
KW  -  Resource elasticity
KW  - Cloud computing
JF  - Computer Methods and Programs in Biomedicine
VL  - 113
IS  - 1
SP  - 346
EP  - 359
DO  - https://doi.org/10.1016/j.cmpb.2013.09.013
UR  - https://www.sciencedirect.com/science/article/pii/S0169260713003209
N2  - The promising potential of cloud computing and its convergence with technologies such as mobile computing, wireless networks, sensor technologies allows for creation and delivery of newer type of cloud services. In this paper, we advocate the use of cloud computing for the creation and management of cloud based health care services. As a representative case study, we design a Cloud Based Intelligent Health Care Service (CBIHCS) that performs real time monitoring of user health data for diagnosis of chronic illness such as diabetes. Advance body sensor components are utilized to gather user specific health data and store in cloud based storage repositories for subsequent analysis and classification. In addition, infrastructure level mechanisms are proposed to provide dynamic resource elasticity for CBIHCS. Experimental results demonstrate that classification accuracy of 92.59% is achieved with our prototype system and the predicted patterns of CPU usage offer better opportunities for adaptive resource elasticity.
ER  - 
TY  - JOUR
T1  - Exploration of text matching methods in Chinese disease Q&A systems: A method using ensemble based on BERT and boosted tree models
A1  - Wu, Ziming
A1  - Liang, Jun
A1  - Zhang, Zhongan
A1  - Lei, Jianbo
Y1  - 2021///
KW  -  BERT
KW  -  Boosted tree model
KW  -  Feature engineering
KW  - Text matching
JF  - Journal of Biomedical Informatics
VL  - 115
SP  - 103683
EP  - 103683
DO  - https://doi.org/10.1016/j.jbi.2021.103683
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000125
N2  - Background
Text matching is one of the basic tasks in the field of natural language processing. Owing to the particularity of Chinese language and medical texts, text matching has greater application and research value in the medical field. In 2019, at the China Health Information Processing Conference (CHIP), 30,000 sets of real disease Q&A data in Chinese on diabetes, hypertension, hepatitis B, AIDS, and breast cancer were released for public evaluation. A total of 90 teams participated in the evaluation.
Purpose
To explore the best method of text matching of Chinese medical Q&A data by participating in an evaluation competition.
Method
After analyzing the Chinese medical Q&A data provided by the competition, we used the bidirectional encoder representations from transformers (BERT) model and a boosted tree model to compare the effects. At the same time, we analyzed the importance of the features extracted through feature engineering. Finally, we integrated the BERT and boosted tree models, and proved the effectiveness of the ensemble through a correlation analysis.
Results
The final F1 score of the ensemble model is 0.90825, ranking first among the 90 participating teams. The highest F1 score of the single BERT model is 0.87443, whereas the highest F1 score of the boosted tree single model is only 0.86915. The F1 score of the BERT multi-model ensemble is 0.87473 (an average increase of 0.756% compared to the single model), and the F1 score of the boosted tree multi-model ensemble is 0.86720 (an average decrease of 0.03% compared to the single model). In the feature importance experiment, the out-degree and in-degree of the Q&A sentence are of utmost importance. In the correlation experiment, the correlation coefficients between models of the same type are all as high as 0.9, which shows a high similarity. The correlation coefficient between different types of models is approximately 0.7, which shows a certain degree of discrimination. With the ensemble of the two types of models, the F1 score reached 0.90825, which is 3.88% higher than that of the optimal single model.
Conclusion
In our study, the proposed model ensemble method was shown to effectively improve the performance of a single model. It achieves good results in Chinese medical Q&A tasks and has a good generalization property.
ER  - 
TY  - JOUR
T1  - Towards non-invasive blood glucose measurement using machine learning: An all-purpose PPG system design
A1  - Sen Gupta, Shantanu
A1  - Kwon, Tae-Ho
A1  - Hossain, Shifat
A1  - Kim, Ki-Doo
Y1  - 2021///
KW  -  Biomedical device
KW  -  Diabetes
KW  -  Features
KW  -  Glucose
KW  -  Machine learning
KW  - Photoplethysmography
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102706
EP  - 102706
DO  - https://doi.org/10.1016/j.bspc.2021.102706
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421003037
N2  - Diabetes, the result of excessive or uncontrolled glucose in the blood, is one of the leading causes of human mortality. Due to the unavailability of non-invasive glucose level checker until now, the most trustworthy day-to-day life glucose test for personal healthcare is the use of glucometer in which case painful finger pricking is an obvious part. However, researches have been done to prove the usage of pulse oximeter to measure the blood glucose level besides other physiological indicators such as heart rate, percentage of blood oxygen, etc. Here, as the first of two studies, we try to develop an all-purpose commercial prototype photoplethysmography (PPG) system to monitor necessary health indicator parameters in a non-invasive way. The developed fingertip PPG device consists of both transmissive and reflective type data acquisition system after illuminating the skin with red, green, and IR LEDs. Next, as the second study, special consideration is given to prove the efficiency of the device for measuring blood glucose level (BGL). To measure blood glucose from PPG signal, a few discriminative and related features are extracted from the obtained PPG signals. Machine learning algorithms are employed to predict the actual value of BGL from the extracted features. The proposed algorithm and system can predict the BGL level with a level of clinical accuracy. In the Clarke error grid plot, 96.15% and 3.85% of data are in the zone A and zone B, respectively, with 0% data in the critical zones.
ER  - 
TY  - JOUR
T1  - Analysis of embedded medical system and nursing care of pediatric severe infection
A1  - Liu, Yanping
A1  - Jin, Bo
A1  - Li, Wenhui
A1  - Wang, Yan
Y1  - 2021///
KW  -  Electrocardiography (ecg)
KW  -  Pediatric Severe Infection
KW  -  Raspberry pi
KW  - Pediatric monitoring system (pms)
JF  - Microprocessors and Microsystems
VL  - 83
SP  - 104003
EP  - 104003
DO  - https://doi.org/10.1016/j.micpro.2021.104003
UR  - https://www.sciencedirect.com/science/article/pii/S0141933121001769
N2  - ABSTRACT
Health is one of the global challenges for humanity. In the past ten years, health care has attracted considerable attention. The primary goal is to establish and monitor the pediatric medical staff who has not been admitted, the pediatric monitoring system, and reliable to carry out their normal daily life activities. Recently, the pediatric monitoring system is one of the significant advances of the improved technology. Currently, a modern way has been required. The critical role that experts traditionally play in health care. Need to visit the pediatric ward for infectious diseases and advice necessary. There are two fundamental problems associated with this approach. Second, the child is still admitted to the hospital with bedside biomedical equipment for some time, and first, medical staff must be present at all times on the pediatric website. To solve these two problems, the pediatric department provides knowledge and information on preventing infections and diseases. Second, there is a need for a reliable, easily accessible Pediatric Surveillance System (PMS). To improve the above situation, it can use technology in smarter ways. In recent years, along with the Raspberry Pi, medical sensors have played a significant role. The wearable sensor monitors the contact between the human body and his or her physiological parameters. Today, buy various sensors in the market, such as Electrocardiography (ECG) sensors, temperature sensors, pulse monitors, etc. The cost of the sensor varies in flexibility and accuracy, depending on its size.
ER  - 
TY  - JOUR
T1  - A Classification Framework using a Diverse Intensified Strawberry Optimized Neural Network (DISON) for Clinical Decision-making
A1  - Sreejith, S
A1  - Khanna Nehemiah, H
A1  - Kannan, A
Y1  - 2020///
KW  -  ANN
KW  -  Bio-inspired computing
KW  -  Extremely randomized trees
KW  -  Strawberry plant optimization
KW  - Clinical data
JF  - Cognitive Systems Research
VL  - 64
SP  - 98
EP  - 116
DO  - https://doi.org/10.1016/j.cogsys.2020.08.003
UR  - https://www.sciencedirect.com/science/article/pii/S1389041720300498
N2  - A novel classification framework for clinical decision making that uses an Extremely Randomized Tree (ERT) based feature selection and a Diverse Intensified Strawberry Optimized Neural network (DISON) is proposed. DISON is a Feed Forward Artificial Neural Network where the optimization of weights and bias is done using a two phase training strategy. Two algorithms namely Strawberry Plant Optimization (SPO) algorithm and Gradient-descent Back-propagation algorithm are used sequentially to identify the optimum weights and bias. The novel two phase training method and the stochastic duplicate-elimination strategy of SPO helps in addressing the issue of local optima associated with conventional neural networks. The relevant attributes are selected based on the feature importance values computed using an ERT classifier. Vertebral Column, Pima Indian diabetes (PID), Cleveland Heart disease (CHD) and Statlog Heart disease (SHD) datasets from the University of California Irvine machine learning repository are used for experimentation. The framework has achieved an accuracy of 87.17% for Vertebral Column, 90.92% for PID, 93.67% for CHD and 94.5% for SHD. The classifier performance has been compared with existing works and is found to be competitive in terms of accuracy, sensitivity and specificity. Wilcoxon test confirms the statistical superiority of the proposed method.
ER  - 
TY  - JOUR
T1  - Risk markers identification in EHR using natural language processing: hemorrhagic and ischemic stroke cases
A1  - Grechishcheva, Sofia
A1  - Efimov, Egor
A1  - Metsker, Oleg
Y1  - 2019///
KW  -  data extraction
KW  -  electronic health records
KW  -  hemorrhagic stroke
KW  -  ischemic stroke
KW  -  latent semantic analysis
KW  - Natural language processing
JF  - Procedia Computer Science
VL  - 156
SP  - 142
EP  - 149
DO  - https://doi.org/10.1016/j.procs.2019.08.189
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919311081
N1  - 8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece
N2  - This article describes the study results in the development of the method of analysis of semi-structured data from electronic health records to improve the quality of data describing patients’ treatment processes. Improving the accuracy of information retrieval from electronic medical records was achieved by using developed problem-solving oriented library. Moreover, the latent-semantic analysis of the electronic health records of chronic patients with chronic heart failure, diabetes mellitus, hypertension was performed. The main tokens characterizing different groups of patients were revealed. The developed library and semantic analysis based on it can be used to accurately automatic extraction of information from semi-structured electronic medical records. Automated markup of medical texts on the Russian language is also possible for the development of artificial intelligence systems and new generation clinical decision support systems.
ER  - 
TY  - JOUR
T1  - Deep supervised learning with mixture of neural networks
A1  - Hu, Yaxian
A1  - Luo, Senlin
A1  - Han, Longfei
A1  - Pan, Limin
A1  - Zhang, Tiemei
Y1  - 2020///
KW  -  Diabetes determination
KW  -  Expectation maximization
KW  -  Mixture model
KW  - Deep neural network
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101764
EP  - 101764
DO  - https://doi.org/10.1016/j.artmed.2019.101764
UR  - https://www.sciencedirect.com/science/article/pii/S093336571830366X
N2  - Deep Neural Network (DNN), as a deep architectures, has shown excellent performance in classification tasks. However, when the data has different distributions or contains some latent non-observed factors, it is difficult for DNN to train a single model to perform well on the classification tasks. In this paper, we propose mixture model based on DNNs (MoNNs), a supervised approach to perform classification tasks with a gating network and multiple local expert models. We use a neural network as a gating function and use DNNs as local expert models. The gating network split the heterogeneous data into several homogeneous components. DNNs are combined to perform classification tasks in each component. Moreover, we use EM (Expectation Maximization) as an optimization algorithm. Experiments proved that our MoNNs outperformed the other compared methods on determination of diabetes, determination of benign or malignant breast cancer, and handwriting recognition. Therefore, the MoNNs can solve the problem of data heterogeneity and have a good effect on classification tasks.
ER  - 
TY  - JOUR
T1  - Hardware Based Analysis on Automated Early Detection of Diabetic-Retinopathy
A1  - Datta, N S
A1  - Banerjee, R
A1  - Dutta, H S
A1  - Mukhopadhyay, S
Y1  - 2012///
KW  -  Adaptive histogram equalization
KW  -  Color-space conversion
KW  -  Fundus image
KW  - Diabetic Retinopathy
JF  - Procedia Technology
VL  - 4
SP  - 256
EP  - 260
DO  - https://doi.org/10.1016/j.protcy.2012.05.039
UR  - https://www.sciencedirect.com/science/article/pii/S2212017312003180
N1  - 2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012
N2  - This paper discusses primarily the hardware based issues on early detection of diabetic retinopathy. Software based algorithms for preprocessing, segmentation, and, classification stages are initially analyzed. Later those techniques were customized and implemented using TMS320C6713 based DSP Kits of Texas instruments with code composer studio for the early detection of diabetic retinopathy through the fundus images of retina. The hardware based implementation shows more effective results as compared to other existing approaches.
ER  - 
TY  - JOUR
T1  - Knowledge-driven machine learning based framework for early-stage disease risk prediction in edge environment
A1  - Hossain, M Anwar
A1  - Ferdousi, Rahatara
A1  - Alhamid, Mohammed F
Y1  - 2020///
KW  -  Disease likelihood
KW  -  Epidemiology
KW  -  Healthcare
KW  -  Self-screening
KW  - Machine learning
JF  - Journal of Parallel and Distributed Computing
VL  - 146
SP  - 25
EP  - 34
DO  - https://doi.org/10.1016/j.jpdc.2020.07.003
UR  - https://www.sciencedirect.com/science/article/pii/S0743731520303324
N2  - Early-stage disease risk prediction can be beneficial to improve the health of the mass and can reduce the economic burden of late treatment. Machine learning has played a pivotal role in predictive systems, which requires achieving a specific degree of accuracy for healthcare systems. Most recently researchers have found the necessity of bridging between epidemiology and machine learning classifications toward health risk prediction. This work proposes an epidemiology knowledge-driven unique model that follows the principle of association rule-based ontology to select features and classification techniques. The goal of this approach is to generalize a framework for future robust systems to predict the likelihood of diseases, which can be executed in the edge computing environment. The framework introduces epidemiological library and structured attribute set along with the library of precaution to derive the disease risk-prediction process. To investigate the adoption of the epidemiology knowledge-driven model, we considered a real dataset of early-stage likelihood prediction of diabetes and carried out a set of experiments for highlighting the significance of several epidemiological factors. The classification aspect of the framework is further compared with widely accepted approaches for machine learning based healthcare, which shows the novelty of the proposed model.
ER  - 
TY  - JOUR
T1  - An automated diagnostic system of polycystic ovary syndrome based on object growing
A1  - Deng, Yinhui
A1  - Wang, Yuanyuan
A1  - Shen, Yuzhong
Y1  - 2011///
KW  -  Automated detection of follicles
KW  -  Polycystic ovary syndrome
KW  -  Ultrasound images
KW  - Object growing
JF  - Artificial Intelligence in Medicine
VL  - 51
IS  - 3
SP  - 199
EP  - 209
DO  - https://doi.org/10.1016/j.artmed.2010.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S0933365710001211
N2  - Objective
Polycystic ovary syndrome (PCOS) is a complex endocrine disorder that seriously affects women's health. The disorder is characterized by the formation of many follicles in the ovary. Currently the predominant diagnosis is to manually count the number of follicles, which may lead to inter-observer and intra-observer variability and low efficiency. A computer-aided PCOS diagnostic system may overcome these problems. However the methods reported in recently published literature are not very effective and often need human interaction. To overcome these problems, we propose an automated PCOS diagnostic system based on ultrasound images.
Methods and materials
The proposed system consists of two major functional blocks: preprocessing phase and follicle identification based on object growing. In the preprocessing phase, speckle noise in the input image is removed by an adaptive morphological filter, then contours of objects are extracted using an enhanced labeled watershed algorithm, and finally the region of interest is automatically selected. The object growing algorithm for follicle identification first computes a cost map to distinguish between the ovary and its external region and assigns each object a cost function based on the cost map. The object growing algorithm initially selects several objects that are likely to be follicles with very high probabilities and dynamically update the set of possible follicles based on their cost functions. The proposed method was applied to 31 real PCOS ultrasound images obtained from patients and its performance was compared with those of three other methods, including level set method, boundary vector field (BVF) method and the fuzzy support vector machine (FSVM) classifier.
Results
Based on the judgment of subject matter experts, the proposed diagnostic system achieved 89.4% recognition rate (RR) and 7.45% misidentification rate (MR) while the RR and MR of the level set method, the BVF method and the FSVM classifier are around 65.3% and 2.11%, 76.1% and 4.53%, and 84.0% and 16.3%, respectively. The proposed diagnostic system also achieved better performance than those reported in recently published literature.
Conclusion
The paper proposed an automated diagnostic system for the PCOS using ultrasound images, which consists of two major functional blocks: preprocessing phase and follicle identification based on object growing. Experimental results showed that the proposed system is very effective in follicle identification for PCOS diagnosis.
ER  - 
TY  - JOUR
T1  - Automatic detection of neovascularization in retinal images using extreme learning machine
A1  - Huang, He
A1  - Ma, He
A1  - JW van Triest, Han
A1  - Wei, Yinghua
A1  - Qian, Wei
Y1  - 2018///
KW  -  Extreme learning machine
KW  -  Retinal image
KW  - Detection of neovascularization
JF  - Neurocomputing
VL  - 277
SP  - 218
EP  - 227
DO  - https://doi.org/10.1016/j.neucom.2017.03.093
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217313991
N1  - Hierarchical Extreme Learning Machines
N2  - Diabetic Retinopathy is one complication of diabetes, which can cause blindness. Diabetic retinopathy can be divided into Non-Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopahy (PDR), and neovascularization is a key symbol to make diagnosis between them. An automatic detection of neovascularization in retinal images using extreme learning machine is proposed. Furthermore, we use a series of filter banks to get the features of neovascularization from retinal images. The detection framework is evaluated with images annotated by expert ophthalmologists based on the images selected from several public retinal image databases. The experimental results illustrate that the framework can mark and show the suspected neovascularization regions to ophthalmologists, and thus support for their decision making.
ER  - 
TY  - JOUR
T1  - Deep learning based semantic personalized recommendation system
A1  - Sharma, Sunny
A1  - Rana, Vijay
A1  - Kumar, Vivek
Y1  - 2021///
KW  -  Personalization
KW  -  Recommendation system
KW  -  Semantics
KW  - Deep learning
JF  - International Journal of Information Management Data Insights
VL  - 1
IS  - 2
SP  - 100028
EP  - 100028
DO  - https://doi.org/10.1016/j.jjimei.2021.100028
UR  - https://www.sciencedirect.com/science/article/pii/S2667096821000215
N2  - The past decade has seen significant development in the number of personalized recommendation applications on the World Wide Web. It aims to assist users to retrieve relevant items from a large repository of contents by providing items or services of likely interest based on examined evidence of the users’ preferences and desires. However, this vision is complex due to the huge amount of information aka media-rich information available on the web. Most of the systems formulated so far use the metadata linked with the digital contents, but such systems fail to generate significant recommendations results. In these circumstances, a semantic personalized recommendation system (SPRS) plays an important role to take away the semantic gap between high-level semantic contents and low-level media features. The proposed system recommends personalized sets of videos to users depending on their previous activity on the site and exploits a domain ontology and user items content to the domain concepts. To evaluate the performance of the framework, items’ prediction is executed by utilizing the proposed framework, and performance is determined by comparing the predicted and actual ratings of the items in terms of Predictive Accuracy Metrics, precision, and recall.
ER  - 
TY  - JOUR
T1  - Adapting existing natural language processing resources for cardiovascular risk factors identification in clinical notes
A1  - Khalifa, Abdulrahman
A1  - Meystre, Stéphane
Y1  - 2015///
KW  -  Cardiovascular disease
KW  -  Clinical narrative
KW  -  Information extraction
KW  -  Machine learning
KW  -  Medical records
KW  -  Risk factors
KW  -  Text mining
KW  - Natural language processing
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - S128
EP  - S132
DO  - https://doi.org/10.1016/j.jbi.2015.08.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001690
N1  - Supplement: Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data
N2  - The 2014 i2b2 natural language processing shared task focused on identifying cardiovascular risk factors such as high blood pressure, high cholesterol levels, obesity and smoking status among other factors found in health records of diabetic patients. In addition, the task involved detecting medications, and time information associated with the extracted data. This paper presents the development and evaluation of a natural language processing (NLP) application conceived for this i2b2 shared task. For increased efficiency, the application main components were adapted from two existing NLP tools implemented in the Apache UIMA framework: Textractor (for dictionary-based lookup) and cTAKES (for preprocessing and smoking status detection). The application achieved a final (micro-averaged) F1-measure of 87.5% on the final evaluation test set. Our attempt was mostly based on existing tools adapted with minimal changes and allowed for satisfying performance with limited development efforts.
ER  - 
TY  - JOUR
T1  - Classification of Rest and Active Periods in Actigraphy Data Using PCA
A1  - Muns, Isaac W
A1  - Lad, Yogesh
A1  - Guardiola, Ivan G
A1  - Thimgan, Matthew
Y1  - 2017///
KW  -  Actigraphy
KW  -  Principal Component Analysis
KW  - Clustering
JF  - Procedia Computer Science
VL  - 114
SP  - 275
EP  - 280
DO  - https://doi.org/10.1016/j.procs.2017.09.041
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917318355
N1  - Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 – November 1, 2017, Chicago, Illinois, USA
N2  - In this paper we highlight a clustering algorithm for the purpose of identifying sleep and wake periods directly from actigraphy signals. The paper makes use of statistical Principal Component Analysis to identify periods of rest and activity. The aim of the proposed methodology is to develop a quick and efficient method to determine the sleep duration of an individual. In addition, a robust method that can identify sleep periods in the accelerometer data when duration, time of day varies by individual. A selected group of 10 individual’s sensor data consisting of actigraphy from an accelerometer (3-axis), near body temperature, and lux sensors from a single GENEActiv watch worn on the non-dominant hand. The actigraphy of each individual was collected 24 hours a day for a period spanning 80 days. We highlight that a simple data preprocessing stage followed with a 2 phase clustering method provides results that align with previously validated methodologies.
ER  - 
TY  - JOUR
T1  - A robust method for the automatic location of the optic disc and the fovea in fundus images
A1  - Romero-Oraá, Roberto
A1  - García, María
A1  - Oraá-Pérez, Javier
A1  - López, María I
A1  - Hornero, Roberto
Y1  - 2020///
KW  -  Computer-assisted diagnostic systems
KW  -  Fovea
KW  -  Fundus image
KW  -  Optic disc
KW  - Diabetic retinopathy
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105599
EP  - 105599
DO  - https://doi.org/10.1016/j.cmpb.2020.105599
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314322
N2  - Background and objective
The location of the optic disc (OD) and the fovea is usually crucial in automatic screening systems for diabetic retinopathy. Previous methods aimed at their location often fail when these structures do not have the standard appearance. The purpose of this work is to propose novel, robust methods for the automatic detection of the OD and the fovea.
Methods
The proposed method comprises a preprocessing stage, a method for retinal background extraction, a vasculature segmentation phase and the computation of various novel saliency maps. The main novelty of this work is the combination of the proposed saliency maps, which represent the spatial relationships between some structures of the retina and the visual appearance of the OD and fovea. Another contribution is the method to extract the retinal background, based on region-growing.
Results
The proposed methods were evaluated over a proprietary database and three public databases: DRIVE, DiaretDB1 and Messidor. For the OD, we achieved 100% accuracy for all databases except Messidor (99.50%). As for the fovea location, we also reached 100% accuracy for all databases except Messidor (99.67%).
Conclusions
Our results suggest that the proposed methods are robust and effective to automatically detect the OD and the fovea. This way, they can be useful in automatic screening systems for diabetic retinopathy as well as other retinal diseases.
ER  - 
TY  - JOUR
T1  - Computer-aided diagnosis of diabetic retinopathy: A review
A1  - Mookiah, Muthu Rama Krishnan
A1  - Acharya, U Rajendra
A1  - Chua, Chua Kuang
A1  - Lim, Choo Min
A1  - Ng, E Y K
A1  - Laude, Augustinus
Y1  - 2013///
KW  -  Computer-aided diagnosis
KW  -  Fundus imaging
KW  -  Image processing
KW  -  Pattern classification
KW  -  Retinopathy
KW  - Retina
JF  - Computers in Biology and Medicine
VL  - 43
IS  - 12
SP  - 2136
EP  - 2155
DO  - https://doi.org/10.1016/j.compbiomed.2013.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513002862
N2  - Diabetes mellitus may cause alterations in the retinal microvasculature leading to diabetic retinopathy. Unchecked, advanced diabetic retinopathy may lead to blindness. It can be tedious and time consuming to decipher subtle morphological changes in optic disk, microaneurysms, hemorrhage, blood vessels, macula, and exudates through manual inspection of fundus images. A computer aided diagnosis system can significantly reduce the burden on the ophthalmologists and may alleviate the inter and intra observer variability. This review discusses the available methods of various retinal feature extractions and automated analysis.
ER  - 
TY  - JOUR
T1  - Microaneurysm detection using color locus detection method
A1  - Yadav, Divakar
A1  - Karn, Arun Kumar
A1  - Giddalur, Anurag
A1  - Dhiman, Arti
A1  - Sharma, Sakshi
A1  - Muskan
A1  - Yadav, Arun Kr.
Y1  - 2021///
KW  -  ANN
KW  -  CNN
KW  -  Classifiers
KW  -  Decision tree
KW  -  KNN
KW  -  Logistics regression
KW  -  Random forest
KW  -  SVM
KW  - Microaneurysm
JF  - Measurement
VL  - 176
SP  - 109084
EP  - 109084
DO  - https://doi.org/10.1016/j.measurement.2021.109084
UR  - https://www.sciencedirect.com/science/article/pii/S0263224121001147
N2  - Diabetes is one of the leading health problem all over the world. Microaneurysms (MAs) are the initial diabetic retinopathy (DR) signs that are detectable at the clinical level in human. Diagnosis and treatment of DR is an important task to be performed. Many methods for identifying microaneurysms, such as: ANN, CNN and AI, have been previously suggested. In this work, we propose machine learning based classifiers to identify MAs with low computational power. The retinal images have been segmented on the basis of histograms. Then, features have been extracted from segmented images using shape, statistical and gray-level co-occurrence matrix (GLCM) methods. Thereafter, classification algorithms: Logistics regression, k-NN, SVM, Kernel SVM, Naive Bayes, decision tree, and random forest have been applied for classifying MAs versus non-MAs. The proposed model obtained accuracy of 69.7% 76.04%, 68.75%, 76.04%, 58.33%, 73.958%, and 83.33% respectively for different classification algorithms i.e. random forest outperformed the other methods with an accuracy of 83.33%.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy techniques in retinal images: A review
A1  - Salamat, Nadeem
A1  - Missen, Malik M Saad
A1  - Rashid, Aqsa
Y1  - 2019///
KW  -  Blood vessels
KW  -  Diabetic retinopathy screening
KW  -  Exudates
KW  -  Optic disc
KW  - Computer aided diagnosis
JF  - Artificial Intelligence in Medicine
VL  - 97
SP  - 168
EP  - 188
DO  - https://doi.org/10.1016/j.artmed.2018.10.009
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718300630
N2  - The diabetic retinopathy is the main reason of vision loss in people. Medical experts recognize some clinical, geometrical and haemodynamic features of diabetic retinopathy. These features include the blood vessel area, exudates, microaneurysm, hemorrhages and neovascularization, etc. In Computer Aided Diagnosis (CAD) systems, these features are detected in fundus images using computer vision techniques. In this paper, we review the methods of low, middle and high level vision for automatic detection and classification of diabetic retinopathy.We give a detailed review of 79 algorithms for detecting different features of diabetic retinopathy during the last eight years.
ER  - 
TY  - JOUR
T1  - Investigation of the severity level of diabetic retinopathy using supervised classifier algorithms
A1  - Mahendran, G
A1  - Dhanasekaran, R
Y1  - 2015///
KW  -  Exudates
KW  -  Neighbourhood component labelling
KW  -  Probabilistic neural network
KW  -  SVM classifier
KW  -  Score computation
KW  - Diabetic retinopathy
JF  - Computers & Electrical Engineering
VL  - 45
SP  - 312
EP  - 323
DO  - https://doi.org/10.1016/j.compeleceng.2015.01.013
UR  - https://www.sciencedirect.com/science/article/pii/S0045790615000191
N2  - Diabetic retinopathy is a condition that occurs in individuals with several years of diabetes mellitus and causes a characteristic group of lesions in the retina and progressively damages it. Detecting retinal fundus diseases in advance helps ophthalmologists to apply proper treatments that may cure the disease or decrease its severity and thus protect patients from vision loss. Diabetic retinopathy is usually diagnosed by ophthalmologists using dilated images that are captured by pouring a chemical solution into the patient’s eye, which causes inconvenience and irritation to the patient. In this paper, we propose a method to detect lesion exudates automatically with the aid of a non-dilated retinal fundus image to help ophthalmologists diagnose the disease. The exudates from the low contrast images are detected and localised using a neighbourhood based segmentation technique. A support vector machine (SVM) and probabilistic neural network (PNN) classifiers are proposed to assess the severity of the disease, and the results are compared with the same segmentation technique. The average classification accuracy for the SVM and PNN classifiers are determined to be 97.89% and 94.76%, respectively.
ER  - 
TY  - JOUR
T1  - Dynamic feature scaling for online learning of binary classifiers
A1  - Bollegala, Danushka
Y1  - 2017///
KW  -  Classification
KW  -  Online learning
KW  - Feature scaling
JF  - Knowledge-Based Systems
VL  - 129
SP  - 97
EP  - 105
DO  - https://doi.org/10.1016/j.knosys.2017.05.010
UR  - https://www.sciencedirect.com/science/article/pii/S0950705117302216
N2  - Scaling feature values is an important step in numerous machine learning tasks. Different features can have different value ranges and some form of a feature scaling is often required in order to learn an accurate classifier. However, feature scaling is conducted as a preprocessing task prior to learning. This is problematic in an online setting because of two reasons. First, it might not be possible to accurately determine the value range of a feature at the initial stages of learning when we have observed only a handful of training instances. Second, the distribution of data can change over time, which render obsolete any feature scaling that we perform in a pre-processing step. We propose a simple but an effective method to dynamically scale features at train time, thereby quickly adapting to any changes in the data stream. We compare the proposed dynamic feature scaling method against more complex methods for estimating scaling parameters using several benchmark datasets for classification. Our proposed feature scaling method consistently outperforms more complex methods on all of the benchmark datasets and improves classification accuracy of a state-of-the-art online classification algorithm.
ER  - 
TY  - JOUR
T1  - Finding representative patterns with ordered projections
A1  - Riquelme, José C
A1  - Aguilar-Ruiz, Jesús S
A1  - Toro, Miguel
Y1  - 2003///
KW  -  Axis-parallel classifiers
KW  -  Pattern analysis
KW  -  Preprocessing techniques
KW  - Data mining
JF  - Pattern Recognition
VL  - 36
IS  - 4
SP  - 1009
EP  - 1018
DO  - https://doi.org/10.1016/S0031-3203(02)00119-X
UR  - https://www.sciencedirect.com/science/article/pii/S003132030200119X
N2  - This paper presents a new approach to finding representative patterns for dataset editing. The algorithm patterns by ordered projections (POP), has some interesting characteristics: important reduction of the number of instances from the dataset; lower computational cost (Θ(mnlogn)) with respect to other typical algorithms due to the absence of distance calculations; conservation of the decision boundaries, especially from the point of view of the application of axis-parallel classifiers. POP works well in practice with both continuous and discrete attributes. The performance of POP is analysed in two ways: percentage of reduction and classification. POP has been compared to IB2, ENN and SHRINK concerning the percentage of reduction and the computational cost. In addition, we have analysed the accuracy of k-NN and C4.5 after applying the reduction techniques. An extensive empirical study using datasets with continuous and discrete attributes from the UCI repository shows that POP is a valuable preprocessing method for the later application of any axis-parallel learning algorithm.
ER  - 
TY  - JOUR
T1  - Microaneurysm detection in color eye fundus images for diabetic retinopathy screening
A1  - Melo, Tânia
A1  - Mendonça, Ana Maria
A1  - Campilho, Aurélio
Y1  - 2020///
KW  -  Fundus image analysis
KW  -  Microaneurysm detection
KW  -  Sliding band filter
KW  - Diabetic retinopathy screening
JF  - Computers in Biology and Medicine
VL  - 126
SP  - 103995
EP  - 103995
DO  - https://doi.org/10.1016/j.compbiomed.2020.103995
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520303267
N2  - Diabetic retinopathy (DR) is a diabetes complication, which in extreme situations may lead to blindness. Since the first stages are often asymptomatic, regular eye examinations are required for an early diagnosis. As microaneurysms (MAs) are one of the first signs of DR, several automated methods have been proposed for their detection in order to reduce the ophthalmologists’ workload. Although local convergence filters (LCFs) have already been applied for feature extraction, their potential as MA enhancement operators was not explored yet. In this work, we propose a sliding band filter for MA enhancement aiming at obtaining a set of initial MA candidates. Then, a combination of the filter responses with color, contrast and shape information is used by an ensemble of classifiers for final candidate classification. Finally, for each eye fundus image, a score is computed from the confidence values assigned to the MAs detected in the image. The performance of the proposed methodology was evaluated in four datasets. At the lesion level, sensitivities of 64% and 81% were achieved for an average of 8 false positives per image (FPIs) in e-ophtha MA and SCREEN-DR, respectively. In the last dataset, an AUC of 0.83 was also obtained for DR detection.
ER  - 
TY  - JOUR
T1  - Risk-based postprandial hypoglycemia forecasting using supervised learning
A1  - Oviedo, Silvia
A1  - Contreras, Ivan
A1  - Quirós, Carmen
A1  - Giménez, Marga
A1  - Conget, Ignacio
A1  - Vehi, Josep
Y1  - 2019///
KW  -  Bolus calculation
KW  -  Hypoglycemia prediction
KW  -  Machine learning
KW  -  Postprandial hypoglycemia
KW  -  Type 1 diabetes
KW  - Blood glucose
JF  - International Journal of Medical Informatics
VL  - 126
SP  - 1
EP  - 8
DO  - https://doi.org/10.1016/j.ijmedinf.2019.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618304970
N2  - Background
Predicting insulin-induced postprandial hypoglycemic events is critical for the safety of type 1 diabetes patients because an early warning of hypoglycemia facilitates correction of the insulin bolus before its administration. The postprandial hypoglycemic event counts can be lowered by reducing the size of the bolus based on a reliable prediction but at the cost of increasing the average blood glucose.
Methods
We developed a method for predicting postprandial hypoglycemia using machine learning techniques personalized to each patient. The proposed system enables on-line therapeutic decision making for patients using a sensor augmented pump therapy. Two risk-based approaches were developed for a window of 240 min after the meal/bolus, and they were tested based on real retrospective data from 10 patients using 70 mg/dL and 54 mg/dL as thresholds according to the consensus for Level 1 and Level 2 hypoglycemia, respectively. Due to the small size of the patient cohort, we trained personalized models for each patient.
Results
The median specificity and sensitivity were 79% and 71% for Level 1 hypoglycemia, respectively, and 81% and 77% for Level 2.
Conclusions
The results demonstrated that it is feasible to anticipate hypoglycemic events with a reasonable false-positive rate. The accuracy of the results and the trade-off between performance metrics allow its use in decision support systems for patients who wear insulin pumps.
ER  - 
TY  - JOUR
T1  - Prediction of death status on the course of treatment in SARS-COV-2 patients with deep learning and machine learning methods
A1  - Kivrak, Mehmet
A1  - Guldogan, Emek
A1  - Colak, Cemil
Y1  - 2021///
KW  -  Data Mining
KW  -  Deep Learning
KW  -  Extreme Gradient Boosting
KW  -  Machine Learning
KW  - SARS-COV-2
JF  - Computer Methods and Programs in Biomedicine
VL  - 201
SP  - 105951
EP  - 105951
DO  - https://doi.org/10.1016/j.cmpb.2021.105951
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721000250
N2  - ABSTRACT
Background and Objective
The new type of Coronavirus (2019-nCov) epidemic spread rapidly, causing more than 250 thousand deaths worldwide. The virus, which first appeared as a sign of pneumonia, was later called the SARS-COV-2 with Severe Acute Respiratory Syndrome by the World Health Organization. The SARS-COV-2 virus is triggered by binding to the Angiotensin-Converting Enzyme 2 (ACE 2) inhibitor, which is vital in cardiovascular diseases and the immune system, especially in conditions such as cerebrovascular, hypertension, and diabetes. This study aims to evaluate the prediction performance of death status based on the demographic/clinical factors (including COVID-19 severity) by data mining methods.
Methods
The dataset consists of 1603 SARS-COV-2 patients and 13 variables obtained from an open-source web address. The current dataset contains age, gender, chronic disease (hypertension, diabetes, renal, cardiovascular, etc.), some enzymes (ACE, angiotensin II receptor blockers), and COVID-19 severity, which are used to predict death status using deep learning and machine learning approaches (random forest, k-nearest neighbor, extreme gradient boosting [XGBoost]). A grid search algorithm tunes hyperparameters of the models, and predictions are assessed through performance metrics. Steps of knowledge discovery in databases are applied to obtain the relevant information.
Results
The accuracy rate of deep learning (97.15%) was more successful than the accuracy rate based on classical machine learning (92.15% for RF and 93.4% for k-NN), but the ensemble classifier XGBoost method gave the highest accuracy (99.7%). While COVID-19 severity and age calculated from XGBoost were the two most important factors associated with death status, the most determining variables for death status estimated from deep learning were COVID-19 severity and hypertension.
Conclusions
The proposed model (XGBoost) achieved the best prediction of death status based on the factors as compared to the other algorithms. The results of this study can guide patients with certain variables to take early measures and access preventive health care services before they become infected with the virus.
ER  - 
TY  - JOUR
T1  - Algorithms for digital image processing in diabetic retinopathy
A1  - Winder, R J
A1  - Morrow, P J
A1  - McRitchie, I N
A1  - Bailie, J R
A1  - Hart, P M
Y1  - 2009///
KW  -  Computer-aided diagnosis
KW  -  Digital imaging
KW  -  Image processing
KW  - Diabetic retinopathy
JF  - Computerized Medical Imaging and Graphics
VL  - 33
IS  - 8
SP  - 608
EP  - 622
DO  - https://doi.org/10.1016/j.compmedimag.2009.06.003
UR  - https://www.sciencedirect.com/science/article/pii/S0895611109000810
N2  - This work examined recent literature on digital image processing in the field of diabetic retinopathy. Algorithms were categorized into 5 steps (preprocessing; localization and segmentation of the optic disk; segmentation of the retinal vasculature; localization of the macula and fovea; localization and segmentation of retinopathy). The variety of outcome measures, use of a gold standard or ground truth, data sample sizes and the use of image databases is discussed. It is intended that our classification of algorithms into a small number of categories, definition of terms and discussion of evolving techniques will provide guidance to algorithm designers for diabetic retinopathy.
ER  - 
TY  - JOUR
T1  - Predicting morbidity by local similarities in multi-scale patient trajectories
A1  - Carrasco-Ribelles, Lucía A
A1  - Pardo-Mas, Jose Ramón
A1  - Tortajada, Salvador
A1  - Sáez, Carlos
A1  - Valdivieso, Bernardo
A1  - García-Gómez, Juan M
Y1  - 2021///
KW  -  Cardiovascular disease
KW  -  Diabetes
KW  -  Dynamic programming
KW  -  Local alignment
KW  -  Risk prediction
KW  - Patient trajectory
JF  - Journal of Biomedical Informatics
VL  - 120
SP  - 103837
EP  - 103837
DO  - https://doi.org/10.1016/j.jbi.2021.103837
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001660
N2  - Patient Trajectories (PTs) are a method of representing the temporal evolution of patients. They can include information from different sources and be used in socio-medical or clinical domains. PTs have generally been used to generate and study the most common trajectories in, for instance, the development of a disease. On the other hand, healthcare predictive models generally rely on static snapshots of patient information. Only a few works about prediction in healthcare have been found that use PTs, and therefore benefit from their temporal dimension. All of them, however, have used PTs created from single-source information. Therefore, the use of longitudinal multi-scale data to build PTs and use them to obtain predictions about health conditions is yet to be explored. Our hypothesis is that local similarities on small chunks of PTs can identify similar patients concerning their future morbidities. The objectives of this work are (1) to develop a methodology to identify local similarities between PTs before the occurrence of morbidities to predict these on new query individuals; and (2) to validate this methodology on risk prediction of cardiovascular diseases (CVD) occurrence in patients with diabetes. We have proposed a novel formal definition of PTs based on sequences of longitudinal multi-scale data. Moreover, a dynamic programming methodology to identify local alignments on PTs for predicting future morbidities is proposed. Both the proposed methodology for PT definition and the alignment algorithm are generic to be applied on any clinical domain. We validated this solution for predicting CVD in patients with diabetes and we achieved a precision of 0.33, a recall of 0.72 and a specificity of 0.38. Therefore, the proposed solution in the diabetes use case can result of utmost utility to secondary screening.
ER  - 
TY  - JOUR
T1  - MPC Model Individualization in Free-Living Conditions: A Proof-of-Concept Case Study
A1  - Toffanin, C
A1  - Del Favero, S
A1  - Aiello, E M
A1  - Messori, M
A1  - Cobelli, C
A1  - Magni, L
Y1  - 2017///
KW  -  artifcial pancreas
KW  -  clinical trial
KW  -  personalized model for control
KW  -  type 1 diabetes
KW  -  validation
KW  - Identifcation
JF  - IFAC-PapersOnLine
VL  - 50
IS  - 1
SP  - 1181
EP  - 1186
DO  - https://doi.org/10.1016/j.ifacol.2017.08.271
UR  - https://www.sciencedirect.com/science/article/pii/S2405896317305888
N1  - 20th IFAC World Congress
N2  - In the last years, Model Predictive Control (MPC) proved to be one of the most promising approaches for an Artifcial Pancreas (AP), a device for closed-loop blood glucose control in subjects afected by Type 1 Diabetes (T1D). MPC performance is highly infuenced by the quality of the model used for prediction. Moreover, the inter-patient variability characterising subjects with T1D increases the need of patient-tailored models. Recently, promising results have been obtained in silico using the UVA/Padova simulator in Soru et al. (2012) and Messori et al. (2016) where diferent individualization techniques have been studied and compared to the “average” model of the UVA/Padova adult population showing signifcant improvements in term of prediction ability. The aim of this paper is to verify the applicability of the technique described in Soru et al. (2012) and extend it to be used on free-living data collected without ad hoc clinical protocols. Data were collected during a 1 month trial in free-living conditions (Renard et al. (2016)). In this proof-of-concept case study, individualized models obtained with diferent identifcation parameters are compared with the “average” model that was used to synthetize the MPC controller used during that trial. The individualized models show superior prediction performance and prove robustness to non-optimal algorithm initialization in a selected test-case.
ER  - 
TY  - JOUR
T1  - Smartphone-based personalized blood glucose prediction
A1  - Li, Juan
A1  - Fernando, Chandima
Y1  - 2016///
KW  -  Blood glucose
KW  -  Personalized care
KW  -  Prediction
KW  -  Smartphone
KW  - Diabetes
JF  - ICT Express
VL  - 2
IS  - 4
SP  - 150
EP  - 154
DO  - https://doi.org/10.1016/j.icte.2016.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S2405959516301126
N1  - Special Issue on Emerging Technologies for Medical Diagnostics
N2  - Effective blood glucose control is essential for patients with diabetes. However, individual patients may not be able to monitor their blood glucose level regularly because of all manner of real-life interference. In this paper, we propose a personalized diabetes prediction mechanism that leverages smartphone-collected patient data and population data to drive personalized prediction. Unlike existing predictive models, this model utilizes pooled population data and captures patient similarities, and eventually produces a personalized blood glucose prediction for an individual. We have implemented the proposed model as a mobile application and have performed extensive experiments to evaluate its performance. The experimental results demonstrate that the proposed prediction mechanism can improve the prediction accuracy and remedy the problem of sparse data in the existing approaches.
ER  - 
TY  - JOUR
T1  - VLSI Wavelet Based Denoising of PPG Signal
A1  - Kasambe, P V
A1  - Rathod, S S
Y1  - 2015///
KW  -  FPGA
KW  -  Xilinx System Generator.
KW  - Wavelet Denoising
JF  - Procedia Computer Science
VL  - 49
SP  - 282
EP  - 288
DO  - https://doi.org/10.1016/j.procs.2015.04.254
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915007632
N1  - Proceedings of 4th International Conference on Advances in Computing, Communication and Control (ICAC3'15)
N2  - Wavelet denoising represents a common preprocessing step for several biomedical applications exposing low SNR. These applica- tions require real time processing along with minimization of power and area, only custom VLSI implementations can be adopted for this fulfillment. In this paper, Photoplethysmography (PPG) signal is used as a biomedical signal as an example. PPG is a non-invasive method in which relative blood volume changes in the blood close to skin is measured. The “pulse waveform” never underwent intensive investigation. Active investigation efforts are opening to reveal its effectiveness beyond oxygen saturation and determination of heart rate. With the introduction of pulse oximeter, this is one of the important waveforms that are normally dis- played in the clinical settings nowadays. But, the acquired PPG signal using PPG sensors are usually corrupted with different kinds of interference like Motion Artifacts, Power Line Noise, etc. We consider Power Line Noise for the performance evaluation of VLSI Wavelet based denoising of PPG signal. Different kinds of Wavelets such as db4, Coif1, Haar for denoising. Also, standard deviation and mean absolute deviation are used as evaluation criteria. Xilinx System Generator for DSP is exploited for the design of the architecture and simulation of proposed denoising method.
ER  - 
TY  - JOUR
T1  - A survey on medical image analysis in diabetic retinopathy
A1  - Stolte, Skylar
A1  - Fang, Ruogu
Y1  - 2020///
KW  -  Deep learning
KW  -  Image mining
KW  -  Lesion detection
KW  - Diabetic retinopathy
JF  - Medical Image Analysis
VL  - 64
SP  - 101742
EP  - 101742
DO  - https://doi.org/10.1016/j.media.2020.101742
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520301067
N2  - Diabetic Retinopathy (DR) represents a highly-prevalent complication of diabetes in which individuals suffer from damage to the blood vessels in the retina. The disease manifests itself through lesion presence, starting with microaneurysms, at the nonproliferative stage before being characterized by neovascularization in the proliferative stage. Retinal specialists strive to detect DR early so that the disease can be treated before substantial, irreversible vision loss occurs. The level of DR severity indicates the extent of treatment necessary - vision loss may be preventable by effective diabetes management in mild (early) stages, rather than subjecting the patient to invasive laser surgery. Using artificial intelligence (AI), highly accurate and efficient systems can be developed to help assist medical professionals in screening and diagnosing DR earlier and without the full resources that are available in specialty clinics. In particular, deep learning facilitates diagnosis earlier and with higher sensitivity and specificity. Such systems make decisions based on minimally handcrafted features and pave the way for personalized therapies. Thus, this survey provides a comprehensive description of the current technology used in each step of DR diagnosis. First, it begins with an introduction to the disease and the current technologies and resources available in this space. It proceeds to discuss the frameworks that different teams have used to detect and classify DR. Ultimately, we conclude that deep learning systems offer revolutionary potential to DR identification and prevention of vision loss.
ER  - 
TY  - JOUR
T1  - Adversarial multi-source transfer learning in healthcare: Application to glucose prediction for diabetic people
A1  - De Bois, Maxime
A1  - El Yacoubi, Mounîm A
A1  - Ammi, Mehdi
Y1  - 2021///
KW  -  Deep learning
KW  -  Diabetes
KW  -  Neural networks
KW  -  Personalized medicine
KW  -  Transfer learning
KW  - Artificial intelligence
JF  - Computer Methods and Programs in Biomedicine
VL  - 199
SP  - 105874
EP  - 105874
DO  - https://doi.org/10.1016/j.cmpb.2020.105874
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720317077
N2  - Background and objectives: Deep learning has yet to revolutionize general practices in healthcare, despite promising results for some specific tasks. This is partly due to data being in insufficient quantities hurting the training of the models. To address this issue, data from multiple health actors or patients could be combined by capitalizing on their heterogeneity through the use of transfer learning. Methods: To improve the quality of the transfer between multiple sources of data, we propose a multi-source adversarial transfer learning framework that enables the learning of a feature representation that is similar across the sources, and thus more general and more easily transferable. We apply this idea to glucose forecasting for diabetic people using a fully convolutional neural network. The evaluation is done by exploring various transfer scenarios with three datasets characterized by their high inter and intra variability. Results: While transferring knowledge is beneficial in general, we show that the statistical and clinical accuracies can be further improved by using of the adversarial training methodology, surpassing the current state-of-the-art results. In particular, it shines when using data from different datasets, or when there is too little data in an intra-dataset situation. To understand the behavior of the models, we analyze the learnt feature representations and propose a new metric in this regard. Contrary to a standard transfer, the adversarial transfer does not discriminate the patients and datasets, helping the learning of a more general feature representation. Conclusion: The adversarial training framework improves the learning of a general feature representation in a multi-source environment, enhancing the knowledge transfer to an unseen target. The proposed method can help improve the efficiency of data shared by different health actors in the training of deep models.
ER  - 
TY  - JOUR
T1  - RF-IDH: An intelligent fall detection system for hemodialysis patients via COTS RFID
A1  - Chen, Yi
A1  - Xiao, Fu
A1  - Huang, Haiping
A1  - Sun, Lijuan
Y1  - 2020///
KW  -  Intelligent RFID
KW  -  Machine learning in IoT
KW  -  Patient monitoring
KW  - Fall detection
JF  - Future Generation Computer Systems
VL  - 113
SP  - 13
EP  - 24
DO  - https://doi.org/10.1016/j.future.2020.06.047
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19327074
N2  - Unhealthy habits lead to a growing population of hemodialysis patients. The single treatment of hemodialysis is about four hours long. Therefore, patients usually go to the toilet during treatment and need to be checked for safety. However, existing fall detection techniques are often limited by factors such as privacy, signal interference, and the like. In this paper, we propose RF-IDH tackle the above issues, a dedicated system for detecting falls caused by complications in hemodialysis patients using RF signal. In RF-IDH, after collecting the signal, we process the collected data by three functional module clusters, namely signal preprocessing, residual feature extraction, hemodialysis patient’s fall detection, all of which are well-designed to achieve high performance in patient’s fall detection. In particular, we design a residual feature extraction (RFE) algorithm based on the hemodialysis patient safety process model, and the fall detection of hemodialysis patients is treated as a machine learning problem where four classification models are built via learning residual feature space. We implement our system on commercial off-the-shelf RFID devices and compared the evaluation metrics of four different methods in terms of system performance, efficiency, robustness, and latency. The evaluation results show that our proposed RF-IDH that optimizes the 2NN–RFE method achieves superior performance compared to other methods.
ER  - 
TY  - JOUR
T1  - MBCGP-FE: A modified balanced cartesian genetic programming feature extractor
A1  - Yazdani, Samaneh
A1  - Shanbehzadeh, Jamshid
A1  - Hadavandi, Esmaeil
Y1  - 2017///
KW  -  Balanced cartesian genetic programming
KW  -  Dimensionality reduction
KW  -  Feature extraction
KW  - Feature interaction
JF  - Knowledge-Based Systems
VL  - 135
SP  - 89
EP  - 98
DO  - https://doi.org/10.1016/j.knosys.2017.08.005
UR  - https://www.sciencedirect.com/science/article/pii/S0950705117303611
N2  - Many data sets are represented by low-level or primitive features. This makes it difficult to discover relevant information via learning algorithm. Changing the way primitive data is represented can be advantageous. This can be performed using data preprocessing algorithms. A successful preprocessing algorithm should be capable of revealing the relationships among features to improve learners. These hidden relations among features can make the relevancy of the aspects of the data opaque to the learner. Automatic feature extraction is a solution to overcome this problem. This article introduces a Modified Balanced Cartesian Genetic Programming Feature Extractor (MBCGP-FE) for transforming the feature space to a smaller one composed of highly informative features through modifying the representation and operators of Balanced Cartesian Genetic Programming (BCGP). The new feature space is composed from original relevant and new constructed features which are created by discovering and compacting hidden relations among features. The size of the new feature space is determined during the optimization process. Experimental results on real data sets show that the MBCGP-FE improves the performance of learners and it is effective in reducing the dimension of data sets through the construction of new informative features. In addition, obtained results indicate the effectiveness of our proposed method in comparison with other feature extraction methods.
ER  - 
TY  - JOUR
T1  - TyG-er: An ensemble Regression Forest approach for identification of clinical factors related to insulin resistance condition using Electronic Health Records
A1  - Bernardini, Michele
A1  - Morettini, Micaela
A1  - Romeo, Luca
A1  - Frontoni, Emanuele
A1  - Burattini, Laura
Y1  - 2019///
KW  -  Laboratory screening
KW  -  Missing values
KW  -  Pattern recognition
KW  -  Pre-diabetes
KW  -  Random forest
KW  - Insulin resistance
JF  - Computers in Biology and Medicine
VL  - 112
SP  - 103358
EP  - 103358
DO  - https://doi.org/10.1016/j.compbiomed.2019.103358
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519302355
N2  - Background
Insulin resistance is an early-stage deterioration of Type 2 diabetes. Identification and quantification of insulin resistance requires specific blood tests; however, the triglyceride-glucose (TyG) index can provide a surrogate assessment from routine Electronic Health Record (EHR) data. Since insulin resistance is a multi-factorial condition, to improve its characterisation, this study aims to discover non-trivial clinical factors in EHR data to determine where the insulin-resistance condition is encoded.
Methods
We proposed a high-interpretable Machine Learning approach (i.e., ensemble Regression Forest combined with data imputation strategies), named TyG-er. We applied three different experimental procedures to test TyG-er reliability on the Italian Federation of General Practitioners dataset, named FIMMG_obs dataset, which is publicly available and reflects the clinical use-case (i.e., not all laboratory exams are prescribed on a regular basis over time).
Results
Results detected non-conventional clinical factors (i.e., uricemia, leukocytes, gamma-glutamyltransferase and protein profile) and provided novel insight into the best combination of clinical factors for detecting early glucose tolerance deterioration. The robustness of these extracted clinical factors was confirmed by the high agreement (from 0.664 to 0.911 of Lin's correlation coefficient (rc)) of the TyG-er approach among different experimental procedures. Moreover, the results of the three experimental procedures outlined the predictive power of the TyG-er approach (up to a mean absolute error of 5.68% and rc=0.666,p<.05).
Conclusions
The TyG-er approach is able to carry information about the identification of the TyG index, strictly correlated with the insulin-resistance condition, while extracting the most relevant non-glycemic features from routine data.
ER  - 
TY  - JOUR
T1  - Deep Bayesian baseline for segmenting diabetic retinopathy lesions: Advances and challenges
A1  - Garifullin, Azat
A1  - Lensu, Lasse
A1  - Uusitalo, Hannu
Y1  - 2021///
KW  -  Diabetic retinopathy
KW  -  Haemorrhage
KW  -  Hard exudate
KW  -  Lesion segmentation
KW  -  Microaneurysm
KW  -  Soft exudate
KW  - Bayesian deep learning
JF  - Computers in Biology and Medicine
VL  - 136
SP  - 104725
EP  - 104725
DO  - https://doi.org/10.1016/j.compbiomed.2021.104725
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521005199
N2  - Early diagnosis of retinopathy is essential for preventing retinal complications and visual impairment due to diabetes. For the detection of retinopathy lesions from retinal images, several automatic approaches based on deep neural networks have been developed in the recent years. Most of the proposed methods produce point estimates of pixels belonging to the lesion areas and give no or little information on the uncertainty of method predictions. However, the latter can be essential in the examination of the medical condition of the patient when the goal is early detection of abnormalities. This work extends the recent research with a Bayesian framework by considering the parameters of a convolutional neural network as random variables and utilizing stochastic variational dropout based approximation for uncertainty quantification. The framework includes an extended validation procedure and it allows analyzing lesion segmentation distributions, model calibration and prediction uncertainties. Also the challenges related to the deep probabilistic model and uncertainty quantification are presented. The proposed method achieves area under precision-recall curve of 0.84 for hard exudates, 0.641 for soft exudates, 0.593 for haemorrhages, and 0.484 for microaneurysms on IDRiD dataset.
ER  - 
TY  - JOUR
T1  - Development of traditional Chinese medicine clinical data warehouse for medical knowledge discovery and decision support
A1  - Zhou, Xuezhong
A1  - Chen, Shibo
A1  - Liu, Baoyan
A1  - Zhang, Runsun
A1  - Wang, Yinghui
A1  - Li, Ping
A1  - Guo, Yufeng
A1  - Zhang, Hua
A1  - Gao, Zhuye
A1  - Yan, Xiufeng
Y1  - 2010///
KW  -  Clinical data mining
KW  -  Clinical decision support
KW  -  Traditional Chinese medicine
KW  - Clinical data warehouse
JF  - Artificial Intelligence in Medicine
VL  - 48
IS  - 2
SP  - 139
EP  - 152
DO  - https://doi.org/10.1016/j.artmed.2009.07.012
UR  - https://www.sciencedirect.com/science/article/pii/S0933365709001055
N1  - Artificial Intelligence in Biomedical Engineering and Informatics
N2  - Objective
Traditional Chinese medicine (TCM) is a scientific discipline, which develops the related theories from the long-term clinical practices. The large-scale clinical data are the core empirical knowledge source for TCM research. This paper introduces a clinical data warehouse (CDW) system, which incorporates the structured electronic medical record (SEMR) data for medical knowledge discovery and TCM clinical decision support (CDS).
Materials and methods
We have developed the clinical reference information model (RIM) and physical data model to manage the various information entities and their relationships in TCM clinical data. An extraction-transformation-loading (ETL) tool is implemented to integrate and normalize the clinical data from different operational data sources. The CDW includes online analytical processing (OLAP) and complex network analysis (CNA) components to explore the various clinical relationships. Furthermore, the data mining and CNA methods are used to discover the valuable clinical knowledge from the data.
Results
The CDW has integrated 20,000 TCM inpatient data and 20,000 outpatient data, which contains manifestations (e.g. symptoms, physical examinations and laboratory test results), diagnoses and prescriptions as the main information components. We propose a practical solution to accomplish the large-scale clinical data integration and preprocessing tasks. Meanwhile, we have developed over 400 OLAP reports to enable the multidimensional analysis of clinical data and the case-based CDS. We have successfully conducted several interesting data mining applications. Particularly, we use various classification methods, namely support vector machine, decision tree and Bayesian network, to discover the knowledge of syndrome differentiation. Furthermore, we have applied association rule and CNA to extract the useful acupuncture point and herb combination patterns from the clinical prescriptions.
Conclusion
A CDW system consisting of TCM clinical RIM, ETL, OLAP and data mining as the core components has been developed to facilitate the tasks of TCM knowledge discovery and CDS. We have conducted several OLAP and data mining tasks to explore the empirical knowledge from the TCM clinical data. The CDW platform would be a promising infrastructure to make full use of the TCM clinical data for scientific hypothesis generation, and promote the development of TCM from individualized empirical knowledge to large-scale evidence-based medicine.
ER  - 
TY  - JOUR
T1  - Similarity classifier with generalized mean applied to medical data
A1  - Luukka, Pasi
A1  - Leppälampi, Tapio
Y1  - 2006///
KW  -  Classification
KW  -  Dimension reduction
KW  -  Łukasiewicz structure
KW  - Fuzzy similarity
JF  - Computers in Biology and Medicine
VL  - 36
IS  - 9
SP  - 1026
EP  - 1040
DO  - https://doi.org/10.1016/j.compbiomed.2005.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S0010482505000673
N2  - A new approach based on fuzzy similarity was presented for the detection of erythemato-squamous diseases, diabetes, liver disorders, breast cancer and thyroid. The domain contained records of patients with known diagnoses. The results were very promising with all data sets and some conclusions can be drawn that a fuzzy similarity model can be used for the diagnosis of patients taking into consideration the error rate. A fuzzy similarity classifier was used to detect the six erythemato-squamous diseases when 34 features defining six disease indications were used as inputs. The results confirmed that the proposed model has potential in detecting erythemato-squamous diseases. The fuzzy similarity model achieved accuracy rates (over 97%) which were higher than that of the stand-alone neural network model or the ANFIS model suggested in [E.D. Übeyli, I. Güler, Comput. Biol. Med. 35(5) (2005) 421–433]. With PIMA Indian diabetes, the detection model has an error rate of about 25% which is much better than the overall rate of 33% for diabetes. The model was also tested with other data sets: thyroid and two breast cancer data sets where the average detection accuracy was over 96% for all cases, which is quite good. Also, the liver disorder data set gave promising results.
ER  - 
TY  - JOUR
T1  - CDC-Net: Cascaded decoupled convolutional network for lesion-assisted detection and grading of retinopathy using optical coherence tomography (OCT) scans
A1  - Hassan, Bilal
A1  - Qin, Shiyin
A1  - Hassan, Taimur
A1  - Akram, Muhammad Usman
A1  - Ahmed, Ramsha
A1  - Werghi, Naoufel
Y1  - 2021///
KW  -  Deep learning
KW  -  Lesion segmentation
KW  -  Retinal analysis
KW  -  Retinopathy grading
KW  - Optical coherence tomography (OCT)
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103030
EP  - 103030
DO  - https://doi.org/10.1016/j.bspc.2021.103030
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421006273
N2  - Retinopathy refers to any injury in the retinal region of the eye that can lead to distorted vision or even blindness. The segmentation of retinal lesions or biomarkers is crucial for the precise classification and grading of retinopathy. Optical coherence tomography imaging is the widely used eye examination tool by ophthalmologists due to its comprehensive visualization of the retinal lesions, which can assist in the prompt treatment of retinal conditions. However, due to vast clinical optical coherence tomography applications and the incidence of ocular syndromes, the number of scans collected daily outweighs ophthalmologists’ capacity to interpret these in a meaningful way. Many studies have been proposed previously to address this issue using optical coherence tomography scans. However, to the best of our knowledge, no framework can perform joint segmentation of the retinal lesions and retinopathy grading. In this paper, we propose a novel framework to address this shortcoming. We propose a new cascaded decoupled convolutional network comprising two separate modules that work together to perform lesion-assisted grading of retinopathy according to the clinical standards. We thoroughly evaluated the proposed framework using 26841 multi-vendor scans spanned over four publicly accessible datasets. The obtained results validate the efficacy of the proposed scheme over other state-of-the-art frameworks, where it achieved the mean Dice score of 0.820 (3.66% improvement) in segmentation of retinal lesions and 98.89% accuracy in the grading of retinopathy with 98.34% true positive rate and 99.17% true negative rate.
ER  - 
TY  - JOUR
T1  - A flexible data-driven comorbidity feature extraction framework
A1  - Sideris, Costas
A1  - Pourhomayoun, Mohammad
A1  - Kalantarian, Haik
A1  - Sarrafzadeh, Majid
Y1  - 2016///
KW  -  Clustering
KW  -  Comorbidity
KW  -  Prediction
KW  - Knowledge discovery
JF  - Computers in Biology and Medicine
VL  - 73
SP  - 165
EP  - 172
DO  - https://doi.org/10.1016/j.compbiomed.2016.04.014
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516301032
N2  - Disease and symptom diagnostic codes are a valuable resource for classifying and predicting patient outcomes. In this paper, we propose a novel methodology for utilizing disease diagnostic information in a predictive machine learning framework. Our methodology relies on a novel, clustering-based feature extraction framework using disease diagnostic information. To reduce the data dimensionality, we identify disease clusters using co-occurrence statistics. We optimize the number of generated clusters in the training set and then utilize these clusters as features to predict patient severity of condition and patient readmission risk. We build our clustering and feature extraction algorithm using the 2012 National Inpatient Sample (NIS), Healthcare Cost and Utilization Project (HCUP) which contains 7 million hospital discharge records and ICD-9-CM codes. The proposed framework is tested on Ronald Reagan UCLA Medical Center Electronic Health Records (EHR) from 3041 Congestive Heart Failure (CHF) patients and the UCI 130-US diabetes dataset that includes admissions from 69,980 diabetic patients. We compare our cluster-based feature set with the commonly used comorbidity frameworks including Charlson's index, Elixhauser's comorbidities and their variations. The proposed approach was shown to have significant gains between 10.7–22.1% in predictive accuracy for CHF severity of condition prediction and 4.65–5.75% in diabetes readmission prediction.
ER  - 
TY  - JOUR
T1  - Convolutional Neural Networks for Diabetic Retinopathy
A1  - Pratt, Harry
A1  - Coenen, Frans
A1  - Broadbent, Deborah M
A1  - Harding, Simon P
A1  - Zheng, Yalin
Y1  - 2016///
KW  -  Convolutional Neural Networks
KW  -  Diabetes
KW  -  Diabetic Retinopathy
KW  -  Image Classification
KW  - Deep Learning
JF  - Procedia Computer Science
VL  - 90
SP  - 200
EP  - 205
DO  - https://doi.org/10.1016/j.procs.2016.07.014
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916311929
N1  - 20th Conference on Medical Image Understanding and Analysis (MIUA 2016)
N2  - The diagnosis of diabetic retinopathy (DR) through colour fundus images requires experienced clinicians to identify the presence and significance of many small features which, along with a complex grading system, makes this a difficult and time consuming task. In this paper, we propose a CNN approach to diagnosing DR from digital fundus images and accurately classifying its severity. We develop a network with CNN architecture and data augmentation which can identify the intricate features involved in the classification task such as micro-aneurysms, exudate and haemorrhages on the retina and consequently provide a diagnosis automatically and without user input. We train this network using a high-end graphics processor unit (GPU) on the publicly available Kaggle dataset and demonstrate impressive results, particularly for a high-level classification task. On the data set of 80,000 images used our proposed CNN achieves a sensitivity of 95% and an accuracy of 75% on 5,000 validation images.
ER  - 
TY  - JOUR
T1  - Clustering datasets with demographics and diagnosis codes
A1  - Zhong, Haodi
A1  - Loukides, Grigorios
A1  - Gwadera, Robert
Y1  - 2020///
KW  -  Demographics
KW  -  Diagnosis codes
KW  -  Pattern mining
KW  - Clustering
JF  - Journal of Biomedical Informatics
VL  - 102
SP  - 103360
EP  - 103360
DO  - https://doi.org/10.1016/j.jbi.2019.103360
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302801
N2  - Clustering data derived from Electronic Health Record (EHR) systems is important to discover relationships between the clinical profiles of patients and as a preprocessing step for analysis tasks, such as classification. However, the heterogeneity of these data makes the application of existing clustering methods difficult and calls for new clustering approaches. In this paper, we propose the first approach for clustering a dataset in which each record contains a patient’s values in demographic attributes and their set of diagnosis codes. Our approach represents the dataset in a binary form in which the features are selected demographic values, as well as combinations (patterns) of frequent and correlated diagnosis codes. This representation enables measuring similarity between records using cosine similarity, an effective measure for binary-represented data, and finding compact, well-separated clusters through hierarchical clustering. Our experiments using two publicly available EHR datasets, comprised of over 26,000 and 52,000 records, demonstrate that our approach is able to construct clusters with correlated demographics and diagnosis codes, and that it is efficient and scalable.
ER  - 
TY  - JOUR
T1  - A machine learning-based approach for predicting the outbreak of cardiovascular diseases in patients on dialysis
A1  - Mezzatesta, Sabrina
A1  - Torino, Claudia
A1  - Meo, Pasquale De
A1  - Fiumara, Giacomo
A1  - Vilasi, Antonio
Y1  - 2019///
KW  -  Cardiovascular outcomes
KW  -  ESRD
KW  -  Prognosis
KW  - Machine learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 177
SP  - 9
EP  - 15
DO  - https://doi.org/10.1016/j.cmpb.2019.05.005
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718317188
N2  - Background and Objective: Patients with End- Stage Kidney Disease (ESKD) have a unique cardiovascular risk. This study aims at predicting, with a certain precision, death and cardiovascular diseases in dialysis patients. Methods: To achieve our aim, machine learning techniques have been used. Two datasets have been taken into consideration: the first is an Italian dataset obtained from the Istituto di Fisiologia Clinica of Consiglio Nazionale delle Ricerche of Reggio Calabria; the second is an American dataset provided by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) repository. From each one we obtained 5 datasets, according to the outcome of interest. We tested different types of algorithm (both linear and non-linear), but the final choice was to use Support Vector Machine. In particular, we obtained the best performances using the non-linear SVC with RBF kernel algorithm, optimizing it with GridSearch. The last is an algorithm useful to search the best combination of hyper-parameters (in our case, to find the best couple (C, γ)), in order to improve the accuracy of the algorithm. Results: The use of non-linear SVC with RBF kernel algorithm, optimized with GridSearch, allowed to obtain an accuracy of 95.25% in the Italian dataset and of 92.15% in the American dataset, in a timeframe of 2.5 years,in the prediction of Ischaemic Heart Disease. A worse performance was obtained for the other outcomes. Conclusions: The machine learning-based approach applied in our study is able to predict, with a high accuracy, the outbreak of cardiovascular diseases in patients on dialysis.
ER  - 
TY  - JOUR
T1  - Glucose forecasting combining Markov chain based enrichment of data, random grammatical evolution and Bagging
A1  - Hidalgo, J Ignacio
A1  - Botella, Marta
A1  - Velasco, J Manuel
A1  - Garnica, Oscar
A1  - Cervigón, Carlos
A1  - Martínez, Remedios
A1  - Aramendi, Aranzazu
A1  - Maqueda, Esther
A1  - Lanchares, Juan
Y1  - 2020///
KW  -  Bagging
KW  -  Data augmentation
KW  -  Diabetes management
KW  -  Ensemble models
KW  -  Random-GE
KW  -  Time series forecasting
KW  - Grammatical evolution
JF  - Applied Soft Computing
VL  - 88
SP  - 105923
EP  - 105923
DO  - https://doi.org/10.1016/j.asoc.2019.105923
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619307045
N2  - Diabetes Mellitus is a disease affecting more and more people every year. Depending on the kind of diabetes and sometimes on the stage of the illness, diabetic patients have to inject some amount of artificial insulin, namely bolus, before the meals, to make up the absence or malfunctioning of their natural insulin. This decision is a difficult task since they need to estimate the number of carbohydrates they are going to ingest, take into account the past and future circumstances, know the past values of glucose, evaluate if the effect of previously injected insulin has already finished and any other relevant information. In this paper, we present and compare a set of methodologies to automate the decision of the insulin bolus, which reduces the number of dangerous predictions. We combine two different data enrichment techniques based on Markov chains with grammatical evolution engines to generate models of blood glucose, and univariate marginal distribution algorithms and bagging techniques to select the set of models to assemble. In particular, we propose the Random-GE procedure, an adaptation of Random Forests to Grammatical Evolution, which leads to excellent prediction models, with a simple configuration and a reduced execution time. The ensemble gives the prediction of glucose for a duple of food and insulins, helping patients in the selection of the appropriate bolus to maintain healthy glucose levels after the meals. Experimental results show that our models get more accurate and robust predictions than previous approaches.
ER  - 
TY  - JOUR
T1  - Color fundus image registration techniques and applications for automated analysis of diabetic retinopathy progression: A review
A1  - Saha, Sajib Kumar
A1  - Xiao, Di
A1  - Bhuiyan, Alauddin
A1  - Wong, Tien Y
A1  - Kanagasingam, Yogesan
Y1  - 2019///
KW  -  Color fundus image
KW  -  Longitudinal registration
KW  -  Registration
KW  -  Retinopathy progression
KW  - Diabetic retinopathy
JF  - Biomedical Signal Processing and Control
VL  - 47
SP  - 288
EP  - 302
DO  - https://doi.org/10.1016/j.bspc.2018.08.034
UR  - https://www.sciencedirect.com/science/article/pii/S1746809418302325
N2  - Diabetic retinopathy (DR) is one of the leading cause of visual impairments in the working age population in the developed world. It is a complication of both types of diabetes mellitus, which affects the light perception part of the retina; and without timely treatment patients could lose their sight and eventually become blind. Automated methods for the detection and progression analysis of DR are considered as potential health-care need to stop disease propagation and to ensure improved management for DR. Aiming for the detection and progression analysis of DR, color fundus photography is considered as one of the best candidates for non-invasive imaging of the eye fundus. A list of methods has already been developed to analyse DR related changes in the retina using color fundus photographs. In this manuscript we review those automated methods. In order to accurately compare the evolution of DR over time, retinal images that are typically collected on an annual or biennial basis must be perfectly superimposed. However, in reality, for two separate photographic-eye examinations the patient is never in exactly the same position and also the camera may vary. Therefore, a registration method is applied prior to evolution computation. Knowing registration as a fundamental preprocessing step for longitudinal (over time) analysis, we also reviewed state-of-the art methods for the registration of color fundus images. The review summarizes the achievement so far and also identifies potential study areas for further improvement and future research toward more efficient and accurate DR progression analysis.
ER  - 
TY  - JOUR
T1  - Glucose-insulin model identified in free-living conditions for hypoglycaemia prevention
A1  - Toffanin, C
A1  - Del Favero, S
A1  - Aiello, E M
A1  - Messori, M
A1  - Cobelli, C
A1  - Magni, L
Y1  - 2018///
KW  -  Artificial pancreas
KW  -  Clinical trial
KW  -  MPC
KW  -  Type 1 diabetes
KW  -  Validation
KW  - Identification
JF  - Journal of Process Control
VL  - 64
SP  - 27
EP  - 36
DO  - https://doi.org/10.1016/j.jprocont.2018.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0959152418300258
N2  - Hypoglycaemia avoidance is one of the main barriers to the optimal management of Type 1 Diabetes (T1D). In order to attenuate the effect of hypoglycaemia, alarm systems support T1D subjects equipped with Continuous Glucose Monitoring (CGM) devices. The development of predictive detection tools for hypoglycaemia and CGM have been accelerated by Artificial Pancreas (AP), a closed-loop systems for automatic blood glucose control in T1D subjects. The methods to generate hypoglycaemia alarms can be divided in two categories: low-threshold detection and prediction. The first notifies the crossing of a critical blood glucose level, while the second predicts this risk in advance and it is typically based on a patient model. Considering the significant inter-patient variability characterizing T1D subjects, patient-tailored models are required. In this regard, different individualization techniques have been proposed showing significant improvements compared to “average” models. This paper proposes an alarm system based on patient-tailored models obtained through an identification technique that exploits impulse response data collected in silico and that is extended here to be used on free-living data. In particular, the data used in this work derive from a 1 month AP trial performed in free-living conditions. Individualized models obtained with different identification parameters are compared. Independently of the selected parameters, the patient-tailored models show superior predictive performance with respect to the “average” model used in the Model Predictive Control (MPC) algorithm used in the trial. The best model is used to design an alarm system which shows significant improvements in hypoglycaemia detection in comparison with the safety system used in the trial: true positive are increased by 31% with a decrease of the false positive by 57%. The promising prediction capabilities of the proposed patient-tailored models can be a key ingredient for a new generation of individualised MPC for AP.
ER  - 
TY  - JOUR
T1  - Machine learning of clinical performance in a pancreatic cancer database
A1  - Hayward, John
A1  - Alvarez, Sergio A
A1  - Ruiz, Carolina
A1  - Sullivan, Mary
A1  - Tseng, Jennifer
A1  - Whalen, Giles
Y1  - 2010///
KW  -  Clinical performance
KW  -  Pancreatic cancer
KW  -  Predictive modeling
KW  -  Quality of life
KW  - Machine learning
JF  - Artificial Intelligence in Medicine
VL  - 49
IS  - 3
SP  - 187
EP  - 195
DO  - https://doi.org/10.1016/j.artmed.2010.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S0933365710000527
N1  - Data Mining for the Study of Disease Genes and Proteins
N2  - Objective
We consider predictive models for clinical performance of pancreatic cancer patients based on machine learning techniques. The predictive performance of machine learning is compared with that of the linear and logistic regression techniques that dominate the medical oncology literature.
Methods and materials
We construct predictive models over a clinical database that we have developed for the University of Massachusetts Memorial Hospital in Worcester, Massachusetts, USA. The database contains retrospective records of 91 patient treatments for pancreatic tumors. Classification and regression targets include patient survival time, Eastern Cooperative Oncology Group (ECOG) quality of life scores, surgical outcomes, and tumor characteristics. The predictive performance of several techniques is described, and specific models are presented.
Results
We show that machine learning techniques attain a predictive performance that is as good as, or better than, that of linear and logistic regression, for target attributes that include tumor N and T stage, survival time, and ECOG quality of life scores. Bayesian techniques are found to provide the best performance overall. For tumor size as the target attribute, however, logistic regression (respectively linear regression in the case of a numerical as opposed to discrete target) performs best. Preprocessing in the form of attribute selection and supervised attribute discretization improves predictive performance for most of the predictive techniques and target attributes considered.
Conclusion
Machine learning provides techniques for improved prediction of clinical performance. These techniques therefore merit consideration as valuable alternatives to traditional multivariate regression techniques in clinical medical studies.
ER  - 
TY  - JOUR
T1  - Properties of the Box–Cox transformation for pattern classification
A1  - Bicego, Manuele
A1  - Baldo, Sisto
Y1  - 2016///
KW  -  Classification
KW  -  Non linear mappings
KW  -  Preprocessing
KW  - Box–Cox transformation
JF  - Neurocomputing
VL  - 218
SP  - 390
EP  - 400
DO  - https://doi.org/10.1016/j.neucom.2016.08.081
UR  - https://www.sciencedirect.com/science/article/pii/S0925231216309778
N2  - The Box–Cox transformation [1,2] (Box and Cox, 1964; Sakia, 1992) has been regarded as a parametric pre-processing technique aimed at making the distribution of a set of points approximately Gaussian. Since normality represents an assumption underlying many statistical data analysis tools, such technique has been widely applied in different fields of Computer Science. In this paper we will provide evidence that this technique can be useful also in the case of Pattern Classification, where Gaussianity of datasets is not so critical. By letting the Box–Cox transform work in operational ranges which do not necessarily correspond to an increase in Gaussianity, we will show that class separability can be improved: this is likely due to the non linear nature of the Box–Cox transformation, which deforms the space in a nonuniform way. We will also provide some suggestions on criteria that can be used to automatically estimate the best parameter of the Box–Cox transformation in the Pattern Classification context.
ER  - 
TY  - JOUR
T1  - A comprehensive diagnosis system for early signs and different diabetic retinopathy grades using fundus retinal images based on pathological changes detection
A1  - AbdelMaksoud, Eman
A1  - Barakat, Sherif
A1  - Elmogy, Mohammed
Y1  - 2020///
KW  -  Blood vessels (BV)
KW  -  Diabetic retinopathy (DR)
KW  -  Exudates (EX)
KW  -  Hemorrhages (HM)
KW  -  Microaneurysms (MA)
KW  -  Multi-label classification (MLC)
KW  - Multi-label computer-aided diagnostic (CAD)
JF  - Computers in Biology and Medicine
VL  - 126
SP  - 104039
EP  - 104039
DO  - https://doi.org/10.1016/j.compbiomed.2020.104039
UR  - https://www.sciencedirect.com/science/article/pii/S001048252030370X
N2  - Multi-label classification (MLC) is deemed as an effective and dynamic research topic in the medical image analysis field. For ophthalmologists, MLC benefits can be utilized to detect early diabetic retinopathy (DR) signs, as well as its different grades. This paper proposes a comprehensive computer-aided diagnostic (CAD) system that exploits the MLC of DR grades using colored fundus photography. The proposed system detects and analyzes various retina pathological changes accompanying DR development. We extracted some significant features to differentiate healthy from DR cases as well as differentiate various DR grades. First, we preprocessed the retinal images to eliminate noise and enhance the image quality by using histogram equalization for brightness preservation based on dynamic stretching technique. Second, the images were segmented to extract four pathology variations, which are blood vessels, exudates, microaneurysms, and hemorrhages. Next, six various features were extracted using a gray level co-occurrence matrix, the four extracting areas, and blood-vessel bifurcation points. Finally, the features were supplied to a support vector machine (SVM) classifier to distinguish normal and different DR grades. To train and test the proposed system, we utilized four benchmark datasets (two of them are multi-label datasets) using six performance metrics. The proposed system achieved an average accuracy of 89.2%, sensitivity of 85.1%, specificity of 85.2%, positive predictive value of 92.8%, area under the curve of 85.2%, and Disc similarity coefficient (DSC) of 88.7%. The experiments show promising results as compared with other systems.
ER  - 
TY  - JOUR
T1  - A foundation of rough sets theoretical and computational hybrid intelligent system for survival analysis
A1  - Pattaraintakorn, Puntip
A1  - Cercone, Nick
Y1  - 2008///
KW  -  Rough sets
KW  -  Soft computing
KW  -  Survival analysis
KW  - Intelligent system
JF  - Computers & Mathematics with Applications
VL  - 56
IS  - 7
SP  - 1699
EP  - 1708
DO  - https://doi.org/10.1016/j.camwa.2008.04.030
UR  - https://www.sciencedirect.com/science/article/pii/S0898122108002472
N2  - What do we (not) know about the association between diabetes and survival time? Our study offers an alternative mathematical framework based on rough sets to analyze medical data and provide epidemiology survival analysis with risk factor diabetes. We experiment on three data sets: geriatric, melanoma and Primary Biliary Cirrhosis. A case study reports from 8547 geriatric Canadian patients at the Dalhousie Medical School. Notification status (dead or alive) is treated as the censor attribute and the time lived is treated as the survival time. The analysis result illustrates diabetes is a very significant risk factor to survival time in our geriatric patients data. This paper offers both theoretical and practical guidelines in the construction of a rough sets hybrid intelligent system, for the analysis of real world data. Furthermore, we discuss the potential of rough sets, artificial neural networks (ANNs) and frailty index in predicting survival tendency.
ER  - 
TY  - JOUR
T1  - A hybrid and exploratory approach to knowledge discovery in metabolomic data
A1  - Grissa, Dhouha
A1  - Comte, Blandine
A1  - Pétéra, Mélanie
A1  - Pujos-Guillot, Estelle
A1  - Napoli, Amedeo
Y1  - 2020///
KW  -  Classification
KW  -  Data and pattern exploration
KW  -  Formal concept analysis
KW  -  Interpretation
KW  -  Metabolomic data
KW  -  Pattern mining
KW  -  Visualization
KW  - Hybrid knowledge discovery
JF  - Discrete Applied Mathematics
VL  - 273
SP  - 103
EP  - 116
DO  - https://doi.org/10.1016/j.dam.2018.11.025
UR  - https://www.sciencedirect.com/science/article/pii/S0166218X18306346
N1  - Advances in Formal Concept Analysis: Traces of CLA 2016
N2  - In this paper, we propose a hybrid and exploratory knowledge discovery approach for analyzing metabolomic complex data based on a combination of supervised classifiers, pattern mining and Formal Concept Analysis (FCA). The approach is based on three main operations, preprocessing, classification, and postprocessing. Classifiers are applied to datasets of the form individuals × features and produce sets of ranked features which are further analyzed. Pattern mining and FCA are used to provide a complementary analysis and support for visualization. A practical application of this framework is presented in the context of metabolomic data, where two interrelated problems are considered, discrimination and prediction of class membership. The dataset is characterized by a small set of individuals and a large set of features, in which predictive biomarkers of clinical outcomes should be identified. The problems of combining numerical and symbolic data mining methods, as well as discrimination and prediction, are detailed and discussed. Moreover, it appears that visualization based on FCA can be used both for guiding knowledge discovery and for interpretation by domain analysts.
ER  - 
TY  - JOUR
T1  - Blood vessel detection from Retinal fundas images using GIFKCN classifier
A1  - Mondal, Sambit S
A1  - Mandal, Nirupma
A1  - Singh, Akansha
A1  - Singh, Krishna Kant
Y1  - 2020///
KW  -  DWT
KW  -  HKFCM-σ
KW  -  PCA
KW  -  SCD
KW  - Clustering
JF  - Procedia Computer Science
VL  - 167
SP  - 2060
EP  - 2069
DO  - https://doi.org/10.1016/j.procs.2020.03.246
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920307122
N1  - International Conference on Computational Intelligence and Data Science
N2  - In this paper, an automated method for blood vessel detection from retinal fundus images is proposed. Initially, the method extracts the green layer of the retinal image as it contains the requisite information. The noise is removed using a noise removal filter this step will lead to the reduction of artifacts in the final results. The image features are highlighted through morphological operations i.e., top hat and bottom hat transformation on the preprocessed image. The resultant image is then clustered into two clusters representing vessel and non-vessel using generalized improved Fuzzy Kohonen Clustering Network (GIFKCN). The experiments are performed on the Digital Retinal Images for Vessel Extraction (DRIVE) database. The effectiveness of the proposed technique is analyzed using performance metrics like accuracy and sensitivity. A comparative study of the proposed technique with some of the well-known existing methods is done. The comparison shows that the proposed method has improved significantly.
ER  - 
TY  - JOUR
T1  - DUNet: A deformable network for retinal vessel segmentation
A1  - Jin, Qiangguo
A1  - Meng, Zhaopeng
A1  - Pham, Tuan D
A1  - Chen, Qi
A1  - Wei, Leyi
A1  - Su, Ran
Y1  - 2019///
KW  -  DUNet
KW  -  Deformable convolution
KW  -  Segmentation
KW  -  U-Net
KW  - Retinal blood vessel
JF  - Knowledge-Based Systems
VL  - 178
SP  - 149
EP  - 162
DO  - https://doi.org/10.1016/j.knosys.2019.04.025
UR  - https://www.sciencedirect.com/science/article/pii/S0950705119301984
N2  - Automatic segmentation of retinal vessels in fundus images plays an important role in the diagnosis of some diseases such as diabetes and hypertension. In this paper, we propose Deformable U-Net (DUNet), which exploits the retinal vessels’ local features with a U-shape architecture, in an end to end manner for retinal vessel segmentation. Inspired by the recently introduced deformable convolutional networks, we integrate the deformable convolution into the proposed network. The DUNet, with upsampling operators to increase the output resolution, is designed to extract context information and enable precise localization by combining low-level features with high-level ones. Furthermore, DUNet captures the retinal vessels at various shapes and scales by adaptively adjusting the receptive fields according to vessels’ scales and shapes. Public datasets: DRIVE, STARE, CHASE_DB1 and HRF are used to test our models. Detailed comparisons between the proposed network and the deformable neural network, U-Net are provided in our study. Results show that more detailed vessels can be extracted by DUNet and it exhibits state-of-the-art performance for retinal vessel segmentation with a global accuracy of 0.9566/0.9641/0.9610/0.9651 and AUC of 0.9802/0.9832/0.9804/0.9831 on DRIVE, STARE, CHASE_DB1 and HRF respectively. Moreover, to show the generalization ability of the DUNet, we use another two retinal vessel data sets, i.e., WIDE and SYNTHE, to qualitatively and quantitatively analyze and compare with other methods. Extensive cross-training evaluations are used to further assess the extendibility of DUNet. The proposed method has the potential to be applied to the early diagnosis of diseases.
ER  - 
TY  - JOUR
T1  - Simple methods for the lesion detection and severity grading of diabetic retinopathy by image processing and transfer learning
A1  - Sugeno, Ayaka
A1  - Ishikawa, Yasuyuki
A1  - Ohshima, Toshio
A1  - Muramatsu, Rieko
Y1  - 2021///
KW  -  Convolutional neural network (CNN)
KW  -  Deep learning
KW  -  Diabetic retinopathy (DR)
KW  -  Image processing
KW  -  Lesion detection
KW  -  Severity grading
KW  - Computer-aided diagnosis
JF  - Computers in Biology and Medicine
VL  - 137
SP  - 104795
EP  - 104795
DO  - https://doi.org/10.1016/j.compbiomed.2021.104795
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521005898
N2  - Diabetic retinopathy (DR) has become one of the major causes of blindness. Due to the increased prevalence of diabetes worldwide, diabetic patients exhibit high probabilities of developing DR. There is a need to develop a labor-less computer-aided diagnosis system to support the clinical diagnosis. Here, we attempted to develop simple methods for severity grading and lesion detection from retinal fundus images. We developed a severity grading system for DR by transfer learning with a recent convolutional neural network called EfficientNet-B3 and the publicly available Kaggle Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 training dataset, which includes artificial noise. After removing the blurred and duplicated images from the dataset using a numerical threshold, the trained model achieved specificity and sensitivity values ≳ 0.98 in the identification of DR retinas. For severity grading, the classification accuracy values of 0.84, 0.95, and 0.98 were recorded for the 1st, 2nd, and 3rd predicted labels, respectively. The utility of EfficientNets-B3 for the severity grading of DR as well as the detailed retinal areas referred were confirmed via visual explanation methods of convolutional neural networks. Lesion extraction was performed by applying an empirically defined threshold value to the enhanced retinal images. Although the extraction of blood vessels and detection of red lesions occurred simultaneously, the red and white lesions, including both soft and hard exudates, were clearly extracted. The detected lesion areas were further confirmed with ground truth using the DIARETDB1 database images with general accuracy. The simple and easily applicable methods proposed in this study will aid in the detection and severity grading of DR, which might help in the selection of appropriate treatment strategies for DR.
ER  - 
TY  - JOUR
T1  - Segmentation of retinal blood vessels from ophthalmologic Diabetic Retinopathy images
A1  - Jebaseeli, T Jemima
A1  - Durai, C Anand Deva
A1  - Peter, J Dinesh
Y1  - 2019///
KW  -  Blood vessel
KW  -  Deep learning
KW  -  Feature extraction
KW  -  Fundus image
KW  -  Image segmentation
KW  -  Neural network
KW  -  Ophthalmology
KW  -  Retina
KW  -  SVM
KW  - Diabetic Retinopathy
JF  - Computers & Electrical Engineering
VL  - 73
SP  - 245
EP  - 258
DO  - https://doi.org/10.1016/j.compeleceng.2018.11.024
UR  - https://www.sciencedirect.com/science/article/pii/S004579061732949X
N2  - The most prominent ophthalmic cause of blindness is Diabetic Retinopathy (DR). This retinal disease is characterized by variation in diameter of the retinal blood vessel and the new blood vessel growth inside the retina. A system to enhance the quality of the segmentation result over the pathological retinal images has been proposed. The proposed method uses Contrast Limited Adaptive Histogram Equalization (CLAHE) for preprocessing and Tandem Pulse Coupled Neural Network (TPCNN) model for automatic feature vectors generation then classification and extraction of the retinal blood vessels via Deep Learning Based Support Vector Machine (DLBSVM). The proposed approach is assessed over the standard public fundus image databases to evaluate the performance. The results render that these techniques improve the segmentation results with an average value of 74.45% sensitivity, 99.40% specificity, and 99.16% accuracy. The results evoke that the proposed method is a suitable alternative for supervised techniques.
ER  - 
TY  - JOUR
T1  - Implementation of a TCM-based computational health informatics diagnostic tool for Sub-Saharan African students
A1  - Oluwagbemi, Olugbenga
A1  - Jatto, Abdulwahab
Y1  - 2019///
KW  -  Diagnostics
KW  -  Facial image recognition
KW  -  Infectious diseases
KW  -  Informatics
KW  -  Non-communicable diseases
KW  -  Traditional Chinese medicine
KW  - Computational health informatics
JF  - Informatics in Medicine Unlocked
VL  - 14
SP  - 43
EP  - 58
DO  - https://doi.org/10.1016/j.imu.2018.12.002
UR  - https://www.sciencedirect.com/science/article/pii/S2352914817302502
N2  - Health status checkup is a crucial step towards early detection of diseases. Health status diagnosis, in university health centers, within the sub-Saharan African region, can be cumbersome and time consuming. In many cases, facilities for health checkup are not available. Traditional Chinese Medicine (TCM) is a promising approach, when integrated with in-silico methods. This study was conducted to implement a TCM-based computational health informatics diagnostic tool. The tool was applied to diagnose African students. This study was also conducted to stimulate further research into in-silico TCM diagnostics. Besides developing a reliable biometric verification system, to ascertain the real identities of patients brought to university health centers, it is assistive to create a platform that provides automated and complementary support for preliminary health diagnostic activities. It also mitigates stress, by helping to efficiently decipher and provide quick objective opinion from the perspective of a computerized decision support system. The diagnostic module of the computational health informatics diagnostic tool adopts knowledge from a TCM facial color diagnosis. A comprehensive literature search was conducted for relevant full-text research papers. Only research publications written in English language were reviewed. The present work was compared qualitatively and quantitatively with the existing works noted in the literature. Facial detection and matching algorithms were implemented for the TCM-based computational health informatics diagnostic tool by using Java programming language. Facial image acquisition processes were conducted. Captured facial images of African students were preprocessed. Facial feature extraction was performed by implementing feature extraction algorithms. An algorithm for the extraction of color information and measurement was also implemented. Knowledge of machine learning was applied to extract and collate facial features, and to machine learn from them. Facial classification and recognition algorithms were implemented. Finally, the results from the computational health informatics diagnostic tool were evaluated, by conducting a performance evaluation and validation. This study provides qualitative and quantitative information on facial recognition, facial color information measurement, as well as prediction of health status, for some sub-Saharan African University students. Performance evaluation was shown using confusion matrix and ROC curves. Statistical analysis of the experimental results was presented. The parameters in each diagnostic illustration were shown with valid range. In order to justify the effectiveness of the computational tool, further explanations were provided from relevant methodology guides on the evaluation of diagnostic tests. The computational health informatics diagnostic tool will complement the diagnostic efforts in university health centers of sub-Saharan African universities. It will also be useful for personal health diagnosis of interested individuals. The tool will also be viable for educating health professionals. TCM will be of immense benefit to developing countries by positively contributing towards diagnosing different non-communicable diseases and some infectious diseases in such countries.
ER  - 
TY  - JOUR
T1  - Exudate-based diabetic macular edema recognition in retinal images using cascaded deep residual networks
A1  - Mo, Juan
A1  - Zhang, Lei
A1  - Feng, Yangqin
Y1  - 2018///
KW  -  Exudate segmentation
KW  -  Fully convolutional network
KW  -  Residual learning
KW  -  Retinal image
KW  - Diabetic macular edema recognition
JF  - Neurocomputing
VL  - 290
SP  - 161
EP  - 171
DO  - https://doi.org/10.1016/j.neucom.2018.02.035
UR  - https://www.sciencedirect.com/science/article/pii/S092523121830170X
N2  - Diabetic macular edema (DME), one of the leading causes of visual impairment and blindness, is usually diagnosed by the presence of exudates. However, exudate detection is challenging due to the large intraclass variation and high interclass similarity. To overcome these challenges, we propose the cascaded deep residual networks to recognize DME. Specifically, we first design a fully convolutional residual network that fuses multi-level hierarchical information to segment exudates accurately with a fast speed. Compared with previous methods, our approach avoids a wide range of preprocessing or postprocessing steps, reducing the impact of subjective factors. Then based on the segmentation results, the region centered on the pixel with the maximum probability is cropped and fed into the other deep residual network (for classification) to distinguish DME from its hard mimics. This makes the classification network to extract more representative features based on the segmentation results instead of the original images, further reducing the influence of complicated background. We evaluate the proposed method on two publicly available databases, the HEI-MED and e-ophtha EX databases. Extensive experiments demonstrate that our approach achieves better performance than the state-of-the-art methods with a fast processing speed, making it suitable for real-world clinical applications.
ER  - 
TY  - JOUR
T1  - Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data
A1  - Yang, Yujie
A1  - Li, Ye
A1  - Chen, Runge
A1  - Zheng, Jing
A1  - Cai, Yunpeng
A1  - Fortino, Giancarlo
Y1  - 2021///
KW  -  Electronic health record
KW  -  Health big data
KW  -  Machine learning
KW  -  Risk prediction
KW  - Renal failure
JF  - Big Data Research
VL  - 25
SP  - 100234
EP  - 100234
DO  - https://doi.org/10.1016/j.bdr.2021.100234
UR  - https://www.sciencedirect.com/science/article/pii/S2214579621000514
N2  - Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.
ER  - 
TY  - JOUR
T1  - Uniqueness of medical data mining
A1  - Cios, Krzysztof J
A1  - William Moore, G
Y1  - 2002///
KW  -  Ethical
KW  -  Security and legal aspects of medical data mining
KW  -  Unique features of medical data mining and knowledge discovery
KW  - Medical data mining
JF  - Artificial Intelligence in Medicine
VL  - 26
IS  - 1
SP  - 1
EP  - 24
DO  - https://doi.org/10.1016/S0933-3657(02)00049-0
UR  - https://www.sciencedirect.com/science/article/pii/S0933365702000490
N1  - Medical Data Mining and Knowledge Discovery
N2  - This article addresses the special features of data mining with medical data. Researchers in other fields may not be aware of the particular constraints and difficulties of the privacy-sensitive, heterogeneous, but voluminous data of medicine. Ethical and legal aspects of medical data mining are discussed, including data ownership, fear of lawsuits, expected benefits, and special administrative issues. The mathematical understanding of estimation and hypothesis formation in medical data may be fundamentally different than those from other data collection activities. Medicine is primarily directed at patient-care activity, and only secondarily as a research resource; almost the only justification for collecting medical data is to benefit the individual patient. Finally, medical data have a special status based upon their applicability to all people; their urgency (including life-or-death); and a moral obligation to be used for beneficial purposes.
ER  - 
TY  - JOUR
T1  - DrugSemantics: A corpus for Named Entity Recognition in Spanish Summaries of Product Characteristics
A1  - Moreno, Isabel
A1  - Boldrini, Ester
A1  - Moreda, Paloma
A1  - Romá-Ferri, M Teresa
Y1  - 2017///
KW  -  Named Entity Recognition
KW  -  Precision
KW  -  Reliability
KW  -  Spanish
KW  -  Summary of Product Characteristics
KW  - Corpus
JF  - Journal of Biomedical Informatics
VL  - 72
SP  - 8
EP  - 22
DO  - https://doi.org/10.1016/j.jbi.2017.06.013
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417301363
N2  - For the healthcare sector, it is critical to exploit the vast amount of textual health-related information. Nevertheless, healthcare providers have difficulties to benefit from such quantity of data during pharmacotherapeutic care. The problem is that such information is stored in different sources and their consultation time is limited. In this context, Natural Language Processing techniques can be applied to efficiently transform textual data into structured information so that it could be used in critical healthcare applications, being of help for physicians in their daily workload, such as: decision support systems, cohort identification, patient management, etc. Any development of these techniques requires annotated corpora. However, there is a lack of such resources in this domain and, in most cases, the few ones available concern English. This paper presents the definition and creation of DrugSemantics corpus, a collection of Summaries of Product Characteristics in Spanish. It was manually annotated with pharmacotherapeutic named entities, detailed in DrugSemantics annotation scheme. Annotators were a Registered Nurse (RN) and two students from the Degree in Nursing. The quality of DrugSemantics corpus has been assessed by measuring its annotation reliability (overall F=79.33% [95%CI: 78.35–80.31]), as well as its annotation precision (overall P=94.65% [95%CI: 94.11–95.19]). Besides, the gold-standard construction process is described in detail. In total, our corpus contains more than 2000 named entities, 780 sentences and 226,729 tokens. Last, a Named Entity Classification module trained on DrugSemantics is presented aiming at showing the quality of our corpus, as well as an example of how to use it.
ER  - 
TY  - JOUR
T1  - MediClass: A System for Detecting and Classifying Encounter-based Clinical Events in Any Electronic Medical Record
A1  - Hazlehurst, Brian
A1  - Frost, H Robert
A1  - Sittig, Dean F
A1  - Stevens, Victor J
Y1  - 2005///
JF  - Journal of the American Medical Informatics Association
VL  - 12
IS  - 5
SP  - 517
EP  - 529
DO  - https://doi.org/10.1197/jamia.M1771
UR  - https://www.sciencedirect.com/science/article/pii/S1067502705000824
N2  - MediClass is a knowledge-based system that processes both free-text and coded data to automatically detect clinical events in electronic medical records (EMRs). This technology aims to optimize both clinical practice and process control by automatically coding EMR contents regardless of data input method (e.g., dictation, structured templates, typed narrative). We report on the design goals, implemented functionality, generalizability, and current status of the system. MediClass could aid both clinical operations and health services research through enhancing care quality assessment, disease surveillance, and adverse event detection.
ER  - 
TY  - JOUR
T1  - A novel fuzzy hybrid quantum artificial immune clustering algorithm based on cloud model
A1  - Zhang, Ren-Long
A1  - Shan, Mi-Yuan
A1  - Liu, Xiao-Hong
A1  - Zhang, Li-Hong
Y1  - 2014///
KW  -  C-FHQAI
KW  -  Cloud model
KW  -  Quantum computing
KW  - Clustering algorithm
JF  - Engineering Applications of Artificial Intelligence
VL  - 35
SP  - 1
EP  - 13
DO  - https://doi.org/10.1016/j.engappai.2014.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S0952197614001262
N2  - In this paper we propose a novel fuzzy hybrid quantum artificial immune clustering algorithm based on cloud model (C-FHQAI) to solve the stochastic problem. Fuzzy hybrid quantum artificial immune algorithm can be developed with some of the advantages of information processing where there is a certain amount of indeterminism with qubits, i.e. quantum bits, replacing classical neurons having deterministic states and also in place of the classical artificial immune algorithm with quantum operators.The fuzzy combinatorial fuzzy hybrid quantum artificial immune clustering algorithm (C-FHQAI) is more expressive than the other fuzzy theories and methods. Finally, numerical examples show that the clustering effectiveness of the C-FHQAI algorithm is fast convergence and improves the accuracy of the fuzzy calculation. We find that the C-FHQAI clustering algorithm has the perspective of widespread application.
ER  - 
TY  - JOUR
T1  - Effective search for genetic-based machine learning systems via estimation of distribution algorithms and embedded feature reduction techniques
A1  - Yang, Jiadong
A1  - Xu, Hua
A1  - Jia, Peifa
Y1  - 2013///
KW  -  Estimation of distribution algorithms
KW  -  Evolutionary computation
KW  -  Features reduction
KW  - Genetic-based machine learning systems
JF  - Neurocomputing
VL  - 113
SP  - 105
EP  - 121
DO  - https://doi.org/10.1016/j.neucom.2013.01.014
UR  - https://www.sciencedirect.com/science/article/pii/S092523121300177X
N2  - Genetic-based machine learning (GBML) systems, which employ evolutionary algorithms (EAs) as search mechanisms, evolve rule-based classification models to represent target concepts. Compared to Michigan-style GBML, Pittsburgh-style GBML is expected to achieve more compact solutions. It has been shown that standard recombination operators in EAs do not assure an effective evolutionary search to solve sophisticated problems that contain strong interactions between features. On the other hand, when dealing with real-world classification tasks, irrelevant features not only complicate the problem but also incur unnecessary matchings in GBML systems, which increase the computational cost a lot. To handle the two problems mentioned above in an integrated manner, a new Pittsburgh-style GBML system is proposed. In the proposed method, classifiers are generated and recombined at two levels. At the high level, classifiers are recombined by rule-wise uniform crossover operators since each classifier consists of a variable-size rule set. At the low level, single rules contained in classifiers are reproduced via sampling Bayesian networks that characterize the global statistical information extracted from promising rules found so far. Furthermore, according to the statistical information in the rule population, an embedded approach is presented to detect and remove redundant features incrementally following the evolution of rule population. Results of empirical evaluation show that the proposed method outperforms the original Pittsburgh-style GBML system in terms of classification accuracy while reducing the computational cost. Furthermore, the proposed method is also competitive to other non-evolutionary, highly used machine learning methods. With respect to the performance of feature reduction, the proposed embedded approach is able to deliver solutions with higher classification accuracy when removing the same number of features as other feature reduction techniques do.
ER  - 
TY  - JOUR
T1  - Detecting health misinformation in online health communities: Incorporating behavioral features into machine learning based approaches
A1  - Zhao, Yuehua
A1  - Da, Jingwei
A1  - Yan, Jiaqi
Y1  - 2021///
KW  -  Misinformation detection
KW  -  Online health community
KW  - Health misinformation
JF  - Information Processing & Management
VL  - 58
IS  - 1
SP  - 102390
EP  - 102390
DO  - https://doi.org/10.1016/j.ipm.2020.102390
UR  - https://www.sciencedirect.com/science/article/pii/S0306457320308852
N2  - Curbing the diffusion of health misinformation on social media has long been a public concern since the spread of such misinformation can have adverse effects on public health. Previous studies mainly relied on linguistic features and textual features to detect online health-related misinformation. Based on the Elaboration Likelihood Model (ELM), this study proposed that the features of online health misinformation can be classified into two levels: central-level and peripheral-level. In this study, a novel health misinformation detection model was proposed which incorporated the central-level features (including topic features) and the peripheral-level features (including linguistic features, sentiment features, and user behavioral features). In addition, the following behavioral features were introduced to reflect the interaction characteristics of users: Discussion initiation, Interaction engagement, Influential scope, Relational mediation, and Informational independence. Due to the lack of a labeled dataset, we collected the dataset from a real online health community in order to provide a real scenario for data analysis. Four types of misinformation were identified through the coding analysis. The proposed model and its individual features were validated on the real-world dataset. The model correctly detected about 85% of the health misinformation. The results also suggested that behavioral features were more informative than linguistic features in detecting misinformation. The findings not only demonstrated the efficacy of behavioral features in health misinformation detection but also offered both methodological and theoretical contributions to misinformation detection from the perspective of integrating the features of messages as well as the features of message creators.
ER  - 
TY  - JOUR
T1  - An improved swarm optimized functional link artificial neural network (ISO-FLANN) for classification
A1  - Dehuri, Satchidananda
A1  - Roy, Rahul
A1  - Cho, Sung-Bae
A1  - Ghosh, Ashish
Y1  - 2012///
KW  -  Data mining
KW  -  FSN
KW  -  Functional link artificial neural networks
KW  -  Improved particle swarm optimization
KW  -  Multi-layer perception
KW  -  Particle swarm optimization
KW  -  SVM
KW  - Classification
JF  - Journal of Systems and Software
VL  - 85
IS  - 6
SP  - 1333
EP  - 1345
DO  - https://doi.org/10.1016/j.jss.2012.01.025
UR  - https://www.sciencedirect.com/science/article/pii/S0164121212000210
N1  - Special Issue: Agile Development
N2  - Multilayer perceptron (MLP) (trained with back propagation learning algorithm) takes large computational time. The complexity of the network increases as the number of layers and number of nodes in layers increases. Further, it is also very difficult to decide the number of nodes in a layer and the number of layers in the network required for solving a problem a priori. In this paper an improved particle swarm optimization (IPSO) is used to train the functional link artificial neural network (FLANN) for classification and we name it ISO-FLANN. In contrast to MLP, FLANN has less architectural complexity, easier to train, and more insight may be gained in the classification problem. Further, we rely on global classification capabilities of IPSO to explore the entire weight space, which is plagued by a host of local optima. Using the functionally expanded features; FLANN overcomes the non-linear nature of problems. We believe that the combined efforts of FLANN and IPSO (IPSO + FLANN=ISO−FLANN) by harnessing their best attributes can give rise to a robust classifier. An extensive simulation study is presented to show the effectiveness of proposed classifier. Results are compared with MLP, support vector machine(SVM) with radial basis function (RBF) kernel, FLANN with gradiend descent learning and fuzzy swarm net (FSN).
ER  - 
TY  - JOUR
T1  - Discovery of clinical pathway patterns from event logs using probabilistic topic models
A1  - Huang, Zhengxing
A1  - Dong, Wei
A1  - Ji, Lei
A1  - Gan, Chenxi
A1  - Lu, Xudong
A1  - Duan, Huilong
Y1  - 2014///
KW  -  Clinical event log
KW  -  Latent Dirichlet Allocation
KW  -  Pattern discovery
KW  -  Topic models
KW  - Clinical pathway analysis
JF  - Journal of Biomedical Informatics
VL  - 47
SP  - 39
EP  - 57
DO  - https://doi.org/10.1016/j.jbi.2013.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046413001445
N2  - Discovery of clinical pathway (CP) patterns has experienced increased attention over the years due to its importance for revealing the structure, semantics and dynamics of CPs, and to its usefulness for providing clinicians with explicit knowledge which can be directly used to guide treatment activities of individual patients. Generally, discovery of CP patterns is a challenging task as treatment behaviors in CPs often have a large variability depending on factors such as time, location and patient individual. Based on the assumption that CP patterns can be derived from clinical event logs which usually record various treatment activities in CP executions, this study proposes a novel approach to CP pattern discovery by modeling CPs using mixtures of an extension to the Latent Dirichlet Allocation family that jointly models various treatment activities and their occurring time stamps in CPs. Clinical case studies are performed to evaluate the proposed approach via real-world data sets recording typical treatment behaviors in patient careflow. The obtained results demonstrate the suitability of the proposed approach for CP pattern discovery, and indicate the promise in research efforts related to CP analysis and optimization.
ER  - 
TY  - CHAP
T1  - Mining scientific data
A1  - Ramakrishnan, Naren
A1  - Grama, Ananth Y
ED  - Zelkowitz, Marvin V
Y1  - 2002///
PB  - Elsevier
VL  - 55
T3  - Advances in Computers
SP  - 119
EP  - VIII
DO  - https://doi.org/10.1016/S0065-2458(01)80028-0
UR  - https://www.sciencedirect.com/science/article/pii/S0065245801800280
N2  - The past two decades have seen rapid advances in high-performance computing and tools for data acquisition in a variety of scientific domains. Coupled with the availability of massive storage systems and fast networking technology to manage and assimilate data, these have given a significant impetus to data mining in the scientific domain. Data mining is now recognized as a key computational technology, supporting traditional analysis, visualization, and design tasks. Diverse applications in domains such as mineral prospecting, computer-aided design, bioinformatics, and computational steering are now being viewed in the data mining framework. This has led to a very effective cross-fertilization of computational techniques from both continuous and discrete perspectives. In this chapter, we characterize the nature of scientific data mining activities and identify dominant recurring themes. We discuss algorithms, techniques, and methodologies for their effective application and present application studies that summarize the state-of-the-art in this emerging field. We conclude by identifying opportunities for future research in emerging domains.
ER  - 
TY  - JOUR
T1  - A synthetic informative minority over-sampling (SIMO) algorithm leveraging support vector machine to enhance learning from imbalanced datasets
A1  - Piri, Saeed
A1  - Delen, Dursun
A1  - Liu, Tieming
Y1  - 2018///
KW  -  Imbalanced data
KW  -  Machine learning
KW  -  Over-sampling
KW  -  Performance metrics
KW  -  Support vector machines
KW  - Predictive modeling
JF  - Decision Support Systems
VL  - 106
SP  - 15
EP  - 29
DO  - https://doi.org/10.1016/j.dss.2017.11.006
UR  - https://www.sciencedirect.com/science/article/pii/S016792361730218X
N2  - Developing decision support systems (DSS) based on imbalanced datasets is one the critical challenges in data mining and decision-analytics. A dataset is called imbalanced when the number of examples from one class outnumbers the number of the instances from another class. Learning from imbalanced datasets is one of the major challenges in machine learning. While a standard classifier could have a very good performance on a balanced dataset, when applied to an imbalanced dataset, its performance deteriorates dramatically. This poor performance is rather troublesome, especially in detecting the minority class, which usually is the class of interest. Therefore, the poor performance of machine learning techniques, which are used to develop DSS, negatively affect the practicality of DSS in real word problems. Over-sampling the minority class is one of the most promising remedies for imbalanced data learning. In this study, we propose a new synthetic informative minority over-sampling (SIMO) algorithm leveraging support vector machine (SVM). In this algorithm, first SVM is applied to the original imbalanced dataset, then, minority examples close to the SVM decision boundary, as the informative minority examples are over-sampled. We also developed another version of SIMO and call it weighted SIMO (W-SIMO). W-SIMO is different from SIMO in the degree of over-sampling the informative minority examples. In W-SIMO, incorrectly classified informative minority examples are over-sampled with a higher degree compared to the correctly classified informative minority examples. In this way, there is more focus on incorrectly classified minority examples. The over-sampled dataset can be used to train any classifier. We applied these algorithms to the 15 publicly available benchmark imbalanced datasets and assessed their performance in comparison with existing approaches in the area of imbalanced data learning. The results showed that our algorithms had the best performance in all datasets compared to other approaches.
ER  - 
TY  - JOUR
T1  - Programmable fluid transport on photolithographically micropatterned cloth devices: Towards the development of facile, multifunctional colorimetric diagnostic platforms
A1  - Li, Huijie
A1  - Liu, Cuiling
A1  - Wang, Dan
A1  - Zhang, Chunsun
Y1  - 2018///
KW  -  Micropatterned cloth devices
KW  -  Multifunctional colorimetric detection
KW  -  Photolithography
KW  -  Programmable fluid transport
KW  - Polyvinyl cinnamate-based photoresist
JF  - Sensors and Actuators B: Chemical
VL  - 255
SP  - 2416
EP  - 2430
DO  - https://doi.org/10.1016/j.snb.2017.08.215
UR  - https://www.sciencedirect.com/science/article/pii/S0925400517316465
N2  - In this work, a novel strategy to construct coin-sized, multifunctional micropatterned cloth devices (μPCDs) based on programmable fluid transport has been reported for the first time. A photolithography technique is used to fabricate μPCDs that operate as simple disposable colorimetric sensors. This fabrication method allows hydrophobic barriers to be well integrated onto the cloth channels to form programmable cloth-based diagnostic devices. In the present study, several different programmable μPCDs were designed and fabricated, including basic μPCDs, μPCDs with parallel or serial fluidic paths, and μPCDs with serial-parallel fluidic paths. The fluid transport in the resulting μPCDs was programmed to provide qualitative or quantitative analyses. The μPCDs with serial-parallel fluidic paths were used as prototypes for qualitative detection of glucose concentration, pH, and protein concentration in artificial urine (AU) samples. Moreover, a single cloth-based device could be programmed for sequential reaction processes, and the quantitative detection of nickel(II) ions in water, nitrite ions in AU, and glucose in AU by two-step enzymatic reactions was achieved. The results indicate that the developed programmable μPCDs can provide a new sensing platform for other biochemical assays.
ER  - 
TY  - JOUR
T1  - Deep attributed network representation learning of complex coupling and interaction
A1  - Li, Zhao
A1  - Wang, Xin
A1  - Li, Jianxin
A1  - Zhang, Qingpeng
Y1  - 2021///
KW  -  Attributed network
KW  -  Autoencoder
KW  -  Structural role proximity
KW  - Network representation learning
JF  - Knowledge-Based Systems
VL  - 212
SP  - 106618
EP  - 106618
DO  - https://doi.org/10.1016/j.knosys.2020.106618
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120307474
N2  - Networks that can describe complex systems in nature are increasingly coupled and interacted, and effective modeling on complex coupling and interaction information is an important research direction of artificial intelligence. Representation learning provides us with a paradigm to solve such issues, but the current network representation learning methods are difficult to capture the coupling and interaction information in complex networks. In this paper, we propose a novel deep attributed network representation learning model framework (RolEANE), which can effectively preserve the highly nonlinear coupling and interactive network topological structure and attribute information. We design two different structural role proximity enhancement strategies for the deep autoencoder in the model framework, so that it can efficiently capture network topological structure and attribute information. In addition, the neighbor-modified Skip-Gram model in our model framework can efficiently and seamlessly integrate network topological structure and attribute information, and the selection of an appropriate representation learning output strategy can significantly improve the final performance of the algorithm. The experiments on four real datasets show that our method consistently outperforms the state-of-the-art network representation learning methods. On the node classification task, the average performance is improved by 4.52%–10.28% than the optimal baseline method; on the link prediction task, the average performance is 4.63% higher than the optimal baseline method.
ER  - 
TY  - JOUR
T1  - Soft computing in medicine
A1  - Yardimci, Ahmet
Y1  - 2009///
KW  -  Fuzzy–genetic algorithms
KW  -  Fuzzy–neural systems
KW  -  Medicine
KW  -  Neural–genetic algorithms
KW  - Soft computing
JF  - Applied Soft Computing
VL  - 9
IS  - 3
SP  - 1029
EP  - 1043
DO  - https://doi.org/10.1016/j.asoc.2009.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S1568494609000246
N2  - Soft computing (SC) is not a new term; we have gotten used to reading and hearing about it daily. Nowadays, the term is used often in computer science and information technology. It is possible to define SC in different ways. Nonetheless, SC is a consortium of methodologies which works synergistically and provides, in one form or another, flexible information processing capability for handling real life ambiguous situations. Its aim is to exploit the tolerance for imprecision, uncertainty, approximate reasoning and partial truth in order to achieve tractability, robustness and low-cost solutions. SC includes fuzzy logic (FL), neural networks (NNs), and genetic algorithm (GA) methodologies. SC combines these methodologies as FL and NN (FL–NN), NN and GA (NN–GA) and FL and GA (FL–GA). Recent years have witnessed the phenomenal growth of bio-informatics and medical informatics by using computational techniques for interpretation and analysis of biological and medical data. Among the large number of computational techniques used, SC, which incorporates neural networks, evolutionary computation, and fuzzy systems, provides unmatched utility because of its demonstrated strength in handling imprecise information and providing novel solutions to hard problems. The aim of this paper is to introduce briefly the various SC methodologies and to present various applications in medicine between the years 2000 and 2008. The scope is to demonstrate the possibilities of applying SC to medicine-related problems. The recent published knowledge about use of SC in medicine is researched in MEDLINE. This study detects which methodology or methodologies of SC are used frequently together to solve the special problems of medicine. According to MEDLINE database searches, the rates of preference of SC methodologies in medicine were found as 68% of FL–NN, 27% of NN–GA and 5% of FL–GA. So far, FL–NN methodology was significantly used in medicine. The rates of using FL–NN in clinical science, diagnostic science and basic science were found as %83, %71 and %48, respectively. On the other hand NN–GA and FL–GA methodologies were mostly preferred by basic science of medicine. Another message emerging from this survey is that the number of papers which used NN–GA methodology has continuously risen until today. Also search results put the case clearly that FL–GA methodology has not applied well enough to medicine yet. Undeniable interest in studying SC methodologies in genetics, physiology, radiology, cardiology, and neurology disciplines proves that studying SC is very fruitful in these disciplines and it is expected that future researches in medicine will use SC more than it is used today to solve more complex problems.
ER  - 
TY  - JOUR
T1  - Parallel multiobjective memetic RBFNNs design and feature selection for function approximation problems
A1  - Guillén, A
A1  - Pomares, H
A1  - González, J
A1  - Rojas, I
A1  - Valenzuela, O
A1  - Prieto, B
Y1  - 2009///
KW  -  Function approximation
KW  -  MPI
KW  -  Neural networks
KW  -  RBF
KW  -  RBFNN
KW  - Parallel genetic algorithms
JF  - Neurocomputing
VL  - 72
IS  - 16
SP  - 3541
EP  - 3555
DO  - https://doi.org/10.1016/j.neucom.2008.12.037
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209001970
N1  - Financial Engineering Computational and Ambient Intelligence (IWANN 2007)
N2  - The design of radial basis function neural networks (RBFNNs) still remains as a difficult task when they are applied to classification or to regression problems. The difficulty arises when the parameters that define an RBFNN have to be set, these are: the number of RBFs, the position of their centers and the length of their radii. Another issue that has to be faced when applying these models to real world applications is to select the variables that the RBFNN will use as inputs. The literature presents several methodologies to perform these two tasks separately, however, due to the intrinsic parallelism of the genetic algorithms, a parallel implementation will allow the algorithm proposed in this paper to evolve solutions for both problems at the same time. The parallelization of the algorithm not only consists in the evolution of the two problems but in the specialization of the crossover and mutation operators in order to evolve the different elements to be optimized when designing RBFNNs. The subjacent genetic algorithm is the non-sorting dominated genetic algorithm II (NSGA-II) that helps to keep a balance between the size of the network and its approximation accuracy in order to avoid overfitted networks. Another of the novelties of the proposed algorithm is the incorporation of local search algorithms in three stages of the algorithm: initialization of the population, evolution of the individuals and final optimization of the Pareto front. The initialization of the individuals is performed hybridizing clustering techniques with the mutual information (MI) theory to select the input variables. As the experiments will show, the synergy of the different paradigms and techniques combined by the presented algorithm allow to obtain very accurate models using the most significant input variables.
ER  - 
TY  - JOUR
T1  - Lexical patterns, features and knowledge resources for coreference resolution in clinical notes
A1  - Gooch, Phil
A1  - Roudsari, Abdul
Y1  - 2012///
KW  -  Algorithms
KW  -  Clinical records
KW  -  Coreference resolution
KW  -  Knowledge engineering
KW  - Natural language processing
JF  - Journal of Biomedical Informatics
VL  - 45
IS  - 5
SP  - 901
EP  - 912
DO  - https://doi.org/10.1016/j.jbi.2012.02.012
UR  - https://www.sciencedirect.com/science/article/pii/S1532046412000421
N1  - Text Mining and Natural Language Processing in Pharmacogenomics
N2  - Generation of entity coreference chains provides a means to extract linked narrative events from clinical notes, but despite being a well-researched topic in natural language processing, general-purpose coreference tools perform poorly on clinical texts. This paper presents a knowledge-centric and pattern-based approach to resolving coreference across a wide variety of clinical records from two corpora (Ontology Development and Information Extraction (ODIE) and i2b2/VA), and describes a method for generating coreference chains using progressively pruned linked lists that reduces the search space and facilitates evaluation by a number of metrics. Independent evaluation results give an F-measure for each corpus of 79.2% and 87.5%, respectively. A baseline of blind coreference of mentions of the same class gives F-measures of 65.3% and 51.9% respectively. For the ODIE corpus, recall is significantly improved over the baseline (p<0.05) but overall there was no statistically significant improvement in F-measure (p>0.05). For the i2b2/VA corpus, recall, precision, and F-measure are significantly improved over the baseline (p<0.05). Overall, our approach offers performance at least as good as human annotators and greatly increased performance over general-purpose tools. The system uses a number of open-source components that are available to download.
ER  - 
TY  - JOUR
T1  - An energy screening and morphology characterization-based hybrid expert scheme for automatic identification of micro-sleep event K-complex
A1  - Zhao, Xian
A1  - Chen, Chen
A1  - Zhou, Wei
A1  - Wang, Yalin
A1  - Fan, Jiahao
A1  - Wang, Zeyu
A1  - Akbarzadeh, Saeed
A1  - Chen, Wei
Y1  - 2021///
KW  -  Automatic identification
KW  -  K-complexes
KW  -  Morphological filtering
KW  -  Morphology characterization
KW  -  Teager energy operator
KW  - Sleep EEG
JF  - Computer Methods and Programs in Biomedicine
VL  - 201
SP  - 105955
EP  - 105955
DO  - https://doi.org/10.1016/j.cmpb.2021.105955
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721000304
N2  - Background and Objective
: K-complexes, as a significant indicator in sleep staging and sleep protection, are an important micro-event in sleep analysis. Clinically, K-complexes are recognized through the expert visual inspection of electroencephalogram (EEG) during sleep. Since this process is laborious and has high inter-observer variability, developing automated K-complex detection methods can alleviate the burden on clinicians while providing reliable recognition results. However, existing methods face the following issues. First, most work only identifies the K-complexes in stage 2, which requires distinguishing the sleep stages as the prerequisite for further events’ identification. Second, most approaches can only detect the occurrence of events without the ability to predict their location and duration, which are also essential to sleep analysis.
Methods
: In this work, a novel hybrid expert scheme for K-complex detection is proposed by integrating signal morphology with expert knowledge into the decision-making process. To eliminate artifacts, and to minimize the individual variability in raw sleep EEG signals, the potential K-complex candidates are first screened by combining Teager energy operator (TEO) and personalized thresholds. Then, to distinguish signal shapes from background activity, a novel frame of filtering based on morphological filtering (MF) is devised to differentiate morphological components of K-complex waveforms from EEG series. Finally, K-complex waveforms are identified from the extracted morphological information by judgment rules, which are inspired by expert knowledge of micro-sleep events.
Results
: Detection performance is evaluated by its application on the public database MASS-C1 (Montreal archives of sleep studies cohort one) which includes the recordings of 19 healthy adults. The detection performance demonstrates an F-measure of 0.63 with a recall of 0.81 and a precision of 0.53 on average. The duration error between events and detections is 0.10 s.
Conclusions
: The presented scheme has detected the occurrence of events. Meanwhile, it has recognized their locations and durations. The favorable results exhibit that the proposed scheme outperforms the state-of-the-art studies and has great potential to help release the burden of experts in sleep EEG analysis.
ER  - 
TY  - JOUR
T1  - Sensor-based and vision-based human activity recognition: A comprehensive survey
A1  - Minh Dang, L
A1  - Min, Kyungbok
A1  - Wang, Hanxiang
A1  - Jalil Piran, Md.
A1  - Hee Lee, Cheol
A1  - Moon, Hyeonjoon
Y1  - 2020///
KW  -  Action recognition
KW  -  Context-awareness
KW  -  Deep learning
KW  -  Human-centric sensing
KW  -  Sensors
KW  -  Vision
KW  - Human activity recognition
JF  - Pattern Recognition
VL  - 108
SP  - 107561
EP  - 107561
DO  - https://doi.org/10.1016/j.patcog.2020.107561
UR  - https://www.sciencedirect.com/science/article/pii/S0031320320303642
N2  - Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type. After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research.
ER  - 
TY  - JOUR
T1  - Anatomy of data integration
A1  - Brazhnik, Olga
A1  - Jones, John F
Y1  - 2007///
KW  -  Clinical data integration
KW  -  Conceptual modeling
KW  -  Data processing
KW  -  Database architecture
KW  - Informational value of data
JF  - Journal of Biomedical Informatics
VL  - 40
IS  - 3
SP  - 252
EP  - 269
DO  - https://doi.org/10.1016/j.jbi.2006.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046406000967
N2  - Producing reliable information is the ultimate goal of data processing. The ocean of data created with the advances of science and technologies calls for integration of data coming from heterogeneous sources that are diverse in their purposes, business rules, underlying models and enabling technologies. Reference models, Semantic Web, standards, ontology, and other technologies enable fast and efficient merging of heterogeneous data, while the reliability of produced information is largely defined by how well the data represent the reality. In this paper, we initiate a framework for assessing the informational value of data that includes data dimensions; aligning data quality with business practices; identifying authoritative sources and integration keys; merging models; uniting updates of varying frequency and overlapping or gapped data sets.
ER  - 
TY  - JOUR
T1  - Towards an efficient and Energy-Aware mobile big health data architecture
A1  - Navaz, Alramzana Nujum
A1  - Serhani, Mohamed Adel
A1  - Al-Qirim, Nabeel
A1  - Gergely, Marton
Y1  - 2018///
KW  -  M-health
KW  -  Mobile big data
KW  -  Mobile offloading
KW  -  Processing
KW  -  Resources optimization
KW  - Analytics customization
JF  - Computer Methods and Programs in Biomedicine
VL  - 166
SP  - 137
EP  - 154
DO  - https://doi.org/10.1016/j.cmpb.2018.10.008
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718305601
N2  - Background and objectives
Mobile and ubiquitous devices are everywhere, generating an exorbitant amount of data. New generations of healthcare systems are using mobile devices to continuously collect large amounts of different types of data from patients with chronic diseases. The challenge with such Mobile Big Data in general, is how to meet the growing performance demands of the mobile resources handling these tasks, while simultaneously minimizing their consumption.
Methods
This research proposes a scalable architecture for processing Mobile Big Data. The architecture is developed around three new algorithms for the effective use of resources in performing mobile data processing and analytics: mobile resources optimization, mobile analytics customization, and mobile offloading. The mobile resources optimization algorithm monitors resources and automatically switches off unused network connections and application services whenever resources are limited. The mobile analytics customization algorithm attempts to save energy by customizing the analytics processes through the implementation of some data-aware schemes. Finally, the mobile offloading algorithm uses some heuristics to intelligently decide whether to process data locally, or delegate it to a cloud back-end server.
Results
The three algorithms mentioned above are tested using Android-based mobile devices on real Electroencephalography (EEG) data streams retrieved from sensors and an online data bank. Results show that the three combined algorithms proved their effectiveness in optimizing the resources of mobile devices in handling, processing, and analyzing EEG data.
Conclusion
We developed an energy-efficient model for Mobile Big Data which addressed key limitations in mobile device processing and analytics and reduced execution time and limited battery resources. This was supported with the development of three new algorithms for the effective use of resources, energy saving, parallel processing and analytics customization.
ER  - 
TY  - JOUR
T1  - A comprehensive health assessment framework to facilitate IoT-assisted smart workouts: A predictive healthcare perspective
A1  - Bhatia, Munish
A1  - Sood, Sandeep K
Y1  - 2017///
KW  -  Internet of Things (IoT)
KW  -  M-healthcare
KW  -  Probabilistic State of Vulnerability (PSoV)
KW  -  Smart workouts
KW  - Artificial Neural Networks
JF  - Computers in Industry
VL  - 92-93
SP  - 50
EP  - 66
DO  - https://doi.org/10.1016/j.compind.2017.06.009
UR  - https://www.sciencedirect.com/science/article/pii/S0166361516303177
N2  - Enormous potential of Internet of Things (IoT) Technology has made it feasible to perceive and analyze real time health conditions in ubiquitous manner. Moreover, incorporation of IoT in healthcare industry has led researchers around the world to develop smart applications like mobile healthcare, health-aware recommendations, and intelligent healthcare systems. Inspired from these aspects, this research presents an intelligent healthcare framework based on IoT Technology to provide ubiquitous healthcare to person during his/her workout sessions. The intelligence of the presented framework lies with its ability to analyze real time health conditions during workouts and predict probabilistic health state vulnerability. For predictive purpose, the proposed framework indulges the utilization of Artificial Neural Network (ANN) model, which is comprised of three phases namely, monitor, learn, and predict. In addition to this, the presented framework is supported by a mathematical foundation to predict probabilistic vulnerability, in terms of Probabilistic State of Vulnerability (PSoV). In order to determine the validity and applicability of the proposed framework, experiments were performed where 5 people with different attributes are monitored for 14 days using numerous smart sensors. Results, upon comparison with various state-of-the-art techniques, depict that the proposed system is superior in performance and is highly effective in delivering healthcare services during workouts.
ER  - 
TY  - JOUR
T1  - Towards an automatic early stress recognition system for office environments based on multimodal measurements: A review
A1  - Alberdi, Ane
A1  - Aztiria, Asier
A1  - Basarab, Adrian
Y1  - 2016///
KW  -  Behaviour
KW  -  Early detection
KW  -  Multimodality
KW  -  Physiology
KW  - Stress
JF  - Journal of Biomedical Informatics
VL  - 59
SP  - 49
EP  - 75
DO  - https://doi.org/10.1016/j.jbi.2015.11.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415002750
N2  - Stress is a major problem of our society, as it is the cause of many health problems and huge economic losses in companies. Continuous high mental workloads and non-stop technological development, which leads to constant change and need for adaptation, makes the problem increasingly serious for office workers. To prevent stress from becoming chronic and provoking irreversible damages, it is necessary to detect it in its early stages. Unfortunately, an automatic, continuous and unobtrusive early stress detection method does not exist yet. The multimodal nature of stress and the research conducted in this area suggest that the developed method will depend on several modalities. Thus, this work reviews and brings together the recent works carried out in the automatic stress detection looking over the measurements executed along the three main modalities, namely, psychological, physiological and behavioural modalities, along with contextual measurements, in order to give hints about the most appropriate techniques to be used and thereby, to facilitate the development of such a holistic system.
ER  - 
TY  - JOUR
T1  - A survey on human-aware robot navigation
A1  - Möller, Ronja
A1  - Furnari, Antonino
A1  - Battiato, Sebastiano
A1  - Härmä, Aki
A1  - Farinella, Giovanni Maria
Y1  - 2021///
KW  -  Active vision
KW  -  Activity recognition
KW  -  Human robot interaction
KW  - Robot navigation
JF  - Robotics and Autonomous Systems
VL  - 145
SP  - 103837
EP  - 103837
DO  - https://doi.org/10.1016/j.robot.2021.103837
UR  - https://www.sciencedirect.com/science/article/pii/S0921889021001226
N2  - Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human–robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.
ER  - 
TY  - JOUR
T1  - Operational research techniques in medical treatment and diagnosis: A review
A1  - Bartolozzi, F
A1  - de Gaetano, A
A1  - Di Lena, E
A1  - Marino, S
A1  - Nieddu, L
A1  - Patrizi, G
Y1  - 2000///
KW  -  Medical modelling
KW  -  Optimal control
KW  -  Simultaneous identification and optimization
KW  - Dynamic systems
JF  - European Journal of Operational Research
VL  - 121
IS  - 3
SP  - 435
EP  - 466
DO  - https://doi.org/10.1016/S0377-2217(99)00017-X
UR  - https://www.sciencedirect.com/science/article/pii/S037722179900017X
N2  - Many modern techniques for the diagnosis of pathological states in humans and for their subsequent treatment can be posed as nonlinear identification problems of essentially nonlinear dynamic systems or as nonlinear optimal control problems. It can be shown that the linearised versions of such models are inadequate and do not represent at all well the complexity of the problem. Thus, nonlinear estimation and control techniques are required for progress to be made in this field. The aim of this review is to examine some models suggested in the medical literature for the modelling of certain medical treatments and diagnoses. Then examine how these models can be enriched by using Operational Research techniques so that a better control is provided on the diagnosis and the treatment, as well as the formulation of more precise models of the phenomenon. The review will present some applications both therapeutic and diagnostic that have appeared in the literature. Special interest will be bestowed on hyperthermic systems in oncological treatment and glucose–insulin dynamics for diabetic patients, while heart dynamics and magnetic resonance imaging will also receive attention. These applications are good examples to show the advantages of Operational Research methods in this field of endeavour. The outline of the paper is the following. After the introduction, in section two a brief description of nonlinear system models of phenomena will be given, for definitional and descriptive purposes. In section three a discussion of how to apply System theory in the medical field will be presented, together with an analysis of the possible benefits. In section four some applications of dynamical models to medical diagnosis and treatment will be described, while in section five the appropriate conclusions will be stated.
ER  - 
TY  - JOUR
T1  - Model-based multidimensional clustering of categorical data
A1  - Chen, Tao
A1  - Zhang, Nevin L
A1  - Liu, Tengfei
A1  - Poon, Kin Man
A1  - Wang, Yi
Y1  - 2012///
KW  -  Categorical data
KW  -  Latent tree models
KW  -  Multidimensional clustering
KW  - Model-based clustering
JF  - Artificial Intelligence
VL  - 176
IS  - 1
SP  - 2246
EP  - 2269
DO  - https://doi.org/10.1016/j.artint.2011.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S000437021100110X
N2  - Existing models for cluster analysis typically consist of a number of attributes that describe the objects to be partitioned and one single latent variable that represents the clusters to be identified. When one analyzes data using such a model, one is looking for one way to cluster data that is jointly defined by all the attributes. In other words, one performs unidimensional clustering. This is not always appropriate. For complex data with many attributes, it is more reasonable to consider multidimensional clustering, i.e., to partition data along multiple dimensions. In this paper, we present a method for performing multidimensional clustering on categorical data and show its superiority over unidimensional clustering.
ER  - 
TY  - JOUR
T1  - GIR-based ensemble sampling approaches for imbalanced learning
A1  - Tang, Bo
A1  - He, Haibo
Y1  - 2017///
KW  -  Adaptive learning
KW  -  Boosting and bagging
KW  -  Generalized imbalance ratio
KW  -  Undersampling and oversampling
KW  - Imbalanced learning
JF  - Pattern Recognition
VL  - 71
SP  - 306
EP  - 319
DO  - https://doi.org/10.1016/j.patcog.2017.06.019
UR  - https://www.sciencedirect.com/science/article/pii/S003132031730242X
N2  - This paper presents two adaptive ensemble sampling approaches for imbalanced learning: one is the undersampling-based approach, and the other one is the oversampling-based approach, with the objectives of bias reduction and adaptive learning. Both of these two approaches are based on a novel class imbalance metric, termed generalized imbalance ratio (GIR), instead of the conventional sample size ratio. Specifically, these two sampling-based approaches adaptively split the imbalanced learning problem into multiple balanced learning subproblems in a probabilistic way, which forces the classifiers trained in the subproblems focus on those difficult to learn samples. In each subproblem, several weak classifiers are trained in a boosting manner. A final stronger classifier is further built by combining all these weak classifiers in a bagging manner. Extensive experiments are conducted on real-life UCI imbalanced data sets to evaluate the performance of the proposed methods. The superior performance demonstrates the effectiveness of the proposed methods and indicates wide potential applications in data mining.
ER  - 
TY  - JOUR
T1  - Posterior probability profiles for the automated assessment of the recovery of patients with stroke from activity of daily living tasks
A1  - Van Dijck, Gert
A1  - Van Vaerenbergh, Jo
A1  - Van Hulle, Marc M
Y1  - 2009///
KW  -  Hybrid filter-wrapper feature selection
KW  -  Mutual information
KW  -  Stroke recovery
KW  - Activity of daily living tasks
JF  - Artificial Intelligence in Medicine
VL  - 46
IS  - 3
SP  - 233
EP  - 249
DO  - https://doi.org/10.1016/j.artmed.2009.03.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365709000517
N2  - Summary
Objective
Assessing recovery after stroke has been so far a time consuming procedure in which trained clinicians are required. A demand for automated assessment techniques arises due to the increasing number of patients with stroke and the continuous growth of new treatment options. In this study, we investigate the applicability of isometric force and torque measurements in activity of daily living tasks to assess the functional recovery after stroke in an automated way.
Methods and materials
A new hybrid filter-wrapper feature subset technology was developed for a new mechatronic platform with the aim to identify the most important features and sensors that can distinguish normal controls from patients with stroke. We compared 3 different classification algorithms to make the distinction: k-nearest neighbors, kernel density estimation and least-squares support vector machines. Based on isometric force and torque measurements obtained from 16 patients with a first-ever ischemic or haemorrhagic stroke within the middle cerebral artery territory, we computed for each subject the probability to belong to the class of normal subjects. These probabilities were computed during a period of 6 months post-stroke to quantify the level of recovery during this period. The posterior probabilities were validated by means of a correlation study with the Lindmark modified Fugl-Meyer assessment.
Results
Patients with stroke and normal controls could be distinguished with an accuracy of 98.25% by means of kernel density estimation. The posterior probability profiles had a correlation of 76.6% and 80.29% with the global score of the Lindmark modified Fugl-Meyer scale and ‘part A’, the upper extremity subscore, respectively. This degree of correlation was as high as obtained with supervised scoring techniques such as the Barthel index.
Conclusion
This study shows that the assessment of recovery after stroke can be automated by means of posterior probability profiles due to their high correlation with the Fugl-Meyer assessment. The posterior probability profiles confirm the importance of a recovery within the first weeks after stroke to obtain a higher recovery plateau compared to later changes in recovery.
ER  - 
TY  - JOUR
T1  - Towards identifying intervention arms in randomized controlled trials: Extracting coordinating constructions
A1  - Chung, Grace Yuet-Chee
Y1  - 2009///
KW  -  Biomedical natural langauge processing
KW  -  Biomedical text mining
KW  -  Medical informatics
KW  - Information extraction
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 5
SP  - 790
EP  - 800
DO  - https://doi.org/10.1016/j.jbi.2008.12.011
UR  - https://www.sciencedirect.com/science/article/pii/S1532046408001573
N1  - Biomedical Natural Language Processing
N2  - Background: Large numbers of reports of randomized controlled trials (RCTs) are published each year, and it is becoming increasingly difficult for clinicians practicing evidence-based medicine to find answers to clinical questions. The automatic machine extraction of RCT experimental details, including design methodology and outcomes, could help clinicians and reviewers locate relevant studies more rapidly and easily. Aim: This paper investigates how the comparison of interventions is documented in the abstracts of published RCTs. The ultimate goal is to use automated text mining to locate each intervention arm of a trial. This preliminary work aims to identify coordinating constructions, which are prevalent in the expression of intervention comparisons. Methods and results: An analysis of the types of constructs that describe the allocation of intervention arms is conducted, revealing that the compared interventions are predominantly embedded in coordinating constructions. A method is developed for identifying the descriptions of the assignment of treatment arms in clinical trials, using a full sentence parser to locate coordinating constructions and a statistical classifier for labeling positive examples. Predicate-argument structures are used along with other linguistic features with a maximum entropy classifier. An F-score of 0.78 is obtained for labeling relevant coordinating constructions in an independent test set. Conclusions: The intervention arms of a randomized controlled trials can be identified by machine extraction incorporating syntactic features derived from full sentence parsing.
ER  - 
TY  - JOUR
T1  - A benchmark and comparison of active learning for logistic regression
A1  - Yang, Yazhou
A1  - Loog, Marco
Y1  - 2018///
KW  -  Benchmark
KW  -  Experimental design
KW  -  Logistic regression
KW  -  Preference maps
KW  - Active learning
JF  - Pattern Recognition
VL  - 83
SP  - 401
EP  - 415
DO  - https://doi.org/10.1016/j.patcog.2018.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318302140
N2  - Logistic regression is by far the most widely used classifier in real-world applications. In this paper, we benchmark the state-of-the-art active learning methods for logistic regression and discuss and illustrate their underlying characteristics. Experiments are carried out on three synthetic datasets and 44 real-world datasets, providing insight into the behaviors of these active learning methods with respect to the area of the learning curve (which plots classification accuracy as a function of the number of queried examples) and their computational costs. Surprisingly, one of the earliest and simplest suggested active learning methods, i.e., uncertainty sampling, performs exceptionally well overall. Another remarkable finding is that random sampling, which is the rudimentary baseline to improve upon, is not overwhelmed by individual active learning techniques in many cases.
ER  - 
TY  - JOUR
T1  - Optimization techniques in respiratory control system models
A1  - Serna, Leidy Y
A1  - Mañanas, Miguel Ángel
A1  - Marín, Jesús
A1  - Hernández, Alher Mauricio
A1  - Benito, Salvador
Y1  - 2016///
KW  -  Mechanical work of breathing
KW  -  Optimal control
KW  -  Optimization algorithms
KW  - Respiratory control system
JF  - Applied Soft Computing
VL  - 48
SP  - 431
EP  - 443
DO  - https://doi.org/10.1016/j.asoc.2016.07.033
UR  - https://www.sciencedirect.com/science/article/pii/S1568494616303684
N2  - One of the most complex physiological systems whose modeling is still an open study is the respiratory control system where different models have been proposed based on the criterion of minimizing the work of breathing (WOB). The aim of this study is twofold: to compare two known models of the respiratory control system which set the breathing pattern based on quantifying the respiratory work; and to assess the influence of using direct-search or evolutionary optimization algorithms on adjustment of model parameters. This study was carried out using experimental data from a group of healthy volunteers under CO2 incremental inhalation, which were used to adjust the model parameters and to evaluate how much the equations of WOB follow a real breathing pattern. This breathing pattern was characterized by the following variables: tidal volume, inspiratory and expiratory time duration and total minute ventilation. Different optimization algorithms were considered to determine the most appropriate model from physiological viewpoint. Algorithms were used for a double optimization: firstly, to minimize the WOB and secondly to adjust model parameters. The performance of optimization algorithms was also evaluated in terms of convergence rate, solution accuracy and precision. Results showed strong differences in the performance of optimization algorithms according to constraints and topological features of the function to be optimized. In breathing pattern optimization, the sequential quadratic programming technique (SQP) showed the best performance and convergence speed when respiratory work was low. In addition, SQP allowed to implement multiple non-linear constraints through mathematical expressions in the easiest way. Regarding parameter adjustment of the model to experimental data, the evolutionary strategy with covariance matrix and adaptation (CMA-ES) provided the best quality solutions with fast convergence and the best accuracy and precision in both models. CMAES reached the best adjustment because of its good performance on noise and multi-peaked fitness functions. Although one of the studied models has been much more commonly used to simulate respiratory response to CO2 inhalation, results showed that an alternative model has a more appropriate cost function to minimize WOB from a physiological viewpoint according to experimental data.
ER  - 
TY  - JOUR
T1  - Modifications of the construction and voting mechanisms of the Random Forests Algorithm
A1  - Tripoliti, Evanthia E
A1  - Fotiadis, Dimitrios I
A1  - Manis, George
Y1  - 2013///
KW  -  Decision tree
KW  -  Ensemble methods
KW  -  Random Forests
KW  -  Weighted voting
KW  - Classification
JF  - Data & Knowledge Engineering
VL  - 87
SP  - 41
EP  - 65
DO  - https://doi.org/10.1016/j.datak.2013.07.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X13000748
N2  - The aim of this work is to propose modifications of the Random Forests algorithm which improve its prediction performance. The suggested modifications intend to increase the strength and decrease the correlation of individual trees of the forest and to improve the function which determines how the outputs of the base classifiers are combined. This is achieved by modifying the node splitting and the voting procedure. Different approaches concerning the number of the predictors and the evaluation measure which determines the impurity of the node are examined. Regarding the voting procedure, modifications based on feature selection, clustering, nearest neighbors and optimization techniques are proposed. The novel feature of the current work is that it proposes modifications, not only for the improvement of the construction or the voting mechanisms but also, for the first time, it examines the overall improvement of the Random Forests algorithm (a combination of construction and voting). We evaluate the proposed modifications using 24 datasets. The evaluation demonstrates that the proposed modifications have positive effect on the performance of the Random Forests algorithm and they provide comparable, and, in most cases, better results than the existing approaches.
ER  - 
TY  - JOUR
T1  - Correntropy-based robust multilayer extreme learning machines
A1  - Liangjun, Chen
A1  - Honeine, Paul
A1  - Hua, Qu
A1  - Jihong, Zhao
A1  - Xia, Sun
Y1  - 2018///
KW  -  Computer aided cancer diagnosis
KW  -  Correntropy
KW  -  Extreme learning machine
KW  -  Unsupervised feature learning
KW  - Deep learning
JF  - Pattern Recognition
VL  - 84
SP  - 357
EP  - 370
DO  - https://doi.org/10.1016/j.patcog.2018.07.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318302401
N2  - In extreme learning machines (ELM), the hidden node parameters are randomly generated and the output weights can be analytically computed. To overcome the bad feature extraction ability of the shallow architecture of ELM, the hierarchical ELM has been extensively studied as a deep architecture with multilayer neural network. However, the commonly used mean square error (MSE) criterion is very sensitive to outliers and impulsive noises, generally existing in real world data. In this paper, we investigate the correntropy to improve the robustness of the multilayer ELM and provide sparser representation. The correntropy, as a nonlinear measure of similarity, is robust to outliers and can approximate different norms (from ℓ0 to ℓ2). A new full correntropy based multilayer extreme learning machine (FC-MELM) algorithm is proposed to handle the classification of datasets which are corrupted by impulsive noises or outliers. The contributions of this paper are three-folds: (1) The MSE based reconstruction loss is replaced by the correntropy based loss function; In this way, the robustness of the ELM based multilayer algorithms is enhanced. (2) The traditional ℓ1-based sparsity penalty term is also replaced by a correntropy-based sparsity penalty term, which can further improve the performance of the proposed algorithm with a sparser representation of the data. The combination of (1) and (2) provides the correntropy-based ELM autoencoder. (3) The FC-MELM is proposed by using the correntropy-based ELM autoencoder as a building block. It is notable that the FC-MELM is trained in a forward manner, which means fine-tuning procedure is not required. Thus, the FC-MELM has great advantage in learning efficiently when compared with traditional deep learning algorithms. The good property of the proposed algorithm is confirmed by the experiments on well-known benchmark datasets, including the MNIST datasets, the NYU Object Recognition Benchmark dataset, and the Moore network traffic dataset. Finally, the proposed FC-MELM algorithm is applied to address Computer Aided Cancer Diagnosis. Experiments conducted on the well-known Wisconsin Breast Cancer Data (Diagnostic) dataset are presented and show that the proposed FC-MELM outperforms state-of-the-art methods in solving computer aided cancer diagnosis problems.
ER  - 
TY  - JOUR
T1  - Itemset mining: A constraint programming perspective
A1  - Guns, Tias
A1  - Nijssen, Siegfried
A1  - De Raedt, Luc
Y1  - 2011///
KW  -  Constraint programming
KW  -  Itemset mining
KW  - Data mining
JF  - Artificial Intelligence
VL  - 175
IS  - 12
SP  - 1951
EP  - 1983
DO  - https://doi.org/10.1016/j.artint.2011.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0004370211000646
N2  - The field of data mining has become accustomed to specifying constraints on patterns of interest. A large number of systems and techniques has been developed for solving such constraint-based mining problems, especially for mining itemsets. The approach taken in the field of data mining contrasts with the constraint programming principles developed within the artificial intelligence community. While most data mining research focuses on algorithmic issues and aims at developing highly optimized and scalable implementations that are tailored towards specific tasks, constraint programming employs a more declarative approach. The emphasis lies on developing high-level modeling languages and general solvers that specify what the problem is, rather than outlining how a solution should be computed, yet are powerful enough to be used across a wide variety of applications and application domains. This paper contributes a declarative constraint programming approach to data mining. More specifically, we show that it is possible to employ off-the-shelf constraint programming techniques for modeling and solving a wide variety of constraint-based itemset mining tasks, such as frequent, closed, discriminative, and cost-based itemset mining. In particular, we develop a basic constraint programming model for specifying frequent itemsets and show that this model can easily be extended to realize the other settings. This contrasts with typical procedural data mining systems where the underlying procedures need to be modified in order to accommodate new types of constraint, or novel combinations thereof. Even though the performance of state-of-the-art data mining systems outperforms that of the constraint programming approach on some standard tasks, we also show that there exist problems where the constraint programming approach leads to significant performance improvements over state-of-the-art methods in data mining and as well as to new insights into the underlying data mining problems. Many such insights can be obtained by relating the underlying search algorithms of data mining and constraint programming systems to one another. We discuss a number of interesting new research questions and challenges raised by the declarative constraint programming approach to data mining.
ER  - 
TY  - JOUR
T1  - A survey of energy-efficient context recognition systems using wearable sensors for healthcare applications
A1  - Rault, Tifenn
A1  - Bouabdallah, Abdelmadjid
A1  - Challal, Yacine
A1  - Marin, Frédéric
Y1  - 2017///
KW  -  Energy efficient wearable sensor networks
KW  -  Human context recognition
KW  - State-of-the-art
JF  - Pervasive and Mobile Computing
VL  - 37
SP  - 23
EP  - 44
DO  - https://doi.org/10.1016/j.pmcj.2016.08.003
UR  - https://www.sciencedirect.com/science/article/pii/S1574119216301298
N2  - Human context recognition (HCR) from on-body sensor networks is an important and challenging task for many healthcare applications because it offers continuous monitoring capability of both personal and environmental parameters. However, these systems still face a major energy issue that prevent their wide adoption. Indeed, in healthcare applications, sensors are used to capture data during daily life or extended stays in hospital. Thus, continuous sampling and communication tasks quickly deplete sensors’ battery reserves, and frequent battery replacement is not convenient. Therefore, there is a need to develop energy-efficient solutions for long-term monitoring applications in order to foster the acceptance of these technologies by the patients. In this paper, we survey existing energy-efficient approaches designed for HCR based on wearable sensor networks. We propose a new classification of the energy-efficient mechanisms for health-related human context recognition applications and we review the related works in detail. Moreover, we provide a qualitative comparison of these solutions in terms of energy-consumption, recognition accuracy and latency. Finally, we discuss open research issue and give directions for future works.
ER  - 
TY  - JOUR
T1  - Incremental filter and wrapper approaches for feature discretization
A1  - Ferreira, Artur J
A1  - Figueiredo, Mário A T
Y1  - 2014///
KW  -  Feature selection
KW  -  Filter
KW  -  Incremental discretization
KW  -  Static discretization
KW  -  Wrapper
KW  - Feature discretization
JF  - Neurocomputing
VL  - 123
SP  - 60
EP  - 74
DO  - https://doi.org/10.1016/j.neucom.2012.10.036
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213003676
N1  - Contains Special issue articles: Advances in Pattern Recognition Applications and Methods
N2  - Discrete data representations are necessary, or at least convenient, in many machine learning problems. While feature selection (FS) techniques aim at finding relevant subsets of features, the goal of feature discretization (FD) is to find concise (quantized) data representations, adequate for the learning task at hand. In this paper, we propose two incremental methods for FD. The first method belongs to the filter family, in which the quality of the discretization is assessed by a (supervised or unsupervised) relevance criterion. The second method is a wrapper, where discretized features are assessed using a classifier. Both methods can be coupled with any static (unsupervised or supervised) discretization procedure and can be used to perform FS as pre-processing or post-processing stages. The proposed methods attain efficient representations suitable for binary and multi-class problems with different types of data, being competitive with existing methods. Moreover, using well-known FS methods with the features discretized by our techniques leads to better accuracy than with the features discretized by other methods or with the original features.
ER  - 
TY  - JOUR
T1  - Current methods in electrocardiogram characterization
A1  - Martis, Roshan Joy
A1  - Acharya, U Rajendra
A1  - Adeli, Hojjat
Y1  - 2014///
KW  -  Arrhythmia
KW  -  Cardiovascular diseases (CVD)
KW  -  Computer aided cardiac diagnosis (CACD)
KW  -  Non-linear methods
KW  -  Transform domain techniques
KW  -  Wavelets
KW  - Electrocardiogram
JF  - Computers in Biology and Medicine
VL  - 48
SP  - 133
EP  - 149
DO  - https://doi.org/10.1016/j.compbiomed.2014.02.012
UR  - https://www.sciencedirect.com/science/article/pii/S0010482514000432
N2  - The Electrocardiogram (ECG) is the P-QRS-T wave depicting the cardiac activity of the heart. The subtle changes in the electric potential patterns of repolarization and depolarization are indicative of the disease afflicting the patient. These clinical time domain features of the ECG waveform can be used in cardiac health diagnosis. Due to the presence of noise and minute morphological parameter values, it is very difficult to identify the ECG classes accurately by the naked eye. Various computer aided cardiac diagnosis (CACD) systems, analysis methods, challenges addressed and the future of cardiovascular disease screening are reviewed in this paper. Methods developed for time domain, frequency transform domain, and time-frequency domain analysis, such as the wavelet transform, cannot by themselves represent the inherent distinguishing features accurately. Hence, nonlinear methods which can capture the small variations in the ECG signal and provide improved accuracy in the presence of noise are discussed in greater detail in this review. A CACD system exploiting these nonlinear features can help clinicians to diagnose cardiovascular disease more accurately.
ER  - 
TY  - JOUR
T1  - Fast and adaptive network of spiking neurons for multi-view visual pattern recognition
A1  - Wysoski, Simei Gomes
A1  - Benuskova, Lubica
A1  - Kasabov, Nikola
Y1  - 2008///
KW  -  Face recognition
KW  -  On-line classification
KW  -  Rank order coding
KW  -  Visual pattern recognition
KW  - Spiking neural network
JF  - Neurocomputing
VL  - 71
IS  - 13
SP  - 2563
EP  - 2575
DO  - https://doi.org/10.1016/j.neucom.2007.12.038
UR  - https://www.sciencedirect.com/science/article/pii/S0925231208002191
N1  - Artificial Neural Networks (ICANN 2006) / Engineering of Intelligent Systems (ICEIS 2006)
N2  - In this paper, we describe and evaluate a new spiking neural network (SNN) architecture and its corresponding learning procedure to perform fast and adaptive multi-view visual pattern recognition. The network is composed of a simplified type of integrate-and-fire neurons arranged hierarchically in four layers of two-dimensional neuronal maps. Using a Hebbian-based training, the network adaptively changes its structure in order to respond optimally to different visual patterns. Neurons in the last layer accumulate information collected over multiple frames to reach a final decision. We tested the network with VidTimit dataset to recognize individuals using facial information from multiple frames. The experiments illustrate and evaluate the two main novelties of the network: structural adaptation and frame-by-frame accumulation of opinions.
ER  - 
TY  - JOUR
T1  - ProFUSO: Business process and ontology-based framework to develop ubiquitous computing support systems for chronic patients’ management
A1  - Jimenez-Molina, Angel
A1  - Gaete-Villegas, Jorge
A1  - Fuentes, Javier
Y1  - 2018///
KW  -  Chronic disease management
KW  -  Clinical decision support systems (CDSS)
KW  -  Interoperability
KW  -  Ubiquitous computing
KW  -  Ubiquitous health (u-health) services
KW  - Data architecture
JF  - Journal of Biomedical Informatics
VL  - 82
SP  - 106
EP  - 127
DO  - https://doi.org/10.1016/j.jbi.2018.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300650
N2  - New advances in telemedicine, ubiquitous computing, and artificial intelligence have supported the emergence of more advanced applications and support systems for chronic patients. This trend addresses the important problem of chronic illnesses, highlighted by multiple international organizations as a core issue in future healthcare. Despite the myriad of exciting new developments, each application and system is designed and implemented for specific purposes and lacks the flexibility to support different healthcare concerns. Some of the known problems of such developments are the integration issues between applications and existing healthcare systems, the reusability of technical knowledge in the creation of new and more sophisticated systems and the usage of data gathered from multiple sources in the generation of new knowledge. This paper proposes a framework for the development of chronic disease support systems and applications as an answer to these shortcomings. Through this framework our pursuit is to create a common ground methodology upon which new developments can be created and easily integrated to provide better support to chronic patients, medical staff and other relevant participants. General requirements are inferred for any support system from the primary attention process of chronic patients by the Business Process Management Notation. Numerous technical approaches are proposed to design a general architecture that considers the medical organizational requirements in the treatment of a patient. A framework is presented for any application in support of chronic patients and evaluated by a case study to test the applicability and pertinence of the solution.
ER  - 
TY  - JOUR
T1  - Optimized treatment of fibromyalgia using system identification and hybrid model predictive control
A1  - Deshpande, Sunil
A1  - Nandola, Naresh N
A1  - Rivera, Daniel E
A1  - Younger, Jarred W
Y1  - 2014///
KW  -  Biomedical applications
KW  -  Fibromyalgia
KW  -  Hybrid model predictive control
KW  -  System identification
KW  - Optimized adaptive behavioral interventions
JF  - Control Engineering Practice
VL  - 33
SP  - 161
EP  - 173
DO  - https://doi.org/10.1016/j.conengprac.2014.09.011
UR  - https://www.sciencedirect.com/science/article/pii/S0967066114002305
N2  - The term adaptive intervention is used in behavioral health to describe individually tailored strategies for preventing and treating chronic, relapsing disorders. This paper describes a system identification approach for developing dynamical models from clinical data, and subsequently, a hybrid model predictive control scheme for assigning dosages of naltrexone as treatment for fibromyalgia, a chronic pain condition. A simulation study that includes conditions of significant plant-model mismatch demonstrates the benefits of hybrid predictive control as a decision framework for optimized adaptive interventions. This work provides insights on the design of novel personalized interventions for chronic pain and related conditions in behavioral health.
ER  - 
TY  - JOUR
T1  - Reasoning about clinical guidelines based on algebraic data types and constraint logic programming
A1  - Pérez, Beatriz
Y1  - 2019///
KW  -  Constraint logic programming
KW  -  Formal verification
KW  -  Model driven development
KW  -  Model transformation
KW  - Clinical guidelines
JF  - Journal of Biomedical Informatics
VL  - 92
SP  - 103134
EP  - 103134
DO  - https://doi.org/10.1016/j.jbi.2019.103134
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419300528
N2  - Previously, the authors presented an overall framework aimed at improving the representation, quality and application of clinical guidelines in daily clinical practice. Regarding the quality improvement of guidelines, we developed a proposal to verify specific requirements in guidelines, using the SPIN model checker as verification tool. Additionally, we established a pattern-based approach for defining commonly occurring types of requirements in guidelines, in order to help non experts in their formal specification. In particular, among such patterns, we identified several which could not be verified by using such a proposal, thus leaving their verification as future work. In this paper, we provide a revised and extended version of that work by providing an overall proposal which mainly addresses previous shortcomings, while providing additional verification functionalities. More specifically, we have defined a complementary proposal to the previous one regarding the verification of guidelines. This proposal uses Formula, a model finding and design space exploration tool that is based on Algebraic Data Types (ADT) and Constraint Logic Programming (CLP). The main contributions of this paper are twofold: (1) providing a more complete set of patterns for defining commonly occurring types of requirements in guidelines, and (2) supporting the verification of a wider range of patterns by combining the use of our previous proposal, based on the SPIN model checker, with our Formula-based method. More specifically, our Formula-based proposal provides us with a solution to the verification of those patterns we were not able to verify previously. Additionally, our proposal has been implemented as an Eclipse plug-in developed based on Model Driven Development (MDD) techniques, which enables us to automatically generate the Formula specification of a guideline, making the process faster and less error-prone than a manual translation. This Formula specification, together with the requirements to be checked in the guideline, are finally taken as input of the Formula tool to check whether the guideline verifies the requirements. We show the feasibility of our overall approach by verifying properties in different clinical guidelines with encouraging results.
ER  - 
TY  - JOUR
T1  - Applications of deep learning in fundus images: A review
A1  - Li, Tao
A1  - Bo, Wang
A1  - Hu, Chunyu
A1  - Kang, Hong
A1  - Liu, Hanruo
A1  - Wang, Kai
A1  - Fu, Huazhu
Y1  - 2021///
KW  -  Deep learning
KW  -  Eye diseases
KW  - Fundus images
JF  - Medical Image Analysis
VL  - 69
SP  - 101971
EP  - 101971
DO  - https://doi.org/10.1016/j.media.2021.101971
UR  - https://www.sciencedirect.com/science/article/pii/S1361841521000177
N2  - The use of fundus images for the early screening of eye diseases is of great clinical importance. Due to its powerful performance, deep learning is becoming more and more popular in related applications, such as lesion segmentation, biomarkers segmentation, disease diagnosis and image synthesis. Therefore, it is very necessary to summarize the recent developments in deep learning for fundus images with a review paper. In this review, we introduce 143 application papers with a carefully designed hierarchy. Moreover, 33 publicly available datasets are presented. Summaries and analyses are provided for each task. Finally, limitations common to all tasks are revealed and possible solutions are given. We will also release and regularly update the state-of-the-art results and newly-released datasets at https://github.com/nkicsl/Fundus_Review to adapt to the rapid development of this field.
ER  - 
TY  - JOUR
T1  - Multiple Empirical Kernel Learning with dynamic pairwise constraints
A1  - Wang, Zhe
A1  - Fan, Qi
A1  - Gao, Daqi
Y1  - 2015///
KW  -  Dynamic pairwise constraints
KW  -  Empirical Kernel Mapping
KW  -  Pattern recognition
KW  -  Prior knowledge
KW  - Multiple Kernel Learning
JF  - Applied Soft Computing
VL  - 30
SP  - 14
EP  - 25
DO  - https://doi.org/10.1016/j.asoc.2015.01.040
UR  - https://www.sciencedirect.com/science/article/pii/S1568494615000617
N2  - Unlike the traditional Multiple Kernel Learning (MKL) with the implicit kernels, Multiple Empirical Kernel Learning (MEKL) explicitly maps the original data space into multiple feature spaces via different empirical kernels. MEKL has been demonstrated to bring good classification performance and to be much easier in processing and analyzing the adaptability of kernels for the input space. In this paper, we incorporate the dynamic pairwise constraints into MEKL to propose a novel Multiple Empirical Kernel Learning with dynamic Pairwise Constraints method (MEKLPC). It is known that the pairwise constraint provides the relationship between two samples, which tells whether these samples belong to the same class or not. In the present work, we boost the original pairwise constraints and design the dynamic pairwise constraints which can pay more attention onto the boundary samples and thus to make the decision hyperplane more reasonable and accurate. Thus, the proposed MEKLPC not only inherits the advantages of the MEKL, but also owns multiple folds of prior information. Firstly, MEKLPC gets the side-information and boosts the classification performance significantly in each feature space. Here, the side-information is the dynamic pairwise constraints which are constructed by the samples near the decision boundary, i.e. the boundary samples. Secondly, in each mapped feature space, MEKLPC still measures the empirical risk and generalization risk. Lastly, different feature spaces mapped by multiple empirical kernels can agree to their outputs for the same input sample as much as possible. To the best of our knowledge, it is the first time to introduce the dynamic pairwise constraints into the MEKL framework in the present work. The experiments on a number of real-world data sets demonstrate the feasibility and effectiveness of MEKLPC.
ER  - 
TY  - JOUR
T1  - Neuro-fuzzy classification of prostate cancer using NEFCLASS-J
A1  - Keles, Ayturk
A1  - Samet Hasiloglu, A
A1  - Keles, Ali
A1  - Aksoy, Yilmaz
Y1  - 2007///
KW  -  Benign prostatic hyperplasia
KW  -  Data analysis
KW  -  Neuro-fuzzy classification
KW  -  Prostate cancer
KW  - Medical diagnosis
JF  - Computers in Biology and Medicine
VL  - 37
IS  - 11
SP  - 1617
EP  - 1628
DO  - https://doi.org/10.1016/j.compbiomed.2007.03.006
UR  - https://www.sciencedirect.com/science/article/pii/S0010482507000595
N2  - Medical diagnosis has been the most proper area for the implementations of artificial intelligence for approximately 20 years. In this paper, a new approach based on neuro-fuzzy classification (NEFCLASS) tool has been presented to classify prostate cancer. The tool has the features of batch learning, automatic cross validation, automatic determination of the rule base size, and handling of missing values to increase its interpretability. We have investigated how good medical data analysis could be done with NEFCLASS-J, and what effects selected parameters have on classifier performances. Medical data were obtained from patients with real prostate cancer and benign prostatic hyperplasia (BPH). The reason for the selection of these two illnesses was the fact that their symptoms are very similar yet their differentiation is very crucial. The results showed that, for creating high performance of classifier appropriate for the data used, firstly it is necessary to decide well on the membership type and the number of fuzzy sets and then validation procedure. After a good classifier has been found, other parameters should be investigated to improve this classifier. In the light of this study, we can present a foresight for the diagnosis of the patients with prostate cancer or BPH.
ER  - 
TY  - JOUR
T1  - An efficient deep learning-based scheme for web spam detection in IoT environment
A1  - Makkar, Aaisha
A1  - Kumar, Neeraj
Y1  - 2020///
KW  -  Cognitive IoT
KW  -  Deep learning
KW  -  Web spam
KW  - Internet of Things(IoT)
JF  - Future Generation Computer Systems
VL  - 108
SP  - 467
EP  - 487
DO  - https://doi.org/10.1016/j.future.2020.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19326779
N2  - From the last few years, Internet of Things has revolutionized the entire world. In this, various smart objects perform the tasks of sensing and computing to provide uninterrupted services to the end users in different applications such as smart transportation, e-healthcare to name a few. With the inherent capabilities of these objects to take adaptive intelligent decisions, Cognitive Internet of Things is another paradigm of Internet of Things which emerges during this era. However, while accessing data from the Internet, web spam is one of the challenges to be handled. It has been observed from the literature review that for accessing data, search engines are preferred mostly by an individual. The search engine’s effective ranking can decrease the computational cost of objects during the data access. The current solutions to this issue are aimed to discover the spam in the search engine after its occurrence. So, in this proposal, we present a cognitive spammer framework that removes spam pages when search engines calculate the web page rank score. The framework detects web spam with the support of Long Short-Term Memory network by training the link features. This training resulted with an accuracy of 95.25, as more than 1,11,000 hosts are being correctly classified. However, the content features are trained by neural network. The proposed scheme has been validated with the WEBSPAM-UK 2007 dataset. Prior to processing, the dataset is pre-processed using a new technique called ‘Split by Over-sampling and Train by Under-fitting’. The ensemble and cross validation approach has been used for optimization of results with an accuracy of 96.96%. So, the proposed scheme outperforms the existing techniques.
ER  - 
TY  - JOUR
T1  - Probabilistic modeling personalized treatment pathways using electronic health records
A1  - Huang, Zhengxing
A1  - Ge, Zhenxiao
A1  - Dong, Wei
A1  - He, Kunlun
A1  - Duan, Huilong
Y1  - 2018///
KW  -  Electronic Health Record
KW  -  Hidden Markov Model
KW  -  Process Mining
KW  - Personalized Treatment Pathway
JF  - Journal of Biomedical Informatics
VL  - 86
SP  - 33
EP  - 48
DO  - https://doi.org/10.1016/j.jbi.2018.08.004
UR  - https://www.sciencedirect.com/science/article/pii/S153204641830159X
N2  - Background
Modeling personalized treatment pathways plays an important role in understanding essential/critical treatment behaviors performed on patients during their hospitalizations and thus provides the opportunity for the improvement of better health service delivery in treatment pathways.
Objective
Unlike traditional business process mining, modeling personalized treatment pathways is more challenging because they are typically case-specific. Although several studies have been devoted to modeling patient treatment pathways, limited efforts have been made on the extraction of latent semantics and their transitions behind patient treatment pathways, which are often ambiguous and poorly understood.
Methods
In this article, we propose an extension of the Hidden Markov Model to mine and model personalized treatment pathways by extracting latent treatment topics and identifying their sequential dependencies in pathways, in the form of probabilistic distributions and transitions of patients’ raw Electronic Health Record (EHR) data.
Results
We evaluated the proposed model on 48,024 patients with cardiovascular diseases. A total of 15 treatment topics and their typical transition routes were discovered from EHR data that contained 1,391,251 treatment events with 2786 types of interventions and that were evaluated by ten clinicians manually. The obtained p-values are 0.000146 and 0.009106 in comparison with both Latent Dirichlet Allocation and Sequent Naïve Bayes models, respectively; this outcome indicate that our approach achieves a better understanding of human evaluators on modeling personalized treatment pathway than that of benchmark models.
Conclusion
The experimental results on a real-world data set clearly suggest that the proposed model has efficiency in mining and modeling personalized treatment pathways. We argue that the discovered treatment topics and their transition routes, as actionable knowledge that represents the practice of treating individual patients in their clinical pathways, can be further exploited to help physicians better understand their specialty and learn from previous experiences for treatment analysis and improvement.
ER  - 
TY  - JOUR
T1  - Advanced mobile and wearable systems
A1  - Jóźwiak, Lech
Y1  - 2017///
KW  -  Automated design technology
KW  -  Heterogeneous systems
KW  -  Massively parallel systems
KW  -  Mobile systems
KW  -  Multi-processor systems on a chip
KW  - Cyber-physical systems
JF  - Microprocessors and Microsystems
VL  - 50
SP  - 202
EP  - 221
DO  - https://doi.org/10.1016/j.micpro.2017.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S0141933117300741
N2  - The recent spectacular progress in the microelectronic, information, communication, material and sensor technologies created a big stimulus towards development of smart communicating cyber-physical systems (CPS) and Internet of Things (IoT). CPS and IoT are undergoing an explosive growth to a large degree related to advanced mobile systems like smart automotive and avionic systems, mobile robots and wearable devices. The huge and rapidly developing markets of sophisticated mobile cyber-physical systems represent great opportunities, but these opportunities come with a price of unusual system complexity, as well as, stringent and difficult to satisfy requirements of many modern applications. Specifically, smart cars and various wearable systems to a growing degree involve big instant data from multiple complex sensors or other systems, and are required to provide continuous autonomous service in a long time. In consequence, they demand a guaranteed (ultra-)high performance and/or (ultra-)low energy consumption, while requiring a high reliability, safety and security. To adequately address these demands, sophisticated embedded computing and embedded design technologies are needed. After an introduction to modern mobile systems, this paper discusses the huge heterogeneous area of these systems, and considers serious issues and challenges in their design. Subsequently, it discusses the embedded computing and design technologies needed to adequately address the issues and overcome the challenges in order to satisfy the stringent requirements of the modern mobile systems.
ER  - 
TY  - JOUR
T1  - Lexicographic preferences for predictive modeling of human decision making: A new machine learning method with an application in accounting
A1  - Bräuning, Michael
A1  - Hüllermeier, Eyke
A1  - Keller, Tobias
A1  - Glaum, Martin
Y1  - 2017///
KW  -  Accounting
KW  -  Artificial intelligence
KW  -  Decision analysis
KW  -  Preference learning
KW  - Lexicographic orders
JF  - European Journal of Operational Research
VL  - 258
IS  - 1
SP  - 295
EP  - 306
DO  - https://doi.org/10.1016/j.ejor.2016.08.055
UR  - https://www.sciencedirect.com/science/article/pii/S0377221716306944
N2  - Lexicographic preferences on a set of attributes provide a cognitively plausible structure for modeling the behavior of human decision makers. Therefore, the induction of corresponding models from revealed preferences or observed decisions constitutes an interesting problem from a machine learning point of view. In this paper, we introduce a learning algorithm for inducing generalized lexicographic preference models from a given set of training data, which consists of pairwise comparisons between objects. Our approach generalizes simple lexicographic orders in the sense of allowing the model to consider several attributes simultaneously (instead of looking at them one by one), thereby significantly increasing the expressiveness of the model class. In order to evaluate our method, we present a case study of a highly complex real-world problem, namely the choice of the recognition method for actuarial gains and losses from occupational pension schemes. Using a unique sample of European companies, this problem is well suited for demonstrating the effectiveness of our lexicographic ranker. Furthermore, we conduct a series of experiments on benchmark data from the machine learning domain.
ER  - 
TY  - JOUR
T1  - Survey on the use of smart and adaptive engineering systems in medicine
A1  - Abbod, M F
A1  - Linkens, D A
A1  - Mahfouf, M
A1  - Dounias, G
Y1  - 2002///
KW  -  Bioengineering
KW  -  Engineering systems
KW  -  Healthcare
KW  -  Intelligent systems
KW  -  Medicine
KW  -  Survey
KW  - Smart and adaptive system
JF  - Artificial Intelligence in Medicine
VL  - 26
IS  - 3
SP  - 179
EP  - 209
DO  - https://doi.org/10.1016/S0933-3657(02)00083-0
UR  - https://www.sciencedirect.com/science/article/pii/S0933365702000830
N2  - In this paper, the current published knowledge about smart and adaptive engineering systems in medicine is reviewed. The achievements of frontier research in this particular field within medical engineering are described. A multi-disciplinary approach to the applications of adaptive systems is observed from the literature surveyed. The three modalities of diagnosis, imaging and therapy are considered to be an appropriate classification method for the analysis of smart systems being applied to specified medical sub-disciplines. It is expected that future research in biomedicine should identify subject areas where more advanced intelligent systems could be applied than is currently evident. The literature provides evidence of hybridisation of different types of adaptive and smart systems with applications in different areas of medical specifications.
ER  - 
TY  - JOUR
T1  - Designing a classifier by a layered multi-population genetic programming approach
A1  - Lin, Jung-Yi
A1  - Ke, Hao-Ren
A1  - Chien, Been-Chian
A1  - Yang, Wei-Pang
Y1  - 2007///
KW  -  Evolutionary computation
KW  -  Multi-population genetic programming
KW  - Classification
JF  - Pattern Recognition
VL  - 40
IS  - 8
SP  - 2211
EP  - 2225
DO  - https://doi.org/10.1016/j.patcog.2007.01.003
UR  - https://www.sciencedirect.com/science/article/pii/S0031320307000192
N1  - Part Special Issue on Visual Information Processing
N2  - This paper proposes a method called layered genetic programming (LAGEP) to construct a classifier based on multi-population genetic programming (MGP). LAGEP employs layer architecture to arrange multiple populations. A layer is composed of a number of populations. The results of populations are discriminant functions. These functions transform the training set to construct a new training set. The successive layer uses the new training set to obtain better discriminant functions. Moreover, because the functions generated by each layer will be composed to a long discriminant function, which is the result of LAGEP, every layer can evolve with short individuals. For each population, we propose an adaptive mutation rate tuning method to increase the mutation rate based on fitness values and remaining generations. Several experiments are conducted with different settings of LAGEP and several real-world medical problems. Experiment results show that LAGEP achieves comparable accuracy to single population GP in much less time.
ER  - 
TY  - JOUR
T1  - Infrared thermography (IRT) applications for building diagnostics: A review
A1  - Kylili, Angeliki
A1  - Fokaides, Paris A
A1  - Christou, Petros
A1  - Kalogirou, Soteris A
Y1  - 2014///
KW  -  Active thermography
KW  -  Locked in thermography
KW  -  Non-destructive testing
KW  -  Passive thermography
KW  -  Pulsed thermography
KW  - Infrared thermography
JF  - Applied Energy
VL  - 134
SP  - 531
EP  - 549
DO  - https://doi.org/10.1016/j.apenergy.2014.08.005
UR  - https://www.sciencedirect.com/science/article/pii/S0306261914008083
N2  - Infrared thermography (IRT) has met an extensive popularity among the non-destructive technologies for building diagnostics, especially with the increasing concerns of energy minimisation and low energy consumption of the building sector. Its popularity for a broad range of applications can be attributed to its non-contact safe nature, its usefulness and effectiveness, as well as the energy and cost savings it can achieve. This paper reviews the state-of-the-art literature and research regarding the passive and active infrared thermography. The fundamentals of IRT are thoroughly explained and the thermographic process for building diagnostics is presented. This work also presents the fields of applicability of IRT with a focus on the building sector, as well as the advantages, limitations and potential sources of errors of IRT employment. Additionally previous non-destructive testing (NDT) studies that employed passive, active pulsed, and active lock-in thermographies for building diagnostics are presented. A review of the thermal image analysis methods and the future trends of thermal imaging are also included in this work. It can be concluded that while IRT is a useful tool for the characterisation of defects in the building sector, there is great prospect for the development of more advanced, effective and accurate approaches that will employ a combination of thermography approaches.
ER  - 
TY  - JOUR
T1  - Application of global optimization methods to model and feature selection
A1  - Boubezoul, Abderrahmane
A1  - Paris, Sébastien
Y1  - 2012///
KW  -  Feature selection
KW  -  Hyper-parameters optimization
KW  -  Particle swarm optimization
KW  -  Support vector machines
KW  - Cross-Entropy Method
JF  - Pattern Recognition
VL  - 45
IS  - 10
SP  - 3676
EP  - 3686
DO  - https://doi.org/10.1016/j.patcog.2012.04.015
UR  - https://www.sciencedirect.com/science/article/pii/S0031320312001744
N2  - Many data mining applications involve the task of building a model for predictive classification. The goal of this model is to classify data instances into classes or categories of the same type. The use of variables not related to the classes can reduce the accuracy and reliability of classification or prediction model. Superfluous variables can also increase the costs of building a model particularly on large datasets. The feature selection and hyper-parameters optimization problem can be solved by either an exhaustive search over all parameter values or an optimization procedure that explores only a finite subset of the possible values. The objective of this research is to simultaneously optimize the hyper-parameters and feature subset without degrading the generalization performances of the induction algorithm. We present a global optimization approach based on the use of Cross-Entropy Method to solve this kind of problem.
ER  - 
TY  - JOUR
T1  - Scale invariant texture descriptors for classifying celiac disease
A1  - Hegenbart, Sebastian
A1  - Uhl, Andreas
A1  - Vécsei, Andreas
A1  - Wimmer, Georg
Y1  - 2013///
KW  -  Celiac disease
KW  -  Texture recognition
KW  - Scale invariance
JF  - Medical Image Analysis
VL  - 17
IS  - 4
SP  - 458
EP  - 474
DO  - https://doi.org/10.1016/j.media.2013.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S136184151300008X
N2  - Scale invariant texture recognition methods are applied for the computer assisted diagnosis of celiac disease. In particular, emphasis is given to techniques enhancing the scale invariance of multi-scale and multi-orientation wavelet transforms and methods based on fractal analysis. After fine-tuning to specific properties of our celiac disease imagery database, which consists of endoscopic images of the duodenum, some scale invariant (and often even viewpoint invariant) methods provide classification results improving the current state of the art. However, not each of the investigated scale invariant methods is applicable successfully to our dataset. Therefore, the scale invariance of the employed approaches is explicitly assessed and it is found that many of the analyzed methods are not as scale invariant as they theoretically should be. Results imply that scale invariance is not a key-feature required for successful classification of our celiac disease dataset.
ER  - 
TY  - JOUR
T1  - Query-oriented evidence extraction to support evidence-based medicine practice
A1  - Sarker, Abeed
A1  - Mollá, Diego
A1  - Paris, Cecile
Y1  - 2016///
KW  -  Evidence-based medicine
KW  -  Medical text processing
KW  -  Query-focused text summarisation
KW  - Automatic text summarisation
JF  - Journal of Biomedical Informatics
VL  - 59
SP  - 169
EP  - 184
DO  - https://doi.org/10.1016/j.jbi.2015.11.010
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415002786
N2  - Background
Evidence-based medicine practice requires medical practitioners to rely on the best available evidence, in addition to their expertise, when making clinical decisions. The medical domain boasts a large amount of published medical research data, indexed in various medical databases such as MEDLINE. As the size of this data grows, practitioners increasingly face the problem of information overload, and past research has established the time-associated obstacles faced by evidence-based medicine practitioners. In this paper, we focus on the problem of automatic text summarisation to help practitioners quickly find query-focused information from relevant documents.
Methods
We utilise an annotated corpus that is specialised for the task of evidence-based summarisation of text. In contrast to past summarisation approaches, which mostly rely on surface level features to identify salient pieces of texts that form the summaries, our approach focuses on the use of corpus-based statistics, and domain-specific lexical knowledge for the identification of summary contents. We also apply a target-sentence-specific summarisation technique that reduces the problem of underfitting that persists in generic summarisation models.
Results
In automatic evaluations run over a large number of annotated summaries, our extractive summarisation technique statistically outperforms various baseline and benchmark summarisation models with a percentile rank of 96.8%. A manual evaluation shows that our extractive summarisation approach is capable of selecting content with high recall and precision, and may thus be used to generate bottom-line answers to practitioners’ queries.
Conclusions
Our research shows that the incorporation of specialised data and domain-specific knowledge can significantly improve text summarisation performance in the medical domain. Due to the vast amounts of medical text available, and the high growth of this form of data, we suspect that such summarisation techniques will address the time-related obstacles associated with evidence-based medicine.
ER  - 
TY  - JOUR
T1  - Translational integrity and continuity: Personalized biomedical data integration
A1  - Wang, Xiaoming
A1  - Liu, Lili
A1  - Fackenthal, James
A1  - Cummings, Shelly
A1  - Olopade, Oluwatobi I
A1  - Hope, Kisha
A1  - Silverstein, Jonathan C
A1  - Olopade, Olufunmilayo I
Y1  - 2009///
KW  -  Data continuity
KW  -  Data curation
KW  -  Data integrity
KW  -  Translational research
KW  - Data integration
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 1
SP  - 100
EP  - 112
DO  - https://doi.org/10.1016/j.jbi.2008.08.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046408001007
N2  - Translational research data are generated in multiple research domains from the bedside to experimental laboratories. These data are typically stored in heterogeneous databases, held by segregated research domains, and described with inconsistent terminologies. Such inconsistency and fragmentation of data significantly impedes the efficiency of tracking and analyzing human-centered records. To address this problem, we have developed a data repository and management system named TraM (http://tram.uchicago.edu), based on a domain ontology integrated entity relationship model. The TraM system has the flexibility to recruit dynamically evolving domain concepts and the ability to support data integration for a broad range of translational research. The web-based application interfaces of TraM allow curators to improve data quality and provide robust and user-friendly cross-domain query functions. In its current stage, TraM relies on a semi-automated mechanism to standardize and restructure source data for data integration and thus does not support real-time data application.
ER  - 
TY  - JOUR
T1  - Independent component analysis: Mining microarray data for fundamental human gene expression modules
A1  - Engreitz, Jesse M
A1  - Daigle, Bernie J
A1  - Marshall, Jonathan J
A1  - Altman, Russ B
Y1  - 2010///
KW  -  Data mining
KW  -  Gene modules
KW  -  Independent component analysis
KW  -  Parthenolide
KW  - Microarrays
JF  - Journal of Biomedical Informatics
VL  - 43
IS  - 6
SP  - 932
EP  - 944
DO  - https://doi.org/10.1016/j.jbi.2010.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046410001000
N2  - As public microarray repositories rapidly accumulate gene expression data, these resources contain increasingly valuable information about cellular processes in human biology. This presents a unique opportunity for intelligent data mining methods to extract information about the transcriptional modules underlying these biological processes. Modeling cellular gene expression as a combination of functional modules, we use independent component analysis (ICA) to derive 423 fundamental components of human biology from a 9395-array compendium of heterogeneous expression data. Annotation using the Gene Ontology (GO) suggests that while some of these components represent known biological modules, others may describe biology not well characterized by existing manually-curated ontologies. In order to understand the biological functions represented by these modules, we investigate the mechanism of the preclinical anti-cancer drug parthenolide (PTL) by analyzing the differential expression of our fundamental components. Our method correctly identifies known pathways and predicts that N-glycan biosynthesis and T-cell receptor signaling may contribute to PTL response. The fundamental gene modules we describe have the potential to provide pathway-level insight into new gene expression datasets.
ER  - 
TY  - JOUR
T1  - Large-scale regulatory network analysis from microarray data: modified Bayesian network learning and association rule mining
A1  - Huang, Zan
A1  - Li, Jiexun
A1  - Su, Hua
A1  - Watts, George S
A1  - Chen, Hsinchun
Y1  - 2007///
KW  -  Association rules
KW  -  Bayesian networks
KW  -  Microarray
KW  - Genetic regulatory networks
JF  - Decision Support Systems
VL  - 43
IS  - 4
SP  - 1207
EP  - 1225
DO  - https://doi.org/10.1016/j.dss.2006.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S0167923606000248
N1  - Special Issue Clusters
N2  - We present two algorithms for learning large-scale gene regulatory networks from microarray data: a modified information-theory-based Bayesian network algorithm and a modified association rule algorithm. Simulation-based evaluation using six datasets indicated that both algorithms outperformed their unmodified counterparts, especially when analyzing large numbers of genes. Both algorithms learned about 20% (50% if directionality and relation type were not considered) of the relations in the actual models. In our empirical evaluation based on two real datasets, domain experts evaluated subsets of learned relations with high confidence and identified 20–30% to be “interesting” or “maybe interesting” as potential experiment hypotheses.
ER  - 
TY  - JOUR
T1  - Extended pre-processing pipeline for text classification: On the role of meta-feature representations, sparsification and selective sampling
A1  - Cunha, Washington
A1  - Canuto, Sérgio
A1  - Viegas, Felipe
A1  - Salles, Thiago
A1  - Gomes, Christian
A1  - Mangaravite, Vitor
A1  - Resende, Elaine
A1  - Rosa, Thierson
A1  - Gonçalves, Marcos André
A1  - Rocha, Leonardo
Y1  - 2020///
KW  -  Experimental evaluation
KW  -  Meta-features
KW  -  Pre-processing
KW  -  Selective sampling
KW  -  Sparsification
KW  - Text classification pipelines
JF  - Information Processing & Management
VL  - 57
IS  - 4
SP  - 102263
EP  - 102263
DO  - https://doi.org/10.1016/j.ipm.2020.102263
UR  - https://www.sciencedirect.com/science/article/pii/S030645731931461X
N2  - Text Classification pipelines are a sequence of tasks needed to be performed to classify documents into a set of predefined categories. The pre-processing phase (before training) of these pipelines involve different ways of transforming and manipulating the documents for the next (learning) phase. In this paper, we introduce three new steps into the pre-processing phase of text classification pipelines to improve effectiveness while reducing the associated costs. The distance-based Meta-Features (MFs) generation step aims at reducing the dimensionality of the original term-document matrix while producing a potentially more informative space that explicitly exploits discriminative labeled information. The second step is a sparsification one aimed at making the MF representation less dense to reduce training costs and noise. The third step is a selective sampling (SS) aimed at removing lines (documents) of the matrix obtained in the previous step, by carefully selecting the “best” documents for the learning phase. Our experiments show that the proposed extended pre-processing pipeline can achieve significant gains in effectiveness when compared to the original TF-IDF (up to 52%) and embedding-based representations (up to 46%), at a much lower cost (up to 9.7x faster in some datasets). Other main contributions of our work include a thorough and rigorous evaluation of the trade-offs between cost and effectiveness associated with the introduction of these new steps into the pipeline as well as a comprehensive comparative experimental evaluation of many alternatives in terms of representations, approaches, etc.
ER  - 
TY  - JOUR
T1  - A review of deep learning with special emphasis on architectures, applications and recent trends
A1  - Sengupta, Saptarshi
A1  - Basak, Sanchita
A1  - Saikia, Pallabi
A1  - Paul, Sayak
A1  - Tsalavoutis, Vasilios
A1  - Atiah, Frederick
A1  - Ravi, Vadlamani
A1  - Peters, Alan
Y1  - 2020///
KW  -  Applications of deep learning
KW  -  Evolutionary computation
KW  -  Supervised learning
KW  -  Testing neural networks
KW  -  Unsupervised learning
KW  - Deep neural network architectures
JF  - Knowledge-Based Systems
VL  - 194
SP  - 105596
EP  - 105596
DO  - https://doi.org/10.1016/j.knosys.2020.105596
UR  - https://www.sciencedirect.com/science/article/pii/S095070512030071X
N2  - Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.
ER  - 
TY  - JOUR
T1  - Internet of Things: A survey of enabling technologies in healthcare and its applications
A1  - Dhanvijay, Mrinai M
A1  - Patil, Shailaja C
Y1  - 2019///
KW  -  Body sensor
KW  -  Healthcare system
KW  -  WBAN
KW  - IoT
JF  - Computer Networks
VL  - 153
SP  - 113
EP  - 131
DO  - https://doi.org/10.1016/j.comnet.2019.03.006
UR  - https://www.sciencedirect.com/science/article/pii/S1389128619302695
N2  - Internet of Things (IoT) on the Wireless Body Area Network (WBAN) for healthcare applications is an operative scenario for IoT devices that has gained attention from vast research fields in recent years. The IoT connects all subjects and the healthcare system seamlessly. This paper describes the WBAN based IoT healthcare system and reviews the state-of-the-art of the network architecture topology and applications in the IoT based healthcare solutions. Moreover, this paper analyzes the security and the privacy features consisting of privacy, authentication, energy, power, resource management, Quality of Services and the real-time wireless health monitoring that are quite problematic in many IoT healthcare architectures. Because, system architecture is not well-defined, data restriction and its integrity preservation is still a challenge. At present 90% of the information available is acquired in the recent two years. This survey mainly aims at analyzing healthcare purpose which is based on digital healthcare system. Further, it reports many IoT and the e-healthcare policies and systems that decide how to ease all bearable development. Thus, the overall system provides large possibilities for future research based on IoT healthcare system. Finally, research gaps are reviewed and the possible future aspects have been discussed.
ER  - 
TY  - JOUR
T1  - Globality and locality incorporation in distance metric learning
A1  - Wang, Wei
A1  - Hu, Bao-Gang
A1  - Wang, Zeng-Fu
Y1  - 2014///
KW  -  Classification
KW  -  Convex optimization
KW  -  Metric learning
KW  - Mahalanobis distance
JF  - Neurocomputing
VL  - 129
SP  - 185
EP  - 198
DO  - https://doi.org/10.1016/j.neucom.2013.09.041
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213009685
N2  - Supervised distance metric learning plays a substantial role to the success of statistical classification and information retrieval. Although many related algorithms are proposed, it is still an open problem about incorporating both the geometric information (i.e., locality) and the label information (i.e., globality) in metric learning. In this paper, we propose a novel metric learning framework, called “Dependence Maximization based Metric Learning” (DMML), which can efficiently integrate these two sources of information into a unified structure as instances of convex programming without requiring balance weights. In DMML, the metric is trained by maximizing the dependence between data distributions in the reproducing kernel Hilbert spaces (RKHSs). Unlike learning in the existing information theoretic algorithms, however, DMML requires no estimation or assumption of data distributions. Under this proposed framework, we present two methods by employing different independence criteria respectively, i.e., Hilbert–Schmidt Independence Criterion and the generalized Distance Covariance. Comprehensive experimental results for classification, visualization and image retrieval demonstrate that DMML favorably outperforms state-of-the-art metric learning algorithms, meanwhile illustrate the respective advantages of these two proposed methods in the related applications.
ER  - 
TY  - JOUR
T1  - Particle swarm classification: A survey and positioning
A1  - Nouaouria, Nabila
A1  - Boukadoum, Mounir
A1  - Proulx, Robert
Y1  - 2013///
KW  -  Classification
KW  -  High dimensional data sets
KW  -  Mixed attribute data sets
KW  - Particle swarm optimization
JF  - Pattern Recognition
VL  - 46
IS  - 7
SP  - 2028
EP  - 2044
DO  - https://doi.org/10.1016/j.patcog.2012.12.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313000022
N2  - This paper offers a survey of recent work on particle swarm classification (PSC), a promising offshoot of particle swarm optimization (PSO), with the goal of positioning it in the overall classification domain. The richness of the related literature shows that this new classification approach may be an efficient alternative, in addition to existing paradigms. After describing the various PSC approaches found in the literature, the paper identifies and discusses two data-related problems that may affect PSC efficiency: high-dimensional datasets and mixed-attribute data. The solutions that have been proposed in the literature for each of these issues are described including recent improvements by a novel PSC algorithm developed by the authors. Subsequently, a positioning PSC for these problems with respect to other classification approaches is made. This is accomplished by using one proprietary and five well known benchmark datasets to determine the performances of PSC algorithm and comparing the obtained results with those reported for various other classification approaches. It is concluded that PSC can be efficiently applied to classification problems with large numbers of instances, both in continuous and mixed-attribute problem description spaces. Moreover, the obtained results show that PSC may not only be applied to more demanding problem domains, but it can also be a competitive alternative to well established classification techniques.
ER  - 
TY  - JOUR
T1  - Industrial batch process monitoring with limited data
A1  - Tulsyan, Aditya
A1  - Garvin, Christopher
A1  - Undey, Cenk
Y1  - 2019///
KW  -  Bayesian methods
KW  -  Machine-learning
KW  -  Real-time multivariate process monitoring
KW  - Biopharmaceutical manufacturing
JF  - Journal of Process Control
VL  - 77
SP  - 114
EP  - 133
DO  - https://doi.org/10.1016/j.jprocont.2019.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S0959152419301581
N2  - This article addresses the problem of real-time statistical batch process monitoring (BPM) for processes with limited production history; herein, referred to as the ‘Low-N’ problem. The Low-N problem is a longstanding, industry-wide problem in biopharmaceutical manufacturing that challenges the theoretical foundations and practical applicability of the existing BPM platform. In this article, we propose an approach to transition from a Low-N scenario to a Large-N scenario by generating an arbitrarily large number of insilico batch data sets. The proposed method is a combination of hardware exploitation and algorithm development. To this effect, we propose a block-learning method for a Bayesian non-parametric model of a batch process, and then use probabilistic programming to generate an arbitrarily large number of dynamic insilico campaign data sets. The proposed solution not only alleviates the monitoring issues associated with a Low-N scenario, it is also compatible with the industrial BPM framework. To the best of authors’ knowledge, this is the first article that describes a systematic approach to address the small data problem using the tools for large data sets. The efficacy of the proposed solution is elucidated on an industrial biopharmaceutical process.
ER  - 
TY  - JOUR
T1  - Exploiting efficient and effective lazy Semi-Bayesian strategies for text classification
A1  - Viegas, Felipe
A1  - Rocha, Leonardo
A1  - Resende, Elaine
A1  - Salles, Thiago
A1  - Martins, Wellignton
A1  - e Freitas, Mateus Ferreira
A1  - Gonçalves, Marcos André
Y1  - 2018///
KW  -  Feature weighting techniques
KW  -  GPU parallelization
KW  -  Naive Bayes classifier
KW  -  Semi-Naive Bayes heuristics
KW  - Text classification
JF  - Neurocomputing
VL  - 307
SP  - 153
EP  - 171
DO  - https://doi.org/10.1016/j.neucom.2018.04.033
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218304636
N2  - Automatic Document Classification (ADC) has become the basis of many important applications, e.g., authorship identification, opinion mining, spam filtering, content organizers, etc. Due to their simplicity, efficiency, absence of parameters, and effectiveness in several scenarios, Naive Bayes (NB) approaches are widely used as a classification paradigm. Due to some characteristics of real document collections, e.g., class imbalance and feature sparseness, NB solutions do not present competitive effectiveness in some ADC tasks when compared to other supervised learning strategies, e.g., SVMs. In this article, we investigate whether a proper combination of some alternative NB learning models with different feature weighting techniques is able to improve the NB effectiveness in ADC tasks and verify that comparable or even superior results when compared to the state-of-the-art in ADC can be achieved. Moreover, we also present an investigation on the relaxation of the NB attribute independence assumption (aka, Semi-Naive approaches) in large text collections, something missing in the literature. Given the high computational costs of these investigations, we take advantage of current many core GPU and multi-GPU architectures to perform such investigation, presenting a massively parallelized version of the NB approach. Finally, supported by the parallel implementations, we propose four novel Lazy Semi-NB approaches to overcome potential overfitting problems. In our experiments, the new lazy solutions are not only more efficient and effective than existing Semi-NB approaches, but also surpass, in terms of effectiveness, all other alternatives in the majority of the cases.
ER  - 
TY  - JOUR
T1  - Contextual activity based Healthcare Internet of Things, Services, and People (HIoTSP): An architectural framework for healthcare monitoring using wearable sensors
A1  - Khowaja, Sunder Ali
A1  - Prabono, Aria Ghora
A1  - Setiawan, Feri
A1  - Yahya, Bernardo Nugroho
A1  - Lee, Seok-Lyong
Y1  - 2018///
KW  -  Activity recognition
KW  -  Data- and knowledge-driven techniques
KW  -  Real-time systems
KW  -  Wearable sensors
KW  - Healthcare IoT
JF  - Computer Networks
VL  - 145
SP  - 190
EP  - 206
DO  - https://doi.org/10.1016/j.comnet.2018.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S1389128618308594
N2  - Healthcare industry is gaining a lot of attention due to its technological advancement and the miniaturization in the form of wearable sensors. IoT-driven healthcare industry has mainly focused on the integration of sensors rather than the integration of services and people. Nonetheless, the framework for IoT-driven healthcare applications are significantly lacking. In addition, the use of semantics for ontological reasoning and the integration of mobile applications into a single framework have also been ignored in many existing studies. This work presents the implementation of Healthcare Internet of Things, Services, and People (HIoTSP) framework using wearable sensor technology. It is designed to achieve the low-cost (consumer devices), the easiness to use (interface), and the pervasiveness (wearable sensors) for healthcare monitoring along with the integration of services and agents like doctors or caregivers. The proposed framework provides the functionalities for data acquisition from wearable sensors, contextual activity recognition, automatic selection of services and applications, user interface, and value-added services such as alert generation, recommendations, and visualization. We used the publicly available dataset, PAMAP2 which is a physical activity monitoring dataset, for deriving the contextual activity. Fall and stress detection services are implemented as case studies for validating the realization of the proposed framework. Experimental analysis shows that we achieve, 87.16% accuracy for low-level contextual activities and 84.06%–86.36% for high-level contextual activities, respectively. We also achieved 91.68% and 82.93% accuracies for fall and stress detection services, respectively. The result is quite satisfactory, considering that all these services have been implemented using pervasive devices with the low-sampling rate. The real-time applicability of the proposed framework is validated by performing the response time analysis for both the services. We also provide suggestions to cope with the scalability and security issues using the HIoTSP framework and we intend to implement those suggestions in our future work.
ER  - 
TY  - JOUR
T1  - Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation
A1  - Zhang, Yu-Dong
A1  - Dong, Zhengchao
A1  - Wang, Shui-Hua
A1  - Yu, Xiang
A1  - Yao, Xujing
A1  - Zhou, Qinghua
A1  - Hu, Hua
A1  - Li, Min
A1  - Jiménez-Mesa, Carmen
A1  - Ramirez, Javier
A1  - Martinez, Francisco J
A1  - Gorriz, Juan Manuel
Y1  - 2020///
KW  -  Applications
KW  -  Assessment
KW  -  Fusion rules
KW  -  Magnetic resonance imaging
KW  -  Neuroimaging
KW  -  PET
KW  -  Partial volume effect
KW  -  SPECT
KW  - Multimodal data fusion
JF  - Information Fusion
VL  - 64
SP  - 149
EP  - 187
DO  - https://doi.org/10.1016/j.inffus.2020.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S1566253520303183
N2  - Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamental limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and various sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread education and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging.
ER  - 
TY  - JOUR
T1  - Image understanding for iris biometrics: A survey
A1  - Bowyer, Kevin W
A1  - Hollingsworth, Karen
A1  - Flynn, Patrick J
Y1  - 2008///
KW  -  Identity verification
KW  -  Iris recognition
KW  -  Texture analysis
KW  - Biometrics
JF  - Computer Vision and Image Understanding
VL  - 110
IS  - 2
SP  - 281
EP  - 307
DO  - https://doi.org/10.1016/j.cviu.2007.08.005
UR  - https://www.sciencedirect.com/science/article/pii/S1077314207001373
N2  - This survey covers the historical development and current state of the art in image understanding for iris biometrics. Most research publications can be categorized as making their primary contribution to one of the four major modules in iris biometrics: image acquisition, iris segmentation, texture analysis and matching of texture representations. Other important research includes experimental evaluations, image databases, applications and systems, and medical conditions that may affect the iris. We also suggest a short list of recommended readings for someone new to the field to quickly grasp the big picture of iris biometrics.
ER  - 
TY  - JOUR
T1  - Unsupervised detection and analysis of changes in everyday physical activity data
A1  - Sprint, Gina
A1  - Cook, Diane J
A1  - Schmitter-Edgecombe, Maureen
Y1  - 2016///
KW  -  Change point detection
KW  -  Data mining
KW  -  Unsupervised learning
KW  -  Wearable sensors
KW  - Physical activity monitoring
JF  - Journal of Biomedical Informatics
VL  - 63
SP  - 54
EP  - 65
DO  - https://doi.org/10.1016/j.jbi.2016.07.020
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416300740
N2  - Sensor-based time series data can be utilized to monitor changes in human behavior as a person makes a significant lifestyle change, such as progress toward a fitness goal. Recently, wearable sensors have increased in popularity as people aspire to be more conscientious of their physical health. Automatically detecting and tracking behavior changes from wearable sensor-collected physical activity data can provide a valuable monitoring and motivating tool. In this paper, we formalize the problem of unsupervised physical activity change detection and address the problem with our Physical Activity Change Detection (PACD) approach. PACD is a framework that detects changes between time periods, determines significance of the detected changes, and analyzes the nature of the changes. We compare the abilities of three change detection algorithms from the literature and one proposed algorithm to capture different types of changes as part of PACD. We illustrate and evaluate PACD on synthetic data and using Fitbit data collected from older adults who participated in a health intervention study. Results indicate PACD detects several changes in both datasets. The proposed change algorithms and analysis methods are useful data mining techniques for unsupervised, window-based change detection with potential to track users’ physical activity and motivate progress toward their health goals.
ER  - 
TY  - JOUR
T1  - BodyCloud: A SaaS approach for community Body Sensor Networks
A1  - Fortino, Giancarlo
A1  - Parisi, Daniele
A1  - Pirrone, Vincenzo
A1  - Di Fatta, Giuseppe
Y1  - 2014///
KW  -  Analytics as a service
KW  -  Cloud computing
KW  -  SaaS
KW  -  Sensor data as a service
KW  -  Software engineering
KW  - Body Sensor Networks
JF  - Future Generation Computer Systems
VL  - 35
SP  - 62
EP  - 79
DO  - https://doi.org/10.1016/j.future.2013.12.015
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X13002793
N1  - Special Section: Integration of Cloud Computing and Body Sensor Networks; Guest Editors: Giancarlo Fortino and Mukaddim Pathan
N2  - Body Sensor Networks (BSNs) have been recently introduced for the remote monitoring of human activities in a broad range of application domains, such as health care, emergency management, fitness and behavior surveillance. BSNs can be deployed in a community of people and can generate large amounts of contextual data that require a scalable approach for storage, processing and analysis. Cloud computing can provide a flexible storage and processing infrastructure to perform both online and offline analysis of data streams generated in BSNs. This paper proposes BodyCloud, a SaaS approach for community BSNs that supports the development and deployment of Cloud-assisted BSN applications. BodyCloud is a multi-tier application-level architecture that integrates a Cloud computing platform and BSN data streams middleware. BodyCloud provides programming abstractions that allow the rapid development of community BSN applications. This work describes the general architecture of the proposed approach and presents a case study for the real-time monitoring and analysis of cardiac data streams of many individuals.
ER  - 
TY  - JOUR
T1  - A survey on health monitoring systems for health smart homes
A1  - Mshali, Haider
A1  - Lemlouma, Tayeb
A1  - Moloney, Maria
A1  - Magoni, Damien
Y1  - 2018///
KW  -  Context-aware
KW  -  Health smart home
KW  -  Healthcare
KW  -  Sensor networks
KW  -  e-Health
KW  - Health monitoring system
JF  - International Journal of Industrial Ergonomics
VL  - 66
SP  - 26
EP  - 56
DO  - https://doi.org/10.1016/j.ergon.2018.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169814117300082
N2  - Aging population ratios are rising significantly. Health monitoring systems (HMS) in smart environments have evolved rapidly to become a viable alternative to traditional healthcare solutions. The aim of HMS is to not only reduce costs but to also provide timely e-health services to individuals wishing to maintain their independence. In this way, elderly people can avoid, for as long as possible, any interaction with healthcare institutions (e.g. nursing homes and hospitals), which in turn reduces pressure on the health system. To fully realise this vision of seamless e-health services supporting people in need of them, a number of challenges that need further investigation still exist. To this end, we provide an overview of the current state of the art for smart health monitoring systems. We review HMS in smart environments from a general perspective and with a particular focus on systems for the elderly and dependent people. We look at the challenges for these systems from the perspective of developing the technology itself, system requirements, system design and modelling. We present a consolidated picture of the most important functions and services offered by HMS for monitoring and detecting human behaviour including its concepts, approaches, and processing techniques. Moreover, we provide an extensive, in-depth analysis and evaluation of the existing research findings in the area of e-health systems. Finally, we present challenges and open issues facing the smart HMS field and we make recommendations on how to improve future systems.
ER  - 
TY  - JOUR
T1  - A constructive algorithm to synthesize arbitrarily connected feedforward neural networks
A1  - Puma-Villanueva, Wilfredo J
A1  - dos Santos, Eurípedes P
A1  - Von Zuben, Fernando J
Y1  - 2012///
KW  -  Arbitrary architectures
KW  -  Classification
KW  -  Constructive learning
KW  - Neural networks
JF  - Neurocomputing
VL  - 75
IS  - 1
SP  - 14
EP  - 32
DO  - https://doi.org/10.1016/j.neucom.2011.05.025
UR  - https://www.sciencedirect.com/science/article/pii/S0925231211004061
N1  - Brazilian Symposium on Neural Networks (SBRN 2010) International Conference on Hybrid Artificial Intelligence Systems (HAIS 2010)
N2  - In this work we present a constructive algorithm capable of producing arbitrarily connected feedforward neural network architectures for classification problems. Architecture and synaptic weights of the neural network should be defined by the learning procedure. The main purpose is to obtain a parsimonious neural network, in the form of a hybrid and dedicate linear/nonlinear classification model, which can guide to high levels of performance in terms of generalization. Though not being a global optimization algorithm, nor a population-based metaheuristics, the constructive approach has mechanisms to avoid premature convergence, by mixing growing and pruning processes, and also by implementing a relaxation strategy for the learning error. The synaptic weights of the neural networks produced by the constructive mechanism are adjusted by a quasi-Newton method, and the decision to grow or prune the current network is based on a mutual information criterion. A set of benchmark experiments, including artificial and real datasets, indicates that the new proposal presents a favorable performance when compared with alternative approaches in the literature, such as traditional MLP, mixture of heterogeneous experts, cascade correlation networks and an evolutionary programming system, in terms of both classification accuracy and parsimony of the obtained classifier.
ER  - 
TY  - JOUR
T1  - Optimized behavioral interventions: what does system identification and control engineering have to offer?*
A1  - Rivera, Daniel E
Y1  - 2012///
KW  -  adaptive behavioral interventions
KW  -  control engineering
KW  -  experiment design
KW  -  hybrid model predictive control
KW  -  system identification
KW  - social and behavioral sciences
JF  - IFAC Proceedings Volumes
VL  - 45
IS  - 16
SP  - 882
EP  - 893
DO  - https://doi.org/10.3182/20120711-3-BE-2027.00427
UR  - https://www.sciencedirect.com/science/article/pii/S1474667015380654
N1  - 16th IFAC Symposium on System Identification
N2  - The last decade has witnessed an increasing interest in applying systems science concepts for problems in behavioral health, and using these to inform the design, analysis, and implementation of optimized interventions. How can system identification and control engineering impact interventions for chronic, relapsing disorders such as drug abuse, cigarette smoking and obesity? The paper addresses this question by focusing on the problem of time-varying “adaptive” interventions. In an adaptive intervention, dosages of intervention components are assigned based on the assessed values of tailoring variables that reflect some outcome measure (e.g., number of cigarettes smoked, parental function) or adherence (e.g, days abstinent). Because time-varying adaptive interventions constitute closed-loop dynamical systems, they are correspondlngly amenable to control engineering solutions. System identification is enabled by intensive longitudinal data (ILD) that can be obtained in the field via ecological momentary assessment (EMA); this creates the availability of rapidly sampled, continuous-time assessments from which dynamical system behavior can be discerned and modeled. How can system identification and control be applied in this broad setting is demonstrated with a number of illustrative problems: dynamic modeling and hybrid model predictive control of low-dose naltrexone as treatment for fibromyalgia, a chronic pain condition; modeling of a smoking cessation intervention involving bupropion and counseling; constructing a dynamic model of an intervention for preventing excessive weight gain during pregnancy, and Model-on-Demand Model Predictive Control in a hypothetical intervention based on the Fast Track program for assigning the frequency of home counseling visits to families with at-risk children.
ER  - 
TY  - JOUR
T1  - Applications of recursive segmentation to the analysis of DNA sequences
A1  - Li, Wentian
A1  - Bernaola-Galván, Pedro
A1  - Haghighi, Fatameh
A1  - Grosse, Ivo
Y1  - 2002///
KW  -  DNA sequence
KW  -  Dinucleotide
KW  - Recursive segmentation
JF  - Computers & Chemistry
VL  - 26
IS  - 5
SP  - 491
EP  - 510
DO  - https://doi.org/10.1016/S0097-8485(02)00010-4
UR  - https://www.sciencedirect.com/science/article/pii/S0097848502000104
N2  - Recursive segmentation is a procedure that partitions a DNA sequence into domains with a homogeneous composition of the four nucleotides A, C, G and T. This procedure can also be applied to any sequence converted from a DNA sequence, such as to a binary strong(G+C)/weak(A+T) sequence, to a binary sequence indicating the presence or absence of the dinucleotide CpG, or to a sequence indicating both the base and the codon position information. We apply various conversion schemes in order to address the following five DNA sequence analysis problems: isochore mapping, CpG island detection, locating the origin and terminus of replication in bacterial genomes, finding complex repeats in telomere sequences, and delineating coding and noncoding regions. We find that the recursive segmentation procedure can successfully detect isochore borders, CpG islands, and the origin and terminus of replication, but it needs improvement for detecting complex repeats as well as borders between coding and noncoding regions.
ER  - 
TY  - JOUR
T1  - Parallel evolutionary computation for multiobjective gene interaction analysis
A1  - Gonçalves, Francisco
A1  - Santander-Jiménez, Sergio
A1  - Sousa, Leonel
A1  - Granado-Criado, José M
A1  - Ilic, Aleksandar
Y1  - 2020///
KW  -  Bioinformatics
KW  -  Biomedicine
KW  -  Evolutionary computation
KW  -  Multiobjective algorithms
KW  - Parallelism
JF  - Journal of Computational Science
VL  - 40
SP  - 101068
EP  - 101068
DO  - https://doi.org/10.1016/j.jocs.2019.101068
UR  - https://www.sciencedirect.com/science/article/pii/S187775031930794X
N2  - Multiple studies provide evidence on the impact of certain gene interactions in the occurrence of diseases. Due to the complexity of genotype–phenotype relationships, it is required the development of highly efficient algorithmic strategies that successfully identify high-order interactions attending to different evaluation criteria. This work investigates parallel evolutionary computation approaches for multiobjective gene interaction analysis. A multiobjective genetic algorithm, with novel optimized design features, is developed and parallelized under problem-independent and problem-dependent schemes. Experimental results show the relevant performance of the method for complex interaction orders, significantly accelerating execution time (up to 296×) with regard to other state-of-the-art multiobjective tools.
ER  - 
TY  - JOUR
T1  - Reliable distributed data stream management in mobile environments
A1  - Brettlecker, Gert
A1  - Schuldt, Heiko
Y1  - 2011///
KW  -  Checkpointing
KW  -  Formal data stream model
KW  -  Mobile information management
KW  -  Reliability
KW  -  Stream operator migration
KW  - Data streams
JF  - Information Systems
VL  - 36
IS  - 3
SP  - 618
EP  - 643
DO  - https://doi.org/10.1016/j.is.2010.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S0306437910001274
N1  - Special Issue on WISE 2009 - Web Information Systems Engineering
N2  - The proliferation of sensor technology, especially in the context of embedded systems, has brought forward novel types of applications that make use of streams of continuously generated sensor data. Many applications like telemonitoring in healthcare or roadside traffic monitoring and control particularly require data stream management (DSM) to be provided in a distributed, yet reliable way. This is even more important when DSM applications are deployed in a failure-prone distributed setting including resource-limited mobile devices, for instance in applications which aim at remotely monitoring mobile patients. In this paper, we introduce a model for distributed and reliable DSM. The contribution of this paper is threefold. First, in analogy to the SQL isolation levels, we define levels of reliability and describe necessary consistency constraints for distributed DSM that specify the tolerated loss, delay, or re-ordering of data stream elements, respectively. Second, we use this model to design and analyze an algorithm for reliable distributed DSM, namely efficient coordinated operator checkpointing (ECOC). We show that ECOC provides lossless and delay-limited reliable data stream management and thus can be used in critical application domains such as healthcare, where the loss of data stream elements cannot be tolerated. Third, we present detailed performance evaluations of the ECOC algorithm running on mobile, resource-limited devices. In particular, we can show that ECOC provides a high level of reliability while, at the same time, featuring good performance characteristics with moderate resource consumption.
ER  - 
TY  - JOUR
T1  - A review of smart homes—Present state and future challenges
A1  - Chan, Marie
A1  - Estève, Daniel
A1  - Escriba, Christophe
A1  - Campo, Eric
Y1  - 2008///
KW  -  Elderly people
KW  - Smart home
JF  - Computer Methods and Programs in Biomedicine
VL  - 91
IS  - 1
SP  - 55
EP  - 81
DO  - https://doi.org/10.1016/j.cmpb.2008.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S0169260708000436
N2  - In the era of information technology, the elderly and disabled can be monitored with numerous intelligent devices. Sensors can be implanted into their home for continuous mobility assistance and non-obtrusive disease prevention. Modern sensor-embedded houses, or smart houses, cannot only assist people with reduced physical functions but help resolve the social isolation they face. They are capable of providing assistance without limiting or disturbing the resident's daily routine, giving him or her greater comfort, pleasure, and well-being. This article presents an international selection of leading smart home projects, as well as the associated technologies of wearable/implantable monitoring systems and assistive robotics. The latter are often designed as components of the larger smart home environment. The paper will conclude by discussing future challenges of the domain.
ER  - 
TY  - JOUR
T1  - One class random forests
A1  - Désir, Chesner
A1  - Bernard, Simon
A1  - Petitjean, Caroline
A1  - Heutte, Laurent
Y1  - 2013///
KW  -  Decision trees
KW  -  Ensemble methods
KW  -  Outlier detection
KW  -  Outlier generation
KW  -  Random forests
KW  -  Supervised learning
KW  - One class classification
JF  - Pattern Recognition
VL  - 46
IS  - 12
SP  - 3490
EP  - 3506
DO  - https://doi.org/10.1016/j.patcog.2013.05.022
UR  - https://www.sciencedirect.com/science/article/pii/S003132031300246X
N2  - One class classification is a binary classification task for which only one class of samples is available for learning. In some preliminary works, we have proposed One Class Random Forests (OCRF), a method based on a random forest algorithm and an original outlier generation procedure that makes use of classifier ensemble randomization principles. In this paper, we propose an extensive study of the behavior of OCRF, that includes experiments on various UCI public datasets and comparison to reference one class namely, Gaussian density models, Parzen estimators, Gaussian mixture models and One Class SVMs—with statistical significance. Our aim is to show that the randomization principles embedded in a random forest algorithm make the outlier generation process more efficient, and allow in particular to break the curse of dimensionality. One Class Random Forests are shown to perform well in comparison to other methods, and in particular to maintain stable performance in higher dimension, while the other algorithms may fail.
ER  - 
TY  - JOUR
T1  - Optimizing an artificial immune system algorithm in support of flow-Based internet traffic classification
A1  - Schmidt, Brian
A1  - Al-Fuqaha, Ala
A1  - Gupta, Ajay
A1  - Kountanis, Dionysios
Y1  - 2017///
KW  -  Internet traffic classification
KW  -  Machine learning
KW  -  Multi-class classification
KW  - Artificial immune system
JF  - Applied Soft Computing
VL  - 54
SP  - 1
EP  - 22
DO  - https://doi.org/10.1016/j.asoc.2017.01.016
UR  - https://www.sciencedirect.com/science/article/pii/S1568494617300285
N2  - The problem of classifying traffic flows in networks has become more and more important in recent times, and much research has been dedicated to it. In recent years, there has been a lot of interest in classifying traffic flows by application, based on the statistical features of each flow. Information about the applications that are being used on a network is very useful in network design, accounting, management, and security. In our previous work we proposed a classification algorithm for Internet traffic flow classification based on Artificial Immune Systems (AIS). We also applied the algorithm on an available data set, and found that the algorithm performed as well as other algorithms, and was insensitive to input parameters, which makes it valuable for embedded systems. It is also very simple to implement, and generalizes well from small training data sets. In this research, we expanded on the previous research by introducing several optimizations in the training and classification phases of the algorithm. We improved the design of the original algorithm in order to make it more predictable. We also give the asymptotic complexity of the optimized algorithm as well as draw a bound on the generalization error of the algorithm. Lastly, we also experimented with several different distance formulas to improve the classification performance. In this paper we have shown how the changes and optimizations applied to the original algorithm do not functionally change the original algorithm, while making its execution 50–60% faster. We also show that the classification accuracy of the Euclidian distance is superseded by the Manhattan distance for this application, giving 1–2% higher accuracy, making the accuracy of the algorithm comparable to that of a Naïve Bayes classifier in previous research that uses the same data set.
ER  - 
TY  - JOUR
T1  - Recent advances in medical image processing for the evaluation of chronic kidney disease
A1  - Alnazer, Israa
A1  - Bourdon, Pascal
A1  - Urruty, Thierry
A1  - Falou, Omar
A1  - Khalil, Mohamad
A1  - Shahin, Ahmad
A1  - Fernandez-Maloigne, Christine
Y1  - 2021///
KW  -  Automatic renal segmentation
KW  -  Deep learning
KW  -  Textural analysis
KW  - Chronic kidney disease
JF  - Medical Image Analysis
VL  - 69
SP  - 101960
EP  - 101960
DO  - https://doi.org/10.1016/j.media.2021.101960
UR  - https://www.sciencedirect.com/science/article/pii/S1361841521000062
N2  - Assessment of renal function and structure accurately remains essential in the diagnosis and prognosis of Chronic Kidney Disease (CKD). Advanced imaging, including Magnetic Resonance Imaging (MRI), Ultrasound Elastography (UE), Computed Tomography (CT) and scintigraphy (PET, SPECT) offers the opportunity to non-invasively retrieve structural, functional and molecular information that could detect changes in renal tissue properties and functionality. Currently, the ability of artificial intelligence to turn conventional medical imaging into a full-automated diagnostic tool is widely investigated. In addition to the qualitative analysis performed on renal medical imaging, texture analysis was integrated with machine learning techniques as a quantification of renal tissue heterogeneity, providing a promising complementary tool in renal function decline prediction. Interestingly, deep learning holds the ability to be a novel approach of renal function diagnosis. This paper proposes a survey that covers both qualitative and quantitative analysis applied to novel medical imaging techniques to monitor the decline of renal function. First, we summarize the use of different medical imaging modalities to monitor CKD and then, we show the ability of Artificial Intelligence (AI) to guide renal function evaluation from segmentation to disease prediction, discussing how texture analysis and machine learning techniques have emerged in recent clinical researches in order to improve renal dysfunction monitoring and prediction. The paper gives a summary about the role of AI in renal segmentation.
ER  - 
TY  - JOUR
T1  - Machine learning for integrating data in biology and medicine: Principles, practice, and opportunities
A1  - Zitnik, Marinka
A1  - Nguyen, Francis
A1  - Wang, Bo
A1  - Leskovec, Jure
A1  - Goldenberg, Anna
A1  - Hoffman, Michael M
Y1  - 2019///
KW  -  Heterogeneous data
KW  -  Machine learning
KW  -  Personalized medicine
KW  -  Systems biology
KW  - Computational biology
JF  - Information Fusion
VL  - 50
SP  - 71
EP  - 91
DO  - https://doi.org/10.1016/j.inffus.2018.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S1566253518304482
N2  - New technologies have enabled the investigation of biology and human health at an unprecedented scale and in multiple dimensions. These dimensions include a myriad of properties describing genome, epigenome, transcriptome, microbiome, phenotype, and lifestyle. No single data type, however, can capture the complexity of all the factors relevant to understanding a phenomenon such as a disease. Integrative methods that combine data from multiple technologies have thus emerged as critical statistical and computational approaches. The key challenge in developing such approaches is the identification of effective models to provide a comprehensive and relevant systems view. An ideal method can answer a biological or medical question, identifying important features and predicting outcomes, by harnessing heterogeneous data across several dimensions of biological variation. In this Review, we describe the principles of data integration and discuss current methods and available implementations. We provide examples of successful data integration in biology and medicine. Finally, we discuss current challenges in biomedical integrative methods and our perspective on the future development of the field.
ER  - 
TY  - JOUR
T1  - An efficient feature selection method for mobile devices with application to activity recognition
A1  - Peng, Jian-Xun
A1  - Ferguson, Stuart
A1  - Rafferty, Karen
A1  - Kelly, Paul D
Y1  - 2011///
KW  -  Activity recognition
KW  -  Data classification
KW  -  Mobile devices
KW  - Feature selection algorithm
JF  - Neurocomputing
VL  - 74
IS  - 17
SP  - 3543
EP  - 3552
DO  - https://doi.org/10.1016/j.neucom.2011.06.023
UR  - https://www.sciencedirect.com/science/article/pii/S0925231211004309
N2  - This paper presents a feature selection method for data classification, which combines a model-based variable selection technique and a fast two-stage subset selection algorithm. The relationship between a specified (and complete) set of candidate features and the class label is modeled using a non-linear full regression model which is linear-in-the-parameters. The performance of a sub-model measured by the sum of the squared-errors (SSE) is used to score the informativeness of the subset of features involved in the sub-model. The two-stage subset selection algorithm approaches a solution sub-model with the SSE being locally minimized. The features involved in the solution sub-model are selected as inputs to support vector machines (SVMs) for classification. The memory requirement of this algorithm is independent of the number of training patterns. This property makes this method suitable for applications executed in mobile devices where physical RAM memory is very limited. An application was developed for activity recognition, which implements the proposed feature selection algorithm and an SVM training procedure. Experiments are carried out with the application running on a PDA for human activity recognition using accelerometer data. A comparison with an information gain-based feature selection method demonstrates the effectiveness and efficiency of the proposed algorithm.
ER  - 
TY  - JOUR
T1  - A center sliding Bayesian binary classifier adopting orthogonal polynomials
A1  - Sun, Lei
A1  - Toh, Kar-Ann
A1  - Lin, Zhiping
Y1  - 2015///
KW  -  Bayesian inference
KW  -  Orthogonal polynomials
KW  -  Pattern recognition
KW  - Binary classification
JF  - Pattern Recognition
VL  - 48
IS  - 6
SP  - 2013
EP  - 2028
DO  - https://doi.org/10.1016/j.patcog.2014.12.010
UR  - https://www.sciencedirect.com/science/article/pii/S0031320314005196
N2  - A center sliding Bayesian design adopting orthogonal polynomials for binary pattern classification is studied in this paper. Essentially, a Bayesian weight solution is coupled with a center sliding scheme in feature space which provides an easy tuning capability for binary classification. The proposed method is compared with several state-of-the-art binary classifiers in terms of their solution forms, decision thresholds and decision boundaries. Based on the center sliding Bayesian framework, a novel orthogonal polynomial classifier is subsequently developed. The orthogonal polynomial classifier is evaluated using two representative orthogonal polynomials for feature mapping. Our experimental results show promising potential of the orthogonal polynomial classifier since it achieves both desired accuracy and computational efficiency.
ER  - 
TY  - JOUR
T1  - Predicting graft survival among kidney transplant recipients: A Bayesian decision support model
A1  - Topuz, Kazim
A1  - Zengul, Ferhat D
A1  - Dag, Ali
A1  - Almehmi, Ammar
A1  - Yildirim, Mehmet Bayram
Y1  - 2018///
KW  -  Bayesian belief network
KW  -  Elastic net
KW  -  Healthcare analytics
KW  -  Information fusion
KW  - Kidney transplantation
JF  - Decision Support Systems
VL  - 106
SP  - 97
EP  - 109
DO  - https://doi.org/10.1016/j.dss.2017.12.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167923617302233
N2  - Predicting the graft survival for kidney transplantation is a high stakes undertaking considering the shortage of available organs and the utilization of healthcare resources. The strength of any predictive model depends on the selection of proper predictors. However, despite improvements in acute rejection management and short-term graft survival, the accurate prediction of kidney transplant outcomes remains suboptimal. Among other approaches, machine-learning techniques have the potential to offer solutions to this prediction problem in kidney transplantation. This study offers a novel methodological solution to this prediction problem by: (a) analyzing the retrospective database including >31,000 U.S. patients; (b) introducing a comprehensive feature selection framework that accounts for medical literature, data analytics methods and elastic net (EN) regression (c) using sensitivity analyses and information fusion to evaluate and combine features from several machine learning approaches (i.e., support vector machines (SVM), artificial neural networks (ANN), and Bootstrap Forest (BF)); (d) constructing several different scenarios by merging different sets of features that are optioned through these fused data mining models and statistical models in addition to expert knowledge; and (e) using best performing sets in Bayesian belief network (BBN) algorithm to identify non-linear relationships and the interactions between explanatory factors and risk levels for kidney graft survival. The results showed that the predictor set obtained through fused data mining model and literature review outperformed the all other alternative predictors sets with the scores of 0.602, 0.684, 0.495 for F-Measure, Average Accuracy, and G-Mean, respectively. Overall, our findings provide novel insights about risk prediction that could potentially help in improving the outcome of kidney transplants. This methodology can also be applied to other similar transplant data sets.
ER  - 
TY  - JOUR
T1  - LGscore: A method to identify disease-related genes using biological literature and Google data
A1  - Kim, Jeongwoo
A1  - Kim, Hyunjin
A1  - Yoon, Youngmi
A1  - Park, Sanghyun
Y1  - 2015///
KW  -  Data mining
KW  -  Disease
KW  -  Gene
KW  -  Google
KW  - Text-mining
JF  - Journal of Biomedical Informatics
VL  - 54
SP  - 270
EP  - 282
DO  - https://doi.org/10.1016/j.jbi.2015.01.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415000052
N2  - Since the genome project in 1990s, a number of studies associated with genes have been conducted and researchers have confirmed that genes are involved in disease. For this reason, the identification of the relationships between diseases and genes is important in biology. We propose a method called LGscore, which identifies disease-related genes using Google data and literature data. To implement this method, first, we construct a disease-related gene network using text-mining results. We then extract gene–gene interactions based on co-occurrences in abstract data obtained from PubMed, and calculate the weights of edges in the gene network by means of Z-scoring. The weights contain two values: the frequency and the Google search results. The frequency value is extracted from literature data, and the Google search result is obtained using Google. We assign a score to each gene through a network analysis. We assume that genes with a large number of links and numerous Google search results and frequency values are more likely to be involved in disease. For validation, we investigated the top 20 inferred genes for five different diseases using answer sets. The answer sets comprised six databases that contain information on disease–gene relationships. We identified a significant number of disease-related genes as well as candidate genes for Alzheimer’s disease, diabetes, colon cancer, lung cancer, and prostate cancer. Our method was up to 40% more accurate than existing methods.
ER  - 
TY  - JOUR
T1  - A Fast Localization and Extraction of Microaneurysm for Early Detection of Diabetic Retionopathy
A1  - Cheng, Yun
A1  - Liu, Weirong
A1  - Wang, Chenglong
A1  - Gu, Xin
A1  - Cheng, Yijun
A1  - Wang, Shengnan
A1  - Peng, Jun
Y1  - 2020///
KW  -  CLAHE
KW  -  Diabetic Retionopathy
KW  -  Edge detection
KW  -  Random Forest
KW  - Microaneurysm
JF  - IFAC-PapersOnLine
VL  - 53
IS  - 2
SP  - 16400
EP  - 16405
DO  - https://doi.org/10.1016/j.ifacol.2020.12.684
UR  - https://www.sciencedirect.com/science/article/pii/S2405896320309976
N1  - 21st IFAC World Congress
N2  - Diabetic Retinopathy (DR) is regarded as one of the leading causes of blindness globally. Microaneurysms (MAs) detection is essential to the computer aided diagnosis of DR at an early stage. However, the automatic detection of MAs is still a challenging problem as they are too tiny to be recognized and hard to be distinguished from other similar lesions. Therefore, we propose an efficient localization and extraction method for MAs, where the edge detection and Random Forest are utilized to enhance the accuracy of detection results. Finally, the proposed method is evaluated based on the public retinal image database MESSIDOR. Numerical results show that high accuracy and timely detection can be obtained with the proposed solution.
ER  - 
TY  - JOUR
T1  - Automatic detection and characterisation of retinal vessel tree bifurcations and crossovers in eye fundus images
A1  - Calvo, David
A1  - Ortega, Marcos
A1  - Penedo, Manuel G
A1  - Rouco, Jose
Y1  - 2011///
KW  -  Automatic classification
KW  -  Blood vessel tree
KW  -  Feature point
KW  -  Segmentation
KW  - Eye fundus
JF  - Computer Methods and Programs in Biomedicine
VL  - 103
IS  - 1
SP  - 28
EP  - 38
DO  - https://doi.org/10.1016/j.cmpb.2010.06.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169260710001446
N2  - Analysis of retinal vessel tree characteristics is an important task in medical diagnosis, specially in cases of diseases like vessel occlusion, hypertension or diabetes. The detection and classification of feature points in the arteriovenous eye tree will increase the information about the structure allowing its use for medical diagnosis. In this work a method for detection and classification of retinal vessel tree feature points is presented. The method applies and combines imaging techniques such as filters or morphologic operations to obtain an adequate structure for the detection. Classification is performed by analysing the feature points environment. Detection and classification of feature points is validated using the VARIA database. Experimental results are compared to previous approaches showing a much higher specificity in the characterisation of feature points while slightly increasing the sensitivity. These results provide a more reliable methodology for retinal structure analysis.
ER  - 
TY  - JOUR
T1  - Detection of glaucomatous change based on vessel shape analysis
A1  - Matsopoulos, George K
A1  - Asvestas, Pantelis A
A1  - Delibasis, Konstantinos K
A1  - Mouravliansky, Nikolaos A
A1  - Zeyen, Thierry G
Y1  - 2008///
KW  -  -Nearest Neighbor classifier
KW  -  Artificial neural networks
KW  -  Classification
KW  -  Registration
KW  -  Self organizing maps
KW  -  Sequential float forward search
KW  -  Wavelet coefficients
KW  - Glaucoma
JF  - Computerized Medical Imaging and Graphics
VL  - 32
IS  - 3
SP  - 183
EP  - 192
DO  - https://doi.org/10.1016/j.compmedimag.2007.11.003
UR  - https://www.sciencedirect.com/science/article/pii/S0895611107001681
N2  - Glaucoma, a leading cause of blindness worldwide, is a progressive optic neuropathy with characteristic structural changes in the optic nerve head and concomitant visual field defects. Ocular hypertension (i.e. elevated intraocular pressure without glaucoma) is the most important risk factor to develop glaucoma. Even though a number of variables, including various optic disc and visual field parameters, have been used in order to identify early glaucomatous damage, there is a need for computer-based methods that can detect early glaucomatous progression so that treatment to prevent further progression can be initiated. This paper is focused on the description of a system based on image processing and classification techniques for the estimation of quantitative parameters to define vessel deformation and the classification of image data into two classes: patients with ocular hypertension who develop glaucomatous damage and patients with ocular hypertension who remain stable. The proposed system consists of the retinal image preprocessing module for vessel central axis segmentation, the automatic retinal image registration module based on a novel application of self organizing maps (SOMs) to define automatic point correspondence, the retinal vessel attributes calculation module to select the vessel shape attributes and the data classification module, using an artificial neural network classifier, to perform the necessary subject classification. Implementation of the system to optic disc data from 127 subjects obtained by a fundus camera at regular intervals provided a classification rate of 87.5%, underscoring the value of the proposed system to assist in the detection of early glaucomatous change.
ER  - 
TY  - JOUR
T1  - An efficient two-pass classifier system for patient opinion mining to analyze drugs satisfaction
A1  - Padmavathy, P
A1  - Pakkir Mohideen, S
Y1  - 2020///
KW  -  Artificial neural network
KW  -  Customer reviews
KW  -  Drug satisfaction
KW  -  Opinion mining
KW  -  Support vector machine
KW  - Two-pass classifier
JF  - Biomedical Signal Processing and Control
VL  - 57
SP  - 101755
EP  - 101755
DO  - https://doi.org/10.1016/j.bspc.2019.101755
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419303362
N2  - Opinion mining is a well-known problem in natural language processing that increasing attention in recent years. With the rapid growth in e-commerce, reviews for popular products on the web have grown rapidly. In opinion mining, the greater part of the scientists has dealt with general domains, for example, electronic items, movies, and restaurants audit not much on health and medical domains. Therefore, in this paper, we focus on predicting the drug satisfaction level among the other patient who already experienced the effect of a drug using a novel Two-pass classifier. The Two-pass classifier is a combination of Support Vector Machine and Artificial Neural Network (SVMNN). Here, at first, we collect customer reviews from healthcare domain. After that, we extract the important features from each review and based on the features we generate the feature vector. Then, we apply two-pass classifier in order to predict the given customer review is positive or negative. The performance of the proposed approach is analyzed using precision, recall, and F-measures. The experimentation results show that the proposed system attains the better result associated with the available methods.
ER  - 
TY  - JOUR
T1  - Referable diabetic retinopathy identification from eye fundus images with weighted path for convolutional neural network
A1  - Liu, Yi-Peng
A1  - Li, Zhanqing
A1  - Xu, Cong
A1  - Li, Jing
A1  - Liang, Ronghua
Y1  - 2019///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Eye fundus images
KW  - Diabetic retinopathy
JF  - Artificial Intelligence in Medicine
VL  - 99
SP  - 101694
EP  - 101694
DO  - https://doi.org/10.1016/j.artmed.2019.07.002
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718307747
N2  - Diabetic retinopathy (DR) is the most common cause of blindness in middle-age subjects and low DR screening rates demonstrates the need for an automated image assessment system, which can benefit from the development of deep learning techniques. Therefore, the effective classification performance is significant in favor of the referable DR identification task. In this paper, we propose a new strategy, which applies multiple weighted paths into convolutional neural network, called the WP-CNN, motivated by the ensemble learning. In WP-CNN, multiple path weight coefficients are optimized by back propagation, and the output features are averaged for redundancy reduction and fast convergence. The experiment results show that with the efficient training convergence rate WP-CNN achieves an accuracy of 94.23% with sensitivity of 90.94%, specificity of 95.74%, an area under the receiver operating curve of 0.9823 and F1-score of 0.9087. By taking full advantage of the multipath mechanism, the proposed WP-CNN is shown to be accurate and effective for referable DR identification compared to the state-of-art algorithms.
ER  - 
TY  - JOUR
T1  - Nanoparticle-structured thin film sensor arrays for breath sensing
A1  - Luo, Jinghu
A1  - Luo, Jin
A1  - Wang, Lingyan
A1  - Shi, Xiajing
A1  - Yin, Jun
A1  - Crew, Elizabeth
A1  - Lu, Susan
A1  - Lesperance, Leann M
A1  - Zhong, Chuan-Jian
Y1  - 2012///
KW  -  Acetone
KW  -  Chemiresistor array
KW  -  Human breath
KW  -  Nanostructured sensing materials
KW  -  Pattern recognition
KW  - Breath sensor
JF  - Sensors and Actuators B: Chemical
VL  - 161
IS  - 1
SP  - 845
EP  - 854
DO  - https://doi.org/10.1016/j.snb.2011.11.045
UR  - https://www.sciencedirect.com/science/article/pii/S0925400511010501
N2  - Chemiresistor sensor arrays with nanoparticle-structured thin films and coupled with pattern recognition engine could enable chemical sensing of biomarker molecules such as acetone in human breath samples with high sensitivity and selectivity, which serves as an intriguing approach to developing potential medical device for noninvasive monitoring of diabetes. This report describes the results of a proof-of-concept investigation to demonstrate the viability of such sensor arrays with the nanostructured sensing films for the detection of human breath and acetone vapor (a volatile biomarker in diabetics’ breath). The nanostructured sensing films were prepared by self-assembly of monolayer-capped gold nanoparticles on an array of chemiresistors using functionalized alkyl thiols as linking molecules. In addition to showing the viability for quantitative detection of acetone, the investigation focused on the understanding of how differences in human breaths could impact the sensor array recognition characteristics. The results have revealed that the difference in data scattering between male and female's breath samples was relatively small. The data distribution between breath samples obtained under controlled fasting and food conditions depends on the design parameters of the sensor arrays. The results have demonstrated the potential of the sensor arrays coupled with pattern recognition for the detection of acetone in diabetic breath, which upon further refinements and testing could lead to a useful portable sensor device for human breath recognition.
ER  - 
TY  - JOUR
T1  - Identifying risk factors for heart disease over time: Overview of 2014 i2b2/UTHealth shared task Track 2
A1  - Stubbs, Amber
A1  - Kotfila, Christopher
A1  - Xu, Hua
A1  - Uzuner, Özlem
Y1  - 2015///
KW  -  CAD
KW  -  Clinical narratives
KW  -  Diabetes
KW  - Natural language processing
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - S67
EP  - S77
DO  - https://doi.org/10.1016/j.jbi.2015.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001409
N1  - Supplement: Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data
N2  - The second track of the 2014 i2b2/UTHealth natural language processing shared task focused on identifying medical risk factors related to Coronary Artery Disease (CAD) in the narratives of longitudinal medical records of diabetic patients. The risk factors included hypertension, hyperlipidemia, obesity, smoking status, and family history, as well as diabetes and CAD, and indicators that suggest the presence of those diseases. In addition to identifying the risk factors, this track of the 2014 i2b2/UTHealth shared task studied the presence and progression of the risk factors in longitudinal medical records. Twenty teams participated in this track, and submitted 49 system runs for evaluation. Six of the top 10 teams achieved F1 scores over 0.90, and all 10 scored over 0.87. The most successful system used a combination of additional annotations, external lexicons, hand-written rules and Support Vector Machines. The results of this track indicate that identification of risk factors and their progression over time is well within the reach of automated systems.
ER  - 
TY  - JOUR
T1  - Facilitating Clinical Outcomes Assessment through the Automated Identification of Quality Measures for Prostate Cancer Surgery
A1  - D'Avolio, Leonard W
A1  - Litwin, Mark S
A1  - Rogers, Selwyn O
A1  - Bui, Alex A T
Y1  - 2008///
JF  - Journal of the American Medical Informatics Association
VL  - 15
IS  - 3
SP  - 341
EP  - 348
DO  - https://doi.org/10.1197/jamia.M2649
UR  - https://www.sciencedirect.com/science/article/pii/S1067502708000169
N2  - Objectives
The College of American Pathologists (CAP) Category 1 quality measures, tumor stage, Gleason score, and surgical margin status, are used by physicians and cancer registrars to categorize patients into groups for clinical trials and treatment planning. This study was conducted to evaluate the effectiveness of an application designed to automatically extract these quality measures from the postoperative pathology reports of patients having undergone prostatectomies for treatment of prostate cancer.
Design
An application was developed with the Clinical Outcomes Assessment Toolkit that uses an information pipeline of regular expressions and support vector machines to extract CAP Category 1 quality measures. System performance was evaluated against a gold standard of 676 pathology reports from the University of California at Los Angeles Medical Center and Brigham and Women's Hospital. To evaluate the feasibility of clinical implementation, all pathology reports were gathered using administrative codes with no manual preprocessing of the data performed.
Measurements
The sensitivity, specificity, and overall accuracy of system performance were measured for all three quality measures. Performance at both hospitals was compared, and a detailed failure analysis was conducted to identify errors caused by poor data quality versus system shortcomings.
Results
Accuracies for Gleason score were 99.7%, tumor stage 99.1%, and margin status 97.2%, for an overall accuracy of 98.67%. System performance on data from both hospitals was comparable. Poor clinical data quality led to a decrease in overall accuracy of only 0.3% but accounted for 25.9% of the total errors.
Conclusion
Despite differences in document format and pathologists' reporting styles, strong system performance indicates the potential of using a combination of regular expressions and support vector machines to automatically extract CAP Category 1 quality measures from postoperative prostate cancer pathology reports.
ER  - 
TY  - JOUR
T1  - Pancreatlas: Applying an Adaptable Framework to Map the Human Pancreas in Health and Disease
A1  - Saunders, Diane C
A1  - Messmer, James
A1  - Kusmartseva, Irina
A1  - Beery, Maria L
A1  - Yang, Mingder
A1  - Atkinson, Mark A
A1  - Powers, Alvin C
A1  - Cartailler, Jean-Philippe
A1  - Brissova, Marcela
Y1  - 2020///
KW  -  data integration
KW  -  data publication and archiving
KW  -  diabetes
KW  -  human pancreas development
KW  -  imaging databases
KW  -  microscopy
KW  -  open source
KW  -  pancreas imaging
KW  -  software
KW  -  web resource
KW  - application programming interface
JF  - Patterns
VL  - 1
IS  - 8
SP  - 100120
EP  - 100120
DO  - https://doi.org/10.1016/j.patter.2020.100120
UR  - https://www.sciencedirect.com/science/article/pii/S2666389920301616
N2  - Summary
Human tissue phenotyping generates complex spatial information from numerous imaging modalities, yet images typically become static figures for publication, and original data and metadata are rarely available. While comprehensive image maps exist for some organs, most resources have limited support for multiplexed imaging or have non-intuitive user interfaces. Therefore, we built a Pancreatlas resource that integrates several technologies into a unique interface, allowing users to access richly annotated web pages, drill down to individual images, and deeply explore data online. The current version of Pancreatlas contains over 800 unique images acquired by whole-slide scanning, confocal microscopy, and imaging mass cytometry, and is available at https://www.pancreatlas.org. To create this human pancreas-specific biological imaging resource, we developed a React-based web application and Python-based application programming interface, collectively called Flexible Framework for Integrating and Navigating Data (FFIND), which can be adapted beyond Pancreatlas to meet countless imaging or other structured data-management needs.
ER  - 
TY  - JOUR
T1  - Demonstrating the stability of support vector machines for classification
A1  - Buciu, I
A1  - Kotropoulos, C
A1  - Pitas, I
Y1  - 2006///
KW  -  Bagging
KW  -  Decomposition of the prediction error
KW  -  Stability
KW  - Support vector machines
JF  - Signal Processing
VL  - 86
IS  - 9
SP  - 2364
EP  - 2380
DO  - https://doi.org/10.1016/j.sigpro.2005.11.005
UR  - https://www.sciencedirect.com/science/article/pii/S0165168405003798
N1  - Special Section: Signal Processing in UWB Communications
N2  - In this paper, we deal with the stability of support vector machines (SVMs) in classification tasks. We decompose the average prediction error of SVMs into the bias and the variance terms, and we define the aggregation effect. By estimating the aforementioned terms with bootstrap smoothing techniques, we demonstrate that support vector machines are stable classifiers. To investigate the stability of the SVM several experiments were conducted. The first experiment deals with face detection. The second experiment conducted is related to the binary classification of three artificially generated data sets stemming from known distributions and an additional synthetic data set known as “Waveform”. Finally, in order to support our claim on the stability of SVMs, two more binary classification experiments were carried out on the “Pime Indian Diabetes” and the “Wisconsin Breast Cancer” data sets. In general, bagging is not expected to improve the classification accuracy of SVMs.
ER  - 
TY  - JOUR
T1  - SERS hydrogel pellets for highly repeatable and reliable detections of significant small biomolecules in complex samples without pretreatment
A1  - Sun, Dan
A1  - Cao, Fanghao
A1  - Wang, Huimin
A1  - Guan, Shulin
A1  - Su, Ailing
A1  - Xu, Weiqing
A1  - Xu, Shuping
Y1  - 2021///
KW  -  blood glucose
KW  -  hydrogel micropellets
KW  -  melamine
KW  -  pretreatment-free
KW  - Surface-enhanced Raman scattering
JF  - Sensors and Actuators B: Chemical
VL  - 327
SP  - 128943
EP  - 128943
DO  - https://doi.org/10.1016/j.snb.2020.128943
UR  - https://www.sciencedirect.com/science/article/pii/S0925400520312909
N2  - The detections of significant small molecules in biological fluids is always challenging due to the complicated sample pretreatment. Here, a universal SERS-hydrogel micropellet was developed for pretreatment-free, reliable detections of small molecules (glucose and melamine) in complex sample (whole blood and milk). The SERS-hydrogel micropellet has an adjustable pore size, which is acquired by the ultraviolet light solidification of the water-in-oil microdroplets in which the hydrogel monomers and the SERS-active metal nanoparticles (MNPs) were encapsulated. These micropellets have a pore size selectivity to allow small molecules to access in and exclude larger molecules, which is helpful for the highly selective, label-free/labeling SERS determinations of small molecules. This SERS substrate ensures high reproducibility of SERS detections since MNPs are uniformly dispersed in each micropellet. The hydrogel matrix can well protect MNPs from the surrounding environments to guarantee long-term stability. The lowest detectable concentration is 10 μM for glucose in whole blood and 10 nM for melamine in milk, and the linear ranges are 0.1-20 mM and 10-8-10-3 M, respectively. This method avoids the complicated preprocessing steps, requires a small volume of samples, has a fast response time and low-cost, which provides the possibility for multiplex SERS detections in liquid biopsy.
ER  - 
TY  - JOUR
T1  - Multilevel mental stress detection using ultra-short pulse rate variability series
A1  - Zubair, Muhammad
A1  - Yoon, Changwoo
Y1  - 2020///
KW  -  Mental stress detection
KW  -  PPG signals
KW  -  Poincare plot
KW  -  Quadratic discriminant analysis
KW  -  Support Vector Machine
KW  -  Wearable sensors
KW  - Pulse rate variability
JF  - Biomedical Signal Processing and Control
VL  - 57
SP  - 101736
EP  - 101736
DO  - https://doi.org/10.1016/j.bspc.2019.101736
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419303179
N2  - Prolonged exposure to mental stress reduces human work efficiency in daily life and may increase the risk of diabetes and cardiovascular diseases. However, identification of the true degree of stress in its initial stage can reduce the risk of life threatening diseases. In this paper, we proposed a multilevel stress detection system using ultra-short term recordings of a low cost wearable sensor. We designed an experimental paradigm based on Mental Arithmetic Tasks (MAT) to properly stimulate different levels of stress. During the experiment, Photoplethysmogram (PPG) signals were recorded along with subjective feedback for validation of stress induction. The beat-to-beat interval series, estimated from sixty seconds long segments of PPG signals, were used to extract different features based on their reliability. In order to capture the temporal information in the ultra-short term segments of PPG, we introduced a new set of features which have the potential to quantify the temporal information at point-to-point level in the Poincare plot. We also used a Sequential Forward Floating Selection (SFFS) algorithm to mitigate the issues of irrelevancy and redundancy among features. We investigated two classifiers based on quadratic discriminant analysis (QDA) and Support Vector Machine (SVM). The results of the proposed method produced 94.33% accuracy with SVM for five-level identification of mental stress. Moreover, we validated the generalizability of the system by evaluating its performance on a dataset recorded with a different stressor (Stroop). In conclusion, we found that the proposed multilevel stress detection system in conjunction with new parameters of the Poincare plot has the potential to detect five different mental stress states using ultra-short term recordings of a low-cost PPG sensor.
ER  - 
TY  - JOUR
T1  - Analysis of functional and pathway association of differential co-expressed genes: A case study in drug addiction
A1  - Li, Zi-hui
A1  - Liu, Yu-feng
A1  - Li, Ke-ning
A1  - DuanMu, Hui-zi
A1  - Chang, Zhi-qiang
A1  - Li, Zhen-qi
A1  - Zhang, Shan-zhen
A1  - Xu, Yan
Y1  - 2012///
KW  -  Functional association
KW  -  Gene co-expression meta-analysis
KW  -  Pathway association
KW  - Drug addiction
JF  - Journal of Biomedical Informatics
VL  - 45
IS  - 1
SP  - 30
EP  - 36
DO  - https://doi.org/10.1016/j.jbi.2011.08.014
UR  - https://www.sciencedirect.com/science/article/pii/S1532046411001419
N2  - Drug addiction has been considered as a kind of chronic relapsing brain disease influenced by both genetic and environmental factors. At present, many causative genes and pathways related to diverse kinds of drug addiction have been discovered, while less attention has been paid to common mechanisms shared by different drugs underlying addiction. By applying a co-expression meta-analysis method to mRNA expression profiles of alcohol, cocaine, heroin addicted and normal samples, we identified significant gene co-expression pairs. As co-expression networks of drug group and control group constructed, associated function term pairs and pathway pairs reflected by co-expression pattern changes were discovered by integrating functional and pathway information respectively. The results indicated that respiratory electron transport chain, synaptic transmission, mitochondrial electron transport, signal transduction, locomotory behavior, response to amphetamine, negative regulation of cell migration, glucose regulation of insulin secretion, signaling by NGF, diabetes pathways, integration of energy metabolism, dopamine receptors may play an important role in drug addiction. In addition, the results can provide theory support for studies of addiction mechanisms.
ER  - 
TY  - JOUR
T1  - Effective blood vessels reconstruction methodology for early detection and classification of diabetic retinopathy using OCTA images by artificial neural network
A1  - Abdelsalam, Mohamed M
Y1  - 2020///
KW  -  And artificial neural network classifier
KW  -  Artificial intelligence
KW  -  Optical coherence tomography angiography
KW  - Diabetic retinopathy
JF  - Informatics in Medicine Unlocked
VL  - 20
SP  - 100390
EP  - 100390
DO  - https://doi.org/10.1016/j.imu.2020.100390
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820303981
N2  - Background
Diabetic retinopathy (DR) refers to the ocular effect of diabetes. It is one of the retinal vascular diseases that can cause loss of vision. DR leads to alterations in vascular networks, including angiogenesis and capillary regression.
Objective
The objective of this research is to provide an effective robust and accurate automatic methodology for the early detection of DR subjects. The methodology depends on two steps: 1) Blood vessel reconstruction, enhancement, and re-continuity using written custom programs, and 2) An Artificial Neural Network (ANN) as an automatic classifier between the diabetic without diabetic retinopathy (DR) and the Mild to Moderate Non-Proliferative Diabetic Retinopathy (NPDR) subjects.
Methods
This approach depends on extracting the seven features, which are the most changeable features according to the morphological retinal vascular network changes. These features are the mean of the intercapillary areas as regions of interest for the largest 10 and 20 selected regions, either including or excluding the Foveal Avascular Zone (FAZ) region, FAZ perimeter, circularity index, and vascular density. The OCTA images were obtained and approved by the Ophthalmology Center in Mansoura University-Egypt.
Results
One hundred images were processed, distributed as follows: 40 eyes were normal, 30 eyes were diabetic without DR, and 30 eyes were NPDR subjects. The total system accuracy reached 97%. The performance parameters of the classification system for normal versus diabetic were 97.5% for sensitivity, 96.67% for specificity, and 95.2% for precision. While, the measures for a diabetic without DR versus non-proliferative DR (mild to moderate) were 96.67% for sensitivity, 96.67% for specificity, and 96.67% for precision. The maximum misclassification error was 3.33%.
Conclusion
The proposed methodology is capable of accurate classification of the diabetic without DR and Non-proliferative diabetic retinopathy subjects. This methodology depends on using written custom programs and a plugin for MATLAB and Fiji based Image-J software with a supervised artificial neural network. This technique achieves high accuracy, resolution, specificity, and precision with only a short time needed for diagnosis.
ER  - 
TY  - JOUR
T1  - Supervised machine learning for the assessment of Chronic Kidney Disease advancement
A1  - Ventrella, Piervincenzo
A1  - Delgrossi, Giovanni
A1  - Ferrario, Gianmichele
A1  - Righetti, Marco
A1  - Masseroli, Marco
Y1  - 2021///
KW  -  chronicity management
KW  -  personalized care
KW  -  predicting renal failure
KW  -  supervised machine learning
KW  - Chronic Kidney Disease
JF  - Computer Methods and Programs in Biomedicine
VL  - 209
SP  - 106329
EP  - 106329
DO  - https://doi.org/10.1016/j.cmpb.2021.106329
UR  - https://www.sciencedirect.com/science/article/pii/S016926072100403X
N2  - ABSTRACT
Background and objective: Chronic Kidney Disease (CKD) is a condition characterized by a progressive loss of kidney function over time caused by many diseases. The most effective weapons against CKD are early diagnosis and treatment, which in most of the cases can only postpone the onset of complete kidney failure. The CKD grading system is classified based on the estimated Glomerular Filtration Rate (eGFR), and it helps to stratify patients for risk, follow up and management planning. This study aims to effectively predict how soon a CKD patient will need to be dialyzed, thus allowing personalized care and strategic planning of treatment. Methods: To accurately predict the time frame within which a CKD patient will necessarily have to be dialyzed, a computational model based on a supervised machine learning approach is developed. Many techniques, regarding both information extraction and model training phases, are compared in order to understand which approaches are most effective. The different models compared are trained on the data extracted from the Electronic Medical Records of the Vimercate Hospital. Results: As final model, we propose a set of Extremely Randomized Trees classifiers considering 27 features, including creatinine level, urea, red blood cells count, eGFR trend (which is not even the most important), age and associated comorbidities. In predicting the occurrence of complete renal failure within the next year rather than later, it obtains a test accuracy of 94%, specificity of 91% and sensitivity of 96%. More and shorter time-frame intervals, up to 6 months of granularity, can be specified without relevantly worsening the model performance. Conclusions: The developed computational model provides nephrologists with a great support in predicting the patient's clinical pathway. The model promising results, coupled with the knowledge and experience of the clinicians, can effectively lead to better personalized care and strategic planning of both patient's needs and hospital resources.
ER  - 
TY  - JOUR
T1  - Spatially guided functional correlation tensor: A new method to associate body mass index and white matter neuroimaging
A1  - Byeon, Kyoungseob
A1  - Park, Bo-yong
A1  - Park, Hyunjin
Y1  - 2019///
KW  -  Classification
KW  -  Functional correlation tensor
KW  -  Imaging biomarker
KW  -  Prediction
KW  - Obesity
JF  - Computers in Biology and Medicine
VL  - 107
SP  - 137
EP  - 144
DO  - https://doi.org/10.1016/j.compbiomed.2019.02.010
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519300575
N2  - Obesity causes critical health problems including cardiovascular disease, diabetes, and stroke. Various neuroimaging methods including diffusion tensor imaging (DTI) are used to explore white matter (WM) alterations in obesity. The functional correlation tensor (FCT) is a method to simulate DTI in WM using resting-state functional magnetic resonance imaging (rs-fMRI). In this study, we enhanced the FCT with additional anatomical information from T1-weighted data in a regression framework. The goal was to 1) develop a spatially guided enhanced FCT (s-eFCT) and to 2) use it to identify imaging biomarkers for obesity. We computed fractional anisotropy (FA) and the mean diffusivity (MD) from the s-eFCT. The regional FA and MD values that can explain body mass index (BMI) well were chosen. The identified regional FA and MD values were used to predict BMI values. The correlation between real and predicted BMIs was 0.57. There was no significant correlation between real and predicted DTI using the MD. The BMI predicted using FA was used to classify participants into three obesity subgroups. The classification accuracy was 57.20%. In summary, we found potential imaging biomarkers of obesity based on the s-eFCT.
ER  - 
TY  - JOUR
T1  - Safe instance screening for primal multi-label ProSVM
A1  - Zhang, Ying
A1  - Xu, Yitian
A1  - Xu, Chang
A1  - Zhong, Peiwei
Y1  - 2021///
KW  -  DCDM
KW  -  ProSVM
KW  -  Safe screening rule
KW  - Multi-label learning
JF  - Knowledge-Based Systems
VL  - 229
SP  - 107362
EP  - 107362
DO  - https://doi.org/10.1016/j.knosys.2021.107362
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121006249
N2  - As an important model in multi-label learning, ProSVM considers two problems simultaneously: one is to distinguish the relevant labels from the irrelevant labels of instances, and the other is to rank the relevant labels. Thus it produces better generalization ability in comparison with other multi-label models. However, ProSVM leads to high computational cost when the label space is enormous since it employs the label-pairs instead of the instances in the training process. To effectively deal with this defect, a safe screening rule is proposed to speed up the learning process, named PSSR. Different from the existing safe screening rule, our screening method is constructed based on solving the primal problem of ProSVM rather than the dual one. So it can address large-scale problems. To the best of our knowledge, it is the first safe screening rule for the primal problem in multi-label learning. Experimental results on eight benchmark datasets demonstrate the superiority of PSSR. Finally, our PSSR is applied to the clinical data of diabetic patients, and obtain better performance.
ER  - 
TY  - JOUR
T1  - Longitudinal K-means approaches to clustering and analyzing EHR opioid use trajectories for clinical subtypes
A1  - Mullin, Sarah
A1  - Zola, Jaroslaw
A1  - Lee, Robert
A1  - Hu, Jinwei
A1  - MacKenzie, Brianne
A1  - Brickman, Arlen
A1  - Anaya, Gabriel
A1  - Sinha, Shyamashree
A1  - Li, Angie
A1  - Elkin, Peter L
Y1  - 2021///
KW  -  Electronic health records
KW  -  Opioids
KW  -  Patient subtypes
KW  -  Trajectory analysis
KW  - Longitudinal k-means clustering
JF  - Journal of Biomedical Informatics
VL  - 122
SP  - 103889
EP  - 103889
DO  - https://doi.org/10.1016/j.jbi.2021.103889
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421002185
N2  - Identification of patient subtypes from retrospective Electronic Health Record (EHR) data is fraught with inherent modeling issues, such as missing data and variable length time intervals, and the results obtained are highly dependent on data pre-processing strategies. As we move towards personalized medicine, assessing accurate patient subtypes will be a key factor in creating patient specific treatment plans. Partitioning longitudinal trajectories from irregularly spaced and variable length time intervals is a well-established, but open problem. In this work, we present and compare k-means approaches for subtyping opioid use trajectories from EHR data. We then interpret the resulting subtypes using decision trees, examining how each subtype is influenced by opioid medication features and patient diagnoses, procedures, and demographics. Finally, we discuss how the subtypes can be incorporated in static machine learning models as features in predicting opioid overdose and adverse events. The proposed methods are general, and can be extended to other EHR prescription dosage trajectories.
ER  - 
TY  - JOUR
T1  - Optic disc segmentation using the sliding band filter
A1  - Dashtbozorg, Behdad
A1  - Mendonça, Ana Maria
A1  - Campilho, Aurélio
Y1  - 2015///
KW  -  Boundary extraction
KW  -  Retinal images
KW  -  Segmentation evaluation
KW  -  Sliding band filter
KW  - Optic disc segmentation
JF  - Computers in Biology and Medicine
VL  - 56
SP  - 1
EP  - 12
DO  - https://doi.org/10.1016/j.compbiomed.2014.10.009
UR  - https://www.sciencedirect.com/science/article/pii/S0010482514002832
N2  - Background: The optic disc (OD) centre and boundary are important landmarks in retinal images and are essential for automating the calculation of health biomarkers related with some prevalent systemic disorders, such as diabetes, hypertension, cerebrovascular and cardiovascular diseases. Methods: This paper presents an automatic approach for OD segmentation using a multiresolution sliding band filter (SBF). After the preprocessing phase, a low-resolution SBF is applied on a downsampled retinal image and the locations of maximal filter response are used for focusing the analysis on a reduced region of interest (ROI). A high-resolution SBF is applied to obtain a set of pixels associated with the maximum response of the SBF, giving a coarse estimation of the OD boundary, which is regularized using a smoothing algorithm. Results: Our results are compared with manually extracted boundaries from public databases (ONHSD, MESSIDOR and INSPIRE-AVR datasets) outperforming recent approaches for OD segmentation. For the ONHSD, 44% of the results are classified as Excellent, while the remaining images are distributed between the Good (47%) and Fair (9%) categories. An average overlapping area of 83%, 89% and 85% is achieved for the images in ONHSD, MESSIDOR and INSPIR-AVR datasets, respectively, when comparing with the manually delineated OD regions. Discussion: The evaluation results on the images of three datasets demonstrate the better performance of the proposed method compared to recently published OD segmentation approaches and prove the independence of this method when from changes in image characteristics such as size, quality and camera field of view.
ER  - 
TY  - JOUR
T1  - Automatic Characterization of the Serous Retinal Detachment Associated with the Subretinal Fluid Presence in Optical Coherence Tomography Images
A1  - de Moura, Joaquim
A1  - Novo, Jorge
A1  - Penas, Susana
A1  - Ortega, Marcos
A1  - Silva, Jorge
A1  - Mendonça, Ana Maria
Y1  - 2018///
KW  -  diabetic macular edema
KW  -  optical coherence tomography
KW  -  serous retinal detachment
KW  - Computer-aided diagnosis
JF  - Procedia Computer Science
VL  - 126
SP  - 244
EP  - 253
DO  - https://doi.org/10.1016/j.procs.2018.07.258
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918312341
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia
N2  - An accurate detection of the macular edema (ME) presence constitutes a crucial ophthalmological issue as it provides useful information for the identification, diagnosis and treatment of different relevant ocular and systemic diseases. Serous Retinal Detachment (SRD) is a particular type of ME, which is characterized by the leakage of fluid that has a propensity of being accumulated in the macular region. This paper proposes a new methodology for the automatic identification and characterization of the SRD edema using Optical Coherence Tomography (OCT) images. The subretinal fluids and the External Limiting Membrane (ELM) retinal layers are identified and characterized to measure the disease severity. Four different visualization modules were designed including representative derived parameters to facilitate the doctor’s work in the diagnostic evaluation of ME. The different steps of this method were validated using the manual labelling provided by an expert clinician. The validation of the proposed method offered satisfactory results, constituting a suitable scenario with intuitive visual representations that also include different relevant biomarkers.
ER  - 
TY  - JOUR
T1  - Hybrid predictive modelling: Thyrotoxic atrial fibrillation case
A1  - Derevitskii, Ilia V
A1  - Savitskaya, Daria A
A1  - Babenko, Alina Yu.
A1  - Kovalchuk, Sergey V
Y1  - 2021///
KW  -  Disease graph
KW  -  Disease pathways
KW  -  Hybrid approach
KW  -  Thyrotoxic atrial fibrillation
KW  - Predictive modelling
JF  - Journal of Computational Science
VL  - 51
SP  - 101365
EP  - 101365
DO  - https://doi.org/10.1016/j.jocs.2021.101365
UR  - https://www.sciencedirect.com/science/article/pii/S1877750321000570
N2  - In this work, we propose a new approach to predictive modelling of disease complications development. This approach is based on hybrid methods that have several advantages in comparison with classic methods. The main advantage is the inclusion of the complex information about the dynamics of a patient’s conditions using pathways analysis and graph-based predictive modelling method. Hybrid approaches integrate results of classic machine learning (ML) models and dynamic analysis methods for better modelling and prediction. We present this method’s application to the practical case of predictive modelling of Thyrotoxicosis Atrial Fibrillation (TAF) development. Medical specialists need tools to estimate the level of risk of developing TAF. Using the proposed predictive modelling method, our team developed such a tool. The method was validated using common ML metrics and expert evaluation and can be used as part of a decision support system for medical staff who work with thyrotoxicosis patients. This manuscript presents an extended version of the work described in the paper [1]. In this work, we proposed several methods for calculating the probability of TAF development. Our methods include arterial fibrillation risk questionnaire for use in practical diagnostic tasks and tools for analyzing TAF dynamic. The extended study presents further development of the approach within the hybrid modelling approach.
ER  - 
TY  - JOUR
T1  - Automated identification and grading system of diabetic retinopathy using deep neural networks
A1  - Zhang, Wei
A1  - Zhong, Jie
A1  - Yang, Shijun
A1  - Gao, Zhentao
A1  - Hu, Junjie
A1  - Chen, Yuanyuan
A1  - Yi, Zhang
Y1  - 2019///
KW  -  Diabetic retinopathy
KW  -  Ensemble learning
KW  -  Fundus images
KW  -  Image classification
KW  -  Transfer learning
KW  - Deep learning
JF  - Knowledge-Based Systems
VL  - 175
SP  - 12
EP  - 25
DO  - https://doi.org/10.1016/j.knosys.2019.03.016
UR  - https://www.sciencedirect.com/science/article/pii/S0950705119301303
N2  - Diabetic retinopathy (DR) is a major cause of human vision loss worldwide. Slowing down the progress of the disease requires early screening. However, the clinical diagnosis of DR presents a considerable challenge in low-resource settings where few ophthalmologists are available to care for all patients with diabetes. In this study, an automated DR identification and grading system called DeepDR is proposed. DeepDR directly detects the presence and severity of DR from fundus images via transfer learning and ensemble learning. It comprises a set of state-of-the-art neural networks based on combinations of popular convolutional neural networks and customised standard deep neural networks. The DeepDR system is developed by constructing a high-quality dataset of DR medical images and then labelled by clinical ophthalmologists. We further explore the relationship between the number of ideal component classifiers and the number of class labels, as well as the effects of different combinations of component classifiers on the best integration performance to construct an optimal model. We evaluate the models on the basis of validity and reliability using nine metrics. Results show that the identification model performs best with a sensitivity of 97.5%, a specificity of 97.7% and an area under the curve of 97.7%. Meanwhile, the grading model achieves a sensitivity of 98.1% and a specificity of 98.9%. On the basis of the methods above, DeepDR can detect DR satisfactorily. Experiment results indicate the importance and effectiveness of the ideal number and combinations of component classifiers in relation to model performance. DeepDR provides reproducible and consistent detection results with high sensitivity and specificity instantaneously. Hence, this work provides ophthalmologists with insights into the diagnostic process.
ER  - 
TY  - JOUR
T1  - Radiomics analysis on CT images for prediction of radiation-induced kidney damage by machine learning models
A1  - Amiri, Sepideh
A1  - Akbarabadi, Mina
A1  - Abdolali, Fatemeh
A1  - Nikoofar, Alireza
A1  - Esfahani, Azam Janati
A1  - Cheraghi, Susan
Y1  - 2021///
KW  -  Computed tomography
KW  -  Machine learning
KW  -  Radiation therapy
KW  -  Radiomics
KW  - Chronic kidney disease
JF  - Computers in Biology and Medicine
VL  - 133
SP  - 104409
EP  - 104409
DO  - https://doi.org/10.1016/j.compbiomed.2021.104409
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521002031
N2  - Introduction
We aimed to assess the power of radiomic features based on computed tomography to predict risk of chronic kidney disease in patients undergoing radiation therapy of abdominal cancers.
Methods
50 patients were evaluated for chronic kidney disease 12 months after completion of abdominal radiation therapy. At the first step, the region of interest was automatically extracted using deep learning models in computed tomography images. Afterward, a combination of radiomic and clinical features was extracted from the region of interest to build a radiomic signature. Finally, six popular classifiers, including Bernoulli Naive Bayes, Decision Tree, Gradient Boosting Decision Trees, K-Nearest Neighbor, Random Forest, and Support Vector Machine, were used to predict chronic kidney disease. Evaluation criteria were as follows: accuracy, sensitivity, specificity, and area under the ROC curve.
Results
Most of the patients (58%) experienced chronic kidney disease. A total of 140 radiomic features were extracted from the segmented area. Among the six classifiers, Random Forest performed best with the accuracy and AUC of 94% and 0.99, respectively.
Conclusion
Based on the quantitative results, we showed that a combination of radiomic and clinical features could predict chronic kidney radiation toxicities. The effect of factors such as renal radiation dose, irradiated renal volume, and urine volume 24-h on CKD was proved in this study.
ER  - 
TY  - JOUR
T1  - Comparison of various approaches to combine logistic regression with genetic algorithms in survival prediction of hepatocellular carcinoma
A1  - Książek, Wojciech
A1  - Gandor, Michał
A1  - Pławiak, Paweł
Y1  - 2021///
KW  -  Genetic algorithms
KW  -  Hepatocellular carcinoma
KW  -  Machine learning
KW  - Logistic regression
JF  - Computers in Biology and Medicine
VL  - 134
SP  - 104431
EP  - 104431
DO  - https://doi.org/10.1016/j.compbiomed.2021.104431
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521002250
N2  - Hepatocellular carcinoma (HCC) is the most common liver cancer in adults. Many different factors make it difficult to diagnose in humans.. In this paper, a novel diagnostics approach based on machine learning techniques is presented. Logistic regression is one of the most classic machine learning models used to solve the problem of binary classification. In typical implementations, logistic regression coefficients are optimized using iterative methods. Additionally, parameters such as solver, C - a regularization parameter or the number of iterations of the algorithm operation should be selected. In our research, we propose a combination of logistic regression with genetic algorithms. We present three experiments showing the fusion of those methods. In the first experiment, we genetically select the logistic regression parameters, while the second experiment extends this approach by including a genetic selection of features. The third experiment presents a novel approach to train the logistic regression model - the genetic selection of coefficients (weights). Our models are tested for the survival prediction of hepatocellular carcinoma based on patient data collected at Coimbra's Hospital and Universitary Center (CHUC), Portugal. The model we proposed achieved a classification accuracy of 94.55% and an f1-score of 93.56%. Our algorithm shows that machine learning techniques optimized by the proposed concept can bring a new and accurate approach in HCC diagnosis with high accuracy.
ER  - 
TY  - JOUR
T1  - Multimodal registration of retinal images using domain-specific landmarks and vessel enhancement
A1  - Hervella, Álvaro S
A1  - Rouco, José
A1  - Novo, Jorge
A1  - Ortega, Marcos
Y1  - 2018///
KW  -  color fundus retinography
KW  -  fluorescein angiography
KW  -  multimodal
KW  -  retinal imaging
KW  -  vascular tree
KW  - image registration
JF  - Procedia Computer Science
VL  - 126
SP  - 97
EP  - 104
DO  - https://doi.org/10.1016/j.procs.2018.07.213
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918311876
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia
N2  - The analysis of different image modalities is frequently performed in ophthalmology as it provides complementary information for the diagnosis and follow-up of relevant diseases, like hypertension or diabetes. This work presents a hybrid method for the multimodal registration of color fundus retinography and fluorescein angiography. The proposed method combines a feature-based approach, using domain-specific landmarks, with an intensity-based approach that employs a domain-adapted similarity metric. The methodology is tested on a dataset of 59 image pairs containing both healthy and pathological cases. The results show a satisfactory performance of the proposed combined approach in this multimodal scenario, improving the registration accuracy achieved by the feature-based and the intensity-based approaches.
ER  - 
TY  - JOUR
T1  - An improved rank based disease prediction using web navigation patterns on bio-medical databases
A1  - Dhanalakshmi, P
A1  - Ramani, K
A1  - Eswara Reddy, B
Y1  - 2017///
KW  -  PubMed
KW  -  Rank prediction
KW  -  Server log
KW  -  User patterns
KW  -  Web usage
KW  - Biomedical documents
JF  - Future Computing and Informatics Journal
VL  - 2
IS  - 2
SP  - 133
EP  - 147
DO  - https://doi.org/10.1016/j.fcij.2017.10.003
UR  - https://www.sciencedirect.com/science/article/pii/S2314728816300198
N2  - Applying machine learning techniques to on-line biomedical databases is a challenging task, as this data is collected from large number of sources and it is multi-dimensional. Also retrieval of relevant document from large repository such as gene document takes more processing time and an increased false positive rate. Generally, the extraction of biomedical document is based on the stream of prior observations of gene parameters taken at different time periods. Traditional web usage models such as Markov, Bayesian and Clustering models are sensitive to analyze the user navigation patterns and session identification in online biomedical database. Moreover, most of the document ranking models on biomedical database are sensitive to sparsity and outliers. In this paper, a novel user recommendation system was implemented to predict the top ranked biomedical documents using the disease type, gene entities and user navigation patterns. In this recommendation system, dynamic session identification, dynamic user identification and document ranking techniques were used to extract the highly relevant disease documents on the online PubMed repository. To verify the performance of the proposed model, the true positive rate and runtime of the model was compared with that of traditional static models such as Bayesian and Fuzzy rank. Experimental results show that the performance of the proposed ranking model is better than the traditional models.
ER  - 
TY  - JOUR
T1  - Computer aided diagnosis of diabetic foot using infrared thermography: A review
A1  - Adam, Muhammad
A1  - Ng, Eddie Y K
A1  - Tan, Jen Hong
A1  - Heng, Marabelle L
A1  - Tong, Jasper W K
A1  - Acharya, U Rajendra
Y1  - 2017///
KW  -  Atherosclerosis
KW  -  Diabetes
KW  -  Infrared image
KW  -  Neuropathy
KW  -  Plantar
KW  - Foot
JF  - Computers in Biology and Medicine
VL  - 91
SP  - 326
EP  - 336
DO  - https://doi.org/10.1016/j.compbiomed.2017.10.030
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517303566
N2  - Diabetes mellitus (DM) is a chronic metabolic disorder that requires regular medical care to prevent severe complications. The elevated blood glucose level affects the eyes, blood vessels, nerves, heart, and kidneys after the onset. The affected blood vessels (usually due to atherosclerosis) may lead to insufficient blood circulation particularly in the lower extremities and nerve damage (neuropathy), which can result in serious foot complications. Hence, an early detection and treatment can prevent foot complications such as ulcerations and amputations. Clinicians often assess the diabetic foot for sensory deficits with clinical tools, and the resulting foot severity is often manually evaluated. The infrared thermography is a fast, nonintrusive and non-contact method which allows the visualization of foot plantar temperature distribution. Several studies have proposed infrared thermography-based computer aided diagnosis (CAD) methods for diabetic foot. Among them, the asymmetric temperature analysis method is more superior, as it is easy to implement, and yielded satisfactory results in most of the studies. In this paper, the diabetic foot, its pathophysiology, conventional assessments methods, infrared thermography and the different infrared thermography-based CAD analysis methods are reviewed.
ER  - 
TY  - JOUR
T1  - Mean Glucose Slope – Principal Component Analysis Classification to Detect Insulin Infusion Set Failure
A1  - Rojas, Rubén
A1  - Garcia-Gabin, Winston
A1  - Bequette, B Wayne
Y1  - 2011///
KW  -  biomedical systems
KW  -  fault detection
KW  -  multivariate analysis
KW  -  type 1 diabetes
KW  - Artificial pancreas
JF  - IFAC Proceedings Volumes
VL  - 44
IS  - 1
SP  - 14127
EP  - 14132
DO  - https://doi.org/10.3182/20110828-6-IT-1002.03147
UR  - https://www.sciencedirect.com/science/article/pii/S1474667016458969
N1  - 18th IFAC World Congress
N2  - Abstract
The bivariate classification technique using the mean glucose slope (MGS) and the first component of the principal component analysis (PCA), is applied to insulin infusion set failure detection (IISF), a challenging problem faced by individuals with type 1 diabetes that are on continuous insulin infusion pump therapy. The objective of this study was to determine if the proposed approach could be used to distinguish between normal patient data and data from patients under IISF online, in a reasonably short period of time. The proposed approach was applied to simulated glucose concentrations for 10 patients, based on a nonlinear physiological model of insulin and glucose dynamics. Although it presents few false alarms, it was capable of detecting most drifting (ramp) infusion set failures before complete failure occurred.
ER  - 
TY  - JOUR
T1  - IntelliHealth: A medical decision support application using a novel weighted multi-layer classifier ensemble framework
A1  - Bashir, Saba
A1  - Qamar, Usman
A1  - Khan, Farhan Hassan
Y1  - 2016///
KW  -  Bagging
KW  -  Classification
KW  -  Disease prediction
KW  -  Ensemble technique
KW  -  Multi-layer
KW  - Machine learning
JF  - Journal of Biomedical Informatics
VL  - 59
SP  - 185
EP  - 200
DO  - https://doi.org/10.1016/j.jbi.2015.12.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415002816
N2  - Accuracy plays a vital role in the medical field as it concerns with the life of an individual. Extensive research has been conducted on disease classification and prediction using machine learning techniques. However, there is no agreement on which classifier produces the best results. A specific classifier may be better than others for a specific dataset, but another classifier could perform better for some other dataset. Ensemble of classifiers has been proved to be an effective way to improve classification accuracy. In this research we present an ensemble framework with multi-layer classification using enhanced bagging and optimized weighting. The proposed model called “HM-BagMoov” overcomes the limitations of conventional performance bottlenecks by utilizing an ensemble of seven heterogeneous classifiers. The framework is evaluated on five different heart disease datasets, four breast cancer datasets, two diabetes datasets, two liver disease datasets and one hepatitis dataset obtained from public repositories. The analysis of the results show that ensemble framework achieved the highest accuracy, sensitivity and F-Measure when compared with individual classifiers for all the diseases. In addition to this, the ensemble framework also achieved the highest accuracy when compared with the state of the art techniques. An application named “IntelliHealth” is also developed based on proposed model that may be used by hospitals/doctors for diagnostic advice.
ER  - 
TY  - JOUR
T1  - Photothermal spectral-domain optical coherence reflectometry for direct measurement of hemoglobin concentration of erythrocytes
A1  - Yim, Jinyeong
A1  - Kim, Hun
A1  - Ryu, Suho
A1  - Song, Sungwook
A1  - Kim, Hyun Ok
A1  - Hyun, Kyung-A
A1  - Jung, Hyo-Il
A1  - Joo, Chulmin
Y1  - 2014///
KW  -  Erythrocytes
KW  -  Hemoglobin concentration
KW  -  Low coherence interferometry
KW  -  Refractive index
KW  - Photothermal effect
JF  - Biosensors and Bioelectronics
VL  - 57
SP  - 59
EP  - 64
DO  - https://doi.org/10.1016/j.bios.2014.01.052
UR  - https://www.sciencedirect.com/science/article/pii/S0956566314000700
N2  - A novel optical detection method for hemoglobin concentration is described. The hemoglobin molecules consisting mainly of iron generate heat upon their absorption of light energy at 532nm, which subsequently changes the refractive index of the blood. We exploit this photothermal effect to determine the hemoglobin concentration of erythrocytes without any preprocessing of blood. Highly sensitive measurement of refractive index alteration of blood samples is enabled by a spectral-domain low coherence reflectometric sensor with subnanometer-level optical path-length sensitivity. The performance and validity of the sensor are presented by comparing the measured results against the reference data acquired from an automatic hematology analyzer.
ER  - 
TY  - JOUR
T1  - Wavelet-based computationally-efficient computer-aided characterization of liver steatosis using conventional B-mode ultrasound images
A1  - Amin, Manar N
A1  - Rushdi, Muhammad A
A1  - Marzaban, Raghda N
A1  - Yosry, Ayman
A1  - Kim, Kang
A1  - Mahmoud, Ahmed M
Y1  - 2019///
KW  -  Computer-aided diagnosis (CAD)
KW  -  Steatosis
KW  -  Ultrasound images
KW  -  Wavelet packet transform
KW  - Fatty liver disease
JF  - Biomedical Signal Processing and Control
VL  - 52
SP  - 84
EP  - 96
DO  - https://doi.org/10.1016/j.bspc.2019.03.010
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419300850
N2  - Hepatic steatosis occurs when lipids accumulate in the liver leading to steatohepatitis, which can evolve into cirrhosis and consequently may end with hepatocellular carcinoma. Several automatic classification algorithms have been proposed to detect liver diseases. However, some algorithms are manufacturer-dependent, while others require extensive calculations and consequently prolonged computational time. This may limit the development of real-time and manufacturer-independent computer-aided detection of liver steatosis. This work demonstrates the feasibility of a computationally-efficient and manufacturer-independent wavelet-based computer-aided liver steatosis detection system using conventional B-mode ultrasound (US) imaging. Seven features were extracted from the approximation part of the second-level wavelet packet transform (WPT) of US images. The proposed technique was tested on two datasets of ex-vivo mice livers with and without gelatin embedding, in addition to a third dataset of in-vivo human livers acquired using two different US machines. Using the gelatin-embedded mice liver dataset, the technique exhibited 98.8% accuracy, 97.8% sensitivity, and 100% specificity, and the frame classification time was reduced from0.4814s using original US images to 0.1444s after WPT preprocessing. When the other mice liver dataset was used, the technique showed 85.74% accuracy, 84.4% sensitivity, and 88.5% specificity, and the frame classification time was reduced from 0.5612s to 0.2903s. Using human liver image data, the best classifier exhibited 92.5% accuracy, 93.0% sensitivity, 91.0% specificity, and the classification time was reduced from 0.660s to 0.146s. This technique can be useful for developing computationally-efficient and manufacturer-independent noninvasive CAD systems for fatty liver detection.
ER  - 
TY  - JOUR
T1  - Phonocardiogram signal analysis for classification of Coronary Artery Diseases using MFCC and 1D adaptive local ternary patterns
A1  - Iqtidar, Khushbakht
A1  - Qamar, Usman
A1  - Aziz, Sumair
A1  - Khan, Muhammad Umar
Y1  - 2021///
KW  -  1D-adaptive local ternary patterns
KW  -  Classification
KW  -  Computer-aided diagnosis
KW  -  Coronary artery disease
KW  -  Feature extraction
KW  - Phonocardiogram (PCG)
JF  - Computers in Biology and Medicine
VL  - 138
SP  - 104926
EP  - 104926
DO  - https://doi.org/10.1016/j.compbiomed.2021.104926
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521007204
N2  - Coronary Artery Diseases (CADs) are a dominant cause of worldwide fatalities. The development of accurate and timely diagnosis routines is imperative to reduce these risks and mortalities. Coronary angiography, an invasive and expensive technique, is currently used as a diagnostic tool for the detection of CAD but it has some procedural hazards, i.e., it requires arterial puncture, and the subject gets exposed to iodinated radiation. Phonocardiography (PCG), a non-invasive and inexpensive technique, is a modality employing heart sounds to diagnose heart diseases but it requires only trained medical personnel to apprehend cardiac murmurs in clinical environments. Furthermore, there is a strong compulsion to characterize CAD into its types, such as Single vessel coronary artery disease (SVCAD), Double vessel coronary artery disease (DVCAD), and Triple vessel coronary artery disease (TVCAD) to assist the cardiologist in decision making about the treatment procedure followed. This paper presents a computer-aided diagnosis system for the categorization of CAD and its types based on Phonocardiogram (PCG) signal analysis. The raw PCG signals were denoised via empirical mode decomposition (EMD) to remove redundant information and noise. Next, we extract MFCC and proposed 1D-Adaptive Local Ternary Patterns (1D-ALTP) and fused them serially to get a strong feature representation of multiple PCG signal classes. Features were further reduced through Multidimensional Scaling (MDS) and subjected to several classification methods such as support vector machines (SVM), Decision Tree (DT), and K-nearest neighbors (KNN) in a comparative fashion. The best classification performances of 98.3% and 97.2% mean accuracies were obtained through SVM with the cubic kernel for binary and multiclass experiments, respectively. The performance of the proposed system is comprehensively tested through 10-fold cross-validation and hold-out train-test techniques to avoid model overfitting. Comparative analysis with existing approaches advocates the superiority of the proposed approach.
ER  - 
TY  - JOUR
T1  - Exploring big data analytics in health care
A1  - Ramesh, T
A1  - Santhi, V
Y1  - 2020///
KW  -  Big data
KW  -  Cost optimization
KW  -  Knowledge management
KW  -  Patient management
KW  - Data mining
JF  - International Journal of Intelligent Networks
VL  - 1
SP  - 135
EP  - 140
DO  - https://doi.org/10.1016/j.ijin.2020.11.003
UR  - https://www.sciencedirect.com/science/article/pii/S2666603020300154
N2  - Health care Industries are facing lot of challenges in maintaining patient information across various databases due to storage issues. In order to extract patient information, preprocessing techniques can be applied in the process of data mining across databases. But as the data is growing enormously with rapid speed, data mining techniques are becoming obsolete due to issues such as Storage, Speed. So, cost optimization has become one of the major requirements in health industry as there is huge burden in maintaining large volumes of patient’s information using traditional databases. Here Big Data plays a vital role in storing huge volumes of patient information using storage mechanisms such as HDFS, HBase. Many issues in health care are discussed in this paper such as prediction of diseases, getting patients information across databases as a single view.
ER  - 
TY  - JOUR
T1  - Refractive Index Estimation from Spectral Measurements of a Plasmonic Glucose Sensor and Wavelength Selection **The project was funded by Baden-Württemberg Stiftung gGmbH. The authors would also like to thank MWK BW, ERC COMPLEX-PLAS and AvH Stiftung.
A1  - Teutsch, Tanja
A1  - Mesch, Martin
A1  - Giessen, Harald
A1  - Tarín, Cristina
Y1  - 2017///
KW  -  biomedical systems
KW  -  input estimation
KW  -  medical applications
KW  -  optical response
KW  -  optical spectroscopy
KW  -  reduction
KW  -  sensors
KW  -  signal analysis
KW  - Estimation algorithms
JF  - IFAC-PapersOnLine
VL  - 50
IS  - 1
SP  - 4406
EP  - 4411
DO  - https://doi.org/10.1016/j.ifacol.2017.08.913
UR  - https://www.sciencedirect.com/science/article/pii/S2405896317313794
N1  - 20th IFAC World Congress
N2  - Noninvasive glucose monitoring is a desired objective in diabetes therapy and monitoring the glucose levels in the tear fluid is one possibility to approach this objective. The plasmonic glucose sensor presented within this work consists of metal nanostructures which cause a resonance in the optical response and the resonance wavelength position depends on the refractive index of the surrounding. Glucose selectivity is guaranteed by functionalizing the sensor with a hydrogel that swells with glucose and thus changes its refractive index. The focus of this work lies on the relationship between the refractive index and the resulting transmission spectra of the sensor. Based on this understanding, an estimation algorithm is presented, which provides a refractive index value for any measured spectrum. It is shown that this new estimation algorithm improves previous results. For a mobile application of the sensor, the transmission can only be measured at few discrete wavelengths rather than the whole spectrum. The choice of these discrete wavelengths has a huge influence on the estimation results. The developed estimation algorithm is used to build an objective function to determine the best wavelengths via sequential forward feature selection. The results of the feature selection show that even reducing the discrete wavelengths to a number of four, refractive index estimation can be performed with a relative error below 0.05%.
ER  - 
TY  - JOUR
T1  - DRNet: Segmentation and localization of optic disc and Fovea from diabetic retinopathy image
A1  - Hasan, Md. Kamrul
A1  - Alam, Md. Ashraful
A1  - Elahi, Md. Toufick E
A1  - Roy, Shidhartho
A1  - Martí, Robert
Y1  - 2021///
KW  -  Encoder-decoder network
KW  -  Ophthalmology
KW  -  Segmentation and localization
KW  -  Skip connection
KW  - Diabetic retinopathy and glaucoma
JF  - Artificial Intelligence in Medicine
VL  - 111
SP  - 102001
EP  - 102001
DO  - https://doi.org/10.1016/j.artmed.2020.102001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365720312665
N2  - Background and objective
In modern ophthalmology, automated Computer-aided Screening Tools (CSTs) are crucial non-intrusive diagnosis methods, where an accurate segmentation of Optic Disc (OD) and localization of OD and Fovea centers are substantial integral parts. However, designing such an automated tool remains challenging due to small dataset sizes, inconsistency in spatial, texture, and shape information of the OD and Fovea, and the presence of different artifacts.
Methods
This article proposes an end-to-end encoder-decoder network, named DRNet, for the segmentation and localization of OD and Fovea centers. In our DRNet, we propose a skip connection, named residual skip connection, for compensating the spatial information lost due to pooling in the encoder. Unlike the earlier skip connection in the UNet, the proposed skip connection does not directly concatenate low-level feature maps from the encoder's beginning layers with the corresponding same scale decoder. We validate DRNet using different publicly available datasets, such as IDRiD, RIMONE, DRISHTI-GS, and DRIVE for OD segmentation; IDRiD and HRF for OD center localization; and IDRiD for Fovea center localization.
Results
The proposed DRNet, for OD segmentation, achieves mean Intersection over Union (mIoU) of 0.845, 0.901, 0.933, and 0.920 for IDRiD, RIMONE, DRISHTI-GS, and DRIVE, respectively. Our OD segmentation result, in terms of mIoU, outperforms the state-of-the-art results for IDRiD and DRIVE datasets, whereas it outperforms state-of-the-art results concerning mean sensitivity for RIMONE and DRISHTI-GS datasets. The DRNet localizes the OD center with mean Euclidean Distance (mED) of 20.23 and 13.34 pixels, respectively, for IDRiD and HRF datasets; it outperforms the state-of-the-art by 4.62 pixels for IDRiD dataset. The DRNet also successfully localizes the Fovea center with mED of 41.87 pixels for the IDRiD dataset, outperforming the state-of-the-art by 1.59 pixels for the same dataset.
Conclusion
As the proposed DRNet exhibits excellent performance even with limited training data and without intermediate intervention, it can be employed to design a better-CST system to screen retinal images. Our source codes, trained models, and ground-truth heatmaps for OD and Fovea center localization will be made publicly available upon publication at GitHub.11https://github.com/kamruleee51/DRNet_Segmentation_Localization_OD_Fovea.
ER  - 
TY  - JOUR
T1  - A low-cost screening method for the detection of the carotid artery diseases
A1  - Seddik, Ahmed F
A1  - Shawky, Doaa M
Y1  - 2013///
KW  -  Artificial neural networks
KW  -  Carotid artery diseases
KW  -  Doppler signal classification
KW  -  K-nearest neighbor
KW  - Automatic diagnosis
JF  - Knowledge-Based Systems
VL  - 52
SP  - 236
EP  - 245
DO  - https://doi.org/10.1016/j.knosys.2013.08.007
UR  - https://www.sciencedirect.com/science/article/pii/S0950705113002372
N2  - Carotid artery diseases are defined as the narrowing or the blockage of the carotid arteries. These two conditions are called carotid artery stenosis or occlusion respectively. Stenosis and occlusion are usually caused by cholesterol deposits and fatty substances which are called plaque. In addition, they represent significant causes of strokes. Thus, they should be a part of regular physical examinations. An important and preliminary diagnosis is to listen to the arteries in the neck using a stethoscope or a Doppler ultrasound (US) device. However, it is sometimes very difficult for a non-professional physician to differentiate between a normal and an abnormal sound due to blood flow blockage. This paper presents a low-cost efficient method that can be used in the automatic screening of carotid artery diseases, especially in areas with high population. Doppler US signals are preprocessed for noise elimination. Then, some features for normal, stenosis and occlusion signals are extracted from the frequency domain of these signals using their spectrograms. A multi-layer feed forward neural-network (MLFFNN) and a k-nearest neighbor (KNN) classifiers were used to automatically diagnose the input signals. The approach is applied to 72 samples divided into three equal sets which represent the three main classes to be identified, i.e., normal, stenosis and occlusion patterns. We used in the training phase 75% of each set and the rest was used in the test phase. Experimental results show the simplicity and efficiency of the presented approach for automatic diagnosis of carotid artery diseases. The maximum obtained classification accuracies are 91.67%, 100%, and 95.89% for the normal, stenosis and occlusion patterns respectively when the MLFFNN classifier is used. In comparison with similar approaches, the proposed approach is less complex, hence runs faster which suggests its suitability as an efficient screening method for the detection of carotid artery diseases.
ER  - 
TY  - JOUR
T1  - Multivariate analysis of the population representativeness of related clinical studies
A1  - He, Zhe
A1  - Ryan, Patrick
A1  - Hoxha, Julia
A1  - Wang, Shuang
A1  - Carini, Simona
A1  - Sim, Ida
A1  - Weng, Chunhua
Y1  - 2016///
KW  -  Knowledge representation
KW  -  Selection bias
KW  - Clinical trial
JF  - Journal of Biomedical Informatics
VL  - 60
SP  - 66
EP  - 76
DO  - https://doi.org/10.1016/j.jbi.2016.01.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416000083
N2  - Objective
To develop a multivariate method for quantifying the population representativeness across related clinical studies and a computational method for identifying and characterizing underrepresented subgroups in clinical studies.
Methods
We extended a published metric named Generalizability Index for Study Traits (GIST) to include multiple study traits for quantifying the population representativeness of a set of related studies by assuming the independence and equal importance among all study traits. On this basis, we compared the effectiveness of GIST and multivariate GIST (mGIST) qualitatively. We further developed an algorithm called “Multivariate Underrepresented Subgroup Identification” (MAGIC) for constructing optimal combinations of distinct value intervals of multiple traits to define underrepresented subgroups in a set of related studies. Using Type 2 diabetes mellitus (T2DM) as an example, we identified and extracted frequently used quantitative eligibility criteria variables in a set of clinical studies. We profiled the T2DM target population using the National Health and Nutrition Examination Survey (NHANES) data.
Results
According to the mGIST scores for four example variables, i.e., age, HbA1c, BMI, and gender, the included observational T2DM studies had superior population representativeness than the interventional T2DM studies. For the interventional T2DM studies, Phase I trials had better population representativeness than Phase III trials. People at least 65years old with HbA1c value between 5.7% and 7.2% were particularly underrepresented in the included T2DM trials. These results confirmed well-known knowledge and demonstrated the effectiveness of our methods in population representativeness assessment.
Conclusions
mGIST is effective at quantifying population representativeness of related clinical studies using multiple numeric study traits. MAGIC identifies underrepresented subgroups in clinical studies. Both data-driven methods can be used to improve the transparency of design bias in participation selection at the research community level.
ER  - 
TY  - JOUR
T1  - Retinal image quality assessment using deep learning
A1  - Zago, Gabriel Tozatto
A1  - Andreão, Rodrigo Varejão
A1  - Dorizzi, Bernadette
A1  - Teatini Salles, Evandro Ottoni
Y1  - 2018///
KW  -  Convolutional neural networks
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Image quality
KW  - Retinal images
JF  - Computers in Biology and Medicine
VL  - 103
SP  - 64
EP  - 70
DO  - https://doi.org/10.1016/j.compbiomed.2018.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S001048251830297X
N2  - Poor-quality retinal images do not allow an accurate medical diagnosis, and it is inconvenient for a patient to return to a medical center to repeat the fundus photography exam. In this paper, a robust automatic system is proposed to assess the quality of retinal images at the moment of the acquisition, aiming at assisting health care professionals during a fundus photography exam. We propose a convolutional neural network (CNN) pretrained on non-medical images for extracting general image features. The weights of the CNN are further adjusted via a fine-tuning procedure, resulting in a performant classifier obtained only with a small quantity of labeled images. The CNN performance was evaluated on two publicly available databases (i.e., DRIMDB and ELSA-Brasil) using two different procedures: intra-database and inter-database cross-validation. The CNN achieved an area under the curve (AUC) of 99.98% on DRIMDB and an AUC of 98.56% on ELSA-Brasil in the inter-database experiment, where training and testing were not performed on the same database. These results show the robustness of the proposed model to various image acquisitions without requiring special adaptation, thus making it a good candidate for use in operational clinical scenarios.
ER  - 
TY  - JOUR
T1  - R-Ensembler: A greedy rough set based ensemble attribute selection algorithm with kNN imputation for classification of medical data
A1  - Bania, Rubul Kumar
A1  - Halder, Anindya
Y1  - 2020///
KW  -  Classification
KW  -  Dependency
KW  -  Ensemble
KW  -  NN Imputation
KW  - Rough set
JF  - Computer Methods and Programs in Biomedicine
VL  - 184
SP  - 105122
EP  - 105122
DO  - https://doi.org/10.1016/j.cmpb.2019.105122
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719306972
N2  - Background and Objective
Retrieving meaningful information from high dimensional dataset is an important and challenging task. Normally, medical dataset suffers from several issues such as curse of dimensionality problem, uncertainty, presence of missing values, non-relevant and redundant attributes, etc. Any machine learning technique applied on such data (without any preprocessing) by and large takes a considerable amount of computational time and may degrade the performance of the model.
Methods
In this article, R-Ensembler, a parameter free greedy ensemble attribute selection method is proposed adopting the concept of rough set theory by using the attribute-class, attribute-significance and attribute-attribute relevance measures to select a subset of attributes which are most relevant, significant and non-redundant from a pool of different attribute subsets in order to predict the presence or absence of different diseases in medical dataset. The main role of the proposed ensembler is to combine multiple subsets of attributes produced by different rough set filters and to produce an optimal subset of attributes for subsequent classification task. A novel n number of set intersection method is also proposed to reduce the biasness during the time of attribute selection process. Before selecting the minimal attribute set from a given data by the proposed R-Ensembler method, the dataset is preprocessed by the k nearest neighbour (kNN) imputation method for missing value treatment.
Results
Experiments are carried out on seven benchmark medical datasets collected from University of California at Irvine (UCI) repository. The performance of the proposed ensemble method is compared with five state-of-the-art attribute selection algorithms, results of which are measured using three benchmark classifiers viz., Naïve Bayes, decision trees and random forest. Experimental results clearly justify the superiority of the proposed R-Ensembler method over other attribute selection algorithms. Results of paired t-test performed on average accuracies produced by different classifiers simulated on the reduced data sets achieved by the proposed and counter part attribute selection methods confirm the statistical significance of the better reduced attribute subsets achieved by the proposed R-Ensembler method compared to others.
Conclusion
The proposed ensemble method turned out to be very effective for selecting high relevant, high significant and less redundant attributes from a pool of different subsets of attributes.
ER  - 
TY  - JOUR
T1  - Auto-Regressive Time Delayed jump neural network for blood glucose levels forecasting
A1  - D’Antoni, Federico
A1  - Merone, Mario
A1  - Piemonte, Vincenzo
A1  - Iannello, Giulio
A1  - Soda, Paolo
Y1  - 2020///
KW  -  Bio-medical patterns
KW  -  Diabetes
KW  -  Neural networks
KW  -  Precision medicine
KW  - Time series forecasting
JF  - Knowledge-Based Systems
VL  - 203
SP  - 106134
EP  - 106134
DO  - https://doi.org/10.1016/j.knosys.2020.106134
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120303890
N2  - Diabetes mellitus is a widespread chronic disease and is one of the main causes of death worldwide. In order to improve the quality of life of people with diabetes and reduce the occurrence of complications, it is fundamental to prevent glycemic levels from exceeding the physiologic range. With this purpose, many works in recent years have been developed to forecast future glycemic trends using machine learning algorithms that exploit the reading of continuous glucose monitoring sensors, which gather glycemic data from diabetic patients 24 h a day. However, their application is limited in practice by the fact that they usually require a large amount of training data and other heterogeneous features gathered from patients. For this reason, in this work we present a novel neural network capable of predicting future glycemic levels using only the past glucose values as input while needing a small amount of training data. The model is a jump neural network with the addition of feedback connections from the output to the hidden layer, and time delays for each of the input-to-hidden, output-to-hidden and input-to-output connections. Experiments were conducted on a private and a public dataset. We evaluated performance in terms of RMSE and of adverse event detection. The proposed model outperforms other methods suited for time series forecasting, as well as models for blood glucose level prediction present in the literature.
ER  - 
TY  - JOUR
T1  - A new deep learning approach for the retinal hard exudates detection based on superpixel multi-feature extraction and patch-based CNN
A1  - Huang, Chenxi
A1  - Zong, Yongshuo
A1  - Ding, Yimin
A1  - Luo, Xin
A1  - Clawson, Kathy
A1  - Peng, Yonghong
Y1  - 2021///
KW  -  Automatic diagnosis
KW  -  Deep learning
KW  -  Feature extraction
KW  -  Superpixel
KW  - Retinal hard exudates
JF  - Neurocomputing
VL  - 452
SP  - 521
EP  - 533
DO  - https://doi.org/10.1016/j.neucom.2020.07.145
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220319196
N2  - Diabetic Retinopathy (DR) is a severe complication of chronic diabetes causing significant visual deterioration and may lead to blindness with delay of being treated. Exudative diabetic maculopathy, a form of macular edema where hard exudates (HE) develop, is a frequent cause of visual deterioration in DR. The detection of HE comprises a significant role in the DR diagnosis. In this paper, an automatic exudates detection method based on superpixel multi-feature extraction and patch-based deep convolutional neural network is proposed. Firstly, superpixels, regarded as candidates, are generated on each resized image using the superpixel segmentation algorithm called Simple Linear Iterative Clustering (SLIC). Then, 25 features extracted from resized images and patches are generated on each feature. Patches are subsequently used to train a deep convolutional neural network, which distinguishes the hard exudates from the background. Experiments conducted on three publicly available datasets (DiaretDB1, e-ophtha EX and IDRiD) demonstrate that our proposed methodology achieved superior HE detection when compared with current state-of-art algorithms.
ER  - 
TY  - JOUR
T1  - Application of knowledge discovery process on the prediction of stroke
A1  - Colak, Cemil
A1  - Karaman, Esra
A1  - Turtay, M Gokhan
Y1  - 2015///
KW  -  Knowledge discovery process (KDP)
KW  -  Stroke
KW  -  Support vector machine (SVM)
KW  - Artificial neural networks (ANN)
JF  - Computer Methods and Programs in Biomedicine
VL  - 119
IS  - 3
SP  - 181
EP  - 185
DO  - https://doi.org/10.1016/j.cmpb.2015.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715000565
N2  - Objective
Stroke is a prominent life-threatening disease in the world. The current study was performed to predict the outcome of stroke using knowledge discovery process (KDP) methods, artificial neural networks (ANN) and support vector machine (SVM) models.
Materials and methods
The records of 297 (130 sick and 167 healthy) individuals were acquired from the databases of the department of emergency medicine. Nine predictors (coronary artery disease, diabetes mellitus, hypertension, history of cerebrovascular disease, atrial fibrillation, smoking, the findings of carotid Doppler ultrasonography [normal, plaque, plaque+stenosis≥50%], the levels of cholesterol and C-reactive protein) were used for predicting the stroke. Feature selection based on the Cramer's V test was carried out for reducing the predictors. Multilayer perceptron (MLP) ANN and SVM with radial basis function (RBF) kernel were used for the prediction based on the selected predictors.
Results
The accuracy values were 81.82% for ANN and 80.38% for SVM in the training dataset (n=209), and 85.9% for ANN and 84.62% for SVM in the testing dataset (n=78), respectively. ANN and SVM models yielded area under curve (AUC) values of 0.905 and 0.899 in the training dataset, and 0.928 and 0.91 in the testing dataset, consecutively.
Conclusion
The findings of the current study pointed out that ANN had more predictive performance when compared with SVM in predicting stroke. The proposed ANN model would be useful when making clinical decisions regarding stroke.
ER  - 
TY  - JOUR
T1  - Automatic image classification for the urinoculture screening
A1  - Andreini, Paolo
A1  - Bonechi, Simone
A1  - Bianchini, Monica
A1  - Garzelli, Andrea
A1  - Mecocci, Alessandro
Y1  - 2016///
KW  -  Artificial neural networks
KW  -  Clustering techniques
KW  -  Support vector machines
KW  -  Urinoculture screening
KW  - Color image processing
JF  - Computers in Biology and Medicine
VL  - 70
SP  - 12
EP  - 22
DO  - https://doi.org/10.1016/j.compbiomed.2015.12.025
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516000020
N2  - Urinary tract infections (UTIs) are considered to be the most common bacterial infection and, actually, it is estimated that about 150 million UTIs occur world wide yearly, giving rise to roughly $6 billion in healthcare expenditures and resulting in 100,000 hospitalizations. Nevertheless, it is difficult to carefully assess the incidence of UTIs, since an accurate diagnosis depends both on the presence of symptoms and on a positive urinoculture, whereas in most outpatient settings this diagnosis is made without an ad hoc analysis protocol. On the other hand, in the traditional urinoculture test, a sample of midstream urine is put onto a Petri dish, where a growth medium favors the proliferation of germ colonies. Then, the infection severity is evaluated by a visual inspection of a human expert, an error prone and lengthy process. In this paper, we propose a fully automated system for the urinoculture screening that can provide quick and easily traceable results for UTIs. Based on advanced image processing and machine learning tools, the infection type recognition, together with the estimation of the bacterial load, can be automatically carried out, yielding accurate diagnoses. The proposed AID (Automatic Infection Detector) system provides support during the whole analysis process: first, digital color images of Petri dishes are automatically captured, then specific preprocessing and spatial clustering algorithms are applied to isolate the colonies from the culture ground and, finally, an accurate classification of the infections and their severity evaluation are performed. The AID system speeds up the analysis, contributes to the standardization of the process, allows result repeatability, and reduces the costs. Moreover, the continuous transition between sterile and external environments (typical of the standard analysis procedure) is completely avoided.
ER  - 
TY  - JOUR
T1  - Improving the segmentation of magnetic resonance brain images using the LSHADE optimization algorithm
A1  - Aranguren, Itzel
A1  - Valdivia, Arturo
A1  - Morales-Castañeda, Bernardo
A1  - Oliva, Diego
A1  - Abd Elaziz, Mohamed
A1  - Perez-Cisneros, Marco
Y1  - 2021///
KW  -  Metaheuristic algorithms
KW  -  Minimum cross entropy
KW  -  Multilevel thresholding
KW  - Magnetic resonance images
JF  - Biomedical Signal Processing and Control
VL  - 64
SP  - 102259
EP  - 102259
DO  - https://doi.org/10.1016/j.bspc.2020.102259
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420303864
N2  - Segmentation is an essential preprocessing step in techniques for image analysis. The automatic segmentation of brain magnetic resonance imaging has been exhaustively investigated since the accurate use of this kind of methods permits the diagnosis and identification of several diseases. Thresholding is a straightforward and efficient technique for image segmentation. Nonetheless, thresholding based approaches tend to increase the computational cost based on the number of thresholds used for the segmentation. Therefore, metaheuristic algorithms are an important tool that helps to find the optimal values in multilevel thresholding. The adaptive differential evolution, based in numerous successes through history, with linear population size reduction (LSHADE) is a robust metaheuristic algorithm that efficiently solves numerical optimization problems. The main advantage of LSHADE is its capability to adapt its internal parameters according to prior knowledge acquired along the evolutionary process. Meanwhile, the continuous reduction of the population improves the exploitation process. This article presents a multilevel thresholding approach based on the LSHADE method for the segmentation of magnetic resonance brain imaging. The proposed method has been tested using three groups of reference images— the first group consists of grayscale standard benchmark images, the second group consists of magnetic resonance T2-weighted brain images, and the third group is formed by images of unhealthy brains affected by tumors. In turn, the performance of the intended approach was compared with distinct metaheuristic algorithms and machine learning methods. The statistically verified results demonstrate that the suggested approach improves consistency and segmentation quality.
ER  - 
TY  - JOUR
T1  - Sparse Manifold Clustering and Embedding to discriminate gene expression profiles of glioblastoma and meningioma tumors
A1  - García-Gómez, Juan M
A1  - Gómez-Sanchis, Juan
A1  - Escandell-Montero, Pablo
A1  - Fuster-Garcia, Elies
A1  - Soria-Olivas, Emilio
Y1  - 2013///
KW  -  Automatic classification
KW  -  Bioinformatics
KW  -  Medical applications
KW  -  Microarray data analysis
KW  - Manifolds
JF  - Computers in Biology and Medicine
VL  - 43
IS  - 11
SP  - 1863
EP  - 1869
DO  - https://doi.org/10.1016/j.compbiomed.2013.08.025
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513002400
N2  - Sparse Manifold Clustering and Embedding (SMCE) algorithm has been recently proposed for simultaneous clustering and dimensionality reduction of data on nonlinear manifolds using sparse representation techniques. In this work, SMCE algorithm is applied to the differential discrimination of Glioblastoma and Meningioma Tumors by means of their Gene Expression Profiles. Our purpose was to evaluate the robustness of this nonlinear manifold to classify gene expression profiles, characterized by the high-dimensionality of their representations and the low discrimination power of most of the genes. For this objective, we used SMCE to reduce the dimensionality of a preprocessed dataset of 35 single-labeling cDNA microarrays with 11500 original clones. Afterwards, supervised and unsupervised methodologies were applied to obtain the classification model: the former was based on linear discriminant analysis, the later on clustering using the SMCE embedding data. The results obtained using both approaches showed that all (100%) the samples could be correctly classified and the results of all repetitions but one formed a compatible cluster of predictive labels. Finally, the embedding dimensionality of the dataset extracted by SMCE revealed large discrimination margins between both classes.
ER  - 
TY  - JOUR
T1  - Machine learning for holistic visualization of STEMI registry data
A1  - Nayak, Keshav R
A1  - Skupin, André
A1  - Schempp, Timothy
A1  - Garberich, Ross
A1  - Bhavnani, Sanjeev P
A1  - Henry, Timothy
Y1  - 2021///
KW  -  Artificial neural network
KW  -  Data visualization
KW  -  STEMI outcomes
KW  -  Self-organizing maps
KW  - Machine learning
JF  - Journal of Biomedical Informatics
VL  - 121
SP  - 103869
EP  - 103869
DO  - https://doi.org/10.1016/j.jbi.2021.103869
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001982
N2  - Background
Widespread adoption of evidence-based guidelines and treatment pathways in ST-Elevation Myocardial Infarction (STEMI) patients has considerably improved cardiac survival and decreased the risk of recurrent myocardial infarction. However, survival outcomes appear to have plateaued over the last decade. The hope underpinning the current study is to engage data visualization to develop a more holistic understanding of the patient space, supported by principles and techniques borrowed from traditionally disparate disciplines, like cartography and machine learning.
Methods and Results
The Minnesota Heart Institute Foundation (MHIF) STEMI database is a large prospective regional STEMI registry consisting of 180 variables of heterogeneous data types on more than 5000 patients spanning 15 years. Initial assessment and preprocessing of the registry database was undertaken, followed by a first proof-of-concept implementation of an analytical workflow that involved machine learning, dimensionality reduction, and data visualization. 38 pre-admission variables were analyzed in an all-encompassing representation of pre-index STEMI event data. We aim to generate a holistic visual representation — a map of the multivariate patient space — by training a high-resolution self-organizing neural network consisting of several thousand neurons. The resulting 2-D lattice arrangement of n-dimensional neuron vectors allowed patients to be represented as point locations in a 2-D display space. Patient attributes were then visually examined and contextualized in the same display space, from demographics to pre-existing conditions, event-specific procedures, and STEMI outcomes. Data visualizations implemented in this study include a small-multiple display of neural component planes, composite visualization of the multivariate patient space, and overlay visualization of non-training attributes.
Conclusion
Our study represents the first known marriage of cartography and machine learning techniques to obtain visualizations of the multivariate space of a regional STEMI registry. Combining cartographic mapping techniques and artificial neural networks permitted the transformation of the STEMI database into novel, two-dimensional visualizations of patient characteristics and outcomes. Notably, these visualizations also drive the discovery of anomalies in the data set, informing corrections applied to detected outliers, thereby further refining the registry for integrity and accuracy. Building on these advances, future efforts will focus on supporting further understanding of risk factors and predictors of outcomes in STEMI patients. More broadly, the thorough visual exploration of display spaces generated through a conjunction of dimensionality reduction with the mature technology base of geographic information systems appears a promising direction for biomedical research.
ER  - 
TY  - JOUR
T1  - A novel nonparametric approach for the identification of the glucose-insulin system in Type 1 diabetic patients
A1  - Del Favero, Simone
A1  - Pillonetto, Gianluigi
A1  - Cobelli, Claudio
A1  - De Nicolao, Giuseppe
Y1  - 2011///
JF  - IFAC Proceedings Volumes
VL  - 44
IS  - 1
SP  - 8340
EP  - 8346
DO  - https://doi.org/10.3182/20110828-6-IT-1002.01929
UR  - https://www.sciencedirect.com/science/article/pii/S1474667016449505
N1  - 18th IFAC World Congress
N2  - Abstract
In this paper we consider the problem of predicting future values of subcutaneous glucose (glucose concentration in the interstitial fluid) in Type-1 diabetes patients, exploiting information on injected insulin, carbohydrates intake and past subcutaneous glucose samples measured by a Continuous Glucose Monitor sensor. Prediction can be used to warn the patient of upcoming possibly harmful events, such as hypoglycemia or hyperglycemia. In addition, an effective prediction is a key ingredient in Model Predictive Control, a technique successfully applied in the development of the so called artificial pancreas. Derivation of individualized predictors is crucial to cope with the wide inter-subject variability. For this reason we explored the application of linear black-box identification methods to derive patient-tailored predictors. Together with the mainstream technique in system identification, the Prediction Error Method, we investigate a novel and promising nonparametric method based on Gaussian regression. The considered methods are applied to data collected on 20 subjects, during two 21-hours hospital admissions for each subject. One of the major challenges of this problem is that the available dataset is short and the glucose-insulin system has very high complexity. The quality of the prediction was compared on the basis of three metrics commonly used in system identification, namely Coefficient of Determination, Fit and Root Mean Squared Error. On a system identification perspective the present work is a comparison of a novel system identification approach with the state-of-the-art one on a particularly difficult and relevant case of study. The nonparametric technique improves prediction performance and reduces computational burden associated with predictor identification.
ER  - 
TY  - JOUR
T1  - HMV: A medical decision support framework using multi-layer classifiers for disease prediction
A1  - Bashir, Saba
A1  - Qamar, Usman
A1  - Khan, Farhan Hassan
A1  - Naseem, Lubna
Y1  - 2016///
KW  -  Disease classification
KW  -  Ensemble technique
KW  -  Majority voting
KW  -  Multi-layer
KW  -  Prediction
KW  - Data mining
JF  - Journal of Computational Science
VL  - 13
SP  - 10
EP  - 25
DO  - https://doi.org/10.1016/j.jocs.2016.01.001
UR  - https://www.sciencedirect.com/science/article/pii/S1877750316300011
N2  - Decision support is a crucial function for decision makers in many industries. Typically, Decision Support Systems (DSS) help decision-makers to gather and interpret information and build a foundation for decision-making. Medical Decision Support Systems (MDSS) play an increasingly important role in medical practice. By assisting doctors with making clinical decisions, DSS are expected to improve the quality of medical care. Conventional clinical decision support systems are based on individual classifiers or a simple combination of these classifiers which tend to show moderate performance. In this research, a multi-layer classifier ensemble framework is proposed based on the optimal combination of heterogeneous classifiers. The proposed model named “HMV” overcomes the limitations of conventional performance bottlenecks by utilizing an ensemble of seven heterogeneous classifiers. The framework is evaluated on two different heart disease datasets, two breast cancer datasets, two diabetes datasets, two liver disease datasets, one Parkinson's disease dataset and one hepatitis dataset obtained from public repositories. Effectiveness of the proposed ensemble is investigated by comparison of results with several well-known classifiers as well as ensemble techniques. The experimental evaluation shows that the proposed framework dealt with all types of attributes and achieved high diagnosis accuracy. A case study is also presented based on a real time medical dataset in order to show the high performance and effectiveness of the proposed model.
ER  - 
TY  - JOUR
T1  - Autonomous and deterministic supervised fuzzy clustering with data imputation capabilities
A1  - Ming, Lim Kian
A1  - Kiong, Loo Chu
A1  - Soong, Lim Way
Y1  - 2011///
KW  -  Data imputation
KW  -  Fault diagnosis
KW  -  Global -means
KW  -  Optimal completion strategy
KW  - Supervised fuzzy clustering
JF  - Applied Soft Computing
VL  - 11
IS  - 1
SP  - 1117
EP  - 1125
DO  - https://doi.org/10.1016/j.asoc.2010.02.011
UR  - https://www.sciencedirect.com/science/article/pii/S1568494610000566
N2  - A fuzzy model based on enhanced supervised fuzzy clustering algorithm is presented in this paper. Supervised fuzzy clustering algorithm by Janos Abonyi and Ferenc Szeifert in the year 2003 allows each rule to represent more than one output with different probabilities for each output. This algorithm implements k-means to initialize the fuzzy model. The main drawbacks of this approach are the number of clusters is unknown and the initial positions of clusters are randomly generated. In this work, the initialization is performed by global k-means algorithm [1] which can autonomously determine the actual number of clusters needed and give deterministic clustering result. In addition, fast global k-means [1] is presented to improve the computation time. Besides that, when collecting input data in a feature vector way, it might occur that some of the feature values are lost for a particular vector due to a faulty reading sensor. To deal with missing values in enhanced supervised fuzzy clustering, the efficient way is imputation during data preprocessing. The modified of optimal completion strategy is presented to solve this problem. This method allows imputation of missing data with high reliability and accuracy. The autonomous and deterministic enhanced supervised fuzzy clustering using supervised Gath–Geva clustering method and the modified of optimal completion strategy can be derived from the unsupervised Gath–Geva algorithm. The proposed algorithm is successfully justified based on benchmark data sets and a real vibration data which was collected from U.S. Navy CH-46E helicopter aft gearbox called Westland.
ER  - 
TY  - JOUR
T1  - Exudate detection in color retinal images for mass screening of diabetic retinopathy
A1  - Zhang, Xiwei
A1  - Thibault, Guillaume
A1  - Decencière, Etienne
A1  - Marcotegui, Beatriz
A1  - Laÿ, Bruno
A1  - Danno, Ronan
A1  - Cazuguel, Guy
A1  - Quellec, Gwénolé
A1  - Lamard, Mathieu
A1  - Massin, Pascale
A1  - Chabouis, Agnès
A1  - Victor, Zeynep
A1  - Erginay, Ali
Y1  - 2014///
KW  -  Exudates segmentation
KW  -  Mathematical morphology
KW  -  e-Ophtha EX database
KW  - Diabetic retinopathy screening
JF  - Medical Image Analysis
VL  - 18
IS  - 7
SP  - 1026
EP  - 1043
DO  - https://doi.org/10.1016/j.media.2014.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S1361841514000693
N2  - The automatic detection of exudates in color eye fundus images is an important task in applications such as diabetic retinopathy screening. The presented work has been undertaken in the framework of the TeleOphta project, whose main objective is to automatically detect normal exams in a tele-ophthalmology network, thus reducing the burden on the readers. A new clinical database, e-ophtha EX, containing precisely manually contoured exudates, is introduced. As opposed to previously available databases, e-ophtha EX is very heterogeneous. It contains images gathered within the OPHDIAT telemedicine network for diabetic retinopathy screening. Image definition, quality, as well as patients condition or the retinograph used for the acquisition, for example, are subject to important changes between different examinations. The proposed exudate detection method has been designed for this complex situation. We propose new preprocessing methods, which perform not only normalization and denoising tasks, but also detect reflections and artifacts in the image. A new candidates segmentation method, based on mathematical morphology, is proposed. These candidates are characterized using classical features, but also novel contextual features. Finally, a random forest algorithm is used to detect the exudates among the candidates. The method has been validated on the e-ophtha EX database, obtaining an AUC of 0.95. It has been also validated on other databases, obtaining an AUC between 0.93 and 0.95, outperforming state-of-the-art methods.
ER  - 
TY  - JOUR
T1  - Pressure injury image analysis with machine learning techniques: A systematic review on previous and possible future methods
A1  - Zahia, Sofia
A1  - Garcia Zapirain, Maria Begoña
A1  - Sevillano, Xavier
A1  - González, Alejandro
A1  - Kim, Paul J
A1  - Elmaghraby, Adel
Y1  - 2020///
KW  -  Deep learning
KW  -  Machine learning algorithms
KW  -  Wound image analysis
KW  - Pressure injury
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101742
EP  - 101742
DO  - https://doi.org/10.1016/j.artmed.2019.101742
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718307127
N2  - Pressure injuries represent a tremendous healthcare challenge in many nations. Elderly and disabled people are the most affected by this fast growing disease. Hence, an accurate diagnosis of pressure injuries is paramount for efficient treatment. The characteristics of these wounds are crucial indicators for the progress of the healing. While invasive methods to retrieve information are not only painful to the patients but may also increase the risk of infections, non-invasive techniques by means of imaging systems provide a better monitoring of the wound healing processes without causing any harm to the patients. These systems should include an accurate segmentation of the wound, the classification of its tissue types, the metrics including the diameter, area and volume, as well as the healing evaluation. Therefore, the aim of this survey is to provide the reader with an overview of imaging techniques for the analysis and monitoring of pressure injuries as an aid to their diagnosis, and proof of the efficiency of Deep Learning to overcome this problem and even outperform the previous methods. In this paper, 114 out of 199 papers retrieved from 8 databases have been analyzed, including also contributions on chronic wounds and skin lesions.
ER  - 
TY  - JOUR
T1  - Mining association patterns of drug-interactions using post marketing FDA’s spontaneous reporting data
A1  - Ibrahim, Heba
A1  - Saad, Amr
A1  - Abdo, Amany
A1  - Sharaf Eldin, A
Y1  - 2016///
KW  -  Association rule mining
KW  -  Drug–drug interaction
KW  -  Logistic regression
KW  -  Proportional reporting ratio
KW  -  RxNorm
KW  - Pharmacovigilance
JF  - Journal of Biomedical Informatics
VL  - 60
SP  - 294
EP  - 308
DO  - https://doi.org/10.1016/j.jbi.2016.02.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416000319
N2  - Background and objectives
Pharmacovigilance (PhV) is an important clinical activity with strong implications for population health and clinical research. The main goal of PhV is the timely detection of adverse drug events (ADEs) that are novel in their clinical nature, severity and/or frequency. Drug interactions (DI) pose an important problem in the development of new drugs and post marketing PhV that contribute to 6–30% of all unexpected ADEs. Therefore, the early detection of DI is vital. Spontaneous reporting systems (SRS) have served as the core data collection system for post marketing PhV since the 1960s. The main objective of our study was to particularly identify signals of DI from SRS. In addition, we are presenting an optimized tailored mining algorithm called “hybrid Apriori”.
Methods
The proposed algorithm is based on an optimized and modified association rule mining (ARM) approach. A hybrid Apriori algorithm has been applied to the SRS of the United States Food and Drug Administration’s (U.S. FDA) adverse events reporting system (FAERS) in order to extract significant association patterns of drug interaction-adverse event (DIAE). We have assessed the resulting DIAEs qualitatively and quantitatively using two different triage features: a three-element taxonomy and three performance metrics. These features were applied on two random samples of 100 interacting and 100 non-interacting DIAE patterns. Additionally, we have employed logistic regression (LR) statistic method to quantify the magnitude and direction of interactions in order to test for confounding by co-medication in unknown interacting DIAE patterns.
Results
Hybrid Apriori extracted 2933 interacting DIAE patterns (including 1256 serious ones) and 530 non-interacting DIAE patterns. Referring to the current knowledge using four different reliable resources of DI, the results showed that the proposed method can extract signals of serious interacting DIAEs. Various association patterns could be identified based on the relationships among the elements which composed a pattern. The average performance of the method showed 85% precision, 80% negative predictive value, 81% sensitivity and 84% specificity. The LR modeling could provide the statistical context to guard against spurious DIAEs.
Conclusions
The proposed method could efficiently detect DIAE signals from SRS data as well as, identifying rare adverse drug reactions (ADRs).
ER  - 
TY  - JOUR
T1  - Automatic endpoint detection to support the systematic review process
A1  - Blake, Catherine
A1  - Lucic, Ana
Y1  - 2015///
KW  -  Evidence-based medicine
KW  -  Information extraction
KW  -  Text mining
KW  - Systematic review
JF  - Journal of Biomedical Informatics
VL  - 56
SP  - 42
EP  - 56
DO  - https://doi.org/10.1016/j.jbi.2015.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415000830
N2  - Preparing a systematic review can take hundreds of hours to complete, but the process of reconciling different results from multiple studies is the bedrock of evidence-based medicine. We introduce a two-step approach to automatically extract three facets – two entities (the agent and object) and the way in which the entities are compared (the endpoint) – from direct comparative sentences in full-text articles. The system does not require a user to predefine entities in advance and thus can be used in domains where entity recognition is difficult or unavailable. As with a systematic review, the tabular summary produced using the automatically extracted facets shows how experimental results differ between studies. Experiments were conducted using a collection of more than 2million sentences from three journals Diabetes, Carcinogenesis and Endocrinology and two machine learning algorithms, support vector machines (SVM) and a general linear model (GLM). F1 and accuracy measures for the SVM and GLM differed by only 0.01 across all three comparison facets in a randomly selected set of test sentences. The system achieved the best performance of 92% for objects, whereas the accuracy for both agent and endpoints was 73%. F1 scores were higher for objects (0.77) than for endpoints (0.51) or agents (0.47). A situated evaluation of Metformin, a drug to treat diabetes, showed system accuracy of 95%, 83% and 79% for the object, endpoint and agent respectively. The situated evaluation had higher F1 scores of 0.88, 0.64 and 0.62 for object, endpoint, and agent respectively. On average, only 5.31% of the sentences in a full-text article are direct comparisons, but the tabular summaries suggest that these sentences provide a rich source of currently underutilized information that can be used to accelerate the systematic review process and identify gaps where future research should be focused.
ER  - 
TY  - JOUR
T1  - A Magnified Adaptive Feature Pyramid Network for automatic microaneurysms detection
A1  - Sun, Song
A1  - Cao, Zhicheng
A1  - Liao, Dingying
A1  - Lv, Ruichan
Y1  - 2021///
KW  -  Adaptive feature
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Pyramid network
KW  -  Super-resolution
KW  - Microaneurysms detection
JF  - Computers in Biology and Medicine
VL  - 139
SP  - 105000
EP  - 105000
DO  - https://doi.org/10.1016/j.compbiomed.2021.105000
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521007940
N2  - Diabetic retinopathy (DR), as an important complication of diabetes, is the primary cause of blindness in adults. Automatic DR detection poses a challenge which is crucial for early DR screening. Currently, the vast majority of DR is diagnosed through fundus images, where the microaneurysm (MA) has been widely used as the most distinguishable marker. Research works on automatic DR detection have traditionally utilized manually designed operators, while a few recent researchers have explored deep learning techniques for this topic. But due to issues such as the extremely small size of microaneurysms, low resolution of fundus pictures, and insufficient imaging depth, the DR detection problem is quite challenging and remains unsolved. To address these issues, this research proposes a new deep learning model (Magnified Adaptive Feature Pyramid Network, MAFP-Net) for DR detection, which conducts super-resolution on low quality fundus images and integrates an improved feature pyramid structure while utilizing a standard two-stage detection network as the backbone. Our proposed detection model needs no pre-segmented patches to train the CNN network. When tested on the E-ophtha-MA dataset, the sensitivity value of our method reached as high as 83.5% at false positives per image (FPI) of 8 and the F1 value achieved 0.676, exceeding all those of the state-of-the-art algorithms as well as the human performance of experienced physicians. Similar results were achieved on another public dataset of IDRiD.
ER  - 
TY  - JOUR
T1  - Detection of white matter lesion regions in MRI using SLIC0 and convolutional neural network
A1  - Diniz, Pedro Henrique Bandeira
A1  - Valente, Thales Levi Azevedo
A1  - Diniz, João Otávio Bandeira
A1  - Silva, Aristófanes Corrêa
A1  - Gattass, Marcelo
A1  - Ventura, Nina
A1  - Muniz, Bernardo Carvalho
A1  - Gasparetto, Emerson Leandro
Y1  - 2018///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Medical images
KW  -  SLIC0
KW  -  White matter lesion
KW  - Computer-aided detection
JF  - Computer Methods and Programs in Biomedicine
VL  - 167
SP  - 49
EP  - 63
DO  - https://doi.org/10.1016/j.cmpb.2018.04.011
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717304455
N2  - Background and Objective: White matter lesions are non-static brain lesions that have a prevalence rate up to 98% in the elderly population. Because they may be associated with several brain diseases, it is important that they are detected as soon as possible. Magnetic Resonance Imaging (MRI) provides three-dimensional data with the possibility to detect and emphasize contrast differences in soft tissues, providing rich information about the human soft tissue anatomy. However, the amount of data provided for these images is far too much for manual analysis/interpretation, representing a difficult and time-consuming task for specialists. This work presents a computational methodology capable of detecting regions of white matter lesions of the brain in MRI of FLAIR modality. The techniques highlighted in this methodology are SLIC0 clustering for candidate segmentation and convolutional neural networks for candidate classification. Methods: The methodology proposed here consists of four steps: (1) images acquisition, (2) images preprocessing, (3) candidates segmentation and (4) candidates classification. Results: The methodology was applied on 91 magnetic resonance images provided by DASA, and achieved an accuracy of 98.73%, specificity of 98.77% and sensitivity of 78.79% with 0.005 of false positives, without any false positives reduction technique, in detection of white matter lesion regions. Conclusions: It is demonstrated the feasibility of the analysis of brain MRI using SLIC0 and convolutional neural network techniques to achieve success in detection of white matter lesions regions.
ER  - 
TY  - JOUR
T1  - Detection of COVID-19 from voice, cough and breathing patterns: Dataset and preliminary results
A1  - Despotovic, Vladimir
A1  - Ismael, Muhannad
A1  - Cornil, Maël
A1  - Call, Roderick Mc
A1  - Fagherazzi, Guy
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  COVID-19
KW  -  Cough
KW  -  Digital biomarker
KW  - Voice
JF  - Computers in Biology and Medicine
VL  - 138
SP  - 104944
EP  - 104944
DO  - https://doi.org/10.1016/j.compbiomed.2021.104944
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521007381
N2  - COVID-19 heavily affects breathing and voice and causes symptoms that make patients’ voices distinctive, creating recognizable audio signatures. Initial studies have already suggested the potential of using voice as a screening solution. In this article we present a dataset of voice, cough and breathing audio recordings collected from individuals infected by SARS-CoV-2 virus, as well as non-infected subjects via large scale crowdsourced campaign. We describe preliminary results for detection of COVID-19 from cough patterns using standard acoustic features sets, wavelet scattering features and deep audio embeddings extracted from low-level feature representations (VGGish and OpenL3). Our models achieve accuracy of 88.52%, sensitivity of 88.75% and specificity of 90.87%, confirming the applicability of audio signatures to identify COVID-19 symptoms. We furthermore provide an in-depth analysis of the most informative acoustic features and try to elucidate the mechanisms that alter the acoustic characteristics of coughs of people with COVID-19.
ER  - 
TY  - JOUR
T1  - Blood glucose concentration prediction based on kernel canonical correlation analysis with particle swarm optimization and error compensation
A1  - He, Jinli
A1  - Wang, Youqing
Y1  - 2020///
KW  -  Blood glucose prediction
KW  -  Error compensation
KW  -  Hypoglycemic warning
KW  -  Kernel function
KW  -  Particle swarm optimization
KW  - Canonical correlation analysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105574
EP  - 105574
DO  - https://doi.org/10.1016/j.cmpb.2020.105574
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720305083
N2  - Background and objective: Blood glucose levels in humans change over time. Continuous glucose monitoring system (CGMS), can constantly monitor the change of blood glucose concentration. Given the historical data of blood glucose, predicting the trend of blood glucose in a short term is important for diabetes. Appropriate behaviors can be adopted to prevent hypoglycemia or hyperglycemia. Methods: The method proposed in this paper only uses historical blood glucose data as input, rather than complex multi-dimensional input. Previous articles have demonstrated that canonical correlation analysis (CCA) can effectively predict blood glucose. The linear relationship between historical blood glucose values and predicted values was only considered regrettably. To compensate for this, this paper adds a kernel function to find out the non-linear relationship between blood glucose. In the introduced kernel function, some parameters need to be adjusted. To reduce the deviation caused by manual parameter adjustment, this paper discusses the role of particle swarm optimization (PSO). Besides, this article puts forward an error compensation for CCA to enhance the precision. Finally based on the prediction results of PSO-KCCA, a personalized hypoglycemic warning threshold is proposed. Results: The proposed method is validated using clinical data by the root mean square error (RMSE) and differential coefficient (R2). The average RMSE result in PSO-KCCA was 8.01, 11.98, 12.45, 13.23, 14.53, 16.40 mg/dL in prediction horizon (PH) =5, 10, 15, 20, 25, 30 min. The average R2 was 0.95, 0.95, 0.98, 0.97, 0.98, and 0.97, respectively. The CCA with error compensation (EC-CCA) reduced RMSE by 33.45% compared with CCA. For the hypoglycemic warning, the average sensitivity obtained at 6 different PH values was 94.37%, and the specificity was 92.25%. Conclusions: The experimental results confirm the effectiveness of PSO-KCCA in blood glucose prediction. The proposed EC-CCA successfully reduces the delay in the time series prediction. The personalized hypoglycemic warning threshold consider the influence of the model accuracy on the prediction results. This method guarantees the rate of underreporting during monitoring and ensures patient safety.
ER  - 
TY  - JOUR
T1  - Simultaneous segmentation and classification of the retinal arteries and veins from color fundus images
A1  - Morano, José
A1  - Hervella, Álvaro S
A1  - Novo, Jorge
A1  - Rouco, José
Y1  - 2021///
KW  -  Artery and vein classification
KW  -  Deep learning
KW  -  Ophthalmology
KW  -  Retina
KW  -  Vessel segmentation
KW  - Medical imaging
JF  - Artificial Intelligence in Medicine
VL  - 118
SP  - 102116
EP  - 102116
DO  - https://doi.org/10.1016/j.artmed.2021.102116
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721001093
N2  - Background and objectives
The study of the retinal vasculature represents a fundamental stage in the screening and diagnosis of many high-incidence diseases, both systemic and ophthalmic. A complete retinal vascular analysis requires the segmentation of the vascular tree along with the classification of the blood vessels into arteries and veins. Early automatic methods approach these complementary segmentation and classification tasks in two sequential stages. However, currently, these two tasks are approached as a joint semantic segmentation, because the classification results highly depend on the effectiveness of the vessel segmentation. In that regard, we propose a novel approach for the simultaneous segmentation and classification of the retinal arteries and veins from eye fundus images.
Methods
We propose a novel method that, unlike previous approaches, and thanks to the proposal of a novel loss, decomposes the joint task into three segmentation problems targeting arteries, veins and the whole vascular tree. This configuration allows to handle vessel crossings intuitively and directly provides accurate segmentation masks of the different target vascular trees.
Results
The provided ablation study on the public Retinal Images vessel Tree Extraction (RITE) dataset demonstrates that the proposed method provides a satisfactory performance, particularly in the segmentation of the different structures. Furthermore, the comparison with the state of the art shows that our method achieves highly competitive results in the artery/vein classification, while significantly improving the vascular segmentation.
Conclusions
The proposed multi-segmentation method allows to detect more vessels and better segment the different structures, while achieving a competitive classification performance. Also, in these terms, our approach outperforms the approaches of various reference works. Moreover, in contrast with previous approaches, the proposed method allows to directly detect the vessel crossings, as well as preserving the continuity of both arteries and veins at these complex locations.
ER  - 
TY  - JOUR
T1  - Therapy-driven Deep Glucose Forecasting
A1  - Aiello, Eleonora Maria
A1  - Lisanti, Giuseppe
A1  - Magni, Lalo
A1  - Musci, Mirto
A1  - Toffanin, Chiara
Y1  - 2020///
KW  -  Deep learning
KW  -  Forecasting
KW  -  LSTM
KW  -  Prediction
KW  - Diabetes
JF  - Engineering Applications of Artificial Intelligence
VL  - 87
SP  - 103255
EP  - 103255
DO  - https://doi.org/10.1016/j.engappai.2019.103255
UR  - https://www.sciencedirect.com/science/article/pii/S0952197619302313
N2  - The automatic regulation of blood glucose for Type 1 diabetes patients is the main goal of the artificial pancreas, a closed-loop system that exploits continue glucose monitoring data to define an optimal insulin therapy. One of the most successful approaches for developing the artificial pancreas is the model predictive control, which exhibits promising results on both virtual and real patients. The performance of such controller is highly dependent on the reliability of the glucose–insulin model used for prediction purpose, which is usually implemented with classic mathematical models. The main limitation of these models consists in the difficulties of modeling the physiological nonlinear dynamics typical of this system. The availability of big amount of in silico and in vivo data moved the attention to new data-driven methods which are able to easily overcome this problem. In this paper we propose Deep Glucose Forecasting, a deep learning approach for forecasting glucose levels, based on a novel, two-headed Long-Short Term Memory implementation. It takes in input the previous values obtained through continue glucose monitoring, the carbohydrate intake, the suggested insulin therapy and forecasts the interstitial glucose level of the patient. The proposed architecture has been trained on 100 virtual adult patients of the UVA/Padova simulator, and tested on both virtual and real patients. The proposed solution is able to generalize to new unseen data, outperforms classical population models and reaches performance comparable to classical personalized models when fine-tuning is exploited on real patients.
ER  - 
TY  - JOUR
T1  - An approach for Ewing test selection to support the clinical assessment of cardiac autonomic neuropathy
A1  - Stranieri, Andrew
A1  - Abawajy, Jemal
A1  - Kelarev, Andrei
A1  - Huda, Shamsul
A1  - Chowdhury, Morshed
A1  - Jelinek, Herbert F
Y1  - 2013///
KW  -  Accuracy of classification
KW  -  Cardiac autonomic neuropathy
KW  -  Decision trees
KW  -  Diabetes patients
KW  -  Ewing features
KW  - Optimal sequence of tests
JF  - Artificial Intelligence in Medicine
VL  - 58
IS  - 3
SP  - 185
EP  - 193
DO  - https://doi.org/10.1016/j.artmed.2013.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S0933365713000705
N2  - Objective
This article addresses the problem of determining optimal sequences of tests for the clinical assessment of cardiac autonomic neuropathy (CAN). We investigate the accuracy of using only one of the recommended Ewing tests to classify CAN and the additional accuracy obtained by adding the remaining tests of the Ewing battery. This is important as not all five Ewing tests can always be applied in each situation in practice.
Methods and material
We used new and unique database of the diabetes screening research initiative project, which is more than ten times larger than the data set used by Ewing in his original investigation of CAN. We utilized decision trees and the optimal decision path finder (ODPF) procedure for identifying optimal sequences of tests.
Results
We present experimental results on the accuracy of using each one of the recommended Ewing tests to classify CAN and the additional accuracy that can be achieved by adding the remaining tests of the Ewing battery. We found the best sequences of tests for cost-function equal to the number of tests. The accuracies achieved by the initial segments of the optimal sequences for 2, 3 and 4 categories of CAN are 80.80, 91.33, 93.97 and 94.14, and respectively, 79.86, 89.29, 91.16 and 91.76, and 78.90, 86.21, 88.15 and 88.93. They show significant improvement compared to the sequence considered previously in the literature and the mathematical expectations of the accuracies of a random sequence of tests. The complete outcomes obtained for all subsets of the Ewing features are required for determining optimal sequences of tests for any cost-function with the use of the ODPF procedure. We have also found two most significant additional features that can increase the accuracy when some of the Ewing attributes cannot be obtained.
Conclusions
The outcomes obtained can be used to determine the optimal sequences of tests for each individual cost-function by following the ODPF procedure. The results show that the best single Ewing test for diagnosing CAN is the deep breathing heart rate variation test. Optimal sequences found for the cost-function equal to the number of tests guarantee that the best accuracy is achieved after any number of tests and provide an improvement in comparison with the previous ordering of tests or a random sequence.
ER  - 
TY  - JOUR
T1  - A fast pruning redundant rule method using Galois connection
A1  - Liu, Huawen
A1  - Liu, Lei
A1  - Zhang, Huijie
Y1  - 2011///
KW  -  Formal concept analysis
KW  -  Galois connection
KW  -  Post-analysis
KW  -  Rule pruning
KW  - Data mining
JF  - Applied Soft Computing
VL  - 11
IS  - 1
SP  - 130
EP  - 137
DO  - https://doi.org/10.1016/j.asoc.2009.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S1568494609002178
N2  - Besides preprocessing, post-analysis also plays an important role in knowledge discovery. It can effectively assist users to grasp the obtained knowledge. However, many of data mining algorithms merely take performance into consideration and put the post-analysis of results aside. They generate a modest number of rules for the purpose of improving accuracy. Unfortunately, most induced rules are redundant or insignificant. Their presence not only confuses end-users in post-analysis, but also degrades efficiency in future decision task. Thus, it is necessary to eliminate redundant or irrelevant rules as more as possible. In this paper, we present an efficient post-processing method to prune redundant rules by virtue of the property of Galois connection, which inherently constrains rules with respect to objects. Its advantage is that information will not be lost greatly during pruning step. The experimental evaluation shows that the proposed method is competent for discarding a large number of superfluous rules effectively and a high compression factor will be achieved. What’s more, the computational cost of our method is surprisingly lower than the Apriori method.
ER  - 
TY  - JOUR
T1  - Diabetic Retinopathy Screening Using Computer Vision
A1  - Hann, Christopher E
A1  - Chase, J Geoffrey
A1  - Revie, James A
A1  - Hewett, Darren
A1  - Shaw, Geoffrey M
Y1  - 2009///
KW  -  Automation
KW  -  Diabetes
KW  -  Diagnosis
KW  -  Retinopathy
KW  - Computer Vision
JF  - IFAC Proceedings Volumes
VL  - 42
IS  - 12
SP  - 298
EP  - 303
DO  - https://doi.org/10.3182/20090812-3-DK-2006.0086
UR  - https://www.sciencedirect.com/science/article/pii/S1474667015378903
N1  - 7th IFAC Symposium on Modelling and Control in Biomedical Systems
N2  - Diabetic Retinopathy (DR) is one of the main causes of blindness and visual impairment in developed countries, stemming solely from diabetes mellitus. Current screening methods using fundus images rely on the experience of the operator as they are manually examined. Automated methods based on neural networks and other approaches have not provided sensitivity or specificity above 85%. This work presents a computer vision based method that directly identifies hard exudates and dot haemorrhages (DH) from 100 digital fundus images from a graded database of images using standard computer vision techniques, and clinical observation and knowledge. Sensitivity and specificity in diagnosis are 95–100% in both cases. Positive and negative prediction values (PPV, NPV) were 95–100% for both cases. The overall method is general, computationally efficient and suitable for further clinical trials to test both accuracy and the ability to the track DR status over time.
ER  - 
TY  - JOUR
T1  - Liking, sharing, commenting and reacting on Facebook: User behaviors’ impact on sentiment intensity
A1  - Kaur, Wandeep
A1  - Balakrishnan, Vimala
A1  - Rana, Omer
A1  - Sinniah, Ajantha
Y1  - 2019///
KW  -  Comment
KW  -  Like
KW  -  Reaction
KW  -  Sentiment intensity
KW  -  Share
KW  - Facebook
JF  - Telematics and Informatics
VL  - 39
SP  - 25
EP  - 36
DO  - https://doi.org/10.1016/j.tele.2018.12.005
UR  - https://www.sciencedirect.com/science/article/pii/S0736585318304325
N2  - The form of communication on Facebook is not only limited to posting and commenting, but also includes sharing, liking and reacting. This study looks into how a Facebook diabetes community uses like, comment, share and reaction in expressing themselves online and how these distinctions can be used to improve sentiment classification from text extracted from the said group. An intensity formula using those behaviors was proposed and experimentations conducted using Weka. The findings reveal a model encompassing user behaviors is able to determine sentiment more accurately compared to one without, with a 94.6 percentage of accuracy. Additional analyses reveal behaviors such as liking, commenting and sharing to contribute more to the sentiment classification compared to reacting. This further cement the need to include such behavioral aspects into sentiment polarity calculation, as it would help algorithms achieve better predictability when classifying sentiment.
ER  - 
TY  - JOUR
T1  - Detecting clinically related content in online patient posts
A1  - VanDam, Courtland
A1  - Kanthawala, Shaheen
A1  - Pratt, Wanda
A1  - Chai, Joyce
A1  - Huh, Jina
Y1  - 2017///
KW  -  Classification
KW  -  Clinical topic
KW  -  Diabetes
KW  -  Health information seeking
KW  -  Human-computer interaction
KW  -  Online health communities
KW  -  Patient
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 75
SP  - 96
EP  - 106
DO  - https://doi.org/10.1016/j.jbi.2017.09.015
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417302198
N2  - Patients with chronic health conditions use online health communities to seek support and information to help manage their condition. For clinically related topics, patients can benefit from getting opinions from clinical experts, and many are concerned about misinformation and biased information being spread online. However, a large volume of community posts makes it challenging for moderators and clinical experts, if there are any, to provide necessary information. Automatically identifying forum posts that need validated clinical resources can help online health communities efficiently manage content exchange. This automation can also assist patients in need of clinical expertise by getting proper help. We present our results on testing text classification models that efficiently and accurately identify community posts containing clinical topics. We annotated 1817 posts comprised of 4966 sentences of an existing online diabetes community. We found that our classifier performed the best (F-measure: 0.83, Precision: 0.79, Recall:0.86) when using Naïve Bayes algorithm, unigrams, bigrams, trigrams, and MetaMap Symantic Types. Training took 5 s. The classification process took a fraction of 1 s. We applied our classifier to another online diabetes community, and the results were: F-measure: 0.63, Precision: 0.57, Recall: 0.71. Our results show our model is feasible to scale to other forums on identifying posts containing clinical topic with common errors properly addressed.
ER  - 
TY  - JOUR
T1  - Results on mining NHANES data: A case study in evidence-based medicine
A1  - won Lee, Jun
A1  - Giraud-Carrier, Christophe
Y1  - 2013///
KW  -  Evidence-based medicine
KW  -  NHANES
KW  -  Observational study
KW  - Medical data mining
JF  - Computers in Biology and Medicine
VL  - 43
IS  - 5
SP  - 493
EP  - 503
DO  - https://doi.org/10.1016/j.compbiomed.2013.02.018
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513000693
N2  - The National Health and Nutrition Examination Survey (NHANES), administered annually by the National Center for Health Statistics, is designed to assess the general health and nutritional status of adults and children in the United States. Given to several thousands of individuals, the extent of this survey is very broad, covering demographic, laboratory and examination information, as well as responses to a fairly comprehensive health questionnaire. In this paper, we adapt and extend association rule mining and clustering algorithms to extract useful knowledge regarding diabetes and high blood pressure from the 1999–2008 survey results, thus demonstrating how data mining techniques may be used to support evidence-based medicine.
ER  - 
TY  - JOUR
T1  - A machine learning-based risk stratification model for ventricular tachycardia and heart failure in hypertrophic cardiomyopathy
A1  - Smole, Tim
A1  - Žunkovič, Bojan
A1  - Pičulin, Matej
A1  - Kokalj, Enja
A1  - Robnik-Šikonja, Marko
A1  - Kukar, Matjaž
A1  - Fotiadis, Dimitrios I
A1  - Pezoulas, Vasileios C
A1  - Tachos, Nikolaos S
A1  - Barlocco, Fausto
A1  - Mazzarotto, Francesco
A1  - Popović, Dejana
A1  - Maier, Lars
A1  - Velicki, Lazar
A1  - MacGowan, Guy A
A1  - Olivotto, Iacopo
A1  - Filipović, Nenad
A1  - Jakovljević, Djordje G
A1  - Bosnić, Zoran
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Machine learning
KW  -  Risk stratification
KW  - Hypertrophic cardiomyopathy
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104648
EP  - 104648
DO  - https://doi.org/10.1016/j.compbiomed.2021.104648
UR  - https://www.sciencedirect.com/science/article/pii/S001048252100442X
N2  - Background
Machine learning (ML) and artificial intelligence are emerging as important components of precision medicine that enhance diagnosis and risk stratification. Risk stratification tools for hypertrophic cardiomyopathy (HCM) exist, but they are based on traditional statistical methods. The aim was to develop a novel machine learning risk stratification tool for the prediction of 5-year risk in HCM. The goal was to determine if its predictive accuracy is higher than the accuracy of the state-of-the-art tools.
Method
Data from a total of 2302 patients were used. The data were comprised of demographic characteristics, genetic data, clinical investigations, medications, and disease-related events. Four classification models were applied to model the risk level, and their decisions were explained using the SHAP (SHapley Additive exPlanations) method. Unwanted cardiac events were defined as sustained ventricular tachycardia occurrence (VT), heart failure (HF), ICD activation, sudden cardiac death (SCD), cardiac death, and all-cause death.
Results
The proposed machine learning approach outperformed the similar existing risk-stratification models for SCD, cardiac death, and all-cause death risk-stratification: it achieved higher AUC by 17%, 9%, and 1%, respectively. The boosted trees achieved the best performing AUC of 0.82. The resulting model most accurately predicts VT, HF, and ICD with AUCs of 0.90, 0.88, and 0.87, respectively.
Conclusions
The proposed risk-stratification model demonstrates high accuracy in predicting events in patients with hypertrophic cardiomyopathy. The use of a machine-learning risk stratification model may improve patient management, clinical practice, and outcomes in general.
ER  - 
TY  - JOUR
T1  - A smart healthcare monitoring system for heart disease prediction based on ensemble deep learning and feature fusion
A1  - Ali, Farman
A1  - El-Sappagh, Shaker
A1  - Islam, S M Riazul
A1  - Kwak, Daehan
A1  - Ali, Amjad
A1  - Imran, Muhammad
A1  - Kwak, Kyung-Sup
Y1  - 2020///
KW  -  Deep learning
KW  -  Feature fusion
KW  -  Heart disease prediction
KW  -  Ontology
KW  - Feature extraction
JF  - Information Fusion
VL  - 63
SP  - 208
EP  - 222
DO  - https://doi.org/10.1016/j.inffus.2020.06.008
UR  - https://www.sciencedirect.com/science/article/pii/S1566253520303055
N2  - The accurate prediction of heart disease is essential to efficiently treating cardiac patients before a heart attack occurs. This goal can be achieved using an optimal machine learning model with rich healthcare data on heart diseases. Various systems based on machine learning have been presented recently to predict and diagnose heart disease. However, these systems cannot handle high-dimensional datasets due to the lack of a smart framework that can use different sources of data for heart disease prediction. In addition, the existing systems utilize conventional techniques to select features from a dataset and compute a general weight for them based on their significance. These methods have also failed to enhance the performance of heart disease diagnosis. In this paper, a smart healthcare system is proposed for heart disease prediction using ensemble deep learning and feature fusion approaches. First, the feature fusion method combines the extracted features from both sensor data and electronic medical records to generate valuable healthcare data. Second, the information gain technique eliminates irrelevant and redundant features, and selects the important ones, which decreases the computational burden and enhances the system performance. In addition, the conditional probability approach computes a specific feature weight for each class, which further improves system performance. Finally, the ensemble deep learning model is trained for heart disease prediction. The proposed system is evaluated with heart disease data and compared with traditional classifiers based on feature fusion, feature selection, and weighting techniques. The proposed system obtains accuracy of 98.5%, which is higher than existing systems. This result shows that our system is more effective for the prediction of heart disease, in comparison to other state-of-the-art methods.
ER  - 
TY  - JOUR
T1  - Automated diabetic retinopathy detection using radial basis function
A1  - Kamble, Vaibhav V
A1  - Kokate, Rajendra D
Y1  - 2020///
KW  -  Diabetic retinopathy
KW  -  Exudates
KW  -  Microaneurysms
KW  -  RBFNN
KW  -  Retinal fundus images
KW  -  Tortuosity
KW  - Blood Vessels
JF  - Procedia Computer Science
VL  - 167
SP  - 799
EP  - 808
DO  - https://doi.org/10.1016/j.procs.2020.03.429
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920308954
N1  - International Conference on Computational Intelligence and Data Science
N2  - Diabetic mellitus is a major reason of visual impairment an around the world. Early automatic diagnosis of diabetic retinopathy (DR) may avoid vision loss and blindness. The goal of this paper is to automatically detect retinal image as Non DR or DR based on radial basis function (RBF) neural network classifier. This experiment address to explore ophthalmic features such as blood vessels, exudates & microaneurysms and it’s segmented from retinal background using A-IFS histon based segmentation method. This obtained feature set delivers to train RBF neural network. The Receiver operation characteristics (ROC) curve is plotted based on evaluated result. The projected experiment has been done on 130 DIARETDB0 & 89 DIARETDB1 retinal images database by using RBF neural network. The experiment perceive the accuracy of 71.2%, Sensitivity 0.83 & Specificity 0.043 for DIARETDB0 and the accuracy of 89.4% Sensitivity 0.94 & Specificity 0.16 for DIARETDB1.
ER  - 
TY  - JOUR
T1  - Modeling asynchronous event sequences with RNNs
A1  - Wu, Stephen
A1  - Liu, Sijia
A1  - Sohn, Sunghwan
A1  - Moon, Sungrim
A1  - Wi, Chung-il
A1  - Juhn, Young
A1  - Liu, Hongfang
Y1  - 2018///
KW  -  Asthma
KW  -  Deep learning
KW  -  Electronic health records
KW  - Temporal data
JF  - Journal of Biomedical Informatics
VL  - 83
SP  - 167
EP  - 177
DO  - https://doi.org/10.1016/j.jbi.2018.05.016
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300996
N2  - Sequences of events have often been modeled with computational techniques, but typical preprocessing steps and problem settings do not explicitly address the ramifications of timestamped events. Clinical data, such as is found in electronic health records (EHRs), typically comes with timestamp information. In this work, we define event sequences and their properties: synchronicity, evenness, and co-cardinality; we then show how asynchronous, uneven, and multi-cardinal problem settings can support explicit accountings of relative time. Our evaluation uses the temporally sensitive clinical use case of pediatric asthma, which is a chronic disease with symptoms (and lack thereof) evolving over time. We show several approaches to explicitly incorporating relative time into a recurrent neural network (RNN) model that improve the overall classification of patients into those with no asthma, those with persistent asthma, those in long-term remission, and those who have experienced relapse. We also compare and contrast these results with those in an inpatient intensive care setting.
ER  - 
TY  - JOUR
T1  - A deep learning approach to detect sleep stages
A1  - Stuburić, Klara
A1  - Gaiduk, Maksym
A1  - Seepold, Ralf
Y1  - 2020///
KW  -  biosignal processing
KW  -  convolutional neural network
KW  -  deep learning
KW  - sleep stages
JF  - Procedia Computer Science
VL  - 176
SP  - 2764
EP  - 2772
DO  - https://doi.org/10.1016/j.procs.2020.09.280
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920321840
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 24th International Conference KES2020
N2  - This paper presents the implementation of deep learning methods for sleep stage detection by using three signals that can be measured in a non-invasive way: heartbeat signal, respiratory signal, and movement signal. Since signals are measurements taken during the time, the problem is seen as time-series data classification. Deep learning methods are chosen to solve the problem are convolutional neural network and long-short term memory network. Input data is structured as a time-series sequence of mentioned signals that represent 30 seconds epoch, which is a standard interval for sleep analysis. The records used belong to the overall 23 subjects, which are divided into two subsets. Records from 18 subjects were used for training the data and from 5 subjects for testing the data. For detecting four sleep stages: REM (Rapid Eye Movement), Wake, Light sleep (Stage 1 and Stage 2), and Deep sleep (Stage 3 and Stage 4), the accuracy of the model is 55%, and F1 score is 44%. For five stages: REM, Stage 1, Stage 2, Deep sleep (Stage 3 and 4), and Wake, the model gives an accuracy of 40% and F1 score of 37%.
ER  - 
TY  - JOUR
T1  - Predicting survival time for kidney dialysis patients: a data mining approach
A1  - Kusiak, Andrew
A1  - Dixon, Bradley
A1  - Shah, Shital
Y1  - 2005///
KW  -  Data mining
KW  -  Data preprocessing
KW  -  Data transformations
KW  -  Decision making
KW  -  Dialysis protocol
KW  -  Medical relevance
KW  -  Survival
KW  - Hemodialysis
JF  - Computers in Biology and Medicine
VL  - 35
IS  - 4
SP  - 311
EP  - 327
DO  - https://doi.org/10.1016/j.compbiomed.2004.02.004
UR  - https://www.sciencedirect.com/science/article/pii/S0010482504000319
N2  - The cost for providing care for patients on hemodialysis due to end stage kidney disease is high. Finding ways to improve patient outcomes and reduce the cost of dialysis is important. Dialysis care is intricate and multiple factors may influence patient survival. Over 50 parameters may be monitored on a regular basis in providing kidney dialysis treatments. Understanding the collective role of these parameters in determining outcomes for an individual patient and administering individualized treatments allowing specific interventions is a challenge. Individual patient survival may depend on a complex interrelationship between multiple demographic and clinical parameters, medications, medical interventions, and the dialysis treatment prescription. In this research, data preprocessing, data transformations, and a data mining approach are used to elicit knowledge about the interaction between many of these measured parameters and patient survival. Two different data mining algorithms were employed for extracting knowledge in the form of decision rules. These rules were used by a decision-making algorithm, which predicts survival of new unseen patients. Important parameters identified by data mining are interpreted for their medical significance. The concepts introduced in this research have been applied and tested using data collected at four dialysis sites. The computational results are reported in the paper.
ER  - 
TY  - JOUR
T1  - A retinal vessel detection approach using convolution neural network with reinforcement sample learning strategy
A1  - Guo, Yanhui
A1  - Budak, Ümit
A1  - Vespa, Lucas J
A1  - Khorasani, Elham
A1  - Şengür, Abdulkadir
Y1  - 2018///
KW  -  Convolution neural network
KW  -  Image segmentation
KW  -  Retinal vessels
KW  - Computer-aided detection
JF  - Measurement
VL  - 125
SP  - 586
EP  - 591
DO  - https://doi.org/10.1016/j.measurement.2018.05.003
UR  - https://www.sciencedirect.com/science/article/pii/S0263224118303841
N2  - Computer-aided detection (CAD) provides an efficient way to assist doctors to interpret fundus images. In a CAD system, retinal vessel (RV) detection is an important step to identify the retinal disease regions automatically and accurately. However, RV detection is still a challenging problem due to variations in morphology of the vessels on a noisy background. In this paper, we formulate the detection task as a classification problem and solve it using a convolutional neural network (CNN) as a two-class classifier. The proposed model has 2 convolution layers, 2 pooling layers, 1 dropout layer and 1 loss layer. The contributions of the algorithm are two-fold. First, a new model of CNN is designed to automatically extract features and classify the retinal vessel region. Compared to traditional classification procedures, it is fully automatic and does not need preprocessing and manual extraction and description of features. Second, a novel reinforcement sample learning scheme is proposed to train the CNN with fewer iterations of epochs and less training time. The proposed model is trained and tested using the Digital Retinal Images for Vessel Extraction (DRIVE) and Structured Analysis of the Retina (STARE) data sets. The proposed CNN achieves better performance and significantly outperforms the state-of-the-art for automatic retinal vessel segmentation on the DRIVE data set with 91.99% accuracy and 0.9652 AUC score (area under ROC), and on the STARE data set with 92.20% accuracy and 0.9440 AUC value. We further compare our result with several state-of-the-art methods based on AUC values. The comparison shows that our proposal yields the second best AUC value. This demonstrates the efficiency of the proposed method without pre-processing and with high accuracy and training speed.
ER  - 
TY  - JOUR
T1  - What are the main patient safety concerns of healthcare stakeholders: a mixed-method study of Web-based text
A1  - Cho, Insook
A1  - Lee, Minyoung
A1  - Kim, Yeonjin
Y1  - 2020///
KW  -  Healthcare stakeholders
KW  -  Natural language processing
KW  -  Topic modeling
KW  -  Web-based text
KW  - Patient safety
JF  - International Journal of Medical Informatics
VL  - 140
SP  - 104162
EP  - 104162
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104162
UR  - https://www.sciencedirect.com/science/article/pii/S1386505620301350
N2  - Objectives
Various healthcare stakeholders define quality of care in different ways. Public policy could advocate all these concerns. This study was conducted to identify the main themes on patient safety of stakeholders expressed before and after the Patient Safety Act was enacted in Korea in 2015.
Design
Longitudinal observational study of the interests of healthcare stakeholders generated between January 2014 and September 2018.
Materials and methods
Text data were collected from 2,487 documents on 18 websites that were identified as representative healthcare stakeholder groups of consumers, providers, government, and researchers. A Korean natural language processing (NLP) package, manual review, and synonym dictionary were used for data preprocessing, and we adopted the unsupervised NLP method of probabilistic topic modeling and latent Dirichlet allocation. A linear trend analysis over time, a qualitative step involving two external experts, and original text reviews were performed to validate the identified topics.
Results
Forty-one topics were identified, and the most common concerns of stakeholders were institutional infection control as triggered by the Middle East respiratory syndrome outbreak in early 2015, and infusion-related infection from late 2017 until the middle of 2018. The other top-three concerns of the stakeholder groups were highly similar, while research topics were limited to the perceptions of providers and the activities and culture of hospitals. Five topics showed statistically significant increasing trends over time, while another five showed decreasing trends (both P < 0.05). In the qualitative step, we confirmed 35 themes and revised the other 6.
Conclusions
A common concern among stakeholders was hospital infection control, ranging from nosocomial infections to those brought in by family visiting patients. Government policies and systemic approaches to patient safety were highlighted by different stakeholders. Researchers were focused on hospital sociocultural factors at both the organizational and clinician levels. These identified concerns all should be advocated by the public health policy.
ER  - 
TY  - JOUR
T1  - An automated decision-support system for non-proliferative diabetic retinopathy disease based on MAs and HAs detection
A1  - Saleh, Marwan D
A1  - Eswaran, C
Y1  - 2012///
KW  -  -Maxima transform
KW  -  Centroid distance method
KW  -  Contrast enhancement
KW  -  Dark spots classification
KW  -  Dark spots segmentation
KW  -  Mathematical morphology
KW  -  Multilevel thresholding
KW  - Diabetic retinopathy
JF  - Computer Methods and Programs in Biomedicine
VL  - 108
IS  - 1
SP  - 186
EP  - 196
DO  - https://doi.org/10.1016/j.cmpb.2012.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S016926071200079X
N2  - Diabetic retinopathy (DR) has become a serious threat in our society, which causes 45% of the legal blindness in diabetes patients. Early detection as well as the periodic screening of DR helps in reducing the progress of this disease and in preventing the subsequent loss of visual capability. This paper provides an automated diagnosis system for DR integrated with a user-friendly interface. The grading of the severity level of DR is based on detecting and analyzing the early clinical signs associated with the disease, such as microaneurysms (MAs) and hemorrhages (HAs). The system extracts some retinal features, such as optic disc, fovea, and retinal tissue for easier segmentation of dark spot lesions in the fundus images. That is followed by the classification of the correctly segmented spots into MAs and HAs. Based on the number and location of MAs and HAs, the system quantifies the severity level of DR. A database of 98 color images is used in order to evaluate the performance of the developed system. From the experimental results, it is found that the proposed system achieves 84.31% and 87.53% values in terms of sensitivity for the detection of MAs and HAs respectively. In terms of specificity, the system achieves 93.63% and 95.08% values for the detection of MAs and HAs respectively. Also, the proposed system achieves 68.98% and 74.91% values in terms of kappa coefficient for the detection of MAs and HAs respectively. Moreover, the system yields sensitivity and specificity values of 89.47% and 95.65% for the classification of DR versus normal.
ER  - 
TY  - JOUR
T1  - Inferring food types through sensing and characterizing mastication dynamics
A1  - Wang, Shuangquan
A1  - Zhou, Gang
A1  - Guan, Jiexiong
A1  - Ma, Yongsen
A1  - Liu, Zhenming
A1  - Ren, Bin
A1  - Zhao, Hongyang
A1  - Watson, Amanda
A1  - Jung, Woosub
Y1  - 2021///
KW  -  Food properties
KW  -  Mastication dynamics
KW  -  Motion sensor
KW  -  Wearable device
KW  - Food type recognition
JF  - Smart Health
VL  - 20
SP  - 100191
EP  - 100191
DO  - https://doi.org/10.1016/j.smhl.2021.100191
UR  - https://www.sciencedirect.com/science/article/pii/S2352648321000131
N2  - Unhealthy dietary structure leads to the prevalence of some chronic diseases, such as obesity, diabetes, and heart disease. Automatic food type recognition helps nutritionists and medical professionals understand patients’ nutritional contents, provide accurate and personalized treatments, and evaluate therapeutic effects. Existing wearable sensor-based methods take advantage of microphone, electromyography (EMG), and piezoelectric sensors embedded in the wearable devices. However, these sensors are either easily impacted by ambient acoustic noise or intrusive and uncomfortable to wear. We observe that each type of food has its own intrinsic properties, such as hardness, elasticity, fracturability, adhesiveness, and size. Different food properties result in different mastication dynamics. In this paper, we present the first effort in using wearable motion sensors to sense mastication dynamics and infer food types accordingly. We specifically define six mastication dynamics parameters to represent these food properties. They are chewing speed, the number of chews, chewing time, chewing force, chewing cycle duration and skull vibration. We embed motion sensors in a headband and deploy the sensors on the temporalis muscles to sense mastication dynamics accurately and less intrusively. In addition, we extract 65 hand-crafted features from each chewing sequence to explicitly characterize the mastication dynamics using motion sensor data. A real-world evaluation dataset of 11 food categories (20 types of food in total) is collected from 15 human subjects. The average recognition accuracy of these 15 human subjects is 82.3%. The accuracy of a single human subject is up to 93.3%.
ER  - 
TY  - JOUR
T1  - Improved K-means algorithm based on density Canopy
A1  - Zhang, Geng
A1  - Zhang, Chengchang
A1  - Zhang, Huayu
Y1  - 2018///
KW  -  Initial seed
KW  -  K-means algorithm
KW  -  Optimal value K
KW  -  Weight product
KW  - Density Canopy
JF  - Knowledge-Based Systems
VL  - 145
SP  - 289
EP  - 297
DO  - https://doi.org/10.1016/j.knosys.2018.01.031
UR  - https://www.sciencedirect.com/science/article/pii/S0950705118300479
N2  - In order to improve the accuracy and stability of K-means algorithm and solve the problem of determining the most appropriate number K of clusters and best initial seeds, an improved K-means algorithm based on density Canopy is proposed. Firstly, the density of sample data sets, the average sample distance in clusters and the distance between clusters are calculated, choosing the density maximum sampling point as the first cluster center and removing the density cluster from the data sets. Defining the product of sample density, the reciprocal of the average distance between the samples in the cluster, and the distance between the clusters as weight product, the other initial seeds is determined by the maximum weight product in the remaining data sets until the data sets is empty. The density Canopy is used as the preprocessing procedure of K-means and its result is used as the cluster number and initial clustering center of K-means algorithm. Finally, the new algorithm is tested on some well-known data sets from UCI machine learning repository and on some simulated data sets with different proportions of noise samples. The simulation results show that the improved K-means algorithm based on density Canopy achieves better clustering results and is insensitive to noisy data compared to the traditional K-means algorithm, the Canopy-based K-means algorithm, Semi-supervised K-means++ algorithm and K-means-u* algorithm. The clustering accuracy of the proposed K-means algorithm based on density Canopy is improved by 30.7%, 6.1%, 5.3% and 3.7% on average on UCI data sets, and improved by 44.3%, 3.6%, 9.6% and 8.9% on the simulated data sets with noise signal respectively. With the increase of the noise ratio, the noise immunity of the new algorithm is more obvious, when the noise ratio reached 30%, the accuracy rate is improved 50% and 6% compared to the traditional K-means algorithm and the Canopy-based K-means algorithm.
ER  - 
TY  - JOUR
T1  - Performance analysis of cost-sensitive learning methods with application to imbalanced medical data
A1  - Mienye, Ibomoiye Domor
A1  - Sun, Yanxia
Y1  - 2021///
KW  -  Imbalanced classification
KW  -  Machine learning
KW  -  Medical diagnosis
KW  - Cost-sensitive learning
JF  - Informatics in Medicine Unlocked
VL  - 25
SP  - 100690
EP  - 100690
DO  - https://doi.org/10.1016/j.imu.2021.100690
UR  - https://www.sciencedirect.com/science/article/pii/S235291482100174X
N2  - Many real-world machine learning applications require building models using highly imbalanced datasets. Usually, in medical datasets, the healthy patients or samples are dominant, making them the majority class, while the sick patients are few, making them the minority class. Researchers have proposed numerous machine learning methods to predict medical diagnosis. Still, the class imbalance problem makes it difficult for classifiers to adequately learn and distinguish between the minority and majority classes. Cost-sensitive learning and resampling techniques are used to deal with the class imbalance problem. This research focuses on developing robust cost-sensitive classifiers by modifying the objective functions of some well-known algorithms, such as logistic regression, decision tree, extreme gradient boosting, and random forest, which are then used to efficiently predict medical diagnosis. Meanwhile, as opposed to resampling techniques, our approach does not alter the original data distribution. Firstly, we implement the standard versions of these algorithms to provide a baseline for performance comparison. Secondly, we develop their corresponding cost-sensitive algorithms. For the proposed approaches, it is not necessary to change the distribution of the original data as the modified algorithms consider the imbalanced class distribution during training, thereby resulting in more reliable performance than when the data is resampled. Four popular medical datasets, including the Pima Indians Diabetes, Haberman Breast Cancer, Cervical Cancer Risk Factors, and Chronic Kidney Disease datasets, are used in the experiments to validate the performance of the proposed approach. The experimental results show that the cost-sensitive methods yield superior performance compared to the standard algorithms.
ER  - 
TY  - JOUR
T1  - Evolutionary algorithm-based convolutional neural network for predicting heart diseases
A1  - Samir, Ali A
A1  - Rashwan, Abdullah R
A1  - Sallam, Karam M
A1  - Chakrabortty, Ripon K
A1  - Ryan, Michael J
A1  - Abohany, Amr A
Y1  - 2021///
KW  -  Convolution neural network
KW  -  Heart disease
KW  -  Meta-heuristic optimization
KW  - Deep learning
JF  - Computers & Industrial Engineering
VL  - 161
SP  - 107651
EP  - 107651
DO  - https://doi.org/10.1016/j.cie.2021.107651
UR  - https://www.sciencedirect.com/science/article/pii/S0360835221005556
N2  - Convolutional neural networks (CNNs) have been commonly used in medical decision support systems to predict and diagnose different diseases with good precision. CNNs are extremely successful in developing health support systems because of their ability to identify relationships and hidden patterns in healthcare data. One of the most important and useful applications of such systems is in the prediction of heart diseases by observing cardiac anomalies. Fundamentally, CNNs have multiple hyperparameters and various specific architectures, which are costly and impose challenges in selecting the best value among possible hyperparameters. In addition, CNNs are sensitive to their hyperparameter values which have a significant impact on the efficiency and behavior of CNN architectures. Thus, selecting the right set of parameters is of particular concern among practitioners. Consequently, this paper proposes a CNN-jSO approach for the prediction of heart (cardiac) diseases, in which the jSO optimization algorithm is employed to tune those CNN hyperparameters. The performance of the designed system is tested on the PhysioNet heart sound and Kaggle heartbeat sounds datasets. The proposed CNN-jSO is compared with other algorithms and shown to be better than them. The CNN-jSO system was implemented in Python and yielded 97.76% training accuracy and 94.12% testing accuracy.
ER  - 
TY  - JOUR
T1  - Classification of diabetic retinopathy using unlabeled data and knowledge distillation
A1  - Abbasi, Sajjad
A1  - Hajabdollahi, Mohsen
A1  - Khadivi, Pejman
A1  - Karimi, Nader
A1  - Roshandel, Roshanak
A1  - Shirani, Shahram
A1  - Samavi, Shadrokh
Y1  - 2021///
KW  -  Diabetic retinopathy
KW  -  Knowledge distillation
KW  -  Teacher-student model
KW  -  Transfer learning
KW  -  Unlabeled data
KW  - Convolutional neural networks (CNN)
JF  - Artificial Intelligence in Medicine
VL  - 121
SP  - 102176
EP  - 102176
DO  - https://doi.org/10.1016/j.artmed.2021.102176
UR  - https://www.sciencedirect.com/science/article/pii/S093336572100169X
N2  - Over the last decade, advances in Machine Learning and Artificial Intelligence have highlighted their potential as a diagnostic tool in the healthcare domain. Despite the widespread availability of medical images, their usefulness is severely hampered by a lack of access to labeled data. For example, while Convolutional Neural Networks (CNNs) have emerged as an essential analytical tool in image processing, their impact is curtailed by training limitations due to insufficient labeled data availability. Transfer Learning enables models developed for one task to be reused for a second task. Knowledge distillation enables transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and the two models' constraints need to be architecturally similar. Knowledge distillation addresses some of the shortcomings of transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed approach transfers the complete knowledge of a model to a new smaller one. Unlabeled data are used in an unsupervised manner to transfer the new smaller model's maximum amount of knowledge. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in classifying images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach effectively transfers knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that different small models' performance is improved significantly using unlabeled data and knowledge distillation.
ER  - 
TY  - JOUR
T1  - A systematic comparison of feature space effects on disease classifier performance for phenotype identification of five diseases
A1  - Kotfila, Christopher
A1  - Uzuner, Özlem
Y1  - 2015///
KW  -  Classification
KW  -  Natural language processing
KW  - Phenotyping
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - S92
EP  - S102
DO  - https://doi.org/10.1016/j.jbi.2015.07.016
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001562
N1  - Supplement: Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data
N2  - Automated phenotype identification plays a critical role in cohort selection and bioinformatics data mining. Natural Language Processing (NLP)-informed classification techniques can robustly identify phenotypes in unstructured medical notes. In this paper, we systematically assess the effect of naive, lexically normalized, and semantic feature spaces on classifier performance for obesity, atherosclerotic cardiovascular disease (CAD), hyperlipidemia, hypertension, and diabetes. We train support vector machines (SVMs) using individual feature spaces as well as combinations of these feature spaces on two small training corpora (730 and 790 documents) and a combined (1520 documents) training corpus. We assess the importance of feature spaces and training data size on SVM model performance. We show that inclusion of semantically-informed features does not statistically improve performance for these models. The addition of training data has weak effects of mixed statistical significance across disease classes suggesting larger corpora are not necessary to achieve relatively high performance with these models.
ER  - 
TY  - JOUR
T1  - Neural network and support vector machine for the prediction of chronic kidney disease: A comparative study
A1  - Almansour, Njoud Abdullah
A1  - Syed, Hajra Fahim
A1  - Khayat, Nuha Radwan
A1  - Altheeb, Rawan Kanaan
A1  - Juri, Renad Emad
A1  - Alhiyafi, Jamal
A1  - Alrashed, Saleh
A1  - Olatunji, Sunday O
Y1  - 2019///
KW  -  Artificial Neural Network (ANN)
KW  -  Chronic Kidney Disease (CKD)
KW  -  Support Vector Machine (SVM)
KW  - Machine learning
JF  - Computers in Biology and Medicine
VL  - 109
SP  - 101
EP  - 111
DO  - https://doi.org/10.1016/j.compbiomed.2019.04.017
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519301258
N2  - This paper aims to assist in the prevention of Chronic Kidney Disease (CKD) by utilizing machine learning techniques to diagnose CKD at an early stage. Kidney diseases are disorders that disrupt the normal function of the kidney. As the percentage of patients affected by CKD is significantly increasing, effective prediction procedures should be considered. In this paper, we focus on applying different machine learning classification algorithms to a dataset of 400 patients and 24 attributes related to diagnosis of chronic kidney disease. The classification techniques used in this study include Artificial Neural Network (ANN) and Support Vector Machine (SVM). To perform experiments, all missing values in the dataset were replaced by the mean of the corresponding attributes. Then, the optimized parameters for the Artificial Neural Network (ANN) and Support Vector Machine (SVM) techniques were determined by tuning the parameters and performing several experiments. The final models of the two proposed techniques were developed using the best-obtained parameters and features. The empirical results from the experiments indicated that ANN performed better than SVM, with accuracies of 99.75% and 97.75%, respectively, indicating that the outcome of this study is very promising.
ER  - 
TY  - JOUR
T1  - An evolutionary parallel multiobjective feature selection framework
A1  - Kiziloz, Hakan Ezgi
A1  - Deniz, Ayça
Y1  - 2021///
KW  -  Evolutionary computation
KW  -  Multiobjective optimization
KW  -  Parallel processing
KW  - Feature selection
JF  - Computers & Industrial Engineering
VL  - 159
SP  - 107481
EP  - 107481
DO  - https://doi.org/10.1016/j.cie.2021.107481
UR  - https://www.sciencedirect.com/science/article/pii/S0360835221003855
N2  - Feature selection has become an indispensable preprocessing step in data mining problems as high amount of data become prevalent with the advances in technology. The objective of feature selection is twofold: reducing data amount and improving learning performance. In this study, we leverage the multi-core nature of a regular PC to build a robust framework for feature selection. This framework executes the feature selection algorithm on four processors, in parallel. As per the No Free Lunch Theorem, we facilitate 40 different execution settings for the processors by employing two multiobjective selection algorithms, four initial population generation methods, and five machine learning techniques. Besides, we introduce six setting selection schemes to decide the most fruitful setting for each processor. We carry out extensive experiments on 11 UCI benchmark datasets and analyze the results with statistical tests. Finally, we compare our proposed method with state-of-the-art studies and record remarkable improvement in terms of maximum accuracy.
ER  - 
TY  - JOUR
T1  - Sparse elastic net multi-label rank support vector machine with pinball loss and its applications
A1  - Wang, Hongmei
A1  - Xu, Yitian
Y1  - 2021///
KW  -  Elastic net
KW  -  Elimination rule
KW  -  Pinball loss
KW  -  Rank support vector machine
KW  - Multi-label learning
JF  - Applied Soft Computing
VL  - 104
SP  - 107232
EP  - 107232
DO  - https://doi.org/10.1016/j.asoc.2021.107232
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621001551
N2  - Multi-label rank support vector machine (RankSVM) is an effective technique to deal with multi-label classification problems, which has been widely used in various fields. However, it is sensitive to noise points and cannot delete redundant features for high dimensional problems. Therefore, to address the above two limitations, a sparse elastic net multi-label RankSVM with pinball loss (pin-ENR) is first proposed in this paper. On the one hand, pinball loss is employed to enhance the robustness. On the other hand, it adopts the sparse elastic net regularization, so that it can do variable selection. However, it still has challenges for large-scale problems with a huge number of features, samples, and labels. Therefore, motivated by the sparsity of pin-ENR, a safe simultaneous feature and label-pair elimination rule is further constructed for accelerating pin-ENR, which is termed as FLER-pin-ENR. Its main idea is to delete a large number of inactive features and label-pairs simultaneously before training without sacrificing accuracy. Numerical experiments on four synthetic and seven benchmark datasets demonstrate the feasibility and validity. Moreover, we apply our FLER-pin-ENR to the diagnosis of diabetes complications and the natural scene image classification problems, which further verifies the practicability of our proposed method.
ER  - 
TY  - JOUR
T1  - Analysis of main risk factors causing stroke in Shanxi Province based on machine learning models
A1  - Liu, Junjie
A1  - Sun, Yiyang
A1  - Ma, Jing
A1  - Tu, Jiachen
A1  - Deng, Yuhui
A1  - He, Ping
A1  - Li, Rongshan
A1  - Hu, Fengyun
A1  - Huang, Huaxiong
A1  - Zhou, Xiaoshuang
A1  - Xu, Shixin
Y1  - 2021///
KW  -  Machine learning
KW  -  Risk factor ranking
KW  -  SHAP value
KW  - Stroke
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100712
EP  - 100712
DO  - https://doi.org/10.1016/j.imu.2021.100712
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821001933
N2  - Background:
In China, stroke has been the first leading cause of death in recent years. It is a major cause of long-term physical and cognitive impairment, which bring great pressure on the National Public Health System. On the other hand, China is a big country, evaluation of the risk of getting stroke is important for the prevention and treatment of stroke in China.
Methods:
A data set with 2000 hospitalized stroke patients in 2018 and 27583 residents during the year 2017 to 2020 is analyzed in this study. With the cleaned data, three models on stroke risk levels are built by using machine learning methods. The importance of “8+2” factors from China National Stroke Prevention Project (CSPP) is evaluated via decision tree and random forest models. The importance of more detailed features and their SHAP22SHAP: SHapley Additive exPlanations. values are evaluated and ranked via random forest model. Furthermore, a logistic regression model is applied to evaluate the probability of getting stroke for different risk levels.
Results:
Among all “8+2” risk factors of getting stroke, the decision tree model reveals that top three factors are Hypertension (0.4995),33The value of importance. Physical Inactivity (0.08486) and Diabetes Mellitus (0.07889), and the random forest model shows that top three factors are Hypertension (0.3966), Hyperlipidemia (0.1229) and Physical Inactivity (0.1146). In addition to “8+2” factors the importance of features for lifestyle information, demographic information and medical measurement are evaluated via random forest model. It shows that top five features are Systolic Blood Pressure (SBP) (0.3670), Diastolic Blood Pressure (DBP) (0.1541), Physical Inactivity (0.0904), Body Mass Index (BMI) (0.0721) and Fasting Blood Glucose (FBG)(0.0531). SHAP values show that DBP, Physical Inactivity, SBP, BMI, Smoking, FBG, and Triglyceride(TG) are positively correlated to the risk of getting stroke. High-density Lipoprotein (HDL) is negatively correlated to the risk of getting stroke. Combining with the data of 2000 hospitalized stroke patients, the logistic regression model shows that the average probabilities of getting stroke are 7.20%±0.55%44Confidence Interval with confidence level 95%. for the low-risk level patients, 19.02%±0.94% for the medium-risk level patients and 83.89%±0.97% for the high-risk level patients.
Conclusion:
Based on the census data from Shanxi Province, we investigate stroke risk factors and their ranking. It shows that Hypertension, Physical Inactivity, and Overweight are ranked as the top three high stroke risk factors in Shanxi. The probability of getting a stroke is also estimated through our interpretable machine learning methods.
ER  - 
TY  - JOUR
T1  - EHR2CCAS: A framework for mapping EHR to disease knowledge presenting causal chain of disorders – chronic kidney disease example
A1  - Ma, Xiaojun
A1  - Imai, Takeshi
A1  - Shinohara, Emiko
A1  - Kasai, Satoshi
A1  - Kato, Kosuke
A1  - Kagawa, Rina
A1  - Ohe, Kazuhiko
Y1  - 2021///
KW  -  Chronic kidney disease
KW  -  Electronic health record
KW  -  Machine learning
KW  -  Natural language processing
KW  - Knowledge base
JF  - Journal of Biomedical Informatics
VL  - 115
SP  - 103692
EP  - 103692
DO  - https://doi.org/10.1016/j.jbi.2021.103692
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000216
N2  - Objective
The goal of this work was to capture diseases in patients by comprehending the fine-grained medical conditions and disease progression manifested by transitions in medical conditions. We realize this by introducing our earlier work on a state-of-the-art knowledge presentation, which defines a disease as a causal chain of abnormal states (CCAS). Here, we propose a framework, EHR2CCAS, for constructing a system to map electronic health record (EHR) data to CCAS.
Materials and methods
EHR2CCAS is a framework consisting of modules that access heterogeneous EHR to estimate the presence of abnormal states in a CCAS for a patient in a given time window. EHR2CCAS applies expert-driven (rule-based) and data-driven (machine learning) methods to identify abnormal states from structured and unstructured EHR data. It features data-driven approaches for unlocking clinical texts and imputations based on the EHR temporal properties and the causal CCAS structure. This study presents the CCAS of chronic kidney disease as an example. A mapping system between the EHR from the University of Tokyo Hospital and CCAS of chronic kidney disease was constructed and evaluated against expert annotation.
Results
The system achieved high prediction performance in identifying abnormal states that had strong agreement among annotators. Our handling of narrative varieties in texts and our imputation of the presence of an abnormal state markedly improved the prediction performance. EHR2CCAS presents patient data describing the temporal presence of abnormal states in CCAS, which is useful in individual disease progression management. Further analysis of the differentiation of transition among abnormal states outputted by EHR2CCAS can contribute to detecting disease subtypes.
Conclusion
This work represents the first step toward combining disease knowledge and EHR to extract abnormality related to a disease defined as fine-grained abnormal states and transitions among them. This can aid in disease progression management and deep phenotyping.
ER  - 
TY  - JOUR
T1  - Phe2vec: Automated disease phenotyping based on unsupervised embeddings from electronic health records
A1  - De Freitas, Jessica K
A1  - Johnson, Kipp W
A1  - Golden, Eddye
A1  - Nadkarni, Girish N
A1  - Dudley, Joel T
A1  - Bottinger, Erwin P
A1  - Glicksberg, Benjamin S
A1  - Miotto, Riccardo
Y1  - 2021///
KW  -  electronic medical records
KW  -  informatics
KW  -  machine learning
KW  -  phenotyping
KW  -  unsupervised learning
KW  - electronic health records
JF  - Patterns
VL  - 2
IS  - 9
SP  - 100337
EP  - 100337
DO  - https://doi.org/10.1016/j.patter.2021.100337
UR  - https://www.sciencedirect.com/science/article/pii/S2666389921001859
N2  - Summary
Robust phenotyping of patients from electronic health records (EHRs) at scale is a challenge in clinical informatics. Here, we introduce Phe2vec, an automated framework for disease phenotyping from EHRs based on unsupervised learning and assess its effectiveness against standard rule-based algorithms from Phenotype KnowledgeBase (PheKB). Phe2vec is based on pre-computing embeddings of medical concepts and patients' clinical history. Disease phenotypes are then derived from a seed concept and its neighbors in the embedding space. Patients are linked to a disease if their embedded representation is close to the disease phenotype. Comparing Phe2vec and PheKB cohorts head-to-head using chart review, Phe2vec performed on par or better in nine out of ten diseases. Differently from other approaches, it can scale to any condition and was validated against widely adopted expert-based standards. Phe2vec aims to optimize clinical informatics research by augmenting current frameworks to characterize patients by condition and derive reliable disease cohorts.
ER  - 
TY  - JOUR
T1  - A new discretization algorithm based on range coefficient of dispersion and skewness for neural networks classifier
A1  - Augasta, M Gethsiyal
A1  - Kathirvalavakumar, T
Y1  - 2012///
KW  -  Classification
KW  -  Conjugate gradient training algorithm
KW  -  Data mining
KW  -  Multilayer feedforward neural network
KW  -  Preprocessing
KW  - Supervised discretization
JF  - Applied Soft Computing
VL  - 12
IS  - 2
SP  - 619
EP  - 625
DO  - https://doi.org/10.1016/j.asoc.2011.11.001
UR  - https://www.sciencedirect.com/science/article/pii/S156849461100425X
N2  - In this paper we propose a new static, global, supervised, incremental and bottom-up discretization algorithm based on coefficient of dispersion and skewness of data range. It automates the discretization process by introducing the number of intervals and stopping criterion. The results obtained using this discretization algorithm show that the discretization scheme generated by the algorithm almost has minimum number of intervals and requires smallest discretization time. The feedforward neural network with conjugate gradient training algorithm is used to compute the accuracy of classification from the data discretized by this algorithm. The efficiency of the proposed algorithm is shown in terms of better discretization scheme and better accuracy of classification by implementing it on six different real data sets.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy detection through convolutional neural networks with synaptic metaplasticity
A1  - Vives-Boix, Víctor
A1  - Ruiz-Fernández, Daniel
Y1  - 2021///
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Image processing
KW  -  Metaplasticity
KW  - Convolutional neural networks
JF  - Computer Methods and Programs in Biomedicine
VL  - 206
SP  - 106094
EP  - 106094
DO  - https://doi.org/10.1016/j.cmpb.2021.106094
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721001693
N2  - Background and objectives: Diabetic retinopathy is a type of diabetes that causes vascular changes that can lead to blindness. The ravages of this disease cannot be reversed, so early detection is essential. This work presents an automated method for early detection of this disease using fundus colored images. Methods: A bio-inspired approach is proposed on synaptic metaplasticity in convolutional neural networks. This biological phenomenon is known to directly interfere in both learning and memory by reinforcing less common occurrences during the learning process. Synaptic metaplasticity has been included in the backpropagation stage of a convolution operation for every convolutional layer. Results: The proposed method has been evaluated by using a public small diabetic retinopathy dataset from Kaggle with four award-winning convolutional neural network architectures. Results show that convolutional neural network architectures including synaptic metaplasticity improve both learning rate and accuracy. Furthermore, obtained results outperform other methods in current literature, even using smaller datasets for training. Best results have been obtained for the InceptionV3 architecture with synaptic metaplasticity with a 95.56% accuracy, 94.24% F1-score, 98.9% precision and 90% recall, using 3662 images for training. Conclusions: Convolutional neural networks with synaptic metaplasticity are suitable for early detection of diabetic retinopathy due to their fast convergence rate, training simplicity and high performance.
ER  - 
TY  - JOUR
T1  - Retinal vessel segmentation employing ANN technique by Gabor and moment invariants-based features
A1  - Franklin, S Wilfred
A1  - Rajan, S Edward
Y1  - 2014///
KW  -  Artificial neural networks
KW  -  Retinal images
KW  -  Retinal vasculature
KW  -  Retinal vessel segmentation
KW  - Diabetic retinopathy
JF  - Applied Soft Computing
VL  - 22
SP  - 94
EP  - 100
DO  - https://doi.org/10.1016/j.asoc.2014.04.024
UR  - https://www.sciencedirect.com/science/article/pii/S1568494614001896
N2  - Diabetic retinopathy (DR) is the major ophthalmic pathological cause for loss of eye sight due to changes in blood vessel structure. The retinal blood vessel morphology helps to identify the successive stages of a number of sight threatening diseases and thereby paves a way to classify its severity. This paper presents an automated retinal vessel segmentation technique using neural network, which can be used in computer analysis of retinal images, e.g., in automated screening for diabetic retinopathy. Furthermore, the algorithm proposed in this paper can be used for the analysis of vascular structures of the human retina. Changes in retinal vasculature are one of the main symptoms of diseases like hypertension and diabetes mellitus. Since the size of typical retinal vessel is only a few pixels wide, it is critical to obtain precise measurements of vascular width using automated retinal image analysis. This method segments each image pixel as vessel or nonvessel, which in turn, used for automatic recognition of the vasculature in retinal images. Retinal blood vessels are identified by means of a multilayer perceptron neural network, for which the inputs are derived from the Gabor and moment invariants-based features. Back propagation algorithm, which provides an efficient technique to change the weights in a feed forward network is utilized in our method. The performance of our technique is evaluated and tested on publicly available DRIVE database and we have obtained illustrative vessel segmentation results for those images.
ER  - 
TY  - JOUR
T1  - A new approach to integrating patient-generated data with expert knowledge for personalized goal setting: A pilot study
A1  - Burgermaster, Marissa
A1  - Son, Jung H
A1  - Davidson, Patricia G
A1  - Smaldone, Arlene M
A1  - Kuperman, Gilad
A1  - Feller, Daniel J
A1  - Burt, Katherine Gardner
A1  - Levine, Matthew E
A1  - Albers, David J
A1  - Weng, Chunhua
A1  - Mamykina, Lena
Y1  - 2020///
KW  -  Expert system
KW  -  Knowledge representation
KW  -  Personalized nutrition
KW  -  Suggestion system
KW  - Patient-generated health data
JF  - International Journal of Medical Informatics
VL  - 139
SP  - 104158
EP  - 104158
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104158
UR  - https://www.sciencedirect.com/science/article/pii/S138650561931216X
N2  - Introduction
Self-monitoring technologies produce patient-generated data that could be leveraged to personalize nutritional goal setting to improve population health; however, most computational approaches are limited when applied to individual-level personalization with sparse and irregular self-monitoring data. We applied informatics methods from expert suggestion systems to a challenging clinical problem: generating personalized nutrition goals from patient-generated diet and blood glucose data.
Materials and methods
We applied qualitative process coding and decision tree modeling to understand how registered dietitians translate patient-generated data into recommendations for dietary self-management of diabetes (i.e., knowledge model). We encoded this process in a set of functions that take diet and blood glucose data as an input and output diet recommendations (i.e., inference engine). Dietitians assessed face validity. Using four patient datasets, we compared our inference engine’s output to clinical narratives and gold standards developed by expert clinicians.
Results
To dietitians, the knowledge model represented how recommendations from patient data are made. Inference engine recommendations were 63 % consistent with the gold standard (range = 42 %–75 %) and 74 % consistent with narrative clinical observations (range = 63 %–83 %).
Discussion
Qualitative modeling and automating how dietitians reason over patient data resulted in a knowledge model representing clinical knowledge. However, our knowledge model was less consistent with gold standard than narrative clinical recommendations, raising questions about how best to evaluate approaches that integrate patient-generated data with expert knowledge.
Conclusion
New informatics approaches that integrate data-driven methods with expert decision making for personalized goal setting, such as the knowledge base and inference engine presented here, demonstrate the potential to extend the reach of patient-generated data by synthesizing it with clinical knowledge. However, important questions remain about the strengths and weaknesses of computer algorithms developed to discern signal from patient-generated data compared to human experts.
ER  - 
TY  - JOUR
T1  - Shape-based filter for micro-aneurysm detection
A1  - Zhang, Xinpeng
A1  - Xiao, Zhitao
A1  - Zhang, Fang
A1  - Ogunbona, Philip O
A1  - Xi, Jiangtao
A1  - Tong, Jun
Y1  - 2020///
KW  -  Color retinal image
KW  -  Diabetic retinopathy
KW  -  Local gradient analysis
KW  -  Micro-aneurysm
KW  - Circular bilateral Gabor filter
JF  - Computers & Electrical Engineering
VL  - 84
SP  - 106620
EP  - 106620
DO  - https://doi.org/10.1016/j.compeleceng.2020.106620
UR  - https://www.sciencedirect.com/science/article/pii/S0045790620304754
N2  - Automatic detection of micro-aneurysm in color retinal image is important for early screening and diagnosis of diabetic retinopathy. In this paper, a new method is proposed for micro-aneurysm detection based on circular bilateral Gabor filtering. Firstly, a circular bilateral Gabor filter is developed to extract micro-aneurysm candidates. Secondly, false positives are reduced by eliminating small vessels through a process involving local gradient analysis. The proposed method is tested on the retinal images from the Retinopathy Online Challenge database and Tianjin Medical University Metabolic Diseases Hospital. Evaluation results at both image and lesion level demonstrate the efficacy of the proposed method in detecting micro-aneurysm accurately.
ER  - 
TY  - JOUR
T1  - Multi-proportion channel ensemble model for retinal vessel segmentation
A1  - Tang, Peng
A1  - Liang, Qiaokang
A1  - Yan, Xintong
A1  - Zhang, Dan
A1  - Coppola, Gianmarc
A1  - Sun, Wei
Y1  - 2019///
KW  -  Deep learning
KW  -  Multi-proportion channel ensemble model
KW  -  Retinal fundus images
KW  -  Retinal vessel segmentation
KW  - Computer-aided diagnosis
JF  - Computers in Biology and Medicine
VL  - 111
SP  - 103352
EP  - 103352
DO  - https://doi.org/10.1016/j.compbiomed.2019.103352
UR  - https://www.sciencedirect.com/science/article/pii/S001048251930229X
N2  - Objective
A novel supervised method that is based on the Multi-Proportion Channel Ensemble Model (MPC-EM) is proposed to obtain more vessel details with reduced computational complexity.
Methods
Existing Retinal Vessel Segmentation (RVS) algorithms only work using the single G channel (Green Channel) of fundus images because that channel normally contains the most details with the least noise, while the red and blue channels are usually saturated and noisy. However, we find that the images that are composed of the αG-channel and (1-α) R-channel (Red Channel) with different values of α produce multiple particular global features. This enables the model to detect more local vessel details in fundus images. Therefore, we provide a detailed description and evaluation of the segmentation approach based on the MPC-EM for the RVS. The segmentation approach consists of five identical submodels. Each submodel can capture various vessel details by being trained using different composition images. These probabilistic maps that are produced by five submodels are averaged to achieve the final refined segmentation results.
Results
The proposed approach is evaluated using 4 well-established datasets, i.e., DRIVE, STARE, HRF and CHASE_DB1, with accuracies of 95.74%, 96.95%, 96.31%, and 96.54%, respectively. Additionally, quantitative comparisons with other existing methods and cross-training results are included.
Conclusion
The segmentation results showed that the proposed algorithm based on the MPC-EM with simple submodels can achieve state-of-the-art accuracy with reduced computational complexity.
Significance
Compared with other existing methods that are trained using only the G channel and raw images, the proposed approach based on the MPC-EM, submodels of which are trained using different proportional compositions of R and G channels, obtains better segmentation accuracy and robustness. Additionally, the experimental results show that the R channel of fundus images can also produce performance gains for RVS.
ER  - 
TY  - JOUR
T1  - Implementation and evaluation of a multivariate abstraction-based, interval-based dynamic time-warping method as a similarity measure for longitudinal medical records
A1  - Lion, Matan
A1  - Shahar, Yuval
Y1  - 2021///
KW  -  Classification
KW  -  Interval-based similarity measure
KW  -  Knowledge representation
KW  -  Temporal abstraction
KW  -  Temporal data mining
KW  - Dynamic time warping
JF  - Journal of Biomedical Informatics
VL  - 123
SP  - 103919
EP  - 103919
DO  - https://doi.org/10.1016/j.jbi.2021.103919
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421002483
N2  - Objectives
A common prerequisite for tasks such as classification, prediction, clustering and retrieval of longitudinal medical records is a clinically meaningful similarity measure that considers both [multiple] variable (concept) values and their time. Currently, most similarity measures focus on raw, time-stamped data as these are stored in a medical record. However, clinicians think in terms of clinically meaningful temporal abstractions, such as “decreasing renal functions”, enabling them to ignore minor time and value variations and focus on similarities among the clinical trajectories of different patients. Our objective was to define an abstraction- and interval-based methodology for matching longitudinal, multivariate medical records, and rigorously assess its value, versus the option of using just the raw, time-stamped data.
Methods
We have developed a new methodology for determination of the relative distance between a pair of longitudinal records, by extending the known dynamic time warping (DTW) method into an interval-based dynamic time warping (iDTW) methodology. The iDTW methodology includes (A): A three-steps interval-based representation (iRep) method: [1] abstracting the raw, time-stamped data of the longitudinal records into clinically meaningful interval-based abstractions, using a domain-specific knowledge base, [2] scoping the period of comparison of the records, [3] creating from the intervals a symbolic time series, by partitioning them into a predetermined temporal granularity; (B) An interval-based matching (iMatch) method to match each relevant pair of multivariate longitudinal records, each represented as multiple series of short symbolic intervals in the determined temporal granularity, using a modified DTW version.
Evaluation
Three classification or prediction tasks were defined: (1) classifying 161 records of oncology patients as having had autologous versus allogenic bone-marrow transplantation; (2) classifying the longitudinal records of 125 hepatitis patients as having B or C hepatitis; and (3) predicting micro- or macro-albuminuria in the second year, for 151 diabetes patients who were followed for five years. The raw, time-stamped, multivariate data within each medical record, for one, two, or three concepts out of four or five concepts judged as relevant in each medical domain, were abstracted into clinically meaningful intervals using the Knowledge-Based Temporal-Abstraction method, using previously acquired knowledge. We focused on two temporal-abstraction types: (1) State abstractions, which discretize a concept’s raw value into a predetermined range (e.g., LOW or HIGH Hemoglobin); and (2) Gradient abstractions, which indicate the trend of the concept’s value (e.g., INCREASING, DECREASING Hemoglobin value). We created all of the combinations of either uni-dimensional (State or Gradient) or multi-dimensional (State and Gradient) abstractions, of all of the concepts used. Classification of a record was determined by using a majority of the k-Nearest-Neighbors (KNN) of the given record, k ranging over the odd numbers (to break ties) from 1 to N, N being the size of the training set. We have experimented with all possible configurations of the parameters that our method uses. Overall, a total of 75,936 experiments were performed: 33,600 in the Oncology domain, 28,800 in the Hepatitis domain, and 13,536 in the Diabetes domain. Each experiment involved the performance of a 10-fold Cross Validation to compute the mean performance of a particular iDTW method-configuration set of settings, for a specific subset of one, two, or three concepts out of all of the domain-specific concepts relevant to the classification or prediction task on which the experiment focuses. We measured for each such experimental combination the Area Under the Curve (AUC) and the optimal Specificity/Sensitivity ratio using Youden's Index. We then aggregated the experiments by the types of unidimensional or multidimensional abstractions used in them (including the use of only raw concepts as a special case); for example, two state abstractions of different concepts, and one gradient abstraction of a third concept. We compared the mean AUC when using each such feature representation, or combination of abstractions, across all possible method-setting configurations, to the mean AUC when using as a feature representation, for the same task, only raw concepts, also across all possible method-setting configurations. Finally, we applied a paired t-test, to determine whether the mean difference between the accuracy of each temporal-abstraction representation, across all concept and configuration combinations, and the respective raw-concept combinations, across all concept subset and configuration combinations, is significant (P < 0.05).
Results
The mean performance of the classification and prediction tasks when using, as a feature representation, the various temporal-abstraction combinations, was significantly higher than that performance when using only raw data. Furthermore, in each domain and task, there existed at least one representation using interval-based abstractions whose use led, on average (over all concept subset combinations and method configurations) to a significantly better performance than the use of only subsets of the raw time-stamped data. In seven of nine combinations of domain type (out of three) and number of concepts used (one, two, or three), the variance of the AUCs (for all representations and configurations) was considerably higher across all raw-concept subsets, compared to all abstract combinations. Increasing the number of features used by the matching task enhanced performance. Using multi-dimensional abstractions of the same concept further enhanced the performance. When using only raw data, increasing the number of neighbors monotonically increased the mean performance (over all concept combinations and method configurations) until reaching an optimal saddle-point aroundN; when using abstractions, however, optimal mean performance was often reached after matching only five nearest neighbors.
Conclusions
Using multivariate and multidimensional interval-based, abstraction-based similarity measures is feasible, and consistently and significantly improved the mean classification and prediction performance in time-oriented domains, using DTW-inspired methods, compared to the use of only raw, time-stamped data. It also made the KNN classification more effective. Nevertheless, although the mean performance for the abstract representations was higher than the mean performance when using only raw-data concepts, the actual optimal classification performance in each domain and task depends on the choice of the specific raw or abstract concepts used as features.
ER  - 
TY  - JOUR
T1  - Retinal blood vessel segmentation using pixel-based feature vector
A1  - Toptaş, Buket
A1  - Hanbay, Davut
Y1  - 2021///
KW  -  Feature extraction
KW  -  Image segmentation
KW  -  Retinal blood vessel segmentation
KW  - Biomedical imaging
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103053
EP  - 103053
DO  - https://doi.org/10.1016/j.bspc.2021.103053
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421006509
N2  - A lot of important disease information can be accessed by performing retinal blood vessel analysis on fundus images. Diabetic retinopathy is one of the diseases understood by retinal blood vessel analysis. If this disease is detected at an early stage, vision loss can be prevented. In this paper, a method that performs retinal blood vessel analysis with classical methods is proposed. In this proposed system, pixel-based feature extraction is performed. Five different feature groups are used for feature extraction. These feature groups are edge detection, morphological, statistical, gradient, and Hessian matrix. An 18-D feature vector is created for each pixel. This feature vector is given to the artificial neural network for training. Using test images, the system is tested on two publicly available datasets. Sensitivity, Specificity, and Accuracy performance measures were used as success measures. The similarity index between the segmented image and the ground truth is measure using Dice and Jaccard. The accuracy of the system was measured as 96.18% for DRIVE and 94.56% for STARE, respectively. Experimental results show that the proposed algorithm achieves satisfactory results. This method can be used as an automated retinal blood vessel segmenting system.
ER  - 
TY  - JOUR
T1  - Automatic vessel detection by means of brightness profile characterization in OCT images
A1  - de Moura, Joaquim
A1  - Novo, Jorge
A1  - Rouco, José
A1  - Penedo, Manuel G
A1  - Ortega, Marcos
Y1  - 2017///
KW  -  Optical Coherence Tomography
KW  -  retinal imaging
KW  -  vessel detection
KW  - Computer-aided diagnosis
JF  - Procedia Computer Science
VL  - 112
SP  - 980
EP  - 988
DO  - https://doi.org/10.1016/j.procs.2017.08.142
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917314990
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France
N2  - Optical Coherence Tomography (OCT) is a well-established medical imaging technique that allows the analysis of the eye fundus characteristics in real time. These images enable the experts to make a clinical evaluation of the retinal vasculature, whose morphology provides relevant information for diseases like diabetes, hypertension or arteriosclerosis. In this paper, we present a novel proposal for the automatic vasculature identification in retinal OCT images. To achieve this, we analyse the intensity profiles between representative retinal layers, previously segmented. Then, two statistical models are generated using representative samples of vessel and non-vessel profiles. The analysis of both statistical models let us optimize the discrimination of both cathegories that is used, finally, to identify the vessel locations. The proposed method was adjusted and validated using 256 OCT images, including 1274 vascular structures that were labelled by an expert clinician. Satisfactory results were provided as a precision of 94.55% and a recall of 90.25% were obtained, respectively. The method facilitates the doctors’ work allowing better analysis and treatments of vascular diseases.
ER  - 
TY  - JOUR
T1  - Improving dense conditional random field for retinal vessel segmentation by discriminative feature learning and thin-vessel enhancement
A1  - Zhou, Lei
A1  - Yu, Qi
A1  - Xu, Xun
A1  - Gu, Yun
A1  - Yang, Jie
Y1  - 2017///
KW  -  Convolutional neural network
KW  -  Dense conditional random field
KW  -  Feature learning
KW  -  Image enhancement
KW  - Retinal vessel segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 148
SP  - 13
EP  - 25
DO  - https://doi.org/10.1016/j.cmpb.2017.06.016
UR  - https://www.sciencedirect.com/science/article/pii/S0169260716307106
N2  - Background and objectives
As retinal vessels in color fundus images are thin and elongated structures, standard pairwise based random fields, which always suffer the “shrinking bias” problem, are not competent for such segmentation task. Recently, a dense conditional random field (CRF) model has been successfully used in retinal vessel segmentation. Its corresponding energy function is formulated as a linear combination of several unary features and a pairwise term. However, the hand-crafted unary features can be suboptimal in terms of linear models. Here we propose to learn discriminative unary features and enhance thin vessels for pairwise potentials to further improve the segmentation performance.
Methods
Our proposed method comprises four main steps: firstly, image preprocessing is applied to eliminate the strong edges around the field of view (FOV) and normalize the luminosity and contrast inside FOV; secondly, a convolutional neural network (CNN) is properly trained to generate discriminative features for linear models; thirdly, a combo of filters are applied to enhance thin vessels, reducing the intensity difference between thin and wide vessels; fourthly, by taking the discriminative features for unary potentials and the thin-vessel enhanced image for pairwise potentials, we adopt the dense CRF model to achieve the final retinal vessel segmentation. The segmentation performance is evaluated on four public datasets (i.e. DRIVE, STARE, CHASEDB1 and HRF).
Results
Experimental results show that our proposed method improves the performance of the dense CRF model and outperforms other methods when evaluated in terms of F1-score, Matthews correlation coefficient (MCC) and G-mean, three effective metrics for the evaluation of imbalanced binary classification. Specifically, the F1-score, MCC and G-mean are 0.7942, 0.7656, 0.8835 for the DRIVE dataset respectively; 0.8017, 0.7830, 0.8859 for STARE respectively; 0.7644, 0.7398, 0.8579 for CHASEDB1 respectively; and 0.7627, 0.7402, 0.8812 for HRF respectively.
Conclusions
The discriminative features learned in CNNs are more effective than hand-crafted ones. Our proposed method performs well in retinal vessel segmentation. The architecture of our method is trainable and can be integrated into computer-aided diagnostic (CAD) systems in the future.
ER  - 
TY  - JOUR
T1  - Predicting mortality risk in patients with COVID-19 using machine learning to help medical decision-making
A1  - Pourhomayoun, Mohammad
A1  - Shakibi, Mahdi
Y1  - 2021///
KW  -  Coronavirus
KW  -  Data analytics
KW  -  Machine learning
KW  -  Predictive analytics
KW  - COVID-19
JF  - Smart Health
VL  - 20
SP  - 100178
EP  - 100178
DO  - https://doi.org/10.1016/j.smhl.2020.100178
UR  - https://www.sciencedirect.com/science/article/pii/S2352648320300702
N2  - In the wake of COVID-19 disease, caused by the SARS-CoV-2 virus, we designed and developed a predictive model based on Artificial Intelligence (AI) and Machine Learning algorithms to determine the health risk and predict the mortality risk of patients with COVID-19. In this study, we used a dataset of more than 2,670,000 laboratory-confirmed COVID-19 patients from 146 countries around the world including 307,382 labeled samples. This study proposes an AI model to help hospitals and medical facilities decide who needs to get attention first, who has higher priority to be hospitalized, triage patients when the system is overwhelmed by overcrowding, and eliminate delays in providing the necessary care. The results demonstrate 89.98% overall accuracy in predicting the mortality rate. We used several machine learning algorithms including Support Vector Machine (SVM), Artificial Neural Networks, Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbor (KNN) to predict the mortality rate in patients with COVID-19. In this study, the most alarming symptoms and features were also identified. Finally, we used a separate dataset of COVID-19 patients to evaluate our developed model accuracy, and used confusion matrix to make an in-depth analysis of our classifiers and calculate the sensitivity and specificity of our model.
ER  - 
TY  - JOUR
T1  - Design of hospital IoT system and drug intervention in patients with acute myocardial infarction
A1  - Zhao, Ying
A1  - Zhu, Xiangkun
Y1  - 2021///
KW  -  Internet of things
KW  -  Patients
KW  -  QDD
KW  -  Sensor
KW  -  Smart wearable device
KW  - Acute myocardial infarction
JF  - Microprocessors and Microsystems
VL  - 81
SP  - 103662
EP  - 103662
DO  - https://doi.org/10.1016/j.micpro.2020.103662
UR  - https://www.sciencedirect.com/science/article/pii/S0141933120308085
N2  - Acute Myocardial Infarction (AMI) is a leading cause of death and is a worldwide disorder, despite significant progress in diagnosis over the past decade. AMI is a multifactorial disease that is thought to be due to the interaction of genetic and environmental factors. It requires a high degree of knowledge and experience together to predict acute myocardial infarction is a complicated task. Internet of things (IoT) technology, the Internet, the sensor value of the collection for the diagnosis, and prediction of heart disease, have been adopted in the current medical system. To resolve this problem, it has been proposed to evaluate acute myocardial infarction using a more accurate qualified Quick Decision Diagnosis (QDD) algorithm, the IoT of the framework. Smart wearable device and heart monitoring devices have been attached to the patient's monitoring of blood pressure and Electrocardiogram (ECG). QDD is used to classify the sensor data with the received patient's normal and abnormal data. The simulation results show that the QDD-based myocardial infarction prognosis system is better proposed than other methods offered.
ER  - 
TY  - JOUR
T1  - A review of machine learning methods for retinal blood vessel segmentation and artery/vein classification
A1  - Mookiah, Muthu Rama Krishnan
A1  - Hogg, Stephen
A1  - MacGillivray, Tom J
A1  - Prathiba, Vijayaraghavan
A1  - Pradeepa, Rajendra
A1  - Mohan, Viswanathan
A1  - Anjana, Ranjit Mohan
A1  - Doney, Alexander S
A1  - Palmer, Colin N A
A1  - Trucco, Emanuele
Y1  - 2021///
KW  -  Deep learning
KW  -  Machine learning
KW  -  Retinal vessels
KW  -  Review
KW  -  Segmentation
KW  - Medical imaging
JF  - Medical Image Analysis
VL  - 68
SP  - 101905
EP  - 101905
DO  - https://doi.org/10.1016/j.media.2020.101905
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520302693
N2  - The eye affords a unique opportunity to inspect a rich part of the human microvasculature non-invasively via retinal imaging. Retinal blood vessel segmentation and classification are prime steps for the diagnosis and risk assessment of microvascular and systemic diseases. A high volume of techniques based on deep learning have been published in recent years. In this context, we review 158 papers published between 2012 and 2020, focussing on methods based on machine and deep learning (DL) for automatic vessel segmentation and classification for fundus camera images. We divide the methods into various classes by task (segmentation or artery-vein classification), technique (supervised or unsupervised, deep and non-deep learning, hand-crafted methods) and more specific algorithms (e.g. multiscale, morphology). We discuss advantages and limitations, and include tables summarising results at-a-glance. Finally, we attempt to assess the quantitative merit of DL methods in terms of accuracy improvement compared to other methods. The results allow us to offer our views on the outlook for vessel segmentation and classification for fundus camera images.
ER  - 
TY  - JOUR
T1  - A Rule-based Approach for Identifying Obesity and Its Comorbidities in Medical Discharge Summaries
A1  - Mishra, Ninad K
A1  - Cummo, David M
A1  - Arnzen, James J
A1  - Bonander, Jason
Y1  - 2009///
JF  - Journal of the American Medical Informatics Association
VL  - 16
IS  - 4
SP  - 576
EP  - 579
DO  - https://doi.org/10.1197/jamia.M3086
UR  - https://www.sciencedirect.com/science/article/pii/S1067502709000814
N2  - Objective
Evaluate the effectiveness of a simple rule-based approach in classifying medical discharge summaries according to indicators for obesity and 15 associated co-morbidities as part of the 2008 i2b2 Obesity Challenge.
Methods
The authors applied a rule-based approach that looked for occurrences of morbidity-related keywords and identified the types of assertions in which those keywords occurred. The documents were then classified using a simple scoring algorithm based on a mapping of the assertion types to possible judgment categories.
Measurements
Results for the challenge were evaluated based on macro F-measure. We report micro and macro F-measure results for all morbidities combined and for each morbidity separately.
Results
Our rule-based approach achieved micro and macro F-measures of 0.97 and 0.77, respectively, ranking fifth out of the entries submitted by 28 teams participating in the classification task based on textual judgments and substantially outperforming the average for the challenge.
Conclusions
As shown by its ranking in the challenge results, this approach performed relatively well under conditions in which limited training data existed for some judgment categories. Further, the approach held up well in relation to more complex approaches applied to this classification task. The approach could be enhanced by the addition of expert rules to model more complex medical reasoning.
ER  - 
TY  - JOUR
T1  - High-throughput analysis of the interactions between viral proteins and host cell RNAs
A1  - Lanjanian, Hossein
A1  - Nematzadeh, Sajjad
A1  - Hosseini, Shadi
A1  - Torkamanian-Afshar, Mahsa
A1  - Kiani, Farzad
A1  - Moazzam-Jazi, Maryam
A1  - Aydin, Nizamettin
A1  - Masoudi-Nejad, Ali
Y1  - 2021///
KW  -  COVID-19
KW  -  RNA-Protein affinity
KW  -  RPINBASE
KW  -  Viral nonstructural proteins (NSP)
KW  - Host cell RNA
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104611
EP  - 104611
DO  - https://doi.org/10.1016/j.compbiomed.2021.104611
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521004054
N2  - RNA-protein interactions of a virus play a major role in the replication of RNA viruses. The replication and transcription of these viruses take place in the cytoplasm of the host cell; hence, there is a probability for the host RNA-viral protein and viral RNA-host protein interactions. The current study applies a high-throughput computational approach, including feature extraction and machine learning methods, to predict the affinity of protein sequences of ten viruses to three categories of RNA sequences. These categories include RNAs involved in the protein-RNA complexes stored in the RCSB database, the human miRNAs deposited at the mirBase database, and the lncRNA deposited in the LNCipedia database. The results show that evolution not only tries to conserve key viral proteins involved in the replication and transcription but also prunes their interaction capability. These proteins with specific interactions do not perturb the host cell through undesired interactions. On the other hand, the hypermutation rate of NSP3 is related to its affinity to host cell RNAs. The Gene Ontology (GO) analysis of the miRNA with affiliation to NSP3 suggests that these miRNAs show strongly significantly enriched GO terms related to the known symptoms of COVID-19. Docking and MD simulation study of the obtained miRNA through high-throughput analysis suggest a non-coding RNA (an RNA antitoxin, ToxI) as a natural aptamer drug candidate for NSP5 inhibition. Finally, a significant interplay of the host RNA-viral protein in the host cell can disrupt the host cell's system by influencing the RNA-dependent processes of the host cells, such as a differential expression in RNA. Furthermore, our results are useful to identify the side effects of mRNA-based vaccines, many of which are caused by the off-label interactions with the human lncRNAs.
ER  - 
TY  - JOUR
T1  - Multi-input convolutional neural network for breast cancer detection using thermal images and clinical data
A1  - Sánchez-Cauce, Raquel
A1  - Pérez-Martín, Jorge
A1  - Luque, Manuel
Y1  - 2021///
KW  -  Classification
KW  -  Clinical data
KW  -  Convolutional neural network
KW  -  Thermal images
KW  - Breast cancer
JF  - Computer Methods and Programs in Biomedicine
VL  - 204
SP  - 106045
EP  - 106045
DO  - https://doi.org/10.1016/j.cmpb.2021.106045
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721001206
N2  - Background and objective
Breast cancer is the most common cancer in women. While mammography is the most widely used screening technique for the early detection of this disease, it has several disadvantages such as radiation exposure or high economic cost. Recently, multiple authors studied the ability of machine learning algorithms for early diagnosis of breast cancer using thermal images, showing that thermography can be considered as a complementary test to mammography, or even as a primary test under certain circumstances. Moreover, although some personal and clinical data are considered risk factors of breast cancer, none of these works considered that information jointly with thermal images.
Methods
We propose a novel approach for early detection of breast cancer combining thermal images of different views with personal and clinical data, building a multi-input classification model which exploits the benefits of convolutional neural networks for image analysis. First, we searched for structures using only thermal images. Next, we added the clinical data as a new branch of each of these structures, aiming to improve its performance.
Results
We applied our method to the most widely used public database of breast thermal images, the Database for Mastology Research with Infrared Image. The best model achieves a 97% accuracy and an area under the ROC curve of 0.99, with a specificity of 100% and a sensitivity of 83%.
Conclusions
After studying the impact of thermal images and personal and clinical data on multi-input convolutional neural networks for breast cancer diagnosis, we conclude that: (1) adding the lateral views to the front view improves the performance of the classification model, and (2) including personal and clinical data helps the model to recognize sick patients.
ER  - 
TY  - JOUR
T1  - Retinal image quality assessment using transfer learning: Spatial images vs. wavelet detail subbands
A1  - Abdel-Hamid, Lamiaa
Y1  - 2021///
KW  -  Deep learning
KW  -  Transfer learning
KW  -  VGG network
KW  -  Wavelet transform
KW  - Retinal image quality assessment (RIQA)
JF  - Ain Shams Engineering Journal
VL  - 12
IS  - 3
SP  - 2799
EP  - 2807
DO  - https://doi.org/10.1016/j.asej.2021.02.010
UR  - https://www.sciencedirect.com/science/article/pii/S2090447921001015
N2  - Retinal image quality assessment (RIQA) is essential to assure that images used for medical analysis are of sufficient quality for reliable diagnosis. A modified VGG16 network with transfer learning is introduced in order to classify retinal images into good or bad quality images. Both spatial and wavelet detail subbands are compared as inputs to the modified VGG16 network. Three public retinal image datasets captured with different imaging devices are used, both individually and collectively. Superior performance was attained by the modified VGG16 network, where accuracies in the range of 99–100% were achieved regardless of whether retinal images from the same or different sources were considered and whether the spatial or wavelet images were used. The implemented RIQA algorithm was also found to outperform other RIQA deep learning algorithms from literature by 1.5–10% and to achieve accuracies that are up to 32% higher than traditional RIQA methods for the same dataset.
ER  - 
TY  - JOUR
T1  - Automatic classification of free-text medical causes from death certificates for reactive mortality surveillance in France
A1  - Baghdadi, Yasmine
A1  - Bourrée, Alix
A1  - Robert, Aude
A1  - Rey, Grégoire
A1  - Gallay, Anne
A1  - Zweigenbaum, Pierre
A1  - Grouin, Cyril
A1  - Fouillet, Anne
Y1  - 2019///
KW  -  Evaluation performance
KW  -  Medical causes of death
KW  -  Rule-based method
KW  -  SVM
KW  -  Syndromic surveillance
KW  - Automatic classification
JF  - International Journal of Medical Informatics
VL  - 131
SP  - 103915
EP  - 103915
DO  - https://doi.org/10.1016/j.ijmedinf.2019.06.022
UR  - https://www.sciencedirect.com/science/article/pii/S138650561930245X
N2  - Background
Mortality surveillance is of fundamental importance to public health surveillance. The real-time recording of death certificates, thanks to Electronic Death Registration System (EDRS), provides valuable data for reactive mortality surveillance based on medical causes of death in free-text format. Reactive mortality surveillance is based on the monitoring of mortality syndromic groups (MSGs). An MSG is a cluster of medical causes of death (pathologies, syndromes or symptoms) that meets the objectives of early detection and impact assessment of public health events. The aim of this study is to implement and measure the performance of a rule-based method and two supervised models for automatic free-text cause of death classification from death certificates in order to implement them for routine surveillance.
Method
A rule-based method was implemented using four processing steps: standardization rules, splitting causes of death using delimiters, spelling corrections and dictionary projection. A supervised machine learning method using a linear Support Vector Machine (SVM) classifier was also implemented. Two models were produced using different features (SVM1 based solely on surface features and SVM2 combining surface features and MSGs classified by the rule-based method as feature vectors). The evaluation was conducted using an annotated subset of electronic death certificates received between 2012 and 2016. Classification performance was evaluated on seven MSGs (Influenza, Low respiratory diseases, Asphyxia/abnormal respiration, Acute respiratory disease, Sepsis, Chronic digestive diseases, and Chronic endocrine diseases).
Results
The rule-based method and the SVM2 model displayed a high performance with F-measures over 0.94 for all MSGs. Precision and recall were slightly higher for the rule-based method and the SVM2 model. An error-analysis shows that errors were not specific to an MSG.
Conclusion
The high performance of the rule-based method and SVM2 model will allow us to set-up a reactive mortality surveillance system based on free-text death certificates. This surveillance will be an added-value for public health decision making.
ER  - 
TY  - JOUR
T1  - Continual learning classification method with new labeled data based on the artificial immune system
A1  - Li, Dong
A1  - Liu, Shulin
A1  - Gao, Furong
A1  - Sun, Xin
Y1  - 2020///
KW  -  Classification
KW  -  Continual learning
KW  -  Machine learning
KW  -  New labeled data
KW  - Artificial immune system
JF  - Applied Soft Computing
VL  - 94
SP  - 106423
EP  - 106423
DO  - https://doi.org/10.1016/j.asoc.2020.106423
UR  - https://www.sciencedirect.com/science/article/pii/S156849462030363X
N2  - In this paper, a new supervised learning classification method, continual learning classification method with new labeled data based on the artificial immune system (CLCMNLD), is proposed as a new way to improve the classification performance in real-time by continually learning the new labeled data during the testing stage. It is inspired by the mechanism that vaccines can enhance immunity. New types of memory cells were continuously cultured by learning new labeled data during the testing stage. CLCMNLD will degenerate into a common supervised learning classification method when there is no new labeled data comes out during the testing stage. The effectiveness of the proposed CLCMNLD is tested on twenty well-known datasets from the UCI Machine Learning Repository that are commonly used in the domain of data classification. The experiments reveal that CLCMNLD has better classification performance when it degenerates into a common supervised learning classification method, and it outperforms the other methods when there are some new labeled data comes out during the testing stage. The more types of new labeled data, the more advantages it has.
ER  - 
TY  - JOUR
T1  - Interpreting clinical latent representations using autoencoders and probabilistic models
A1  - Chushig-Muzo, David
A1  - Soguero-Ruiz, Cristina
A1  - de Miguel-Bohoyo, Pablo
A1  - Mora-Jiménez, Inmaculada
Y1  - 2021///
KW  -  Chronic diseases
KW  -  Clustering
KW  -  Electronic health records
KW  -  Gaussian mixture model
KW  -  Learning latent representations
KW  - Autoencoder
JF  - Artificial Intelligence in Medicine
VL  - 122
SP  - 102211
EP  - 102211
DO  - https://doi.org/10.1016/j.artmed.2021.102211
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721002049
N2  - Electronic health records (EHRs) are a valuable data source that, in conjunction with deep learning (DL) methods, have provided important outcomes in different domains, contributing to supporting decision-making. Owing to the remarkable advancements achieved by DL-based models, autoencoders (AE) are becoming extensively used in health care. Nevertheless, AE-based models are based on nonlinear transformations, resulting in black-box models leading to a lack of interpretability, which is vital in the clinical setting. To obtain insights from AE latent representations, we propose a methodology by combining probabilistic models based on Gaussian mixture models and hierarchical clustering supported by Kullback-Leibler divergence. To validate the methodology from a clinical viewpoint, we used real-world data extracted from EHRs of the University Hospital of Fuenlabrada (Spain). Records were associated with healthy and chronic hypertensive and diabetic patients. Experimental outcomes showed that our approach can find groups of patients with similar health conditions by identifying patterns associated with diagnosis and drug codes. This work opens up promising opportunities for interpreting representations obtained by the AE-based model, bringing some light to the decision-making process made by clinical experts in daily practice.
ER  - 
TY  - JOUR
T1  - Heart disease prediction using hyper parameter optimization (HPO) tuning
A1  - Valarmathi, R
A1  - Sheela, T
Y1  - 2021///
KW  -  Grid search
KW  -  Heart disease
KW  -  Randomized search
KW  -  TPOT classifier
KW  - Hyper parameter tuning
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103033
EP  - 103033
DO  - https://doi.org/10.1016/j.bspc.2021.103033
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421006303
N2  - Coronary artery disease prediction is considered to be one of the most challenging tasks in the health care industry. In our research, we propose a prediction system to detect the heart disease. Three Hyper Parameter Optimization (HPO) techniques Grid Search, Randomized Search and Genetic programming (TPOT Classifier) were proposed to optimize the performance of Random forest classifier and XG Boost classifier model. The performance of the two models Random Forest and XG Boost were compared with the existing studies. The performance of the models is evaluated with the publicly available datasets Cleveland Heart disease Dataset (CHD) and Z-Alizadeh Sani dataset. Random Forest along with TPOT Classifier achieved the highest accuracy of 97.52%for CHD Dataset. Random Forest with Randomized Search achieved the highest accuracy of 80.2%, 73.6% and 76.9% for the diagnosis of the stenos is of three vessels LAD, LCX and RCA respectively with Z-Alizadeh Sani Dataset. The results were compared with the existing studies focusing on prediction of heart disease that were found to outperform their results significantly.
ER  - 
TY  - JOUR
T1  - Estimating summary statistics for electronic health record laboratory data for use in high-throughput phenotyping algorithms
A1  - Albers, D J
A1  - Elhadad, N
A1  - Claassen, J
A1  - Perotte, R
A1  - Goldstein, A
A1  - Hripcsak, G
Y1  - 2018///
KW  -  Kullback-Leibler divergence
KW  -  Laboratory tests
KW  -  Summary statistic
KW  -  phenotyping
KW  - Electronic health record
JF  - Journal of Biomedical Informatics
VL  - 78
SP  - 87
EP  - 101
DO  - https://doi.org/10.1016/j.jbi.2018.01.004
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300066
N2  - We study the question of how to represent or summarize raw laboratory data taken from an electronic health record (EHR) using parametric model selection to reduce or cope with biases induced through clinical care. It has been previously demonstrated that the health care process (Hripcsak and Albers, 2012, 2013), as defined by measurement context (Hripcsak and Albers, 2013; Albers et al., 2012) and measurement patterns (Albers and Hripcsak, 2010, 2012), can influence how EHR data are distributed statistically (Kohane and Weber, 2013; Pivovarov et al., 2014). We construct an algorithm, PopKLD, which is based on information criterion model selection (Burnham and Anderson, 2002; Claeskens and Hjort, 2008), is intended to reduce and cope with health care process biases and to produce an intuitively understandable continuous summary. The PopKLD algorithm can be automated and is designed to be applicable in high-throughput settings; for example, the output of the PopKLD algorithm can be used as input for phenotyping algorithms. Moreover, we develop the PopKLD-CAT algorithm that transforms the continuous PopKLD summary into a categorical summary useful for applications that require categorical data such as topic modeling. We evaluate our methodology in two ways. First, we apply the method to laboratory data collected in two different health care contexts, primary versus intensive care. We show that the PopKLD preserves known physiologic features in the data that are lost when summarizing the data using more common laboratory data summaries such as mean and standard deviation. Second, for three disease-laboratory measurement pairs, we perform a phenotyping task: we use the PopKLD and PopKLD-CAT algorithms to define high and low values of the laboratory variable that are used for defining a disease state. We then compare the relationship between the PopKLD-CAT summary disease predictions and the same predictions using empirically estimated mean and standard deviation to a gold standard generated by clinical review of patient records. We find that the PopKLD laboratory data summary is substantially better at predicting disease state. The PopKLD or PopKLD-CAT algorithms are not meant to be used as phenotyping algorithms, but we use the phenotyping task to show what information can be gained when using a more informative laboratory data summary. In the process of evaluation our method we show that the different clinical contexts and laboratory measurements necessitate different statistical summaries. Similarly, leveraging the principle of maximum entropy we argue that while some laboratory data only have sufficient information to estimate a mean and standard deviation, other laboratory data captured in an EHR contain substantially more information than can be captured in higher-parameter models.
ER  - 
TY  - JOUR
T1  - Filtering free-text medical data based on machine learning
A1  - Grechishcheva, Sofia
A1  - Lenivtceva, Iuliia
A1  - Kopanitsa, Georgy
A1  - Panfilov, Dmitry
Y1  - 2021///
KW  -  biomedical informatics
KW  -  data filtering
KW  -  natural language processing
KW  -  supervised machine learning
KW  - classification
JF  - Procedia Computer Science
VL  - 193
SP  - 82
EP  - 91
DO  - https://doi.org/10.1016/j.procs.2021.10.009
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921020500
N1  - 10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021
N2  - This article describes the results of data filtering of electronic health records for patients diagnosed with aortic aneurysm in two different medical centers to prepare data for further feature extraction. The accuracy improvement of filtered data was achieved by using machine learning methods of classification and natural language processing methods, taking into account the specificity of Russian language. Based on accuracy and F-measure, two methods of data filtering were compared: 1) rule-based approach; 2) classification approach. The results show that the designed classification is appropriate in terms of accuracy for data filtering.
ER  - 
TY  - JOUR
T1  - Estimation of Body Mass Index from photographs using deep Convolutional Neural Networks
A1  - Pantanowitz, A
A1  - Cohen, E
A1  - Gradidge, P
A1  - Crowther, N J
A1  - Aharonson, V
A1  - Rosman, B
A1  - Rubin, D M
Y1  - 2021///
KW  -  Body Mass Index
KW  -  Computer vision
KW  -  Deep Convolutional Neural Networks
KW  -  Machine learning
KW  - Anthropomorphism
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100727
EP  - 100727
DO  - https://doi.org/10.1016/j.imu.2021.100727
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821002069
N2  - Obesity is an important concern in public health, and Body Mass Index is one of the useful, common and convenient measures. However, Body Mass Index requires access to accurate scales and a stadiometer for measurements, and could be made more convenient through analysis of photographs. It could be applied to photographs comprising more than one individual, leading to population screening. We use Convolutional Neural Networks to determine Body Mass Index from photographs in a study with 161 participants. The relatively low number of participants in the data, a common problem in medicine, is addressed by reducing the information in the photographs by generating silhouette images. We successfully determine Body Mass Index for unseen test data with high correlation between prediction and actual values, with correlation measurements of greater than 0.93 and a mean absolute error of 1.20.
ER  - 
TY  - JOUR
T1  - Cuffless blood pressure estimation based on composite neural network and graphics information
A1  - Qiu, Ye
A1  - Liu, Dongdong
A1  - Yang, Guoyu
A1  - Qi, Dezhen
A1  - Lu, Yuer
A1  - He, Qingzu
A1  - Qian, Xiangyu
A1  - Li, Xiang
A1  - Cao, Yuping
A1  - Shuai, Jianwei
Y1  - 2021///
KW  -  Compound neural network
KW  -  Cuffless BP prediction
KW  -  Deep learning
KW  -  Electrocardiography (ECG)
KW  -  Photoplethysmography (PPG)
KW  - Blood pressure (BP)
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103001
EP  - 103001
DO  - https://doi.org/10.1016/j.bspc.2021.103001
UR  - https://www.sciencedirect.com/science/article/pii/S174680942100598X
N2  - Blood pressure (BP) assessment and dynamic detection are of great significance for timely detection of the morbidity of hypertension, which is a major risk factor for most cardiovascular diseases (CVDs). It has been proved that the dynamic BP can be effectively predicted by using the combined input of photoplethysmogram (PPG) and electrocardiographic (ECG) signals. In this paper, we proposed a hybrid neural network architecture, which contains CNN-Sequential-Adapt layer, ResNet25_BP layer with squeeze and excitation (SE) block and fully connected layers, for BP estimation. The structure based on the convolutional network aims at the current inputs, which can effectively absorb the graph information of the inputted biological signals and make the model more stable and reliable. We evaluated the performance of two datasets including 1216 and 40 subjects, based on the criterions of British Hypertension Society (BHS) and the Association for the Advancement of Medical Instrumentation (AAMI). According to the BHS and AAMI standards, the outputs of the model achieved grade A on BHS and met the AAMI criteria. The mean absolute errors (MAE) of systolic BP and diastolic BP are 3.70 and 2.81 mmHg in the large dataset, and 1.37 and 0.93 mmHg in the small dataset, respectively.
ER  - 
TY  - JOUR
T1  - A robust ensemble technique in forecasting workload of local healthcare departments
A1  - Piccialli, Francesco
A1  - Giampaolo, Fabio
A1  - Salvi, Alessandro
A1  - Cuomo, Salvatore
Y1  - 2021///
KW  -  Data science
KW  -  Forecasting
KW  -  Smart healthcare
KW  - Machine learning
JF  - Neurocomputing
VL  - 444
SP  - 69
EP  - 78
DO  - https://doi.org/10.1016/j.neucom.2020.02.138
UR  - https://www.sciencedirect.com/science/article/pii/S0925231221001193
N2  - With the exponential growth of the Internet of Things and Cloud Computing, especially in recent years, the potentiality of Machine Learning (ML) has been demonstrated and amplified, together with data mining developments and the availability of large amounts of data. In order to design a ML system capable of producing effective and accurate predictions and results it is necessary to make sure that it is actually working on large data sets; clean, high quality and complete data really representing the information you are trying to analyze. More data are available, the more accurate are evidently predictions. Forecasting represents an important use of extracted knowledge from data. It is the process of predict future demand for an offered product or service, and it allows for optimizing company decisions, reducing risks, managing stocks, planning sales and making many other internal or in-market assessments. In this paper we present and discuss a novel ensemble technique in forecasting workload of local health department. The proposed approach relies on a real dataset composed by over than 20 M of administrative e-health records. Obtained results demonstrate that our ensemble approach outperforms the state-of-the-art.
ER  - 
TY  - JOUR
T1  - ARX model for interstitial glucose prediction during and after physical activities
A1  - Romero-Ugalde, Hector M
A1  - Garnotel, M
A1  - Doron, M
A1  - Jallon, P
A1  - Charpentier, G
A1  - Franc, S
A1  - Huneker, E
A1  - Simon, C
A1  - Bonnet, S
Y1  - 2019///
KW  -  Energy expenditure
KW  -  Insulin
KW  -  Meal
KW  -  Physical activity
KW  -  T1D
KW  - Interstitial glucose prediction
JF  - Control Engineering Practice
VL  - 90
SP  - 321
EP  - 330
DO  - https://doi.org/10.1016/j.conengprac.2019.07.013
UR  - https://www.sciencedirect.com/science/article/pii/S0967066119301121
N2  - This paper presents the first autoregressive with exogenous input (ARX) model using energy expenditure, carbohydrates on board, and insulin on board as input to predict interstitial glucose (IG). The proposed model may be used for predicting IG even during physical activity (PA). A population-based model, obtained from a first database composed of 14 type 1 diabetes (T1D) patients, achieved a root-mean-square error (RMSE) of 16.7±15.6 mg/dL, on IG prediction (30-min ahead) at the end of a PA, on a second database (15 T1D patients). Patient-specific ARX models, obtained on the second database, improved prediction accuracy (RMSE = 7.8±4.5 mg/dL), outperforming the results found in the literature.
ER  - 
TY  - JOUR
T1  - Deep learning based computer-aided diagnosis systems for diabetic retinopathy: A survey
A1  - Asiri, Norah
A1  - Hussain, Muhammad
A1  - Al Adel, Fadwa
A1  - Alzaidi, Nazih
Y1  - 2019///
KW  -  Autoencoder
KW  -  CNN
KW  -  DBN
KW  -  Diabetic macular edema
KW  -  Exudate
KW  -  Hemorrhages
KW  -  Lesion
KW  -  Macula
KW  -  Microaneurysms
KW  -  Optic disc
KW  -  RNN
KW  - Diabetic Retinopathy
JF  - Artificial Intelligence in Medicine
VL  - 99
SP  - 101701
EP  - 101701
DO  - https://doi.org/10.1016/j.artmed.2019.07.009
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718307607
N2  - Diabetic retinopathy (DR) results in vision loss if not treated early. A computer-aided diagnosis (CAD) system based on retinal fundus images is an efficient and effective method for early DR diagnosis and assisting experts. A computer-aided diagnosis (CAD) system involves various stages like detection, segmentation and classification of lesions in fundus images. Many traditional machine-learning (ML) techniques based on hand-engineered features have been introduced. The recent emergence of deep learning (DL) and its decisive victory over traditional ML methods for various applications motivated the researchers to employ it for DR diagnosis, and many deep-learning-based methods have been introduced. In this paper, we review these methods, highlighting their pros and cons. In addition, we point out the challenges to be addressed in designing and learning about efficient, effective and robust deep-learning algorithms for various problems in DR diagnosis and draw attention to directions for future research.
ER  - 
TY  - JOUR
T1  - Computer-Aided Diagnosis system for diagnosis of pulmonary emphysema using bio-inspired algorithms
A1  - Isaac, Anisha
A1  - Nehemiah, H Khanna
A1  - Isaac, Anubha
A1  - Kannan, A
Y1  - 2020///
KW  -  Bio-inspired algorithm
KW  -  Feature selection
KW  -  Moth–Flame Optimization (MFO)
KW  -  Segmentation
KW  -  Spatial Intuitionistic Fuzzy C-Means (SIFCM)
KW  - Emphysema
JF  - Computers in Biology and Medicine
VL  - 124
SP  - 103940
EP  - 103940
DO  - https://doi.org/10.1016/j.compbiomed.2020.103940
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520302754
N2  - Pulmonary emphysema is a condition characterized by the destruction and permanent enlargement of the alveoli of the lungs. The destruction of gas-exchanging alveoli causes shortness of breath followed by a chronic cough and sputum production. A Computer-Aided Diagnosis (CAD) framework for diagnosing pulmonary emphysema from chest Computed Tomography (CT) slices has been designed and implemented in this study. The process of implementing the CAD framework includes segmenting the lung tissues and extracting the regions of interest (ROIs) using the Spatial Intuitionistic Fuzzy C-Means clustering algorithm. The ROIs that were considered in this work were emphysematous lesions — namely, centrilobular, paraseptal, and bullae that were labelled by an expert radiologist. The shape, texture, and run-length features were extracted from each ROI. A wrapper approach that employed four bio-inspired algorithms — namely, Moth–Flame Optimization (MFO), Firefly Optimization (FFO), Artificial Bee Colony Optimization, and Ant Colony Optimization — with the accuracy of the support vector machine classifier as the fitness function was used to select the optimal feature subset. The selected features of each bio-inspired algorithm were trained independently using the Extreme Learning Machine classifier based on the tenfold cross-validation technique. The framework was tested on real-time and public emphysema datasets to perform binary classification of lung CT slices of patients with and without the presence of emphysema. The framework that used MFO and FFO for feature selection produced superior results regarding accuracy, precision, recall, and specificity for the real-time dataset and the public dataset, respectively, when compared to the other bio-inspired algorithms.
ER  - 
TY  - JOUR
T1  - An unsupervised machine learning model for discovering latent infectious diseases using social media data
A1  - Lim, Sunghoon
A1  - Tucker, Conrad S
A1  - Kumara, Soundar
Y1  - 2017///
KW  -  Information retrieval
KW  -  Sentiment analysis
KW  -  Social media
KW  -  Unsupervised machine learning
KW  - Latent infectious diseases
JF  - Journal of Biomedical Informatics
VL  - 66
SP  - 82
EP  - 94
DO  - https://doi.org/10.1016/j.jbi.2016.12.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416301812
N2  - Introduction
The authors of this work propose an unsupervised machine learning model that has the ability to identify real-world latent infectious diseases by mining social media data. In this study, a latent infectious disease is defined as a communicable disease that has not yet been formalized by national public health institutes and explicitly communicated to the general public. Most existing approaches to modeling infectious-disease-related knowledge discovery through social media networks are top-down approaches that are based on already known information, such as the names of diseases and their symptoms. In existing top-down approaches, necessary but unknown information, such as disease names and symptoms, is mostly unidentified in social media data until national public health institutes have formalized that disease. Most of the formalizing processes for latent infectious diseases are time consuming. Therefore, this study presents a bottom-up approach for latent infectious disease discovery in a given location without prior information, such as disease names and related symptoms.
Methods
Social media messages with user and temporal information are extracted during the data preprocessing stage. An unsupervised sentiment analysis model is then presented. Users’ expressions about symptoms, body parts, and pain locations are also identified from social media data. Then, symptom weighting vectors for each individual and time period are created, based on their sentiment and social media expressions. Finally, latent-infectious-disease-related information is retrieved from individuals’ symptom weighting vectors.
Datasets and results
Twitter data from August 2012 to May 2013 are used to validate this study. Real electronic medical records for 104 individuals, who were diagnosed with influenza in the same period, are used to serve as ground truth validation. The results are promising, with the highest precision, recall, and F1 score values of 0.773, 0.680, and 0.724, respectively.
Conclusion
This work uses individuals’ social media messages to identify latent infectious diseases, without prior information, quicker than when the disease(s) is formalized by national public health institutes. In particular, the unsupervised machine learning model using user, textual, and temporal information in social media data, along with sentiment analysis, identifies latent infectious diseases in a given location.
ER  - 
TY  - JOUR
T1  - Computational Traditional Chinese Medicine diagnosis: A literature survey
A1  - Zhang, Qi
A1  - Zhou, Jianhang
A1  - Zhang, Bob
Y1  - 2021///
KW  -  Smart healthcare
KW  -  Survey
KW  -  Traditional Chinese medicine
KW  - Computational diagnosis
JF  - Computers in Biology and Medicine
VL  - 133
SP  - 104358
EP  - 104358
DO  - https://doi.org/10.1016/j.compbiomed.2021.104358
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521001529
N2  - Background and objective
Traditional Chinese Medicine (TCM) diagnosis is based on the theoretical principles and knowledge, where it is steeped in thousands of years of history to diagnose various types of diseases and syndromes. It can be generally divided into four main diagnostic approaches: 1. Inspection, 2. Auscultation and olfaction, 3. Inquiry, and 4. Palpation, which are widely used in TCM hospitals in China and around the world. With the development of intelligent computing technology in recent years, computational TCM diagnosis has grown rapidly.
Methods
In this paper, we aim to systematically summarize the development of computational TCM diagnosis based on four diagnostic approaches, mainly focusing on digital acquisition devices, collected datasets, and computational detection approaches (algorithms). Furthermore, all related works of this field are compared and explored in detail.
Results
This survey provides the principles, applications, and current progress in computing for readers and researchers in terms of computational TCM diagnosis. Moreover, the future development direction, prospect, and technological trend of computational TCM diagnosis will also be discussed in this study.
Conclusions
Recent computational TCM diagnosis works are compared in detail to show the pros/cons, where we provide some meaningful suggestions and opinions on the future research approaches in this area. This work is useful for disease detection in computational TCM diagnosis as well as health management in the smart healthcare area.
Index terms
Computational diagnosis, Traditional Chinese Medicine, survey, smart healthcare.
ER  - 
TY  - JOUR
T1  - Automatic detection of rare pathologies in fundus photographs using few-shot learning
A1  - Quellec, Gwenolé
A1  - Lamard, Mathieu
A1  - Conze, Pierre-Henri
A1  - Massin, Pascale
A1  - Cochener, Béatrice
Y1  - 2020///
KW  -  Deep learning
KW  -  Few-shot learning
KW  -  Rare conditions
KW  - Diabetic retinopathy screening
JF  - Medical Image Analysis
VL  - 61
SP  - 101660
EP  - 101660
DO  - https://doi.org/10.1016/j.media.2020.101660
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520300256
N2  - In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.
ER  - 
TY  - JOUR
T1  - An automated early diabetic retinopathy detection through improved blood vessel and optic disc segmentation
A1  - Kumar, Shailesh
A1  - Adarsh, Abhinav
A1  - Kumar, Basant
A1  - Singh, Amit Kumar
Y1  - 2020///
KW  -  Blood vessels
KW  -  Mathematical morphology
KW  -  Optic disc
KW  -  Radial basis function neural network
KW  -  Watershed transform
KW  - Fundus image
JF  - Optics & Laser Technology
VL  - 121
SP  - 105815
EP  - 105815
DO  - https://doi.org/10.1016/j.optlastec.2019.105815
UR  - https://www.sciencedirect.com/science/article/pii/S0030399219311089
N2  - This paper presents an automated early diabetic retinopathy detection scheme from color fundus images through improved segmentation strategies for optic disc and blood vessels. The red lesions, microaneurysms and hemorrhages are the earliest signs of diabetic retinopathy. This paper essentially proposes improved techniques for microaneurysm as well as hemorrhages detection, which eventually contribute in the overall improvement in the early detection of diabetic retinopathy. The proposed method consists of five stages- pre-processing, detection of blood vessels, segmentation of optic disc, localization of fovea, feature extraction and classification. Mathematical morphology operation is used for pre-processing and blood vessel detection. Watershed transform is used for optic disc segmentation. The main contribution of this model is to propose an improved blood vessel and optic disc segmentation methods. Radial basis function neural network is used for classification of the diseases. The parameters of radial basis function neural network are trained by the features of microaneurysm and hemorrhages. The accuracy of the proposed algorithm is evaluated based on sensitivity and specificity, which are 87% and 93% respectively.
ER  - 
TY  - JOUR
T1  - Automatic microaneurysm detection in fundus image based on local cross-section transformation and multi-feature fusion
A1  - Du, Jingyu
A1  - Zou, Beiji
A1  - Chen, Changlong
A1  - Xu, Ziwen
A1  - Liu, Qing
Y1  - 2020///
KW  -  Diabetic retinopathy
KW  -  Fundus images
KW  -  Local cross-section transformation
KW  -  Microaneurysm detection
KW  - Medical image analysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105687
EP  - 105687
DO  - https://doi.org/10.1016/j.cmpb.2020.105687
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720315200
N2  - Background and objective: Retinal microaneurysm (MA) is one of the earliest clinical signs of diabetic retinopathy(DR). Its detection is essential for controlling DR and preventing vision loss. However, the spatial scale of MA is extremely small and the contrast to surrounding background is subtle, which make MA detection challenging. The purpose of this work is to automatically detect MAs from fundus images. Methods: Our MA detector involves two stages: MA candidate extraction and classification. In MA candidate extraction stage, local minimum region extraction and block filtering are used to exploit the regions where MA may exist. In this way, most of irrelavent background regions are discarded , which subsequently facilitates the training of MA classifier. In the second stage, multiple features are extracted to train the MA classifier. To distinguish MA from vascular regions, we propose a series of descriptors according to the cross-section profile of MA. Specially, as MAs are small and their contrast to surroundings is subtle, we propose local cross-section transformation (LCT) to amplify the difference between the MA and confusing structures. Finally, an under-sampling boosting-based classifier (RUSBoost) is trained to determine whether the candidate is an MA. Results: The proposed method is evaluated on three public available databases i.e. e-ophtha-MA, DiaretDB1 and ROC training set. It achieves high sensitivities for low false positive rates on the three databases. Using the FROC metric, the final scores are 0.516, 0.402 and 0.293 respectively, which are comparable to existing state-of-the-art methods. Conclusions: The proposed local cross-section transformation enhances the discrimination of descriptors by amplifying difference between MAs and confusing structures, which facilitates the classification and improves the detection performances. With the powerful descriptors, our method achieves state-of-the-art performances on three public datasets consistently.
ER  - 
TY  - JOUR
T1  - Early diagnosis of esophageal varices using Boosted-Naïve Bayes Tree: A multicenter cross-sectional study on chronic hepatitis C patients
A1  - Abd-Elsalam, Shimaa M
A1  - Ezz, Mohamed M
A1  - Gamalel-Din, Shehab
A1  - Esmat, Gamal
A1  - Salama, Ahmed
A1  - ElHefnawi, Mahmoud
Y1  - 2020///
KW  -  Biomedical informatics
KW  -  Boosting ensemble method
KW  -  Esophageal varices
KW  -  Liver cirrhosis
KW  -  Liver disease diagnosis
KW  -  Naïve bayes tree
KW  - Alternating decision tree
JF  - Informatics in Medicine Unlocked
VL  - 20
SP  - 100421
EP  - 100421
DO  - https://doi.org/10.1016/j.imu.2020.100421
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820305712
N2  - The standard method for diagnosing varices by upper endoscopy is invasive, costly, and has many drawbacks. To overcome these drawbacks, this study aims to build a predictive Esophageal Varices diagnosing model that uses a minimum number of the most significant variables, trying to avoid unneeded endoscopy procedures. A dataset of a prospective cohort of 5013 chronic hepatitis C Egyptian patients collected from 2006 to 2017 was used in this study. The dataset included more than forty individual clinical laboratory variables, only ten of them were found to be significant, which was achieved via merging the correlation coefficient and p-value filtering methods to acquire improved results. All the 5013 patients of the sample dataset underwent endoscopic assessment, a costly procedure that could be avoided in most cases; hence, a highly accurate non-invasive diagnosing model is justifiably mandatory. To improve the overall performance of the predictive diagnosis model, this research introduced a novel algorithm that improves the traditional Naïve Bayes Tree by adding a boosting technique, dubbed “Boosted-Naïve Bayes Tree” (B-NBT). Applying B-NBT on our dataset revealed an improved performance in both AUROC (Area Under Receiver Operating Characteristic Curve) of 0.865 and Accuracy of 79%. In conclusion, this study revealed that there are only ten most significant variables that are sufficient for the noninvasive diagnosing model to preemptively predict EV with acceptable performance. This could advise physicians an efficient choice to save time and money— a medical contribution to this research. Additionally, an engineering contribution to the research was adding a proposed feature selection method to the boosting technique which improves the predictive performance.
ER  - 
TY  - JOUR
T1  - Higuchi fractal dimension: An efficient approach to detection of brain entrainment to theta binaural beats
A1  - Shamsi, Elham
A1  - Ahmadi-Pajouh, Mohammad Ali
A1  - Seifi Ala, Tirdad
Y1  - 2021///
KW  -  Brain entrainment
KW  -  Electroencephalogram
KW  -  Higuchi fractal dimension
KW  -  K-nearest neighbors classification
KW  - Theta binaural beats
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102580
EP  - 102580
DO  - https://doi.org/10.1016/j.bspc.2021.102580
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421001774
N2  - Binaural beats (BBs) are two pure tones with a small frequency difference (i.e., beat) separately presented to each ear. They cause the beat perception in the brain. BBs are used in both clinical and basic science applications. Studies in BB literature have mainly focused on linear analysis of the brain signals. Even though these approaches have produced promising findings, there may still be some facets left to be considered, which cannot be studied by linear measures. BBs entrain the brain and generate synchronous responses. Previous studies have proved that increasing brain synchronicity reduced its complexity measured by fractal dimension (FD). In this study, Higuchi fractal dimension (HFD) was used to test whether: 1) BB stimulation decreases the electroencephalogram (EEG) complexity, and 2) HFD is a reliable alternative to its common linear counterpart (i.e., relative power of the band in which the beat frequency lies) in terms of the brain entrainment detection. Results revealed that 3-min BB stimulation significantly decreased the HFD in temporal and parietal lobes, which was about half the time required to probe any changes in EEG power. Moreover, there was significant negative correlation between the relative power and HFD in these regions. In comparison to the relative power, HFD produced mostly higher classification accuracies and areas under empirical receiver operating characteristic (ROC) curve in these lobes. Our findings suggest that HFD can be a reliable replacement for relative power in terms of entrainment detection and response classification.
ER  - 
TY  - JOUR
T1  - Systems engineering principles for the design of biomedical signal processing systems
A1  - Faust, Oliver
A1  - Acharya U, Rajendra
A1  - Sputh, Bernhard H C
A1  - Min, Lim Choo
Y1  - 2011///
KW  -  B method
KW  -  CSP
KW  -  CSPB
KW  -  Diabetes type 2
KW  -  Fast Fourier transform
KW  -  Neuropathy
KW  -  Plantar pressure
KW  -  Testing
KW  - Systems engineering
JF  - Computer Methods and Programs in Biomedicine
VL  - 102
IS  - 3
SP  - 267
EP  - 276
DO  - https://doi.org/10.1016/j.cmpb.2010.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169260710001173
N2  - Systems engineering aims to produce reliable systems which function according to specification. In this paper we follow a systems engineering approach to design a biomedical signal processing system. We discuss requirements capturing, specification definition, implementation and testing of a classification system. These steps are executed as formal as possible. The requirements, which motivate the system design, are based on diabetes research. The main requirement for the classification system is to be a reliable component of a machine which controls diabetes. Reliability is very important, because uncontrolled diabetes may lead to hyperglycaemia (raised blood sugar) and over a period of time may cause serious damage to many of the body systems, especially the nerves and blood vessels. In a second step, these requirements are refined into a formal CSP‖ B model. The formal model expresses the system functionality in a clear and semantically strong way. Subsequently, the proven system model was translated into an implementation. This implementation was tested with use cases and failure cases. Formal modeling and automated model checking gave us deep insight in the system functionality. This insight enabled us to create a reliable and trustworthy implementation. With extensive tests we established trust in the reliability of the implementation.
ER  - 
TY  - JOUR
T1  - Recognizing lung cancer using a homemade e-nose: A comprehensive study
A1  - Li, Wang
A1  - Jia, Ziru
A1  - Xie, Dandan
A1  - Chen, Ke
A1  - Cui, Jianguo
A1  - Liu, Hongying
Y1  - 2020///
KW  -  Breath-prints
KW  -  Lung cancer
KW  -  Pattern recognition
KW  -  Smart diagnostics
KW  - Diverse sensor array
JF  - Computers in Biology and Medicine
VL  - 120
SP  - 103706
EP  - 103706
DO  - https://doi.org/10.1016/j.compbiomed.2020.103706
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520300937
N2  - In recent years, breath analysis has been used as a tool for lung cancer detection and many gas sensors were developed for this purpose. Although they are fabricated with advanced materials, for now, gas sensors are still limited in their medical application due to their unfavorable performance. Here, we hypothesized that a combination of diverse types of sensors could aid in improving the detection performance. We fabricated an e-nose based on 10 gas sensors of 4 types and directly tested it using samples from 153 healthy participants and 115 lung cancer patients, without gas pre-concentration. Additionally, we studied and compared five feature extraction algorithms. The extracted features were then used in 2 optimized clustering algorithms and 3 supervised classification strategies, and their performance was investigated. As a result, “breath-prints” for all subjects were successfully obtained. The combined features extracted by LDA and Fast ICA formed the best feature space. Within this feature space, both clustering algorithms grouped all “breath-prints” into exactly 2 clusters with an Adjusted Rand Index greater than 0.95. Among the 3 supervised classification strategies, random forest with 3-fold cross validation showed the best performance with 86.42% of mean classification accuracy and 0.87 of AUC, which was somewhat better than many recently reported sensor arrays. It can be concluded that, the diversity of sensors may play a role in improving the performance of the e-nose though to what extent still requires evaluation.
ER  - 
TY  - JOUR
T1  - Stress detection using ECG and EMG signals: A comprehensive study
A1  - Pourmohammadi, Sara
A1  - Maleki, Ali
Y1  - 2020///
KW  -  Electromyogram
KW  -  Erector spinae muscle
KW  -  Heart rate variability
KW  -  Multi-level stress detection
KW  -  Stress-inducing protocol
KW  -  Trapezius muscle
KW  - Electrocardiogram
JF  - Computer Methods and Programs in Biomedicine
VL  - 193
SP  - 105482
EP  - 105482
DO  - https://doi.org/10.1016/j.cmpb.2020.105482
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719320528
N2  - Background and Objective
In recent years, stress and mental health have been considered as important worldwide concerns. Stress detection using physiological signals such as electrocardiogram (ECG), skin conductance (SC), electromyogram (EMG) and electroencephalogram (EEG) is a traditional approach. However, the effect of stress on the EMG signal of different muscles and the efficacy of combination of the EMG and other biological signals for stress detection have not been taken into account yet. This paper presents a comprehensive review of the EMG signal of the right and left trapezius and right and left erector spinae muscles for multi-level stress recognition. Also, the ECG signal was employed to evaluate the efficacy of EMG signals for stress detection.
Methods
Both EMG and ECG signals were acquired simultaneously from 34 healthy students (23 females and 11 males, aged 20-37 years). Mental arithmetic, Stroop color-word test, time pressure, and stressful environment were employed to induce stress in the laboratory.
Results
The accuracies of stress recognition in two, three and four levels were 100%, 97.6%, and 96.2%, respectively, obtained from the distinct combination of feature selection and machine learning algorithms.
Conclusions
The comparison of stress detection accuracies resulted from EMG and ECG indicators demonstrated the strong ability and the effectiveness of EMG signal for multi-level stress detection.
ER  - 
TY  - JOUR
T1  - Performance evaluation of the spectral autocorrelation function and autoregressive models for automated sleep apnea detection using single-lead ECG signal
A1  - Zarei, Asghar
A1  - Mohammadzadeh Asl, Babak
Y1  - 2020///
KW  -  AR Model
KW  -  Electrocardiogram (ECG)
KW  -  Obstructive sleep apnea
KW  - Spectral autocorrelation function
JF  - Computer Methods and Programs in Biomedicine
VL  - 195
SP  - 105626
EP  - 105626
DO  - https://doi.org/10.1016/j.cmpb.2020.105626
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314590
N2  - Background and objective: This paper addresses the automated recognition of obstructive sleep apnea (OSA) from the analysis of single-lead ECG signals. This is one of the most important problems that is, critical to the realization of monitoring patients with sleep apnea. Methods: In the present study, a novel solution based on autoregressive (AR) modeling of the single-lead ECG, and spectral autocorrelation function as an ECG feature extraction method is presented. The more effective features are opted by sequential forward feature selection (SFFS) technique and fed into the random forest for binary classification between the apnea and normal events. Results: Experimental results on Apnea-ECG database proved that the introduced algorithm resulted in an accuracy of 93.90% (sensitivity of 92.26% and specificity of 94.92%) in per-segment classification, which outperforms the other cutting-edge automatic OSA recognition techniques. Moreover, the proposed algorithm provided an accuracy of 97.14% (sensitivity of 95.65% and specificity of 100%) in discrimination of apnea patients from the normal subjects, which is comparable to the traditional and existing approaches. Conclusions: This study suggests that automatic OSA recognition from single-lead ECG signals is possible, which can be used as an inexpensive and low complexity burden alternative to more conventional methods such as Polysomnography.
ER  - 
TY  - JOUR
T1  - Proposing a convolutional neural network for stress assessment by means of derived heart rate from functional near infrared spectroscopy
A1  - Hakimi, Naser
A1  - Jodeiri, Ata
A1  - Mirbagheri, Mahya
A1  - Setarehdan, S Kamaledin
Y1  - 2020///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Heart rate
KW  -  Independent component analysis
KW  -  Stress assessment
KW  - Functional near infrared spectroscopy
JF  - Computers in Biology and Medicine
VL  - 121
SP  - 103810
EP  - 103810
DO  - https://doi.org/10.1016/j.compbiomed.2020.103810
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520301773
N2  - Background
Stress is known as one of the major factors threatening human health. A large number of studies have been performed in order to either assess or relieve stress by analyzing the brain and heart-related signals.
Method
In this study, a method based on the Convolutional Neural Network (CNN) approach is proposed to assess stress induced by the Montreal Imaging Stress Task. The proposed model is trained on the heart rate signal derived from functional Near-Infrared Spectroscopy (fNIRS), which is referred to as HRF. In this regard, fNIRS signals of 20 healthy volunteers were recorded using a configuration of 23 channels located on the prefrontal cortex. The proposed deep learning system consists of two main parts where in the first part, the one-dimensional convolutional neural network is employed to build informative activation maps, and then in the second part, a stack of deep fully connected layers is used to predict the stress existence probability. Thereafter, the employed CNN method is compared with the Dense Neural Network, Support Vector Machine, and Random Forest regarding various classification metrics.
Results
Results clearly showed the superiority of CNN over all other methods. Additionally, the trained HRF model significantly outperforms the model trained on the filtered fNIRS signals, where the HRF model could achieve 98.69 ± 0.45% accuracy, which is 10.09% greater than the accuracy obtained by the fNIRS model.
Conclusions
Employment of the proposed deep learning system trained on the HRF measurements leads to higher stress classification accuracy than the accuracy reported in the existing studies where the same experimental procedure has been done. Besides, the proposed method suggests better stability with lower variation in prediction. Furthermore, its low computational cost opens up the possibility to be applied in real-time monitoring of stress assessment.
ER  - 
TY  - JOUR
T1  - BNPA: An R package to learn path analysis input models from a data set semi-automatically using Bayesian networks
A1  - de Carvalho, Elias Cesar Araujo
A1  - Vissoci, Joao Ricardo Nickenig
A1  - de Andrade, Luciano
A1  - de Lara Machado, Wagner
A1  - Paraiso, Emerson Cabrera
A1  - Nievola, Julio Cesar
Y1  - 2021///
KW  -  Causal inference
KW  -  Path analysis
KW  -  R-package
KW  - Bayesian networks
JF  - Knowledge-Based Systems
VL  - 223
SP  - 107042
EP  - 107042
DO  - https://doi.org/10.1016/j.knosys.2021.107042
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121003051
N2  - Epidemiologists constantly search for methodologies that help them better understand how diseases work. Populations urge these improvements to combat these diseases more effectively. The literature presents several authors defending the idea that epidemiologists should be able to develop causal models. In this area, the technique of structural equation models (SEM) has stood out in scientific research. Although SEM has been widely used in several research areas, it has been little explored by epidemiologists. Despite its evolution and efficiency, SEM has a gap in terms of discovering causalities. To fill this gap, this study developed an R package called BNPA, whose methodology joins the best of Bayesian network structural learning algorithms (BNSL) from data and path analysis (PA) a SEM subarea. The BNPA was built with pre-processing functions. Its main algorithm allows creating an input model to start the PA from a data set semi-automatically generating information to analyze the PA performance. An analysis of cardiovascular disease’s main predictors was performed using the BNPA with data from the Canadian Community Health Survey (CCHS). Multiple linear regression (MR) was used as a gold standard methodology; the results of BNPA matched 85% of MR results. In conclusion, BNPA is efficient and can benefit researchers, mainly novices, by enabling them to build PA models from data. Furthermore, statisticians and PA experts will have more time to support these researchers instead of creating an initial model.
ER  - 
TY  - JOUR
T1  - Mortality prediction in intensive care units (ICUs) using a deep rule-based fuzzy classifier
A1  - Davoodi, Raheleh
A1  - Moradi, Mohammad Hassan
Y1  - 2018///
KW  -  Fuzzy classifier
KW  -  Intensive care units
KW  -  Mixed data
KW  -  Mortality prediction
KW  - Deep learning
JF  - Journal of Biomedical Informatics
VL  - 79
SP  - 48
EP  - 59
DO  - https://doi.org/10.1016/j.jbi.2018.02.008
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300273
N2  - Electronic health records (EHRs) contain critical information useful for clinical studies. Early assessment of patients’ mortality in intensive care units is of great importance. In this paper, a Deep Rule-Based Fuzzy System (DRBFS) was proposed to develop an accurate in-hospital mortality prediction in the intensive care unit (ICU) patients employing a large number of input variables. Our main contribution is proposing a system, which is capable of dealing with big data with heterogeneous mixed categorical and numeric attributes. In DRBFS, the hidden layer in each unit is represented by interpretable fuzzy rules. Benefiting the strength of soft partitioning, a modified supervised fuzzy k-prototype clustering has been employed for fuzzy rule generation. According to the stacked approach, the same input space is kept in every base building unit of DRBFS. The training set in addition to random shifts, obtained from random projections of prediction results of the current base building unit is presented as the input of the next base building unit. A cohort of 10,972 adult admissions was selected from Medical Information Mart for Intensive Care (MIMIC-III) data set, where 9.31% of patients have died in the hospital. A heterogeneous feature set of first 48 h from ICU admissions, were extracted for in-hospital mortality rate. Required preprocessing and appropriate feature extraction were applied. To avoid biased assessments, performance indexes were calculated using holdout validation. We have evaluated our proposed method with several common classifiers including naïve Bayes (NB), decision trees (DT), Gradient Boosting (GB), Deep Belief Networks (DBN) and D-TSK-FC. The area under the receiver operating characteristics curve (AUROC) for NB, DT, GB, DBN, D-TSK-FC and our proposed method were 73.51%, 61.81%, 72.98%, 70.07%, 66.74% and 73.90% respectively. Our results have demonstrated that DRBFS outperforms various methods, while maintaining interpretable rule bases. Besides, benefiting from specific clustering methods, DRBFS can be well scaled up for large heterogeneous data sets.
ER  - 
TY  - JOUR
T1  - Fully automated segmentation on brain ischemic and white matter hyperintensities lesions using semantic segmentation networks with squeeze-and-excitation blocks in MRI
A1  - Lee, A-Reum
A1  - Woo, Ilsang
A1  - Kang, Dong-Wha
A1  - Jung, Seung Chai
A1  - Lee, Hyunna
A1  - Kim, Namkug
Y1  - 2020///
KW  -  Deep learning
KW  -  Diffusion-weighted imaging (DWI)
KW  -  Medical image segmentation
KW  -  Semantic segmentation
KW  -  Squeeze-and-excitation (SE) block
KW  - Brain ischemic lesion segmentation
JF  - Informatics in Medicine Unlocked
VL  - 21
SP  - 100440
EP  - 100440
DO  - https://doi.org/10.1016/j.imu.2020.100440
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820305906
N2  - Ischemic stroke, a common disease in the elderly, can cause long-term disability and death. Quantitative measurement of brain ischemic lesions at the acute stage is vital for accurate diagnosis and treatment decisions for stroke patients. Currently, the manual segmentation of ischemic lesions is time-consuming, and it is difficult to extract potentially quantifiable information. These limitations can be overcome via deep learning, which has recently become a popular method to segment brain ischemic lesions in MRI. Fully automatic segmentation methods using deep learning quantitatively evaluate the infarct lesions and accurately identify lesions, localize fast lesions, and improve treatment planning. The state-of-the-art methods are variants of U-Net and are fully convolutional networks (FCN); however, these methods have a few limitations, including their inability to capture global features because of the encoder- and decoder-networks. To overcome this limitation, a semantic segmentation method using U-Net with squeeze-and-excitation (SE) blocks is proposed in this study. This method can improve channel-wise information with feature maps compared to a conventional network, to enhance the accuracy of brain lesion segmentation. Patients with acute infarction (N = 429) were retrospectively enrolled in this study with the approval of IRB. Various types of semantic segmentation networks with or without SE blocks were developed, and the accuracies were compared with conventional in-house and commercial software. The Dice similarity coefficient (DSC) of U-Net and Dense U-Net with SE blocks was 85.39±0.84 and 84.23±1.60, respectively. The DSCs increase by 3.5% and 0.9% on average. Accuracies of basic U-Net and Dense U-Net with SE blocks were significantly better than those of conventional image processing methods and those without SE blocks (p-values: 1.0e-08, 2.272e-05, and 0.0003, respectively). To additionally evaluate the methods, a public dataset of white matter hyperintensities (WMHs) segmentation challenge at MICCAI 2017 was used. In the WMH dataset, the DSCs of U-Net with or without SE blocks were 74.64±1.11 and 76.92±0.78, respectively. The DSC increases by 2.3% on average. The semantic segmentation with SE blocks exhibited significantly better performances than those without SE blocks. The solution can be applied to measure various brain segmentation problems, including infarct volume (at the acute stage of stroke patients) and WMH volume.
ER  - 
TY  - JOUR
T1  - Automatic detection of diabetic retinopathy exudates from non-dilated retinal images using mathematical morphology methods
A1  - Sopharak, Akara
A1  - Uyyanonvara, Bunyarit
A1  - Barman, Sarah
A1  - Williamson, Thomas H
Y1  - 2008///
KW  -  Exudates
KW  -  Morphology
KW  -  Non-dilated retinal images
KW  -  Retinal image
KW  - Diabetic retinopathy
JF  - Computerized Medical Imaging and Graphics
VL  - 32
IS  - 8
SP  - 720
EP  - 727
DO  - https://doi.org/10.1016/j.compmedimag.2008.08.009
UR  - https://www.sciencedirect.com/science/article/pii/S0895611108000931
N2  - Diabetic retinopathy is a complication of diabetes that is caused by changes in the blood vessels of the retina. The symptoms can blur or distort the patient’s vision and are a main cause of blindness. Exudates are one of the primary signs of diabetic retinopathy. Detection of exudates by ophthalmologists normally requires pupil dilation using a chemical solution which takes time and affects patients. This paper investigates and proposes a set of optimally adjusted morphological operators to be used for exudate detection on diabetic retinopathy patients’ non-dilated pupil and low-contrast images. These automatically detected exudates are validated by comparing with expert ophthalmologists’ hand-drawn ground-truths. The results are successful and the sensitivity and specificity for our exudate detection is 80% and 99.5%, respectively.
ER  - 
TY  - JOUR
T1  - Improved decision tree construction based on attribute selection and data sampling for fault diagnosis in rotating machines
A1  - Karabadji, Nour El Islem
A1  - Seridi, Hassina
A1  - Khelf, Ilyes
A1  - Azizi, Nabiha
A1  - Boulkroune, Ramzi
Y1  - 2014///
KW  -  Attribute selection
KW  -  Data sampling
KW  -  Pruning
KW  -  Research graph
KW  - Decision tree construction
JF  - Engineering Applications of Artificial Intelligence
VL  - 35
SP  - 71
EP  - 83
DO  - https://doi.org/10.1016/j.engappai.2014.06.010
UR  - https://www.sciencedirect.com/science/article/pii/S0952197614001328
N2  - This paper presents a new approach that avoids the over-fitting and complexity problems suffered in the construction of decision trees. Decision trees are an efficient means of building classification models, especially in industrial engineering. In their construction phase, the two main problems are choosing suitable attributes and database components. In the present work, a combination of attribute selection and data sampling is used to overcome these problems. To validate the proposed approach, several experiments are performed on 10 benchmark datasets, and the results are compared with those from classical approaches. Finally, we present an efficient application of the proposed approach in the construction of non-complex decision rules for fault diagnosis problems in rotating machines.
ER  - 
TY  - JOUR
T1  - Automatic wide field registration and mosaicking of OCTA images using vascularity information
A1  - Díaz, Macarena
A1  - de Moura, Joaquim
A1  - Novo, Jorge
A1  - Ortega, Marcos
Y1  - 2019///
KW  -  Mosaicking
KW  -  RANSAC
KW  -  Registration
KW  -  Retinal Vascularity
KW  -  SURF
KW  - Optical Coherence Tomography Angiography
JF  - Procedia Computer Science
VL  - 159
SP  - 505
EP  - 513
DO  - https://doi.org/10.1016/j.procs.2019.09.205
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919313882
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - Optical Coherence Tomography Angiography (OCTA) constitutes a novel ophthalmological image modality that is characterized for being a non-invasive capture technique that allows a profound analysis of the vascular characteristics of the eye fundus. Given the restricted field of view of the eye fundus that offers each scan, the specialists frequently capture several complementary images that may be simultaneously analyzed to offer a complete and accurate diagnosis of the patient. In this work, we propose a fully automatic method to register complementary OCTA images and provide compositions for the same patient, generating a wide field of representation that allows a simpler and more direct analysis than the traditional tedious manual procedures. To achieve this, we based our proposal in a robust combination of representative features that are filtered by an accurate identification of the main retinal vasculature. This way, given the characteristic high irregularity in the fundus of the OCTA images, we avoid many variable areas that may interfere in the registration process, restricting the analysis to the most representative and stable structure of this image modality, the main retinal vasculature. In particular, we use Speeded-Up Robust Features (SURF) algorithm to extract representative features in the main vascular region that is extracted using a method that combines the analysis of the Hessian matrix followed by an hysteresis threshold process. Then, using a K-NN model, we perform the registration of the resulting features from the different OCTA images to be analyzed. Finally, the Random sample consensus (RANSAC) method is exploited to produce the final target mosaic. The proposed method presented satisfactory results in the validation experiments, with accurate values for the MSE index of 1.2566 and 1.6725 pixels for the registration of paired images an mosaics, respectively.
ER  - 
TY  - JOUR
T1  - RedMed: Extending drug lexicons for social media applications
A1  - Lavertu, Adam
A1  - Altman, Russ B
Y1  - 2019///
KW  -  Drug Surveillance
KW  -  Lexicon
KW  -  Pharmacovigilance
KW  -  Social Media
KW  - Natural Language Processing
JF  - Journal of Biomedical Informatics
VL  - 99
SP  - 103307
EP  - 103307
DO  - https://doi.org/10.1016/j.jbi.2019.103307
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302266
N2  - Social media has been identified as a promising potential source of information for pharmacovigilance. The adoption of social media data has been hindered by the massive and noisy nature of the data. Initial attempts to use social media data have relied on exact text matches to drugs of interest, and therefore suffer from the gap between formal drug lexicons and the informal nature of social media. The Reddit comment archive represents an ideal corpus for bridging this gap. We trained a word embedding model, RedMed, to facilitate the identification and retrieval of health entities from Reddit data. We compare the performance of our model trained on a consumer-generated corpus against publicly available models trained on expert-generated corpora. Our automated classification pipeline achieves an accuracy of 0.88 and a specificity of >0.9 across four different term classes. Of all drug mentions, an average of 79% (±0.5%) were exact matches to a generic or trademark drug name, 14% (±0.5%) were misspellings, 6.4% (±0.3%) were synonyms, and 0.13% (±0.05%) were pill marks. We find that our system captures an additional 20% of mentions; these would have been missed by approaches that rely solely on exact string matches. We provide a lexicon of misspellings and synonyms for 2978 drugs and a word embedding model trained on a health-oriented subset of Reddit.
ER  - 
TY  - JOUR
T1  - Interstitial fluid glucose time-lag correction for real-time continuous glucose monitoring
A1  - Barry Keenan, D
A1  - Mastrototaro, John J
A1  - Weinzimer, Stuart A
A1  - Steil, Garry M
Y1  - 2013///
KW  -  Interstitial fluid
KW  -  Time-lag
KW  -  Wiener filter
KW  - Continuous glucose monitoring
JF  - Biomedical Signal Processing and Control
VL  - 8
IS  - 1
SP  - 81
EP  - 89
DO  - https://doi.org/10.1016/j.bspc.2012.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S1746809412000687
N2  - Time lag between subcutaneous interstitial fluid and plasma glucose decreases the accuracy of real-time continuous glucose monitors. However, inverse filters can be designed to correct time lag and attenuate noise enabling the blood–glucose profile to be reconstructed in real time from continuous measurements of the interstitial-fluid glucose. We designed and tested a Wiener filter using a set of 20 sensor-glucose tracings (∼30h each) with a 1-min sample interval. Delays of 10±2min (mean±SD) were introduced into each signal with additive Gaussian white noise (SNR=40dB). Performance of the filter was compared to conventional causal and non-causal seventh-order finite-impulse response (FIR) filters. Time lags introduced an error of 5.3±2.7%. The error increased in the presence of noise (to 5.7±2.6%) and attempts to remove the noise with conventional low-pass filtering increased the error still further (to 7.0±3.5%). In contrast, the Wiener filter decreased the error attributed to time delay by ∼50% in the presence of noise (from 5.7% to 2.60±1.26%) and by ∼75% in the absence of noise (5.3% to 1.3±1%). Introducing time-lag correction without increasing sensitivity to noise can increase CGM accuracy.
ER  - 
TY  - JOUR
T1  - Blood glucose control algorithms for type 1 diabetic patients: A methodological review
A1  - Lunze, Katrin
A1  - Singh, Tarunraj
A1  - Walter, Marian
A1  - Brendel, Mathias D
A1  - Leonhardt, Steffen
Y1  - 2013///
KW  -  Blood glucose control
KW  -  Insulin therapy devices
KW  -  Model predictive control
KW  -  Patient model
KW  - Artificial pancreas
JF  - Biomedical Signal Processing and Control
VL  - 8
IS  - 2
SP  - 107
EP  - 119
DO  - https://doi.org/10.1016/j.bspc.2012.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S1746809412001061
N2  - A method for optimal continuous insulin therapy for diabetes patients has been sought since the early 1970s. Although technical and medical advances have been made, a fully automated artificial pancreas to replace the functions of the natural organ is still a research aim. This review compares recent control algorithms for type 1 diabetic patients which automatically connect continuous glucose monitoring and insulin injection, without patient intervention. Black-box model and gray-box model based control strategies are described and their performances are evaluated, with a focus on their feasibility of implementation in a real-life situation. In conclusion, a satisfactory control strategy has not yet been proposed, mainly because most control algorithms rely on continuous blood glucose measurement which is not yet available. Modeling the effect of glucose ingestion as an external disturbance on the time evolution of blood glucose concentration, is now the norm for the control community. In contrast, the effects of physical activity on the metabolic system is not yet fully understood and remain an open issue. Moreover, clinical studies on evaluation of control performance are scarce. Therefore, research on blood glucose control needs to concentrate on advanced patient modeling, control optimization and control performance evaluation under realistic patient-oriented conditions.
ER  - 
TY  - JOUR
T1  - Predicting length of stay in hospitals intensive care unit using general admission features
A1  - Abd-Elrazek, Merhan A
A1  - Eltahawi, Ahmed A
A1  - Abd Elaziz, Mohamed H
A1  - Abd-Elwhab, Mohamed N
Y1  - 2021///
KW  -  Bias variance tradeoff
KW  -  Machine learning
KW  - Length of stay
JF  - Ain Shams Engineering Journal
VL  - 12
IS  - 4
SP  - 3691
EP  - 3702
DO  - https://doi.org/10.1016/j.asej.2021.02.018
UR  - https://www.sciencedirect.com/science/article/pii/S2090447921001349
N2  - According to the World Health Organization (WHO), patient Length of Stay (LOS) in hospitals is an important performance measurement and monitoring indicator. Prolonged LOS in the Intensive Care Unit (ICU) may lead to consuming hospital resources, manpower, and equipment. Therefore, accurate prediction of patient LOS may aid the healthcare specialists to take medical decisions and allocate medical team and resources. As well, the patient and insurance companies may use this prediction to manage their budget. In this paper, a framework for predicting patient LOS in the ICU using different machine learning (ML) techniques is proposed. Unlike most of the previous studies, this study relies on general medical features collected on patient admission regardless of the patient diagnosis. This provide a broad scope and cover all patients making this approach general and easy to use. The prediction accuracy of the proposed approach was recorded to be very high and different for each ML technique. For example, the best prediction accuracy was achieved by fuzzy with accuracy reach 92%, while classification tree managed to achieve a prediction accuracy of 90% coming in the second place.
ER  - 
TY  - JOUR
T1  - Substituting clinical features using synthetic medical phrases: Medical text data augmentation techniques
A1  - Abdollahi, Mahdi
A1  - Gao, Xiaoying
A1  - Mei, Yi
A1  - Ghosh, Shameek
A1  - Li, Jinyan
A1  - Narag, Michael
Y1  - 2021///
KW  -  Data augmentation
KW  -  Machine learning
KW  -  Medical document classification
KW  -  Natural language processing
KW  - Unified Medical Language System
JF  - Artificial Intelligence in Medicine
VL  - 120
SP  - 102167
EP  - 102167
DO  - https://doi.org/10.1016/j.artmed.2021.102167
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721001603
N2  - Biomedical natural language processing (NLP) has an important role in extracting consequential information in medical discharge notes. Detecting meaningful features from unstructured notes is a challenging task in medical document classification. The domain specific phrases and different synonyms within the medical documents make it hard to analyze them. Analyzing clinical notes becomes more challenging for short documents like abstract texts. All of these can result in poor classification performance, especially when there is a shortage of the clinical data in real life. Two new approaches (an ontology-guided approach and a combined ontology-based with dictionary-based approach) are suggested for augmenting medical data to enrich training data. Three different deep learning approaches are used to evaluate the classification performance of the proposed methods. The obtained results show that the proposed methods improved the classification accuracy in clinical notes classification.
ER  - 
TY  - JOUR
T1  - Detection of microaneurysms using multi-scale correlation coefficients
A1  - Zhang, Bob
A1  - Wu, Xiangqian
A1  - You, Jane
A1  - Li, Qin
A1  - Karray, Fakhri
Y1  - 2010///
KW  -  Diabetic retinopathy (DR)
KW  -  Microaneurysm (red lesion) detection
KW  -  Multi-scale correlation filtering
KW  - Computer-aided diagnosis (CAD)
JF  - Pattern Recognition
VL  - 43
IS  - 6
SP  - 2237
EP  - 2248
DO  - https://doi.org/10.1016/j.patcog.2009.12.017
UR  - https://www.sciencedirect.com/science/article/pii/S003132031000004X
N2  - This paper presents a new approach to the computer aided diagnosis (CAD) of diabetic retinopathy (DR)—a common and severe complication of long-term diabetes which damages the retina and cause blindness. Since microaneurysms are regarded as the first signs of DR, there has been extensive research on effective detection and localization of these abnormalities in retinal images. In contrast to existing algorithms, a new approach based on multi-scale correlation filtering (MSCF) and dynamic thresholding is developed. This consists of two levels, microaneurysm candidate detection (coarse level) and true microaneurysm classification (fine level). The approach was evaluated based on two public datasets—ROC (retinopathy on-line challenge, http://roc.healthcare.uiowa.edu) and DIARETDB1 (standard diabetic retinopathy database, http://www.it.lut.fi/project/imageret/diaretdb1). We conclude our method to be effective and efficient.
ER  - 
TY  - JOUR
T1  - A novel embedded min-max approach for feature selection in nonlinear Support Vector Machine classification
A1  - Jiménez-Cordero, Asunción
A1  - Morales, Juan Miguel
A1  - Pineda, Salvador
Y1  - 2021///
KW  -  Duality theory
KW  -  Feature selection
KW  -  Min-max optimization
KW  -  Nonlinear Support Vector Machine classification
KW  - Machine learning
JF  - European Journal of Operational Research
VL  - 293
IS  - 1
SP  - 24
EP  - 35
DO  - https://doi.org/10.1016/j.ejor.2020.12.009
UR  - https://www.sciencedirect.com/science/article/pii/S0377221720310195
N2  - In recent years, feature selection has become a challenging problem in several machine learning fields, such as classification problems. Support Vector Machine (SVM) is a well-known technique applied in classification tasks. Various methodologies have been proposed in the literature to select the most relevant features in SVM. Unfortunately, all of them either deal with the feature selection problem in the linear classification setting or propose ad-hoc approaches that are difficult to implement in practice. In contrast, we propose an embedded feature selection method based on a min-max optimization problem, where a trade-off between model complexity and classification accuracy is sought. By leveraging duality theory, we equivalently reformulate the min-max problem and solve it without further ado using off-the-shelf software for nonlinear optimization. The efficiency and usefulness of our approach are tested on several benchmark data sets in terms of accuracy, number of selected features and interpretability.
ER  - 
TY  - JOUR
T1  - Relabeling algorithm for retrieval of noisy instances and improving prediction quality
A1  - Shah, Shital
A1  - Kusiak, Andrew
Y1  - 2010///
KW  -  Bladder cancer immunotherapy
KW  -  Confidence index
KW  -  Confusion region
KW  -  Noisy data
KW  -  Non-separable decision boundaries
KW  - Relabeling algorithm
JF  - Computers in Biology and Medicine
VL  - 40
IS  - 3
SP  - 288
EP  - 299
DO  - https://doi.org/10.1016/j.compbiomed.2009.12.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482509002212
N2  - A relabeling algorithm for retrieval of noisy instances with binary outcomes is presented. The relabeling algorithm iteratively retrieves, selects, and re-labels data instances (i.e., transforms a decision space) to improve prediction quality. It emphasizes knowledge generalization and confidence rather than classification accuracy. A confidence index incorporating classification accuracy, prediction error, impurities in the relabeled dataset, and cluster purities was designed. The proposed approach is illustrated with a binary outcome dataset and was successfully tested on the standard benchmark four UCI repository dataset as well as bladder cancer immunotherapy data. A subset of the most stable instances (i.e., 7% to 51% of the sample) with high confidence (i.e., between 64%–99.44%) was identified for each application along with most noisy instances. The domain experts and the extracted knowledge validated the relabeled instances and corresponding confidence indexes. The relabeling algorithm with some modifications can be applied to other medical, industrial, and service domains.
ER  - 
TY  - JOUR
T1  - PMCVec: Distributed phrase representation for biomedical text processing
A1  - Gero, Zelalem
A1  - Ho, Joyce
Y1  - 2019///
KW  -  Biomedical NLP
KW  -  PubMed abstracts
KW  - Phrase embeddings
JF  - Journal of Biomedical Informatics
VL  - 100
SP  - 100047
EP  - 100047
DO  - https://doi.org/10.1016/j.yjbinx.2019.100047
UR  - https://www.sciencedirect.com/science/article/pii/S2590177X19300460
N1  - Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019
N2  - Distributed semantic representation of biomedical text can be beneficial for text classification, named entity recognition, query expansion, human comprehension, and information retrieval. Despite the success of high-quality vector space models such as Word2Vec and GloVe, they only provide unigram word representations and the semantics for multi-word phrases can only be approximated by composition. This is problematic in biomedical text processing where technical phrases for diseases, symptoms, and drugs should be represented as single entities to capture the correct meaning. In this paper, we introduce PMCVec, an unsupervised technique that generates important phrases from PubMed abstracts and learns embeddings for single words and multi-word phrases simultaneously. Evaluations performed on benchmark datasets produce significant performance gains both qualitatively and quantitatively.
ER  - 
TY  - JOUR
T1  - Dynamic modular fuzzy neural classifier with tree-based structure identification
A1  - Pertselakis, Minas
A1  - Stafylopatis, Andreas
Y1  - 2008///
KW  -  Decision trees
KW  -  Modular multi-net systems
KW  -  Neuro-fuzzy modeling
KW  -  Pattern recognition
KW  - Structure identification
JF  - Neurocomputing
VL  - 71
IS  - 4
SP  - 801
EP  - 812
DO  - https://doi.org/10.1016/j.neucom.2007.03.009
UR  - https://www.sciencedirect.com/science/article/pii/S0925231207001014
N1  - Neural Networks: Algorithms and Applications 50 Years of Artificial Intelligence: a Neuronal Approach
N2  - Inspired by a modular way of reasoning, we present a subsethood-product fuzzy neural classifier with a novel, robust and dynamic architecture, involving a main module and a number of submodules. The well-known classification and regression trees (CART) algorithm is employed as a fast preprocess of structure identification, which divides the input space into high certainty and low certainty regions, each representing a primary fuzzy rule. These primary fuzzy rules use a minimum set of attributes and are mapped onto the main neuro-fuzzy module. However, the patterns belonging to a low certainty primary rule get further split into a subset of secondary rules that use an extended set of attributes. Each such rule subset is mapped onto an expert-submodule, which gets activated only when a pattern falls into the respective low certainty region. In other words, we create a rule form of “if—then–if” conditional statement, where the first “IF” concerns the main module and the primary rule, while the second “IF” concerns the respective submodule and the secondary rule set. This dynamic resource-allocating model is optimized through a supervised learning procedure. Experiments in benchmark classification tasks prove that this architecture not only does reduce complexity and computational cost, which is its primary goal, but also offers fast and accurate processing during real-time operation. Moreover, it holds certain properties that make it ideal for soft computing applications of high dimension, especially those that adopt user-profiles or require partial re-training.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy detection using red lesion localization and convolutional neural networks
A1  - Zago, Gabriel Tozatto
A1  - Andreão, Rodrigo Varejão
A1  - Dorizzi, Bernadette
A1  - Teatini Salles, Evandro Ottoni
Y1  - 2020///
KW  -  Convolutional neural networks
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  - Retinal images
JF  - Computers in Biology and Medicine
VL  - 116
SP  - 103537
EP  - 103537
DO  - https://doi.org/10.1016/j.compbiomed.2019.103537
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519303968
N2  - Detecting the early signs of diabetic retinopathy (DR) is essential, as timely treatment might reduce or even prevent vision loss. Moreover, automatically localizing the regions of the retinal image that might contain lesions can favorably assist specialists in the task of detection. In this study, we designed a lesion localization model using a deep network patch-based approach. Our goal was to reduce the complexity of the model while improving its performance. For this purpose, we designed an efficient procedure (including two convolutional neural network models) for selecting the training patches, such that the challenging examples would be given special attention during the training process. Using the labeling of the region, a DR decision can be given to the initial image, without the need for special training. The model is trained on the Standard Diabetic Retinopathy Database, Calibration Level 1 (DIARETDB1) database and is tested on several databases (including Messidor) without any further adaptation. It reaches an area under the receiver operating characteristic curve of 0.912−95%CI(0.897−0.928) for DR screening, and a sensitivity of 0.940−95%CI(0.921−0.959). These values are competitive with other state-of-the-art approaches.
ER  - 
TY  - JOUR
T1  - Biases in feature selection with missing data
A1  - Seijo-Pardo, Borja
A1  - Alonso-Betanzos, Amparo
A1  - Bennett, Kristin P
A1  - Bolón-Canedo, Verónica
A1  - Josse, Julie
A1  - Saeed, Mehreen
A1  - Guyon, Isabelle
Y1  - 2019///
KW  -  De-biased t-test
KW  -  Missing data
KW  - Feature selection
JF  - Neurocomputing
VL  - 342
SP  - 97
EP  - 112
DO  - https://doi.org/10.1016/j.neucom.2018.10.085
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219301493
N1  - Advances in artificial neural networks, machine learning and computational intelligence
N2  - Feature selection is of great importance for two possible scenarios: (1) prediction, i.e., improving (or minimally degrading) the predictions of a target variable while discarding redundant or uninformative features and (2) discovery, i.e., identifying features that are truly dependent on the target and may be genuine causes to be determined in experimental verifications (for example for the task of drug target discovery in genomics). In both cases, if variables have a large number of missing values, imputing them may lead to false positives; features that are not associated with the target become dependent as a result of imputation. In the first scenario, this may not harm prediction, but in the second one, it will erroneously select irrelevant features. In this paper, we study the risk/benefit trade-off of missing value imputation in the context of feature selection, using causal graphs to characterize when structural bias arises. Our aim is also to investigate situations in which imputing missing values may be beneficial to reduce false negatives, a situation that might arise when there is a dependency between feature and target, but the dependency is below the significance level when only complete cases are considered. However, the benefits of reducing false negatives must be balanced against the increased number of false positives. In the case of binary target variable and continuous features, the t-test is often used for univariate feature selection. In this paper, we also introduce a de-biased version of the t-test allowing us to reap the benefits of imputation, while not incurring the penalty of increasing the number of false positives.
ER  - 
TY  - JOUR
T1  - Automated detection of chronic kidney disease using image fusion and graph embedding techniques with ultrasound images
A1  - Gudigar, Anjan
A1  - U, Raghavendra
A1  - Samanth, Jyothi
A1  - Gangavarapu, Mokshagna Rohit
A1  - Kudva, Abhilash
A1  - Paramasivam, Ganesh
A1  - Nayak, Krishnananda
A1  - Tan, Ru-San
A1  - Molinari, Filippo
A1  - Ciaccio, Edward J
A1  - Rajendra Acharya, U
Y1  - 2021///
KW  -  Fusion
KW  -  Graph embedding
KW  -  Support vector machine
KW  -  Ultrasound image
KW  - Chronic kidney disease
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102733
EP  - 102733
DO  - https://doi.org/10.1016/j.bspc.2021.102733
UR  - https://www.sciencedirect.com/science/article/pii/S174680942100330X
N2  - Chronic Kidney disease (CKD) is a progressive disease affecting more than twenty million individuals in the United States. Disease progression is often characterized by complications such as cardiovascular diseases, anemia, hyperlipidemia and metabolic bone diseases etc., Based on estimated GFR values, the disease is categorized in 5 stages which significantly influence patient outcome. Cardiovascular ultrasound (US) (echocardiography) imagery demonstrate significant hemodynamic alterations that are secondary to CKD in the form of volume/ pressure overload. As the CKD pathology directly impacts cardiovascular disease, the US imaging shows structural and hemodynamic adaptation. Hence, the development of a computer-aided diagnosis (CAD) model to predict CKD would be desirable, and can potentially improve treatment. Several prior studies have utilized kidney features for quantitative analysis. In this paper, acquisition of the four-chamber heart US image is employed to predict CKD stage. The method combines image and feature fusion techniques under a graph embedding framework to characterize heart chamber properties. Moreover, a support vector machine is incorporated to classify heart US images. The proposed method achieved 100 % accuracy for a two-class system, and 99.09 % accuracy for a multi-class categorization scenario. Hence, our proposed CAD tool is deployable in both clinic and hospital settings for computer-aided screening of CKD.
ER  - 
TY  - JOUR
T1  - Application of different imaging modalities for diagnosis of Diabetic Macular Edema: A review
A1  - Mookiah, Muthu Rama Krishnan
A1  - Acharya, U Rajendra
A1  - Fujita, Hamido
A1  - Tan, Jen Hong
A1  - Chua, Chua Kuang
A1  - Bhandary, Sulatha V
A1  - Laude, Augustinus
A1  - Tong, Louis
Y1  - 2015///
KW  -  Biomicroscopy
KW  -  Computer-aided diagnosis
KW  -  Fluorescein Angiography
KW  -  Fundus imaging
KW  -  Optical Coherence Tomography
KW  - Diabetic Macular Edema
JF  - Computers in Biology and Medicine
VL  - 66
SP  - 295
EP  - 315
DO  - https://doi.org/10.1016/j.compbiomed.2015.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515003224
N2  - Diabetic Macular Edema (DME) is caused by accumulation of extracellular fluid from hyperpermeable capillaries within the macula. DME is one of the leading causes of blindness among Diabetes Mellitus (DM) patients. Early detection followed by laser photocoagulation can save the visual loss. This review discusses various imaging modalities viz. biomicroscopy, Fluorescein Angiography (FA), Optical Coherence Tomography (OCT) and colour fundus photographs used for diagnosis of DME. Various automated DME grading systems using retinal fundus images, associated retinal image processing techniques for fovea, exudate detection and segmentation are presented. We have also compared various imaging modalities and automated screening methods used for DME grading. The reviewed literature indicates that FA and OCT identify DME related changes accurately. FA is an invasive method, which uses fluorescein dye, and OCT is an expensive imaging method compared to fundus photographs. Moreover, using fundus images DME can be identified and automated. DME grading algorithms can be implemented for telescreening. Hence, fundus imaging based DME grading is more suitable and affordable method compared to biomicroscopy, FA, and OCT modalities.
ER  - 
TY  - JOUR
T1  - Improving biomedical word representation with locally linear embedding
A1  - Zhao, Di
A1  - Wang, Jian
A1  - Chu, Yonghe
A1  - Zhang, Yijia
A1  - Yang, Zhihao
A1  - Lin, Hongfei
Y1  - 2021///
KW  -  Biomedical word embedding
KW  -  Electronic health records
KW  -  Manifold learning
KW  - Distributed word representation
JF  - Neurocomputing
VL  - 447
SP  - 172
EP  - 182
DO  - https://doi.org/10.1016/j.neucom.2021.02.071
UR  - https://www.sciencedirect.com/science/article/pii/S0925231221003271
N2  - Distributed word representation, usually obtained through calculation from large corpora, has been widely used in biomedical text because of its effectiveness in representing word semantic information. High-quality and meaningful biomedical words enable doctors to obtain the gist of information and knowledge in a short time to make clinical decisions quickly. Currently, the distributed word representation ignores the influence of the word embedding geometric structure obtained through calculation on the word semantic information and cannot accurately represent the word information, thus affecting the representation effect of biomedical text. To solve the above problems, we propose a biomedical word embedding framework based on manifold learning. Our work provides new perspectives for representing biomedical word embedding, which is the key concept in biomedical natural language processing tasks. First, the distributed word representation model is used to obtain the pretrained word embedding, and then the manifold learning is used to re-embed the pretrained word embedding. To verify the validity of the proposed framework in the biomedical domain, we evaluate the algorithm by using biomedical texts. Experimental results show that the proposed method can effectively improve the results of electronic health record classification and semantic similarity.
ER  - 
TY  - JOUR
T1  - Artery–vein segmentation in fundus images using a fully convolutional network
A1  - Hemelings, Ruben
A1  - Elen, Bart
A1  - Stalmans, Ingeborg
A1  - Van Keer, Karel
A1  - De Boever, Patrick
A1  - Blaschko, Matthew B
Y1  - 2019///
KW  -  Artery–vein segmentation
KW  -  Fully convolutional network
KW  - Fundus image
JF  - Computerized Medical Imaging and Graphics
VL  - 76
SP  - 101636
EP  - 101636
DO  - https://doi.org/10.1016/j.compmedimag.2019.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S0895611118306025
N2  - Epidemiological studies demonstrate that dimensions of retinal vessels change with ocular diseases, coronary heart disease and stroke. Different metrics have been described to quantify these changes in fundus images, with arteriolar and venular calibers among the most widely used. The analysis often includes a manual procedure during which a trained grader differentiates between arterioles and venules. This step can be time-consuming and can introduce variability, especially when large volumes of images need to be analyzed. In light of the recent successes of fully convolutional networks (FCNs) applied to biomedical image segmentation, we assess its potential in the context of retinal artery–vein (A/V) discrimination. To the best of our knowledge, a deep learning (DL) architecture for simultaneous vessel extraction and A/V discrimination has not been previously employed. With the aim of improving the automation of vessel analysis, a novel application of the U-Net semantic segmentation architecture (based on FCNs) on the discrimination of arteries and veins in fundus images is presented. By utilizing DL, results are obtained that exceed accuracies reported in the literature. Our model was trained and tested on the public DRIVE and HRF datasets. For DRIVE, measuring performance on vessels wider than two pixels, the FCN achieved accuracies of 94.42% and 94.11% on arteries and veins, respectively. This represents a decrease in error of 25% over the previous state of the art reported by Xu et al. (2017). Additionally, we introduce the HRF A/V ground truth, on which our model achieves 96.98% accuracy on all discovered centerline pixels. HRF A/V ground truth validated by an ophthalmologist, predicted A/V annotations and evaluation code are available at https://github.com/rubenhx/av-segmentation.
ER  - 
TY  - JOUR
T1  - Clinical workflow for personalized foot pressure ulcer prevention
A1  - Bucki, M
A1  - Luboz, V
A1  - Perrier, A
A1  - Champion, E
A1  - Diot, B
A1  - Vuillerme, N
A1  - Payan, Y
Y1  - 2016///
KW  -  Finite element method
KW  -  Patient-specific
KW  -  Soft tissues
KW  - Foot pressure ulcer
JF  - Medical Engineering & Physics
VL  - 38
IS  - 9
SP  - 845
EP  - 853
DO  - https://doi.org/10.1016/j.medengphy.2016.04.017
UR  - https://www.sciencedirect.com/science/article/pii/S135045331630073X
N2  - Foot pressure ulcers are a common complication of diabetes because of patient's lack of sensitivity due to neuropathy. Deep pressure ulcers appear internally when pressures applied on the foot create high internal strains nearby bony structures. Monitoring tissue strains in persons with diabetes is therefore important for an efficient prevention. We propose to use personalized biomechanical foot models to assess strains within the foot and to determine the risk of ulcer formation. Our workflow generates a foot model adapted to a patient's morphology by deforming an atlas model to conform it to the contours of segmented medical images of the patient's foot. Our biomechanical model is composed of rigid bodies for the bones, joined by ligaments and muscles, and a finite element mesh representing the soft tissues. Using our registration algorithm to conform three datasets, three new patient models were created. After applying a pressure load below these foot models, the Von Mises equivalent strains and “cluster volumes” (i.e. volumes of contiguous elements with strains above a given threshold) were measured within eight functionally meaningful foot regions. The results show the variability of both location and strain values among the three considered patients. This study also confirms that the anatomy of the foot has an influence on the risk of pressure ulcer.
ER  - 
TY  - JOUR
T1  - Application of data mining techniques for detecting asymptomatic carotid artery stenosis
A1  - Bilge, Ugur
A1  - Bozkurt, Selen
A1  - Durmaz, Sedat
Y1  - 2013///
JF  - Computers & Electrical Engineering
VL  - 39
IS  - 5
SP  - 1499
EP  - 1505
DO  - https://doi.org/10.1016/j.compeleceng.2012.12.010
UR  - https://www.sciencedirect.com/science/article/pii/S0045790612002522
N2  - Asymptomatic carotid stenosis, one of the etiological factors for stroke, has several risk factors such as hypertension, cardiac morbidity, smoking, diabetes, and physical inactivity. Understanding and determining factors that predispose to asymptomatic carotid stenosis will help in the design of acute stroke trials and in prevention programs. The goal of this study is to explore rules and relationships that might be used to detect possible asymptomatic carotid stenosis by using data mining techniques. For this purpose, Genetic Algorithms (GAs), Logistic Regression (LR), and Chi-square tests have been applied to the patient dataset. Results of these tests have also been compared.
ER  - 
TY  - JOUR
T1  - Automatic arteriosclerotic retinopathy grading using four-channel with image merging
A1  - Gao, Shuo
A1  - Gao, Li
A1  - Quan, Xiongwen
A1  - Zhang, Han
A1  - Bai, Hang
A1  - Kang, Chuanze
Y1  - 2021///
KW  -  ArcLossdeep
KW  -  Contour channel
KW  -  Image merge
KW  -  convolutional neural network
KW  - Arteriosclerotic retinopathy grading
JF  - Computer Methods and Programs in Biomedicine
VL  - 208
SP  - 106274
EP  - 106274
DO  - https://doi.org/10.1016/j.cmpb.2021.106274
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721003485
N2  - Background and objective
Arteriosclerosis can reflect the severity of hypertension, which is one of the main diseases threatening human life safety. But Arteriosclerosis retinopathy detection involves costly and time-consuming manual assessment. To meet the urgent needs of automation, this paper developed a novel arteriosclerosis retinopathy grading method based on convolutional neural network.
Methods
Firstly, we propose a good scheme for extracting features facing the fundus blood vessel background using image merging for contour enhancement. In this step, the original image is dealt with adaptive threshold processing to generate the new contour channel, which merge with the original three-channel image. Then, we employ the pre-trained convolutional neural network with transfer learning to speed up training and contour image channel parameter with Kaiming initialization. Moreover, ArcLoss is applied to increase inter-class differences and intra-class similarity aiming to the high similarity of images of different classes in the dataset.
Results
The accuracy of arteriosclerosis retinopathy grading achieved by the proposed method is up to 65.354%, which is nearly 4% higher than those of the exiting methods. The Kappa of our method is 0.508 in arteriosclerosis retinopathy grading.
Conclusions
An experimental study on multiple metrics demonstrates the superiority of our method, which will be a useful to the toolbox for arteriosclerosis retinopathy grading.
ER  - 
TY  - JOUR
T1  - COVID-19 identification from volumetric chest CT scans using a progressively resized 3D-CNN incorporating segmentation, augmentation, and class-rebalancing
A1  - Hasan, Md. Kamrul
A1  - Jawad, Md. Tasnim
A1  - Hasan, Kazi Nasim Imtiaz
A1  - Partha, Sajal Basak
A1  - Masba, Md. Masum Al
A1  - Saha, Shumit
A1  - Moni, Mohammad Ali
Y1  - 2021///
KW  -  3D convolutional neural network
KW  -  3D patches
KW  -  Progressive resizing
KW  -  Volumetric chest CT scans
KW  - COVID-19
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100709
EP  - 100709
DO  - https://doi.org/10.1016/j.imu.2021.100709
UR  - https://www.sciencedirect.com/science/article/pii/S235291482100191X
N2  - The novel COVID-19 is a global pandemic disease overgrowing worldwide. Computer-aided screening tools with greater sensitivity are imperative for disease diagnosis and prognosis as early as possible. It also can be a helpful tool in triage for testing and clinical supervision of COVID-19 patients. However, designing such an automated tool from non-invasive radiographic images is challenging as many manually annotated datasets are not publicly available yet, which is the essential core requirement of supervised learning schemes. This article proposes a 3D Convolutional Neural Network (CNN)-based classification approach considering both the inter-and intra-slice spatial voxel information. The proposed system is trained end-to-end on the 3D patches from the whole volumetric Computed Tomography (CT) images to enlarge the number of training samples, performing the ablation studies on patch size determination. We integrate progressive resizing, segmentation, augmentations, and class-rebalancing into our 3D network. The segmentation is a critical prerequisite step for COVID-19 diagnosis enabling the classifier to learn prominent lung features while excluding the outer lung regions of the CT scans. We evaluate all the extensive experiments on a publicly available dataset named MosMed, having binary- and multi-class chest CT image partitions. Our experimental results are very encouraging, yielding areas under the Receiver Operating Characteristics (ROC) curve of 0.914±0.049 and 0.893±0.035 for the binary- and multi-class tasks, respectively, applying 5-fold cross-validations. Our method’s promising results delegate it as a favorable aiding tool for clinical practitioners and radiologists to assess COVID-19.
ER  - 
TY  - JOUR
T1  - A novel weighted distance threshold method for handling medical missing values
A1  - Cheng, Ching-Hsue
A1  - Chang, Jing-Rong
A1  - Huang, Hao-Hsuan
Y1  - 2020///
KW  -  Distance threshold
KW  -  Imputation technique
KW  -  Missing values
KW  - Stroke disease
JF  - Computers in Biology and Medicine
VL  - 122
SP  - 103824
EP  - 103824
DO  - https://doi.org/10.1016/j.compbiomed.2020.103824
UR  - https://www.sciencedirect.com/science/article/pii/S001048252030189X
N2  - Data in the medical field often contain missing values and may result in biased research results. Therefore, the objective of this work is to propose a new imputation method, a novel weighted distance threshold method, to impute missing values. After several experiments, we find that the proposed imputation method has the following benefits. (1) The proposed method with purity can reassign instances into the nearest class of the dataset, and the purity computation can filter outliers; (2) The proposed method redefines the degree of missing values and can determine attributes and instances relative to the missing values in different datasets; and (3) The proposed method need not set the k value of the nearest neighborhood because this study identifies the k value based on the best threshold to calculate purity to enhance the results of imputation. In addition, the distance threshold can adjust the optimal nearest neighborhood to estimate missing values. This study implements several experiments to compare the proposed method with other imputation methods using different missing types, missing degrees, and types of datasets. The results indicate that the proposed imputation method is better than the listed methods. Moreover, this study uses the stroke dataset from the International Stroke Trial (IST) to verify whether the proposed method can be effectively applied in practice, and the results show that the proposed method achieves 90% accuracy in the Stroke dataset.
ER  - 
TY  - JOUR
T1  - Data processing platform design and algorithm research of wearable sports physiological parameters detection based on medical internet of things
A1  - Wu, Qinqin
A1  - Tang, Panyu
A1  - Yang, Maolin
Y1  - 2020///
KW  -  Data processing platform
KW  -  Medical Internet of things
KW  -  Physiological parameters
KW  -  Wearable
KW  - Electrocardiogram (ECG)
JF  - Measurement
VL  - 165
SP  - 108172
EP  - 108172
DO  - https://doi.org/10.1016/j.measurement.2020.108172
UR  - https://www.sciencedirect.com/science/article/pii/S0263224120307107
N2  - In this paper, the data processing platform for the detection of wearable Sports Physiological Parameters Based on the medical Internet of things can be connected with the network to build a medical Internet of things platform for remote monitoring of patients' lives, which can process physiological data in real time. Finally, the important physiological parameter array of human body is obtained and transmitted to ZigBee network in real time. If the patient needs further diagnosis, the collected data can also be sent to the home gateway through ZigBee and then sent to the telemedicine center through internwt for diagnosis by the doctor and the diagnosis results will be sent back to the patient. The data processing platform can realize the remote and real-time information interaction between doctors and patients.
ER  - 
TY  - JOUR
T1  - Retinal Blood Vessel Segmentation Approach Based on Mathematical Morphology
A1  - Hassan, Gehad
A1  - El-Bendary, Nashwa
A1  - Hassanien, Aboul Ella
A1  - Fahmy, Ali
A1  - Abullah M., Shoeb
A1  - Snasel, Vaclav
Y1  - 2015///
KW  -  k-means clustering
KW  -  mathematical morphology
KW  -  retinal image
KW  -  vessel segmentation
KW  - blood vessel extraction
JF  - Procedia Computer Science
VL  - 65
SP  - 612
EP  - 622
DO  - https://doi.org/10.1016/j.procs.2015.09.005
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915028355
N1  - International Conference on Communications, management, and Information technology (ICCMIT'2015)
N2  - Diabetic retinopathy is a disease, which forms a severe threat on sight. It may reach to blindness among working age people. By analyzing and detecting of vasculature structures in retinal images, we can early detect the diabetes in advanced stages by comparison of its states of retinal blood vessels. In this paper, we present blood vessel segmentation approach, which can be used in computer based retinal image analysis to extract the retinal image vessels. Mathematical morphology and K-means clustering are used to segment the vessels. To enhance the blood vessels and suppress the background information, we perform smoothing operation on the retinal image using mathematical morphology. Then the enhanced image is segmented using K-means clustering algorithm. The proposed approach is tested on the DRIVE dataset and is compared with alternative approaches. Experimental results obtained by the proposed approach showed that it is effective as it achieved average accuracy of 95.10% and best accuracy of 96.25%.
ER  - 
TY  - JOUR
T1  - Modulo 9 model-based learning for missing data imputation
A1  - Ngueilbaye, Alladoumbaye
A1  - Wang, Hongzhi
A1  - Mahamat, Daouda Ahmat
A1  - Junaidu, Sahalu B
Y1  - 2021///
KW  -  Machine learning algorithms
KW  -  Missing data
KW  -  Modulo 9
KW  - Data quality
JF  - Applied Soft Computing
VL  - 103
SP  - 107167
EP  - 107167
DO  - https://doi.org/10.1016/j.asoc.2021.107167
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621000909
N2  - Missing Values Management is one of the challenges faced by Data Analysts. Therefore, the creation of effective data models will be the right decision for missing data imputation. However, learning, training, and Data Analysis must be implemented through machine learning algorithms. Missing Data is a problem with no feedback or variables. This problem (missing data) can result in serious Data Analysis, which may eventually lead to erroneous conclusions. This research paper first studies how missing data can affect Machine Learning Algorithms, and decision-making based on the Data Analysis’s output. Secondly, it proposes Modulo 9 as a novel method for handling missing data problems. The proposed novel method is assessed with wide-ranging experiments compared with robust Machine Learning techniques such as Support Vector Machine (SVM) Algorithm, Linear Regression (LR), K-Nearest Neighbors (KNN), Naïve Bayes (NB), Support Vector Classifier (SVC), Linear Support Vector Classifier (LSVC), Random Forest Classifier (RFC), Decision Tree Regressor (DTR), Deletion Method, Multi-Layer Perceptron (MLP), and the Mean Value. The results show that the novel method outperforms the eleven (11) existing methods.
ER  - 
TY  - JOUR
T1  - Preserving empirical data utility in k-anonymous microaggregation via linear discriminant analysis
A1  - Rodríguez-Hoyos, Ana
A1  - Rebollo-Monedero, David
A1  - Estrada-Jiménez, José
A1  - Forné, Jordi
A1  - Urquiza-Aguiar, Luis
Y1  - 2020///
KW  -  Data utility
KW  -  LDA
KW  -  Microaggregation
KW  -  Statistical disclosure control
KW  - Data privacy
JF  - Engineering Applications of Artificial Intelligence
VL  - 94
SP  - 103787
EP  - 103787
DO  - https://doi.org/10.1016/j.engappai.2020.103787
UR  - https://www.sciencedirect.com/science/article/pii/S0952197620301792
N2  - Today’s countless benefits of exploiting data come with a hefty price in terms of privacy. k-Anonymous microaggregation is a powerful technique devoted to revealing useful demographic information of microgroups of people, whilst protecting the privacy of individuals therein. Evidently, the inherent distortion of data results in the degradation of its utility. This work proposes and analyzes an anonymization method that draws upon the technique of linear discriminant analysis (LDA), with the aim of preserving the empirical utility of data. Further, this utility is measured as the accuracy of a machine learning model trained on the microaggregated data. By transforming the original data records to a different data space, LDA enables k-anonymous microaggregation to build microcells more tailored to an intrinsic classification threshold. To do this, first, data is rotated (projected) towards the direction of maximum discrimination and, second, scaled in this direction by a factor α that penalizes distortion across the classification threshold. The upshot is that thinner cells are built along the threshold, which ends up preserving data utility in terms of the accuracy of machine learned models for a number of standardized data sets.
ER  - 
TY  - JOUR
T1  - Retinal vessels segmentation based on level set and region growing
A1  - Qian Zhao, Yu
A1  - Hong Wang, Xiao
A1  - Fang Wang, Xiao
A1  - Shih, Frank Y
Y1  - 2014///
KW  -  2D Gabor wavelet
KW  -  Level set
KW  -  Region growing
KW  - Retinal vessel segmentation
JF  - Pattern Recognition
VL  - 47
IS  - 7
SP  - 2437
EP  - 2446
DO  - https://doi.org/10.1016/j.patcog.2014.01.006
UR  - https://www.sciencedirect.com/science/article/pii/S0031320314000247
N2  - Retinal vessels play an important role in the diagnostic procedure of retinopathy. Accurate segmentation of retinal vessels is crucial for pathological analysis. In this paper, we propose a new retinal vessel segmentation method based on level set and region growing. Firstly, a retinal vessel image is preprocessed by the contrast-limited adaptive histogram equalization and a 2D Gabor wavelet to enhance the vessels. Then, an anisotropic diffusion filter is used to smooth the image and preserve vessel boundaries. Finally, the region growing method and a region-based active contour model with level set implementation are applied to extract retinal vessels, and their results are combined to achieve the final segmentation. Comparisons are conducted on the publicly available DRIVE and STARE databases using three different measurements. Experimental results show that the proposed method reaches an average accuracy of 94.77% on the DRIVE database and 95.09% on the STARE database.
ER  - 
TY  - JOUR
T1  - Prediction of hospitalization due to heart diseases by supervised learning methods
A1  - Dai, Wuyang
A1  - Brisimi, Theodora S
A1  - Adams, William G
A1  - Mela, Theofanie
A1  - Saligrama, Venkatesh
A1  - Paschalidis, Ioannis Ch.
Y1  - 2015///
KW  -  Electronic Health Records (EHRs)
KW  -  Heart diseases
KW  -  Hospitalization
KW  -  Machine learning
KW  -  Predictive models
KW  - Prevention
JF  - International Journal of Medical Informatics
VL  - 84
IS  - 3
SP  - 189
EP  - 197
DO  - https://doi.org/10.1016/j.ijmedinf.2014.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S1386505614001907
N2  - Background
In 2008, the United States spent $2.2 trillion for healthcare, which was 15.5% of its GDP. 31% of this expenditure is attributed to hospital care. Evidently, even modest reductions in hospital care costs matter. A 2009 study showed that nearly $30.8 billion in hospital care cost during 2006 was potentially preventable, with heart diseases being responsible for about 31% of that amount.
Methods
Our goal is to accurately and efficiently predict heart-related hospitalizations based on the available patient-specific medical history. To the best of our knowledge, the approaches we introduce are novel for this problem. The prediction of hospitalization is formulated as a supervised classification problem. We use de-identified Electronic Health Record (EHR) data from a large urban hospital in Boston to identify patients with heart diseases. Patients are labeled and randomly partitioned into a training and a test set. We apply five machine learning algorithms, namely Support Vector Machines (SVM), AdaBoost using trees as the weak learner, logistic regression, a naïve Bayes event classifier, and a variation of a Likelihood Ratio Test adapted to the specific problem. Each model is trained on the training set and then tested on the test set.
Results
All five models show consistent results, which could, to some extent, indicate the limit of the achievable prediction accuracy. Our results show that with under 30% false alarm rate, the detection rate could be as high as 82%. These accuracy rates translate to a considerable amount of potential savings, if used in practice.
ER  - 
TY  - JOUR
T1  - Machine Learning Models and Big Data Tools for Evaluating Kidney Acceptance
A1  - Ashiku, Lirim
A1  - Al-Amin, Md.
A1  - Madria, Sanjay
A1  - Dagli, Cihan
Y1  - 2021///
KW  -  Apache Spark
KW  -  Big data
KW  -  Machine learning
KW  -  Organ procurement
KW  - Healthcare
JF  - Procedia Computer Science
VL  - 185
SP  - 177
EP  - 184
DO  - https://doi.org/10.1016/j.procs.2021.05.019
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921011005
N1  - Big Data, IoT, and AI for a Smarter Future
N2  - The rise of on-demand healthcare and the unprecedented growth of electronic health records has given rise to big data opportunities and data analysis using machine learning. The massive and disparate data management using conventional databases is incredibly challenging and expensive to manage. It often requires specialized analytical tools for developing advanced data-driven capabilities and performing data analytics. This paper explores the capability of an open-source framework ‘Apache Spark’ capable of processing large amounts of data on clusters of nodes to analyze Big data and integrate technologies to provide decision support systems in healthcare settings. Next, we propose machine learning models on top of Apache Spark to expedite the decision-making in allocating organs such as kidney selection for the right candidate, thus increasing donor utilization by locating a recipient within the allotted time. The proposed models help in identifying waitlisted candidates willing to accept kidneys that may otherwise be discarded.
ER  - 
TY  - JOUR
T1  - Quantum-based subgraph convolutional neural networks
A1  - Zhang, Zhihong
A1  - Chen, Dongdong
A1  - Wang, Jianjia
A1  - Bai, Lu
A1  - Hancock, Edwin R
Y1  - 2019///
KW  -  Quantum walks
KW  -  Spatial construction
KW  -  Subgraph
KW  - Graph convolutional neural networks
JF  - Pattern Recognition
VL  - 88
SP  - 38
EP  - 49
DO  - https://doi.org/10.1016/j.patcog.2018.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318303820
N2  - This paper proposes a new graph convolutional neural network architecture based on a depth-based representation of graph structure deriving from quantum walks, which we refer to as the quantum-based subgraph convolutional neural network (QS-CNNs). This new architecture captures both the global topological structure and the local connectivity structure within a graph. Specifically, we commence by establishing a family of K-layer expansion subgraphs for each vertex of a graph by quantum walks, which captures the global topological arrangement information for substructures contained within a graph. We then design a set of fixed-size convolution filters over the subgraphs, which helps to characterise multi-scale patterns residing in the data. The idea is to apply convolution filters sliding over the entire set of subgraphs rooted at a vertex to extract the local features analogous to the standard convolution operation on grid data. Experiments on eight graph-structured datasets demonstrate that QS-CNNs architecture is capable of outperforming fourteen state-of-the-art methods for the tasks of node classification and graph classification.
ER  - 
TY  - JOUR
T1  - A Decision Tree-Initialised Neuro-fuzzy Approach for Clinical Decision Support
A1  - Chen, Tianhua
A1  - Shang, Changjing
A1  - Su, Pan
A1  - Keravnou-Papailiou, Elpida
A1  - Zhao, Yitian
A1  - Antoniou, Grigoris
A1  - Shen, Qiang
Y1  - 2021///
KW  -  Fuzzy rule-based systems
KW  -  Medical diagnostic systems
KW  - Clinical decision support
JF  - Artificial Intelligence in Medicine
VL  - 111
SP  - 101986
EP  - 101986
DO  - https://doi.org/10.1016/j.artmed.2020.101986
UR  - https://www.sciencedirect.com/science/article/pii/S0933365720312513
N2  - Apart from the need for superior accuracy, healthcare applications of intelligent systems also demand the deployment of interpretable machine learning models which allow clinicians to interrogate and validate extracted medical knowledge. Fuzzy rule-based models are generally considered interpretable that are able to reflect the associations between medical conditions and associated symptoms, through the use of linguistic if-then statements. Systems built on top of fuzzy sets are of particular appealing to medical applications since they enable the tolerance of vague and imprecise concepts that are often embedded in medical entities such as symptom description and test results. They facilitate an approximate reasoning framework which mimics human reasoning and supports the linguistic delivery of medical expertise often expressed in statements such as ‘weight low’ or ‘glucose level high’ while describing symptoms. This paper proposes an approach by performing data-driven learning of accurate and interpretable fuzzy rule bases for clinical decision support. The approach starts with the generation of a crisp rule base through a decision tree learning mechanism, capable of capturing simple rule structures. The crisp rule base is then transformed into a fuzzy rule base, which forms the input to the framework of adaptive network-based fuzzy inference system (ANFIS), thereby further optimising the parameters of both rule antecedents and consequents. Experimental studies on popular medical data benchmarks demonstrate that the proposed work is able to learn compact rule bases involving simple rule antecedents, with statistically better or comparable performance to those achieved by state-of-the-art fuzzy classifiers.
ER  - 
TY  - JOUR
T1  - A hybrid real-valued negative selection algorithm with variable-sized detectors and the k-nearest neighbors algorithm
A1  - Li, Zhiyong
A1  - Li, Tao
A1  - He, Junjiang
A1  - Zhu, Yongbin
A1  - Wang, Yunpeng
Y1  - 2021///
KW  -  Anomaly detection
KW  -  Negative selection algorithm
KW  -  V-Detector
KW  -  k-nearest neighbors
KW  - Artificial immune system
JF  - Knowledge-Based Systems
VL  - 232
SP  - 107477
EP  - 107477
DO  - https://doi.org/10.1016/j.knosys.2021.107477
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121007395
N2  - A negative selection algorithm generates detectors to realize abnormality detection by simulating the maturation process of T cells in human immunity. Holes are areas of feature space that cannot be covered by the detector set and are the major factor in the degradation of algorithm performance. Conventional methods alleviate the hole problem by minimizing the coverage area of the holes. In this study, we approach the issue from a different angle. Holes are prone to form in the boundary area between the self and nonself regions, and when the self and the nonself cross or overlap, the hole problem becomes more serious. The k-nearest neighbors (k-NN) algorithm is more suitable than other methods for pending instance sets where the class domain crosses or overlaps more. Therefore, we propose a hybrid real-valued negative selection algorithm with variable-sized detectors (V-Detector) and the k-NN algorithm, abbreviated as V-Detector-kNN. The V-Detector-kNN hybrid algorithm first uses the V-Detector algorithm to classify, and then, for the problem that the nonself instances in the holes are misclassified as selfs, k-NN is introduced to classify those misclassified instances to improve the detection rate. Theoretical analysis proves that the V-Detector-kNN algorithm that we proposed has a higher detection rate than the V-Detector algorithm in most cases. Comparative experiments with 5 different algorithms on 9 UCI datasets show that our proposed algorithm ranks first in detection rate.
ER  - 
TY  - JOUR
T1  - Neural network-based multi-task learning for inpatient flow classification and length of stay prediction
A1  - He, Lu
A1  - Madathil, Sreenath Chalil
A1  - Servis, Greg
A1  - Khasawneh, Mohammad T
Y1  - 2021///
KW  -  Artificial neural network
KW  -  Inpatient length of stay
KW  -  Multi-task learning
KW  - Inpatient flow
JF  - Applied Soft Computing
VL  - 108
SP  - 107483
EP  - 107483
DO  - https://doi.org/10.1016/j.asoc.2021.107483
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621004063
N2  - Inpatient unit resources are among the most expensive and valuable resources for healthcare organizations. Inpatient resources such as room, bed, and medical devices can be more efficiently managed if we can predict inpatient flow and length of stay (LOS) before admission and inpatient bed assignment [1]. Patient LOS prediction has been researched individually using classical machine learning methods, such as linear regression, regression trees, random forest, and neural networks for a long time. Inpatient LOS and flow share many common features in training predictive models because both are closely related to relevant features such as recovery status and surgery types. Besides, these two tasks are closely related. For example, a patient with a more complex inpatient flow tends to have a longer LOS. This paper is the first comprehensive study that links them together as multi-tasks and develops an artificial neural network-based multi-task learning model (ANNML) for mixed types of task prediction in inpatient LOS and flow identification. The constructed multi-task learning model was tested on a real-life dataset collected from a large hospital in New York City and compared with four single-task learning models. The results show that ANNML can use the most relevant features to achieve a better prediction accuracy for both task types and has less overfitting and testing variance than single-task learning models.
ER  - 
TY  - JOUR
T1  - Nonlinear feature selection on attributed networks
A1  - Lin, Zhongping
A1  - Luo, Minnan
A1  - Peng, Zhen
A1  - Li, Jundong
A1  - Zheng, Qinghua
Y1  - 2020///
KW  -  Attributed networks
KW  -  Graph convolution networks
KW  - Feature selection
JF  - Neurocomputing
VL  - 410
SP  - 161
EP  - 173
DO  - https://doi.org/10.1016/j.neucom.2020.05.077
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220309280
N2  - The acceleratinnsional nodal attributes in various data mining tasks highlights the significance of feature selection on the networked data. Due to the lack of class labels of nodes, many feature selection methods are proposed in semi-supervised or unsupervised manners in various scenarios instead of supervised ones. More often than not, features and (pseudo) labels are correlated in a nonlinear way that is more intricate than linearity. In these circumstances, the vast majority of existing linear algorithms could not work well since they select features according to how well the feature can linearly explain the variance of labels. Moreover, although some methods focus on nonlinear feature selection, with the neglect of the link relations between data, they are difficult to be applied to attributed networks directly. In this paper, we investigate how to achieve nonlinear feature selection on attributed networks with the help of both labeled and unlabeled data. Methodologically, we first propose a novel semi-supervised nonlinear framework FS-GCN based on graph convolutional networks (GCNs) to select high-quality features, which can elaborately catch the nonlinear dependency between nodal attributes and class labels. To verify the importance of nonlinearity precisely, we further explore the possibility of totally removing the label information so that a variant of FS-GCN is proposed in the unsupervised form, referred to as UFS-GCN. Besides, experimental results on several real-world datasets validate the superiority of FS-GCN as well as UFS-GCN in terms of the quality of selected features, suggesting their robustness in the condition of extremely low even zero label ratio.
ER  - 
TY  - JOUR
T1  - Paper based diagnostics for personalized health care: Emerging technologies and commercial aspects
A1  - Mahato, Kuldeep
A1  - Srivastava, Ananya
A1  - Chandra, Pranjal
Y1  - 2017///
KW  -  Biomarker
KW  -  Medical diagnosis
KW  -  Personalized health care
KW  -  Point of care testing
KW  -  Real sample
KW  - Paper based biosensors
JF  - Biosensors and Bioelectronics
VL  - 96
SP  - 246
EP  - 259
DO  - https://doi.org/10.1016/j.bios.2017.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S0956566317303111
N2  - Personalized health care (PHC) is being appreciated globally to combat clinical complexities underlying various metabolic or infectious disorders including diabetes, cardiovascular, communicable diseases etc. Effective diagnoses majorly depend on initial identification of the causes which are nowadays being practiced in disease-oriented approach, where personal health profile is often overlooked. The adoption of PHC has shown significantly improved diagnoses in various conditions including emergency, ambulatory, and remote area. PHC includes personalized health monitoring (PHM), which is its integral part and may provide valuable information's on various clinical conditions. In PHC, bio-fluids are analyzed using various diagnostic devices including lab based equipment and biosensors. Among all types of biosensing systems, paper based biosensors are commercially attracted due to its portability, easy availability, cheaper manufacturing cost, and transportability. Not only these, various intrinsic properties of paper has facilitated the development of paper based miniaturized sensors, which has recently gained ASSURED (Affordable, Sensitive, Specific, User-friendly, Rapid and Robust, Equipment free, Deliverable to all end-users) status for point of care diagnosis in miniaturized settings. In this review, importance of paper based biosensors and their compatibility for affordable and low cost diagnostics has been elaborated with various examples. Limitations and strategies to overcome the challenges of paper biosensor have also been discussed. We have provided elaborated tables which describe the types, model specifications, sensing mechanisms, target biomarkers, and analytical performance of the paper biosensors with their respective applications in real sample matrices. Different commercial aspects of paper biosensor have also been explained using SWOT (Strength, Weakness, Opportunities, Threats) analysis.
ER  - 
TY  - JOUR
T1  - Using data mining techniques to predict hospitalization of hemodialysis patients
A1  - Yeh, Jinn-Yi
A1  - Wu, Tai-Hsi
A1  - Tsao, Chuan-Wei
Y1  - 2011///
KW  -  Data mining
KW  -  Healthcare quality
KW  -  Temporal abstract
KW  - Hemodialysis
JF  - Decision Support Systems
VL  - 50
IS  - 2
SP  - 439
EP  - 448
DO  - https://doi.org/10.1016/j.dss.2010.11.001
UR  - https://www.sciencedirect.com/science/article/pii/S016792361000182X
N2  - Hemodialysis patients might suffer from unhealthy care behaviors or long-term dialysis treatments and need to be hospitalized. If the hospitalization rate of a hemodialysis center is high, its service quality will be low. Therefore, decreasing hospitalization rate is a crucial problem for health care centers. This study combines temporal abstraction with data mining techniques for analyzing dialysis patients' biochemical data to develop a decision support system. The mined temporal patterns are helpful for clinicians to predict hospitalization of hemodialysis patients and to suggest immediate treatments to avoid hospitalization.
ER  - 
TY  - JOUR
T1  - Home sleep monitoring based on wrist movement data processing
A1  - Pan, Qiang
A1  - Brulin, Damien
A1  - Campo, Eric
Y1  - 2021///
KW  -  home monitoring
KW  -  k-means
KW  -  threshold
KW  -  wrist movement
KW  - Sleep monitoring
JF  - Procedia Computer Science
VL  - 183
SP  - 696
EP  - 705
DO  - https://doi.org/10.1016/j.procs.2021.02.117
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921005937
N1  - Proceedings of the 10th International Conference of Information and Communication Technology
N2  - In this paper, two original sleep monitoring algorithms, including threshold and k-means clustering algorithms are presented. All the proposed algorithms use only acceleration data acquired from the non-dominant wrist with a 3-axis accelerometer, allowing the detection of falling asleep and waking up and a classification into 4-sleep stages (“awake”, “light sleep”, “deep sleep” and “REM”). We validate the proposed methods by comparing them to the results of “Fitbit Charge 2” and “Withings Sleep Analyzer”. Based on wrist movement data collected during 10 nights of sleep of a volunteer, we can show that the algorithms obtain promising results that allow us to consider a new non-intrusive method for users and medical staff to follow the trend of sleep quality through long term monitoring. This longitudinal monitoring can help to detect abnormal changes in sleep that are usually a sign of a change in health status.
ER  - 
TY  - JOUR
T1  - The application of support vector regression for prediction of the antiallodynic effect of drug combinations in the mouse model of streptozocin-induced diabetic neuropathy
A1  - Sałat, Robert
A1  - Sałat, Kinga
Y1  - 2013///
KW  -  Dihydrofuran-2-one
KW  -  Mechanical allodynia
KW  -  Pregabalin
KW  -  Streptozocin
KW  -  Support vector regression
KW  - Diabetes-induced neuropathic pain
JF  - Computer Methods and Programs in Biomedicine
VL  - 111
IS  - 2
SP  - 330
EP  - 337
DO  - https://doi.org/10.1016/j.cmpb.2013.04.018
UR  - https://www.sciencedirect.com/science/article/pii/S0169260713001405
N2  - Drug interactions are an important issue of efficacious and safe pharmacotherapy. Although the use of drug combinations carries the potential risk of enhanced toxicity, when carefully introduced it enables to optimize the therapy and achieve pharmacological effects at doses lower than those of single agents. In view of the development of novel analgesic compounds for the neuropathic pain treatment little is known about their influence on the efficacy of currently used analgesic drugs. Below we describe the preliminary evaluation of support vector machine in the regression mode (SVR) application for the prediction of maximal antiallodynic effect of a new derivative of dihydrofuran-2-one (LPP1) used in combination with pregabalin (PGB) in the streptozocin-induced neuropathic pain model in mice. Based on SVR the most effective doses of co-administered LPP1 (4mg/kg) and PGB (1mg/kg) were predicted to cause the paw withdrawal threshold at 6.7g in the von Frey test. In vivo for the same combination of doses the paw withdrawal was observed at 6.5g, which confirms good predictive properties of SVR.
ER  - 
TY  - JOUR
T1  - Consistent discovery of frequent interval-based temporal patterns in chronic patients’ data
A1  - Shknevsky, Alexander
A1  - Shahar, Yuval
A1  - Moskovitch, Robert
Y1  - 2017///
KW  -  Classification
KW  -  Clustering
KW  -  Frequent pattern mining
KW  -  Pattern consistency
KW  -  Pattern repetition
KW  -  Prediction
KW  -  Temporal abstraction
KW  -  Temporal knowledge discovery
KW  -  Time intervals mining
KW  - Temporal data mining
JF  - Journal of Biomedical Informatics
VL  - 75
SP  - 83
EP  - 95
DO  - https://doi.org/10.1016/j.jbi.2017.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417302216
N2  - Increasingly, frequent temporal patterns discovered in longitudinal patient records are proposed as features for classification and prediction, and as means to cluster patient clinical trajectories. However, to justify that, we must demonstrate that most frequent temporal patterns are indeed consistently discoverable within the records of different patient subsets within similar patient populations. We have developed several measures for the consistency of the discovery of temporal patterns. We focus on time-interval relations patterns (TIRPs) that can be discovered within different subsets of the same patient population. We expect the discovered TIRPs (1) to be frequent in each subset, (2) preserve their “local” metrics - the absolute frequency of each pattern, measured by a Proportion Test, and (3) preserve their “global” characteristics - their overall distribution, measured by a Kolmogorov-Smirnov test. We also wanted to examine the effect on consistency, over a variety of settings, of varying the minimal frequency threshold for TIRP discovery, and of using a TIRP-filtering criterion that we previously introduced, the Semantic Adjacency Criterion (SAC). We applied our methodology to three medical domains (oncology, infectious hepatitis, and diabetes). We found that, within the minimal frequency ranges we had examined, 70–95% of the discovered TIRPs were consistently discoverable; 40–48% of them maintained their local frequency. TIRP global distribution similarity varied widely, from 0% to 65%. Increasing the threshold usually increased the percentage of TIRPs that were repeatedly discovered across different patient subsets within the same domain, and the probability of a similar TIRP distribution. Using the SAC principle, enhanced, for most minimal support levels, the percentage of repeating TIRPs, their local consistency and their global consistency. The effect of using the SAC was further strengthened as the minimal frequency threshold was raised.
ER  - 
TY  - JOUR
T1  - IoTFLiP: IoT-based flipped learning platform for medical education
A1  - Ali, Maqbool
A1  - Bilal, Hafiz Syed Muhammad
A1  - Razzaq, Muhammad Asif
A1  - Khan, Jawad
A1  - Lee, Sungyoung
A1  - Idris, Muhammad
A1  - Aazam, Mohammad
A1  - Choi, Taebong
A1  - Han, Soyeon Caren
A1  - Kang, Byeong Ho
Y1  - 2017///
KW  -  Case-based learning
KW  -  Cloud environment
KW  -  Flipped learning
KW  -  Medical education
KW  - Internet of things
JF  - Digital Communications and Networks
VL  - 3
IS  - 3
SP  - 188
EP  - 194
DO  - https://doi.org/10.1016/j.dcan.2017.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S2352864817300974
N2  - Case-Based Learning (CBL) has become an effective pedagogy for student-centered learning in medical education, which is founded on persistent patient cases. Flippped learning and Internet of Things (IoTs) concepts have gained significant attention in recent years. Using these concepts in conjunction with CBL can improve learning ability by providing real evolutionary medical cases. It also enables students to build confidence in their decision making, and efficiently enhances teamwork in the learing environment. We propose an IoT-based Flip Learning Platform, called IoTFLiP, where an IoT infrastructure is exploited to support flipped case-based learning in a cloud environment with state of the art security and privacy measures for personalized medical data. It also provides support for application delivery in private, public, and hybrid approaches. The proposed platform is an extension of our Interactive Case-Based Flipped Learning Tool (ICBFLT), which has been developed based on current CBL practices. ICBFLT formulates summaries of CBL cases through synergy between students' and medical expert knowledge. The low cost and reduced size of sensor device, support of IoTs, and recent flipped learning advancements can enhance medical students' academic and practical experiences. In order to demonstrate a working scenario for the proposed IoTFLiP platform, real-time data from IoTs gadgets is collected to generate a real-world case for a medical student using ICBFLT.
ER  - 
TY  - JOUR
T1  - Automated interpretation of biopsy images for the detection of celiac disease using a machine learning approach
A1  - Koh, Joel En Wei
A1  - De Michele, Simona
A1  - Sudarshan, Vidya K
A1  - Jahmunah, V
A1  - Ciaccio, Edward J
A1  - Ooi, Chui Ping
A1  - Gururajan, Raj
A1  - Gururajan, Rashmi
A1  - Oh, Shu Lih
A1  - Lewis, Suzanne K
A1  - Green, Peter H
A1  - Bhagat, Govind
A1  - Acharya, U Rajendra
Y1  - 2021///
KW  -  Biopsy images
KW  -  Classifiers
KW  -  Image analysis
KW  -  Machine learning
KW  -  Nonlinear features
KW  -  Steerable pyramid transform
KW  - Celiac disease
JF  - Computer Methods and Programs in Biomedicine
VL  - 203
SP  - 106010
EP  - 106010
DO  - https://doi.org/10.1016/j.cmpb.2021.106010
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721000857
N2  - Background and objectives
Celiac disease is an autoimmune disease occurring in about 1 in 100 people worldwide. Early diagnosis and efficient treatment are crucial in mitigating the complications that are associated with untreated celiac disease, such as intestinal lymphoma and malignancy, and the subsequent high morbidity. The current diagnostic methods using small intestinal biopsy histopathology, endoscopy, and video capsule endoscopy (VCE) involve manual interpretation of photomicrographs or images, which can be time-consuming and difficult, with inter-observer variability. In this paper, a machine learning technique was developed for the automation of biopsy image analysis to detect and classify villous atrophy based on modified Marsh scores. This is one of the first studies to employ conventional machine learning to automate the use of biopsy images for celiac disease detection and classification.
Methods
The Steerable Pyramid Transform (SPT) method was used to obtain sub bands from which various types of entropy and nonlinear features were computed. All extracted features were automatically classified into two-class and multi-class, using six classifiers.
Results
An accuracy of 88.89%, was achieved for the classification of two-class villous abnormalities based on analysis of Hematoxylin and Eosin (H&E) stained biopsy images. Similarly, an accuracy of 82.92% was achieved for the two-class classification of red-green-blue (RGB) biopsy images. Also, an accuracy of 72% was achieved in the classification of multi-class biopsy images.
Conclusion
The results obtained are promising, and demonstrate the possibility of automating biopsy image interpretation using machine learning. This can assist pathologists in accelerating the diagnostic process without bias, resulting in greater accuracy, and ultimately, earlier access to treatment.
ER  - 
TY  - JOUR
T1  - Diabetic Retinopathy Detection Based on Eigenvalues of the Hessian Matrix
A1  - Rubini, S Saranya
A1  - Kunthavai, A
Y1  - 2015///
KW  -  AHCS ;
KW  -  Hemorrhages
KW  -  Microaneurysms
KW  -  SHCS
KW  -  SVM
KW  - Diabetic retinopathy
JF  - Procedia Computer Science
VL  - 47
SP  - 311
EP  - 318
DO  - https://doi.org/10.1016/j.procs.2015.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915005025
N1  - Graph Algorithms, High Performance Implementations and Its Applications ( ICGHIA 2014 )
N2  - Diabetic Retinopathy (DR) is a medical condition caused by fluctuating insulin level in the blood which causes vision loss in case of severity. Timely treatment of such risks requires identification of the first clinical symptoms like microaneurysms (MAs) and hemorrhages (HMAs). The presence of those symptoms are visible in the digital color photographs of the retina and appear as round dark red spots in the image. In this paper, two approaches in the detection of MAs and HMAs are proposed. First, the semi automated approach applies semi automated hessian-based candidate selection algorithm (SHCS) followed by thresholding to detect true MAs and HMAs. The automated approach applies automated hessian-based candidate selection algorithm (AHCS) followed by feature extraction and SVM classification that uses twenty images for training manually annotated by medical domain experts. Implementations of both the approaches have been tested on real world images from retinal scan. From the results, the detection rate of automated algorithm when compared with that of the semi automated algorithm has been found to be significantly lesser with a probability p<0.005.
ER  - 
TY  - JOUR
T1  - An effective fovea detection and automatic assessment of diabetic maculopathy in color fundus images
A1  - Medhi, Jyoti Prakash
A1  - Dandapat, Samarendra
Y1  - 2016///
KW  -  Diabetic maculopathy
KW  -  Exudate
KW  -  Fovea
KW  -  Fundus image
KW  -  Macula
KW  - Diabetic retinopathy
JF  - Computers in Biology and Medicine
VL  - 74
SP  - 30
EP  - 44
DO  - https://doi.org/10.1016/j.compbiomed.2016.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516300968
N2  - Prolonged diabetes causes severe damage to the vision through leakage of blood and blood constituents over the retina. The effect of the leakage becomes more threatening when these abnormalities involve the macula. This condition is known as diabetic maculopathy and it leads to blindness, if not treated in time. Early detection and proper diagnosis can help in preventing this irreversible damage. To achieve this, the possible way is to perform retinal screening at regular intervals. But the ratio of ophthalmologists to patients is very small and the process of evaluation is time consuming. Here, the automatic methods for analyzing retinal/fundus images prove handy and help the ophthalmologists to screen at a faster rate. Motivated from this aspect, an automated method for detection and analysis of diabetic maculopathy is proposed in this work. The method is implemented in two stages. The first stage involves preprocessing required for preparing the image for further analysis. During this stage the input image is enhanced and the optic disc is masked to avoid false detection during bright lesion identification. The second stage is maculopathy detection and its analysis. Here, the retinal lesions including microaneurysms, hemorrhages and exudates are identified by processing the green and hue plane color images. The macula and the fovea locations are determined using intensity property of processed red plane image. Different circular regions are thereafter marked in the neighborhood of the macula. The presence of lesions in these regions is identified to confirm positive maculopathy. Later, the information is used for evaluating its severity. The principal advantage of the proposed algorithm is, utilization of the relation of blood vessels with optic disc and macula, which enhances the detection process. Proper usage of various color plane information sequentially enables the algorithm to perform better. The method is tested on various publicly available databases consisting of both normal and maculopathy images. The algorithm detects fovea with an accuracy of 98.92% when applied on 1374 images. The average specificity and sensitivity of the proposed method for maculopathy detection are obtained as 98.05% and 98.86% respectively.
ER  - 
TY  - JOUR
T1  - Task offloading in edge computing for machine learning-based smart healthcare
A1  - Aazam, Mohammad
A1  - Zeadally, Sherali
A1  - Flushing, Eduardo Feo
Y1  - 2021///
KW  -  Edge computing
KW  -  Fog computing
KW  -  Healthcare
KW  -  Internet of Things (IoT)
KW  -  Machine learning
KW  -  Middleware
KW  -  Offloading
KW  - Cloud computing
JF  - Computer Networks
VL  - 191
SP  - 108019
EP  - 108019
DO  - https://doi.org/10.1016/j.comnet.2021.108019
UR  - https://www.sciencedirect.com/science/article/pii/S1389128621001298
N2  - Recent advances in networking and mobile technologies such as 5G, long-term evolution (LTE), LiFi, wireless broadband (WiBro), WiFi-Direct, Bluetooth Low Energy (BLE) have paved the way for intelligent and smart services. With an average of more than 6.5 devices per person, a plethora of applications are being developed especially related to healthcare. Although, current edge devices such as smartphone and smartwatch are becoming increasingly more powerful and more affordable, there are certain tasks such as those involving machine learning that require higher computational resources, thereby resulting in higher energy consumption in the case of edge devices. Offloading tasks to co-located edge nodes such as fog (a cloud-like localized, smaller resource pool), or a femto-cloud (integration of multiple edge nodes) is one viable solution to address the issues such as performing compute-intensive tasks, and managing energy consumption. The outbreak of coronavirus disease 2019 (COVID-19) and becoming a pandemic has also made a case for edge computing (involving smartphone, wearables, health sensors) for the detection of symptoms to quarantine potential carriers of the virus. We focus on how various forms of smart and opportunistic healthcare (oHealth) can be provided by leveraging edge computing that makes use of a machine learning-based approach. We apply k-nearest neighbors (kNN), naive Bayes (NB), and support vector classification (SVC) algorithms on real data trace for the healthcare and safety-related scenarios we considered. The empirical results obtained provide useful insights into machine learning-based task offloading in edge computing.
ER  - 
TY  - JOUR
T1  - TeleOphta: Machine learning and image processing methods for teleophthalmology
A1  - Decencière, E
A1  - Cazuguel, G
A1  - Zhang, X
A1  - Thibault, G
A1  - Klein, J.-C.
A1  - Meyer, F
A1  - Marcotegui, B
A1  - Quellec, G
A1  - Lamard, M
A1  - Danno, R
A1  - Elie, D
A1  - Massin, P
A1  - Viktor, Z
A1  - Erginay, A
A1  - Laÿ, B
A1  - Chabouis, A
Y1  - 2013///
JF  - IRBM
VL  - 34
IS  - 2
SP  - 196
EP  - 203
DO  - https://doi.org/10.1016/j.irbm.2013.01.010
UR  - https://www.sciencedirect.com/science/article/pii/S1959031813000237
N1  - Special issue : ANR TECSAN : Technologies for Health and Autonomy
N2  - A complete prototype for the automatic detection of normal examinations on a teleophthalmology network for diabetic retinopathy screening is presented. The system combines pathological pattern mining methods, with specific lesion detection methods, to extract information from the images. This information, plus patient and other contextual data, is used by a classifier to compute an abnormality risk. Such a system should reduce the burden on readers on teleophthalmology networks.
ER  - 
TY  - JOUR
T1  - Mining and exploring care pathways from electronic medical records with visual analytics
A1  - Perer, Adam
A1  - Wang, Fei
A1  - Hu, Jianying
Y1  - 2015///
KW  -  Data-driven care plans
KW  -  Frequent sequence mining
KW  -  Temporal event visualization
KW  - Visual analytics
JF  - Journal of Biomedical Informatics
VL  - 56
SP  - 369
EP  - 378
DO  - https://doi.org/10.1016/j.jbi.2015.06.020
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001306
N2  - Objective
In order to derive data-driven insights, we develop Care Pathway Explorer, a system that mines and visualizes a set of frequent event sequences from patient EMR data. The goal is to utilize historical EMR data to extract common sequences of medical events such as diagnoses and treatments, and investigate how these sequences correlate with patient outcome.
Materials and methods
The Care Pathway Explorer uses a frequent sequence mining algorithm adapted to handle the real-world properties of EMR data, including techniques for handling event concurrency, multiple levels-of-detail, temporal context, and outcome. The mined patterns are then visualized in an interactive user interface consisting of novel overview and flow visualizations.
Results
We use the proposed system to analyze the diagnoses and treatments of a cohort of hyperlipidemic patients with hypertension and diabetes pre-conditions, and demonstrate the clinical relevance of patterns mined from EMR data. The patterns that were identified corresponded to clinical and published knowledge, some of it unknown to the physician at the time of discovery.
Conclusion
Care Pathway Explorer, which combines frequent sequence mining techniques with advanced visualizations supports the integration of data-driven insights into care pathway discovery.
ER  - 
TY  - JOUR
T1  - Retinal blood vessel localization approach based on bee colony swarm optimization, fuzzy c-means and pattern search
A1  - Hassanien, Aboul Ella
A1  - Emary, E
A1  - Zawbaa, Hossam M
Y1  - 2015///
KW  -  Artificial bee colony
KW  -  Clustering
KW  -  Fuzzy c-means
KW  -  Image enhancement
KW  -  Pattern search
KW  -  Retinal vessel segmentation
KW  -  Swarm optimization
KW  - Retinal blood vessel
JF  - Journal of Visual Communication and Image Representation
VL  - 31
SP  - 186
EP  - 196
DO  - https://doi.org/10.1016/j.jvcir.2015.06.019
UR  - https://www.sciencedirect.com/science/article/pii/S1047320315001182
N2  - Accurate segmentation of retinal blood vessels is an important task in computer aided diagnosis and surgery planning of retinopathy. Despite the high resolution of photographs in fundus photography, the contrast between the blood vessels and retinal background tends to be poor. Furthermore, pathological changes of the retinal vessel tree can be observed in a variety of diseases such as diabetes and glaucoma. Vessels with small diameters are much liable to effects of diseases and imaging problems. In this paper, an automated retinal blood vessels segmentation approach based on two levels optimization principles is proposed. The proposed approach makes use of the artificial bee colony optimization in conjunction with fuzzy cluster compactness fitness function with partial belongness in the first level to find coarse vessels. The dependency on the vessel reflectance is problematic as the confusion with background and vessel distortions especially for thin vessels, so we made use of a second level of optimization. In the second level of optimization, pattern search is further used to enhance the segmentation results using shape description as a complementary feature. Thinness ratio is used as a fitness function for the pattern search optimization. The pattern search is a powerful tool for local search while artificial bee colony is a global search with high convergence speed. The proposed retinal blood vessels segmentation approach is tested on two publicly available databases DRIVE and STARE of retinal images. The results demonstrate that the performance of the proposed approach is comparable with state of the art techniques in terms of sensitivity, specificity and accuracy.
ER  - 
TY  - JOUR
T1  - A dynamic decision model for diagnosis of dementia, Alzheimer's disease and Mild Cognitive Impairment
A1  - Carvalho, Carolina M
A1  - Seixas, Flávio L
A1  - Conci, Aura
A1  - Muchaluat-Saade, Débora C
A1  - Laks, Jerson
A1  - Boechat, Yolanda
Y1  - 2020///
KW  -  Alzheimer's disease
KW  -  Clinical decision support system
KW  -  Computer-aided diagnosis
KW  -  Dementia
KW  -  Mild cognitive impairment
KW  - Dynamic decision model
JF  - Computers in Biology and Medicine
VL  - 126
SP  - 104010
EP  - 104010
DO  - https://doi.org/10.1016/j.compbiomed.2020.104010
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520303413
N2  - CDSS (Clinical Decision Support System) is a domain within digital health that aims at supporting clinicians by suggesting the most probable diagnosis based on knowledge obtained from patient data. Usually, decision models used by current CDSS are static, i.e., they are not updated when new data are included, which could allow them to acquire new knowledge and enhance system accuracy. This paper proposes a dynamic decision model that automatically updates itself from classifier models using supervised machine learning algorithms. Our supervised learning process ranks several decision models using classifier performance measures, considering available patient data, filled by the health center, or local clinical guidelines. The decision model with the best performance is then selected to be used in our CDSS, which is designed for the diagnosis of D (Dementia), AD (Alzheimer's Disease), and MCI (Mild Cognitive Impairment). Patient datasets from CAD (Center for Alzheimer's Disease), at the Institute of Psychiatry of UFRJ (Federal University of Rio de Janeiro), and CRASI (Center of Reference in Attention to Health of the Elderly), at Antonio Pedro Hospital of UFF (Fluminense Federal University), are used. The main conclusion is that the proposed dynamic decision model, which offers the ability to be continuously refined with more recent diagnostic criteria or even personalized according to the local domain or clinical guidelines, provides an efficient alternative for diagnosis of Dementia, AD, and MCI.
ER  - 
TY  - JOUR
T1  - Recognizing lung cancer and stages using a self-developed electronic nose system
A1  - Chen, Ke
A1  - Liu, Lei
A1  - Nie, Bo
A1  - Lu, Binchun
A1  - Fu, Lidan
A1  - He, Zichun
A1  - Li, Wang
A1  - Pi, Xitian
A1  - Liu, Hongying
Y1  - 2021///
KW  -  Electronic nose
KW  -  Extreme gradient boosting
KW  -  Kernel principal component analysis
KW  -  Volatile organic compounds
KW  - Lung cancer
JF  - Computers in Biology and Medicine
VL  - 131
SP  - 104294
EP  - 104294
DO  - https://doi.org/10.1016/j.compbiomed.2021.104294
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521000883
N2  - Exhaled breath contains thousands of gaseous volatile organic compounds (VOCs) that could be used as non-invasive biomarkers of lung cancer. Breath-based lung cancer screening has attracted wide attention on account of its convenience, low cost and easy popularization. In this paper, the research of lung cancer detection and staging is conducted by the self-developed electronic nose (e-nose) system. In order to investigate the performance of the device in distinguishing lung cancer patients from healthy controls, two feature extraction methods and two different classification models were adopted. Among all the models, kernel principal component analysis (KPCA) combined with extreme gradient boosting (XGBoost) achieved the best results among 235 breath samples. The accuracy, sensitivity and specificity of e-nose system were 93.59%, 95.60% and 91.09%, respectively. Meanwhile, the device could innovatively classify stages of 90 lung cancer patients (i.e., 44 stage III and 46 stage IV). Experimental results indicated that the recognition accuracy of lung cancer stages was more than 80%. Further experiments of this research also showed that the combination of sensor array and pattern recognition algorithms could identify and distinguish the expiratory characteristics of lung cancer, smoking and other respiratory diseases.
ER  - 
TY  - JOUR
T1  - A learning method for the class imbalance problem with medical data sets
A1  - Li, Der-Chiang
A1  - Liu, Chiao-Wen
A1  - Hu, Susan C
Y1  - 2010///
KW  -  Classification
KW  -  Fuzzy set theory
KW  -  Mega-trend diffusion (MTD)
KW  -  Support vector machine (SVM)
KW  - Class imbalance
JF  - Computers in Biology and Medicine
VL  - 40
IS  - 5
SP  - 509
EP  - 518
DO  - https://doi.org/10.1016/j.compbiomed.2010.03.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482510000405
N2  - In medical data sets, data are predominately composed of “normal” samples with only a small percentage of “abnormal” ones, leading to the so-called class imbalance problems. In class imbalance problems, inputting all the data into the classifier to build up the learning model will usually lead a learning bias to the majority class. To deal with this, this paper uses a strategy which over-samples the minority class and under-samples the majority one to balance the data sets. For the majority class, this paper builds up the Gaussian type fuzzy membership function and α-cut to reduce the data size; for the minority class, we use the mega-trend diffusion membership function to generate virtual samples for the class. Furthermore, after balancing the data size of classes, this paper extends the data attribute dimension into a higher dimension space using classification related information to enhance the classification accuracy. Two medical data sets, Pima Indians’ diabetes and the BUPA liver disorders, are employed to illustrate the approach presented in this paper. The results indicate that the proposed method has better classification performance than SVM, C4.5 decision tree and two other studies.
ER  - 
TY  - JOUR
T1  - Retinal blood vessel segmentation from diabetic retinopathy images using tandem PCNN model and deep learning based SVM
A1  - Jebaseeli, T Jemima
A1  - Deva Durai, C Anand
A1  - Peter, J Dinesh
Y1  - 2019///
KW  -  Blood vessel
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Feature extraction
KW  -  Fundus image
KW  -  Neural network
KW  -  Support vector machine
KW  - Image segmentation
JF  - Optik
VL  - 199
SP  - 163328
EP  - 163328
DO  - https://doi.org/10.1016/j.ijleo.2019.163328
UR  - https://www.sciencedirect.com/science/article/pii/S0030402619312264
N2  - Diabetic Retinopathy (DR) occurs due to Type-II diabetes. It causes damages to the retinal blood vessels and reason for visual impairment. The predicted center is around the probability of variation in the estimation of retinal veins, and the crisp enrolls vessel development inside the retina. To witness the changes segmentation of retinal blood vessels has to be made. A framework to upgrade the quality of the segmentation results over morbid retinal images is proposed. This framework utilizes Contrast Limited Adaptive Histogram Equalization (CLAHE) for eliminating the background from the source image and enhances the foreground blood vessel pixels, Tandem Pulse Coupled Neural Network (TPCNN) model is endorsed for automatic feature vectors generation, and Deep Learning Based Support Vector Machine (DLBSVM) is proposed for classification and extraction of blood vessels. The DLBSVM parameters are fine-tuned via Firefly algorithm. The STARE, DRIVE, HRF, REVIEW, and DRIONS fundus image datasets are deliberated to assess the recommended techniques. The results render that the proposed technologies improve the segmentation with 80.61% Sensitivity, 99.54% Specificity, and 99.49% Accuracy.
ER  - 
TY  - JOUR
T1  - Natural Language Processing Framework to Assess Clinical Conditions
A1  - Ware, Henry
A1  - Mullett, Charles J
A1  - Jagannathan, V
Y1  - 2009///
JF  - Journal of the American Medical Informatics Association
VL  - 16
IS  - 4
SP  - 585
EP  - 589
DO  - https://doi.org/10.1197/jamia.M3091
UR  - https://www.sciencedirect.com/science/article/pii/S1067502709000917
N2  - Objective
The authors developed a natural language processing (NLP) framework that could be used to extract clinical findings and diagnoses from dictated physician documentation.
Design
De-identified documentation was made available by i2b2 Bio-informatics research group as a part of their NLP challenge focusing on obesity and its co-morbidities. The authors describe their approach, which used a combination of concept detection, context validation, and the application of a variety of rules to conclude patient diagnoses.
Results
The framework was successful at correctly identifying diagnoses as judged by NLP challenge organizers when compared with a gold standard of physician annotations. The authors overall kappa values for agreement with the gold standard were 0.92 for explicit textual results and 0.91 for intuited results. The NLP framework compared favorably with those of the other entrants, placing third in textual results and fourth in intuited results in the i2b2 competition.
Conclusions
The framework and approach used to detect clinical conditions was reasonably successful at extracting 16 diagnoses related to obesity. The system and methodology merits further development, targeting clinically useful applications.
ER  - 
TY  - JOUR
T1  - Impact of sensing and infusion site dependent dynamics on insulin bolus based meal compensation
A1  - Kölle, Konstanze
A1  - Fougner, Anders
A1  - Stavdahl, Øyvind
Y1  - 2017///
KW  -  Biomedical system modeling
KW  -  Control of physiological
KW  -  clinical variables
KW  -  simulation
KW  -  visualization
KW  - Artificial pancreas or organs
JF  - IFAC-PapersOnLine
VL  - 50
IS  - 1
SP  - 7749
EP  - 7755
DO  - https://doi.org/10.1016/j.ifacol.2017.08.1154
UR  - https://www.sciencedirect.com/science/article/pii/S240589631731649X
N1  - 20th IFAC World Congress
N2  - Meals are most challenging in the regulation of blood glucose levels (BGL) in diabetes mellitus type 1, whether it is automated, semi-automated or manually controlled. The common subcutaneous (SC) route for glucose sensing and insulin administration suffers from large latencies. This paper investigates the impact of glucose sensing and insulin absorption dynamics on the achievable glucose regulation when insulin boluses are triggered by a meal detection system. In silico patients from the academic version of the UVa/Padova simulator are studied. The sub-models of glucose sensing and insulin absorption are adjusted to allow simulations with different time delays and time constants. Meals are detected with published methods based on threshold-checking of continuous glucose monitoring data. Slow glucose sensing dynamics delay the meal detection. Delayed meal detection can be compensated to some extent by exact knowledge about the insulin absorption. The combination of slow glucose sensing and slow insulin administration reduces the effect of insulin boluses on the postprandial BGL. The classical SC approach is, therefore, at high risk of large BGL excursions despite meal detection.
ER  - 
TY  - JOUR
T1  - A sensor-based wrist pulse signal processing and lung cancer recognition
A1  - Zhang, Zhichao
A1  - Zhang, Yuan
A1  - Yao, Lina
A1  - Song, Houbing
A1  - Kos, Anton
Y1  - 2018///
KW  -  Cubic support vector machine (CSVM)
KW  -  Feature extraction
KW  -  Iterative sliding window (ISW)
KW  -  Jin’s pulse diagnosis (JPD)
KW  -  Pulse signal processing and analysis
KW  - Lung cancer recognition
JF  - Journal of Biomedical Informatics
VL  - 79
SP  - 107
EP  - 116
DO  - https://doi.org/10.1016/j.jbi.2018.01.009
UR  - https://www.sciencedirect.com/science/article/pii/S153204641830011X
N2  - Pulse diagnosis is an efficient method in traditional Chinese medicine for detecting the health status of a person in a non-invasive and convenient way. Jin’s pulse diagnosis (JPD) is a very efficient recent development that is gradually recognized and well validated by the medical community in recent years. However, no acceptable results have been achieved for lung cancer recognition in the field of biomedical signal processing using JPD. More so, there is no standard JPD pulse feature defined with respect to pulse signals. Our work is designed mainly for care giving service conveniently at home to the people having lung cancer by proposing a novel wrist pulse signal processing method, having an insight from JPD. We developed an iterative slide window (ISW) algorithm to segment the de-noised signal into single periods. We analyzed the characteristics of the segmented pulse waveform and for the first time summarized 26 features to classify the pulse waveforms of healthy individuals and lung cancer patients using a cubic support vector machine (CSVM). The result achieved by the proposed method is found to be 78.13% accurate.
ER  - 
TY  - JOUR
T1  - Deep learning based early stage diabetic retinopathy detection using optical coherence tomography
A1  - Li, Xuechen
A1  - Shen, Linlin
A1  - Shen, Meixiao
A1  - Tan, Fan
A1  - Qiu, Connor S
Y1  - 2019///
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Optical coherence tomography
KW  - Computer-aided diagnosis
JF  - Neurocomputing
VL  - 369
SP  - 134
EP  - 144
DO  - https://doi.org/10.1016/j.neucom.2019.08.079
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219312172
N2  - Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal examinations on all diabetic patients is an unmet need, and detection at an early stage can provide better control of the disease. The objective of this study is to provide an optical coherence tomography (OCT) image based diagnostic technology for automated early DR diagnosis, including at both grades 0 and 1. This work can help ophthalmologists with evaluation and treatment, reducing the rate of vision loss, and enabling timely and accurate diagnosis. In this work, we developed and evaluated a novel deep network – OCTD_Net, for early-stage DR detection. While one of the networks extracted features from the original OCT image, the other extracted retinal layer information. The accuracy, sensitivity and specificity was 0.92, 0.90 and 0.95, respectively. Our analysis of retinal layers and the features learned by the proposed network suggests that grade 1 DR patients present with significant changes in the thickness and reflection of certain retinal layers. However, grade 0 DR patients do not have such significant changes. The heatmaps of the trained network also suggest that patients with early DR showed different textures around the myoid and ellipsoid zones, inner nuclear layers, and photoreceptor outer segments, which should all receive dedicated attention for early DR diagnosis.
ER  - 
TY  - JOUR
T1  - A practical approach towards causality mining in clinical text using active transfer learning
A1  - Hussain, Musarrat
A1  - Satti, Fahad Ahmed
A1  - Hussain, Jamil
A1  - Ali, Taqdir
A1  - Ali, Syed Imran
A1  - Bilal, Hafiz Syed Muhammad
A1  - Park, Gwang Hoon
A1  - Lee, Sungyoung
A1  - Chung, TaeChoong
Y1  - 2021///
KW  -  Active transfer learning
KW  -  Clinical text mining
KW  -  Machine learning
KW  - Causality mining
JF  - Journal of Biomedical Informatics
VL  - 123
SP  - 103932
EP  - 103932
DO  - https://doi.org/10.1016/j.jbi.2021.103932
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421002616
N2  - Objective
Causality mining is an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinical text to overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinical text into causal knowledge.
Methods
A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causality mining framework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from clinical text.
Results
The multi-model transfer learning technique when applied over multiple iterations, gains substantial performance improvements. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships.
Conclusion
The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well.
Significance
The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinical text, create personas, discover medical knowledge, and provide evidence to clinical decision making.
ER  - 
TY  - JOUR
T1  - IoT medical embedded system design and Trimetazidine treatment of coronary heart disease and angina pectoris
A1  - Wang, Wanqing
A1  - Zhao, Weitao
A1  - Ailing, Li
A1  - Xu, Zengzheng
Y1  - 2021///
KW  -  Clustering technologies
KW  -  Heart attack
KW  -  Medical and embedded system
KW  - Coronary Heart Disease CHD)
JF  - Microprocessors and Microsystems
VL  - 82
SP  - 103894
EP  - 103894
DO  - https://doi.org/10.1016/j.micpro.2021.103894
UR  - https://www.sciencedirect.com/science/article/pii/S0141933121000739
N2  - ABSTRACT
The coronary network is formed on the surface of the heart that pumps oxygen in the blood vessel. If these arteries narrow, the heart does not get enough oxygen-rich blood, especially in physical activity. Coronary Heart Disease (CHD) is the result of injury or damage to coronary artery lining development. This damage can lead to fatty deposits of plaque build on the site of injury, and these deposits are made of cholesterol and other waste from the cells. This accumulation is called atherosclerosis. In the plaque rupture and break-off case, the platelet aggregation works in the region, trying to repair the vessel. This cluster can block arteries and reduce or block blood flow, leading to a heart attack. Things medical devices connected to a functional disease testing purposes other hardware. The instrument system had a touch screen interface for the user to analyze and process the input data. The purpose is to provide a survey of various clustering technologies for use in the medical field. The aim of this study to improve the clustering technology to enhance the design further. Disease-related symptoms user input data to the system attempts to match a file's occurrence and loaded into the input provided. If the symptoms are preloaded, find a match, resulting in a common prescription drug response and disease names in the system. Angina does not mean that a heart attack has occurred or is about to occur. Angina is the presence of coronary artery disease but does not indicate that part of the heart does not get enough blood supply. Symptomatic and wellbeing observing in the clinical business is a significant issue. Because of time requirements, individuals don't enter the medical clinic, prompting numerous medical issues in a second. The greater part of the medical care framework has been created to foresee and analyze the patient's wellbeing, chiefly because screen their wellbeing consistently, regardless of whether The occupied with their timetables. Numerous investigations have indicated that early forecast is the ideal approach to fix wellbeing, early analysis and help patients and their wellbeing cautioning.
ER  - 
TY  - JOUR
T1  - Detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion
A1  - Prentašić, Pavle
A1  - Lončarić, Sven
Y1  - 2016///
KW  -  Convolutional neural networks
KW  -  Exudates
KW  -  Fundus photographs
KW  -  Machine learning
KW  - Diabetic retinopathy
JF  - Computer Methods and Programs in Biomedicine
VL  - 137
SP  - 281
EP  - 292
DO  - https://doi.org/10.1016/j.cmpb.2016.09.018
UR  - https://www.sciencedirect.com/science/article/pii/S0169260716308033
N2  - Background and objective
Diabetic retinopathy is one of the leading disabling chronic diseases and one of the leading causes of preventable blindness in developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into automated population screening programs. Detection of exudates in color fundus photographs is very important for early diagnosis of diabetic retinopathy.
Methods
We use deep convolutional neural networks for exudate detection. In order to incorporate high level anatomical knowledge about potential exudate locations, output of the convolutional neural network is combined with the output of the optic disc detection and vessel detection procedures.
Results
In the validation step using a manually segmented image database we obtain a maximum F1 measure of 0.78.
Conclusions
As manually segmenting and counting exudate areas is a tedious task, having a reliable automated output, such as automated segmentation using convolutional neural networks in combination with other landmark detectors, is an important step in creating automated screening programs for early detection of diabetic retinopathy.
ER  - 
TY  - JOUR
T1  - Evaluating automated entity extraction with respect to drug and non-drug treatment strategies
A1  - Guo, Jinlong
A1  - Blake, Catherine
A1  - Guan, Yingjun
Y1  - 2019///
KW  -  Entity recognition
KW  -  Machine learning
KW  -  MetaMap
KW  - Treatment extraction
JF  - Journal of Biomedical Informatics
VL  - 94
SP  - 103177
EP  - 103177
DO  - https://doi.org/10.1016/j.jbi.2019.103177
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419300954
N2  - Objectives
Treatment used in a randomized clinical trial is a critical data element both for physicians at the point of care and reviewers who are evaluating different interventions. Much of existing work on treatment extraction from the biomedical literature has focused on the extraction of pharmacological interventions. However, non-pharmacological interventions (e.g., exercise, diet, etc.) that are frequently used to address chronic conditions are less well studied. The goal of this study is to compare knowledge-based and machine learning strategies for the extraction of both drug and non-drug treatments.
Methods
We collected 800 randomized clinical trial abstracts each for breast cancer and diabetes from PubMed. The treatments in the result/conclusion sentences of the abstracts were manually annotated and marked as drug/non-drug treatments. We then designed three methods to identify the treatments and evaluated the systems with respect to drug/non-drug treatments. The first method is solely based on knowledge base (here we used MetaMap). The second method is based on a machine learning model trained mainly on contextual features (ML_only). The third method is a combination approach that integrates the previous two approaches.
Results/discussion
Results show that MetaMap, when used with high precision semantic types, has better performance for drug compared to non-drug treatments (F1 = 0.77 vs. 0.64). The ML_only approach has smaller performance difference between drug and non-drug treatments compared with the KB-based approach (F1 = 0.02 vs. 0.05, 0.07, and 0.13). The combination approach achieves significantly better performance than all MetaMap approaches alone for total treatments (F1 = 0.76 vs. 0.72, p < 0.001). The performance gain mainly comes from the non-drug treatments (0.03–0.08 improvement in F1), while the drug treatments do not benefit much from the combination approach (0–0.03 improvement in F1).
Conclusion
These results suggest that a knowledge-based approach should be employed for medical conditions that are primarily treated with drugs whereas conditions that are treated with either a combination of drug and non-drug interventions or primarily non-drug interventions should use automated tools that combine machine learning and a knowledge-based approach to achieve optimal performance.
ER  - 
TY  - JOUR
T1  - A System for Classifying Disease Comorbidity Status from Medical Discharge Summaries Using Automated Hotspot and Negated Concept Detection
A1  - Ambert, Kyle H
A1  - Cohen, Aaron M
Y1  - 2009///
JF  - Journal of the American Medical Informatics Association
VL  - 16
IS  - 4
SP  - 590
EP  - 595
DO  - https://doi.org/10.1197/jamia.M3095
UR  - https://www.sciencedirect.com/science/article/pii/S1067502709000723
N2  - Objective
Free-text clinical reports serve as an important part of patient care management and clinical documentation of patient disease and treatment status. Free-text notes are commonplace in medical practice, but remain an under-used source of information for clinical and epidemiological research, as well as personalized medicine. The authors explore the challenges associated with automatically extracting information from clinical reports using their submission to the Integrating Informatics with Biology and the Bedside (i2b2) 2008 Natural Language Processing Obesity Challenge Task.
Design
A text mining system for classifying patient comorbidity status, based on the information contained in clinical reports. The approach of the authors incorporates a variety of automated techniques, including hot-spot filtering, negated concept identification, zero-vector filtering, weighting by inverse class-frequency, and error-correcting of output codes with linear support vector machines.
Measurements
Performance was evaluated in terms of the macroaveraged F1 measure.
Results
The automated system performed well against manual expert rule-based systems, finishing fifth in the Challenge's intuitive task, and 13th in the textual task.
Conclusions
The system demonstrates that effective comorbidity status classification by an automated system is possible.
ER  - 
TY  - JOUR
T1  - Predicting the morbidity of chronic obstructive pulmonary disease based on multiple locally weighted linear regression model with K-means clustering
A1  - Huang, Zhi-yong
A1  - Lin, Shuang
A1  - Long, Li-li
A1  - Cao, Jiao-yang
A1  - Luo, Fen
A1  - Qin, Wen-cheng
A1  - Sun, Da-ming
A1  - Gregersen, Hans
Y1  - 2020///
KW  -  K-means clustering
KW  -  Locally weighted linear regression
KW  -  PM2.5
KW  -  SO2
KW  - Chronic obstructive pulmonary disease
JF  - International Journal of Medical Informatics
VL  - 139
SP  - 104141
EP  - 104141
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104141
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619309463
N2  - Chronic Obstructive Pulmonary Disease (COPD) is a common chronic respiratory disease related to inflammation affected by harmful gas and particulate matter in the air. Mathematical prediction models between COPD and air pollutants are helpful for early identification, individualized interventions to slow disease progression, and for reduction of medical expenditures. The aim was to build a regression prediction model for the occurrence of COPD acute exacerbation. We collected hospital admissions for COPD in 2015–2018 from ten hospitals in Chongqing, China, used the increment per week as response, and the local sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO) and particulate matter 2.5 (PM2.5) concentrations as predictor variables to build a multiple prediction model. The Mean Absolute Percentage Error (MAPE) was used to evaluate the efficiency. We found that PM2.5 and SO2 are the most important factors contributing to the improvement of prediction accuracy. Multiple locally weighted linear regression (LWLR) Model based on integrated kernel framework with the K-means algorithm demonstrated minimum prediction error of 9.03 %(k=11).
ER  - 
TY  - JOUR
T1  - Personalized healthcare cloud services for disease risk assessment and wellness management using social media
A1  - Abbas, Assad
A1  - Ali, Mazhar
A1  - Shahid Khan, Muhammad Usman
A1  - Khan, Samee U
Y1  - 2016///
KW  -  Expert users
KW  -  Health big-data
KW  -  Risk assessment
KW  - Cloud computing
JF  - Pervasive and Mobile Computing
VL  - 28
SP  - 81
EP  - 99
DO  - https://doi.org/10.1016/j.pmcj.2015.10.014
UR  - https://www.sciencedirect.com/science/article/pii/S1574119215001984
N1  - Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren
N2  - We propose a cloud based framework that effectively manages the health related Big-data and benefits from the ubiquity of the Internet and social media. The framework facilitates the mobile and desktop users by offering: (a) disease risk assessment service and (b) consultation service with the health experts on Twitter. The disease risk assessment is performed through a collaborative filtering based approach whereas the hubs and authorities based approach is employed to identify the health experts from Twitter. The framework is implemented as Software as a Service (SaaS) to provide the disease risk assessment and expert user interaction services. Experimental results exhibit that the proposed framework achieves high accuracy as compared to the state-of-the-art approaches in terms of disease risk assessment and expert user recommendation.
ER  - 
TY  - JOUR
T1  - An artificial intelligence approach to early predict non-ST-elevation myocardial infarction patients with chest pain
A1  - Wu, Chieh-Chen
A1  - Hsu, Wen-Ding
A1  - Islam, Md. Mohaimenul
A1  - Poly, Tahmina Nasrin
A1  - Yang, Hsuan-Chia
A1  - Nguyen, Phung-Anh (Alex)
A1  - Wang, Yao-Chin
A1  - Li, Yu-Chuan (Jack)
Y1  - 2019///
KW  -  Artificial neural network
KW  -  Chest pain
KW  -  Non-ST elevated MI
KW  - Acute coronary syndrome
JF  - Computer Methods and Programs in Biomedicine
VL  - 173
SP  - 109
EP  - 117
DO  - https://doi.org/10.1016/j.cmpb.2019.01.013
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718316936
N2  - Background and Aims
Hospital admission rate for the patients with chest pain has already been increased worldwide but no existing risk score has been designed to stratify non-ST-elevation myocardial infarction (NSTEMI) from non-cardiogenic chest pain. Clinical diagnosis of chest pain in the emergency department is always highly subjective and variable. We, therefore, aimed to develop an artificial intelligence approach to predict stable NSTEMI that would give valuable insight to reduce misdiagnosis in the real clinical setting.
Methods
A standard protocol was developed to collect data from chest pain patients who had visited the emergency department between December 2016 and February 2017. All the chest pain patients with aged <20 years were primarily included in this study. However, STEMI, previous history of ACS, and out-of-hospital cardiac arrest were excluded from our study. An artificial neural network (ANN) model was then developed to predict NSTEMI patients. The accuracy, sensitivity, specificity, and receiver operating characteristic curve was used to measure the performance of this model.
Results
A total of 268 chest pain patients were included in this study; of those, 47 (17.5%) was stable NSTEMI, and 221 (82.5%) was unstable angina patients. Serval risk factors such as cardiac risk factor, systolic blood pressure, hemoglobin, corrected QT interval (QTc), PR interval, glutamic-oxaloacetic transaminase, glutamic pyruvic transaminase and troponin were independently associated with stable NSTEMI. The area under the receiver operating characteristic (AUROC) and accuracy of ANN were 98.4, and 92.86. Additionally, the sensitivity, specificity, positive predictive value, and negative predictive value of the ANN model was 90.91, 93.33, 76.92, and 97.67 respectively.
Conclusion
Our prediction model showed a higher accuracy to predict NSTEMI patients. This model has a potential application in disease detection, monitoring, and prognosis of chest pain at risk of AMI.
ER  - 
TY  - JOUR
T1  - Learning an expandable EMR-based medical knowledge network to enhance clinical diagnosis
A1  - Xie, Jing
A1  - Jiang, Jingchi
A1  - Wang, Yehan
A1  - Guan, Yi
A1  - Guo, Xitong
Y1  - 2020///
KW  -  Disease diagnosis
KW  -  Electronic medical record
KW  -  Knowledge integration
KW  -  Link prediction
KW  -  Medical knowledge network
KW  - Incremental expansion framework
JF  - Artificial Intelligence in Medicine
VL  - 107
SP  - 101927
EP  - 101927
DO  - https://doi.org/10.1016/j.artmed.2020.101927
UR  - https://www.sciencedirect.com/science/article/pii/S093336571830647X
N2  - Electronic medical records (EMRs) contain a wealth of knowledge that can be used to assist doctors in making clinical decisions like disease diagnosis. Constructing a medical knowledge network (MKN) to link medical concepts in EMRs is an effective way to manage this knowledge. The quality of the diagnostic result made by MKN-based clinical decision support system depends on the accuracy of medical knowledge and the completeness of the network. However, collecting knowledge is a long-lasting and cumulative process, which means it’s hard to construct a complete MKN with limited data. This study was conducted with the objective of developing an expandable EMR-based MKN to enhance capabilities in making an initial clinical diagnosis. A network of symptom-indicate-disease knowledge in 992 Chinese EMRs (CEMRs) was manually constructed as Original-MKN, and an incremental expansion framework was applied to it to obtain an expandable MKN based on new CEMRs. The framework was composed by: (1) integrating external knowledge extracted from the medical information websites and (2) mining potential knowledge with new EMRs. The framework also adopts a diagnosis-driven learning method to estimate the effectiveness of each knowledge in clinical practice. Experimental results indicate that our expanded MKN achieves a precision of 0.837 for a recall of 0.719 in clinical diagnosis, which outperforms Original-MKN and four classical machine learning methods. Furthermore, both external medical knowledge and potential medical knowledge benefit MKN expansion and disease diagnosis. The proposed incremental expansion framework sustains the MKN learning new knowledge.
ER  - 
TY  - JOUR
T1  - Collaborative extreme learning machine with a confidence interval for P2P learning in healthcare
A1  - XIE, Rongjun
A1  - Khalil, Ibrahim
A1  - Badsha, Shahriar
A1  - Atiquzzaman, Mohammed
Y1  - 2019///
KW  -  Collaborative ELM
KW  -  Confidence interval
KW  -  Incremental learning
KW  -  P2P Learning
KW  -  Smart healthcare
KW  - P2P network
JF  - Computer Networks
VL  - 149
SP  - 127
EP  - 143
DO  - https://doi.org/10.1016/j.comnet.2018.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S1389128618307278
N2  - In modern e-healthcare systems, medical institutions can provide more reliable diagnoses by introducing Machine-Learning (ML)-based classifiers. These ML classifiers are frequently trained with huge numbers of patients’ data to keep updated with new diseases and changes in current disease patterns. To increase the accuracy in prediction process, Peer-to-Peer (P2P) learning systems have been explored by many studies by which medical institutions can share their data with others: the more data are available, the more accurate the predictions. However, the traditional P2P network system requires much time in which the training data are shared among the nodes in the network. The system also spends much time on learning from samples where the data labels are unknown. Moreover, some nodes may perform certain computations which had already been computed by other nodes, resulting in redundant computations. In this paper, in order to deal with samples having unknown data labels, we propose a Collaborative Extreme Learning Machine (CELM) with a Confidence Interval (CI), which is an enhanced version of the traditional Extreme Learning Machine (ELM). Our proposed model eliminates redundant calculations of the network nodes (the e-healthcare institutions) to improve the learning efficiency, and improves the prediction accuracy by considering where plausible predictions lie. The extensive experimental analysis shows that the proposed model is efficient and achieves high accuracy (up to 98%) in diagnosing clinical events by analyzing patients’ medical records.
ER  - 
TY  - JOUR
T1  - Pattern analysis in daily physical activity data for personal health management
A1  - Chiang, Jung-Hsien
A1  - Yang, Pei-Ching
A1  - Tu, Hsuan
Y1  - 2014///
KW  -  Daily activity habits
KW  -  Health care
KW  -  Personal health management
KW  - Pattern recognition
JF  - Pervasive and Mobile Computing
VL  - 13
SP  - 13
EP  - 25
DO  - https://doi.org/10.1016/j.pmcj.2013.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S1574119213001545
N2  - Purpose: sedentary lifestyles have resulted in an increasing number of people who are at increased risk of various conditions and diseases, including overweight, obesity, and metabolic syndromes. Our objective was to systematically record the daily life journal on a platform to increase the self-awareness and improve the sedentary lifestyle and to assist clinicians in understanding and facilitating patients’ daily physical activity. Method: we developed a portable activity pattern recognition system designed to automatically recognize the daily activity habits of users, and provide visualized life logs on the wellness self-management platform for patients and clinicians. Based on the participants’ and the clinician’s comments, appropriate modifications were made. Results: persuading people to improve their activities during non-working hours can enhance the general physical activity. Since users’ smartphones automatically monitor their energy expenditure, healthcare professionals can use these data to assist their patients in addressing health problems stemming from the obesity or metabolic syndromes, thus empowering users to avert or delay the progression of diabetes, cardiovascular disease and other complications. Discussion and conclusions: the clinical pilot study showed the feasibility of applying this persuasive technology to improve the physical activity of overweight people. The limitation of the study is the need for Wi-Fi and 3G environments and a smartphone.
ER  - 
TY  - JOUR
T1  - Analysis of heart sound anomalies using ensemble learning
A1  - Baydoun, Mohammed
A1  - Safatly, Lise
A1  - Ghaziri, Hassan
A1  - El Hajj, Ali
Y1  - 2020///
KW  -  Classification
KW  -  Ensemble learning
KW  -  Feature extraction
KW  - Phonocardiogram
JF  - Biomedical Signal Processing and Control
VL  - 62
SP  - 102019
EP  - 102019
DO  - https://doi.org/10.1016/j.bspc.2020.102019
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420301750
N2  - Phonocardiogram (PCG) signal analysis is a common method for evaluating the condition of the heart and detecting possible anomalies such as cardiovascular diseases. This work concentrates on the Physionet challenge database that stores PCG recordings for more than one thousand subjects, including healthy and pathological records. A complete methodology is provided to analyze and classify PCG data. The PCG signals are first filtered and segmented into different parts, then analyzed by applying a feature extraction process, followed by classifying the signal as that of a healthy or unhealthy person. The extracted optimal features subset includes statistical components, such as the mean and standard deviation from different parts of the signal, in addition to wavelet-based features. The classification mainly relies on bagging and boosting algorithms, as well as adequately preparing the data in order to yield an enhanced ensemble classifier. The work further provides the approach to combine multiple classification models to improve accuracy. The effect of segmenting the different beats of the PCG on classification scores is also addressed, and the results are shown to be of high precision with an accuracy score of 86.6% on the hidden test, in comparison with recent and best performing literature that achieved 86%. Also, the proposed methodology is extended to the Pascal heart sounds challenge with accurate results that exceed previous works confirming the performance and robustness of the work since it can be applied to multiple databases and sources. The work further provides important insights regarding analyzing PCG recordings and discusses future work possibilities.
ER  - 
TY  - JOUR
T1  - A comparative study of the leading machine learning techniques and two new optimization algorithms
A1  - Baumann, P
A1  - Hochbaum, D S
A1  - Yang, Y T
Y1  - 2019///
KW  -  Binary classification
KW  -  Comparative study
KW  -  Supervised machine learning
KW  -  Supervised normalized cut
KW  - Data mining
JF  - European Journal of Operational Research
VL  - 272
IS  - 3
SP  - 1041
EP  - 1057
DO  - https://doi.org/10.1016/j.ejor.2018.07.009
UR  - https://www.sciencedirect.com/science/article/pii/S0377221718306143
N2  - We present here a computational study comparing the performance of leading machine learning techniques to that of recently developed graph-based combinatorial optimization algorithms (SNC and KSNC). The surprising result of this study is that SNC and KSNC consistently show the best or close to best performance in terms of their F1-scores, accuracy, and recall. Furthermore, the performance of SNC and KSNC is considerably more robust than that of the other algorithms; the others may perform well on average but tend to vary greatly across data sets. This demonstrates that combinatorial optimization techniques can be competitive as compared to state-of-the-art machine learning techniques. The code developed for SNC and KSNC is publicly available.
ER  - 
TY  - JOUR
T1  - Survival prediction of heart failure patients using machine learning techniques
A1  - Newaz, Asif
A1  - Ahmed, Nadim
A1  - Shahriyar Haq, Farhan
Y1  - 2021///
KW  -  Ejection fraction
KW  -  Heart failure
KW  -  Imbalanced classification
KW  -  Random forest
KW  -  Recursive feature elimination
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100772
EP  - 100772
DO  - https://doi.org/10.1016/j.imu.2021.100772
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821002458
N2  - The goal of this research is to develop a reliable decision-support system for the survival prediction of heart failure patients by utilizing their clinical records and laboratory test results. Forecasting heart-failure related events in clinical practice tend to be quite inaccurate and highly variable. Identifying the key drivers of heart failure is also clinically very important. In this regard, we develop a model to accurately identify the patients who are at risk utilizing machine learning techniques. This can help clinicians make informed decisions regarding the intensity of treatment required for a patient. For this study, we have utilized a heart failure dataset originally collected from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad, Pakistan. Sampling strategy is incorporated into the ensemble learning framework to develop a more robust Random Forest Classifier that can effectively deal with the imbalanced nature of the data and provide a more generalizable result with higher accuracy. Two different feature selection techniques - Chi-square test and Recursive Feature Elimination are utilized to identify the features that are most significant in terms of survival prediction of heart failure patients. Using our proposed approach, a maximum G-mean score of 76.83% with a sensitivity score of 80.21% was achieved, which is significantly higher than what has been reported by other researchers. Thus, our proposed framework has the potential to be an effective tool to identify the patients who are at risk and guide clinicians accordingly to take pertinent measures.
ER  - 
TY  - JOUR
T1  - Identification and classification of microaneurysms for early detection of diabetic retinopathy
A1  - Akram, M Usman
A1  - Khalid, Shehzad
A1  - Khan, Shoab A
Y1  - 2013///
KW  -  Classification
KW  -  Diabetic retinopathy
KW  -  Microaneurysms
KW  -  m-Mediods
KW  - Medical image processing
JF  - Pattern Recognition
VL  - 46
IS  - 1
SP  - 107
EP  - 116
DO  - https://doi.org/10.1016/j.patcog.2012.07.002
UR  - https://www.sciencedirect.com/science/article/pii/S003132031200297X
N2  - Diabetic retinopathy is a progressive eye disease which may cause blindness if not detected and treated in time. The early detection and diagnosis of diabetic retinopathy is important to protect the patient's vision. The accurate detection of microaneurysms (MAs) is a critical step for early detection of diabetic retinopathy because they appear as the first sign of disease. In this paper, we propose a three-stage system for early detection of MAs using filter banks. In the first stage, the system extracts all possible candidate regions for MAs present in retinal image. In order to classify a candidate region as MA or non-MA, the system formulates a feature vector for each region depending upon certain properties, i.e. shape, color, intensity and statistics. We present a hybrid classifier which combines the Gaussian mixture model (GMM), support vector machine (SVM) and an extension of multimodel mediod based modeling approach in an ensemble to improve the accuracy of classification. The proposed system is evaluated using publicly available retinal image databases and achieved higher accuracy which is better than previously published methods.
ER  - 
TY  - JOUR
T1  - Decision support system for Warfarin therapy management using Bayesian networks
A1  - Yet, Barbaros
A1  - Bastani, Kaveh
A1  - Raharjo, Hendry
A1  - Lifvergren, Svante
A1  - Marsh, William
A1  - Bergman, Bo
Y1  - 2013///
KW  -  Anticoagulant therapy
KW  -  Decision support systems
KW  -  Warfarin therapy
KW  - Bayesian networks
JF  - Decision Support Systems
VL  - 55
IS  - 2
SP  - 488
EP  - 498
DO  - https://doi.org/10.1016/j.dss.2012.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S016792361200262X
N1  - 1. Analytics and Modeling for Better HealthCare 2. Decision Making in Healthcare
N2  - Warfarin therapy is known as a complex process because of the variation in the patients' response. Failure to deal with such variation may lead to death as a result of thrombosis or bleeding. The possible sources of variation such as concomitant illnesses and drug interactions have to be investigated by the clinician in order to deal with the variation. This paper describes a decision support system (DSS) using Bayesian networks for assisting clinicians to make better decisions in Warfarin therapy management. The DSS is developed in collaboration with a Swedish hospital group that manages Warfarin therapy for more than 3000 patients. The proposed model can assist the clinician in making dose-adjustment and follow-up interval decisions, investigating variation causes, and evaluating bleeding and thrombosis risks related to therapy. The model is built upon previous findings from medical literature, the knowledge of domain experts, and large dataset of patients.
ER  - 
TY  - JOUR
T1  - Machine learning model for predicting malaria using clinical information
A1  - Lee, You Won
A1  - Choi, Jae Woo
A1  - Shin, Eun-Hee
Y1  - 2021///
KW  -  Case reports
KW  -  Diagnosis
KW  -  Malaria
KW  -  Patient information
KW  - Machine learning
JF  - Computers in Biology and Medicine
VL  - 129
SP  - 104151
EP  - 104151
DO  - https://doi.org/10.1016/j.compbiomed.2020.104151
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520304820
N2  - Background
Rapid diagnosing is crucial for controlling malaria. Various studies have aimed at developing machine learning models to diagnose malaria using blood smear images; however, this approach has many limitations. This study developed a machine learning model for malaria diagnosis using patient information.
Methods
To construct datasets, we extracted patient information from the PubMed abstracts from 1956 to 2019. We used two datasets: a solely parasitic disease dataset and total dataset by adding information about other diseases. We compared six machine learning models: support vector machine, random forest (RF), multilayered perceptron, AdaBoost, gradient boosting (GB), and CatBoost. In addition, a synthetic minority oversampling technique (SMOTE) was employed to address the data imbalance problem.
Results
Concerning the solely parasitic disease dataset, RF was found to be the best model regardless of using SMOTE. Concerning the total dataset, GB was found to be the best. However, after applying SMOTE, RF performed the best. Considering the imbalanced data, nationality was found to be the most important feature in malaria prediction. In case of the balanced data with SMOTE, the most important feature was symptom.
Conclusions
The results demonstrated that machine learning techniques can be successfully applied to predict malaria using patient information.
ER  - 
TY  - JOUR
T1  - Privacy preserving classification on local differential privacy in data centers
A1  - Fan, Weibei
A1  - He, Jing
A1  - Guo, Mengjiao
A1  - Li, Peng
A1  - Han, Zhijie
A1  - Wang, Ruchuan
Y1  - 2020///
KW  -  Classification model
KW  -  Data mining
KW  -  Local Differential privacy
KW  - Data center networks
JF  - Journal of Parallel and Distributed Computing
VL  - 135
SP  - 70
EP  - 82
DO  - https://doi.org/10.1016/j.jpdc.2019.09.009
UR  - https://www.sciencedirect.com/science/article/pii/S0743731519303296
N2  - With the rise of cloud service providers and the continuous virtualization of data centers, data center networks are also developing rapidly. As data centers become more and more complex, the demand for security increases dramatically. This paper discusses the privacy inherent in data centers. However, there is no general solution to the privacy problem in data centers due to the device heterogeneity. In this paper, we proposed a local differential privacy-based classification algorithm for data centers. In data mining of data centers, the differential privacy protection mechanism is added to deal with Laplace noise of sensitive information in the pattern mining process. We designed a method to quantify the quality of privacy protection through strict mathematical proof. Experiments demonstrated that the differential privacy-based classification algorithm proposed in this paper has higher iteration efficiency, better security and feasible accuracy. On the premise of ensuring availability, the algorithm has reliable privacy protection characteristics and excellent timeliness.
ER  - 
TY  - JOUR
T1  - Classification of Obstructive Sleep Apnoea from single-lead ECG signals using convolutional neural and Long Short Term Memory networks
A1  - Almutairi, Haifa
A1  - Hassan, Ghulam Mubashar
A1  - Datta, Amitava
Y1  - 2021///
KW  -  Classification
KW  -  Convolutional Neural Networks
KW  -  Deep learning
KW  -  ECG
KW  -  Long Short Term Memory
KW  - Obstructive Sleep Apnoea
JF  - Biomedical Signal Processing and Control
VL  - 69
SP  - 102906
EP  - 102906
DO  - https://doi.org/10.1016/j.bspc.2021.102906
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421005036
N2  - Obstructive Sleep Apnoea (OSA) is a breathing disorder that happens during sleep. Polysomnography (PSG) is typically used as a reference standard for the diagnosis of OSA which uses different physiological signals such as Electrocardiography (ECG), Electroencephalogram (EEG) and Electromyogram (EMG) in a sleep laboratory. This procedure is time-consuming, expensive and inconvenient. However, detection of OSA by using a wearable sensor to collect Electrocardiography (ECG) signals is a practical and effective alternative. Previous studies of OSA classification from ECG signals focused on feature engineering methods which involves extracting specific features from ECG signals and using the extracted feature as inputs to the machine learning methods. In this study, we propose a novel method of OSA classification of ECG signal where deep learning methods automatically extract the features from the ECG signals and classify them. Our deep learning approach uses a hybrid model involving Convolution Neural Networks (CNN) and Long Short Term Memory (LSTM) networks. PhysioNet Apnea-ECG database is used for training and evaluation of our proposed deep learning model. For the released training dataset, our proposed model achieves the accuracy of 94.27%, sensitivity of 94.57%, specificity of 93.93% and F1 score of 95.41%. While for the testing dataset, the achieved accuracy, sensitivity, specificity and F1 score for the proposed model are 90.92%, 91.24%, 90.36% and 92.76% respectively. The performance of our model is compared with state of the art techniques and we found our model to achieve the best performance to classify OSA and health ECG signals.
ER  - 
TY  - JOUR
T1  - An evaluation of a simple model for predicting surgery duration using a set of surgical procedure parameters
A1  - Yuniartha, Deny Ratna
A1  - Masruroh, Nur Aini
A1  - Herliansyah, Muhammad Kusumawan
Y1  - 2021///
KW  -  Machine learning
KW  -  Prediction
KW  -  Surgery billing
KW  -  Surgical procedure
KW  - Surgery duration
JF  - Informatics in Medicine Unlocked
VL  - 25
SP  - 100633
EP  - 100633
DO  - https://doi.org/10.1016/j.imu.2021.100633
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821001234
N2  - The predicted surgery duration is the main data for operating room scheduling. Existing studies on surgery duration prediction have mostly addressed a large set of predictors. However, the available data for predictors are limited or cannot be easily obtained. In practice, the patient's identity and the surgical procedure name are definitely available when the surgeon reserves the surgery schedule. Other detailed data will be available only after the clinical observation of the patient, which is conducted a few hours before the surgery. Furthermore, the variability in surgery duration contributes to the complexity of the operating room scheduling. This study evaluated a simple model to predict the duration of surgery. The model used fewer predictors, which were the surgical procedure parameters, and reduced the variability of the surgery duration numerical value. The parameters comprised a set of hospital parameters collected for the purpose of surgery billing, representing the surgery complexity and resources needed. Using the estimation algorithm, our results showed that a set of surgical procedure parameters as the sole predictors resulted in slightly better performance than combining them with patient features. To reduce the variability of the surgical duration numerical values, we used discretization to convert them into categorical values represented by time bins. We proposed a modified calculation of error and accuracy based on the mean absolute error (MAE) of the estimation algorithm to evaluate the classification algorithm for predicting surgery duration using categorical values. Our study indicated that the use of categorical values resulted in a performance equivalent to that obtained using numerical values. Our simple model could facilitate a hospital to develop a framework for predicting surgery duration using the limited data available for surgery billing. The impacts of operating room scheduling using predicted surgery duration categorical values on patient waiting time and resource utilization in the operating room will be considered in a further study.
ER  - 
TY  - JOUR
T1  - Systematic analysis of genes and diseases using PheWAS-Associated networks
A1  - Khosravi, Ali
A1  - Kouhsar, Morteza
A1  - Goliaei, Bahram
A1  - Jayaram, B
A1  - Masoudi-Nejad, Ali
Y1  - 2019///
KW  -  Associated disease network
KW  -  Associated gene network
KW  -  Systems biology
KW  - PheWAS associations
JF  - Computers in Biology and Medicine
VL  - 109
SP  - 311
EP  - 321
DO  - https://doi.org/10.1016/j.compbiomed.2019.04.037
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519301453
N2  - Several scientific sources have reported different causes of various diseases. One of these factors is genetic variation. Natural selection, molecular evolution and susceptibility to external conditions are the main causes of genetic variations. Phenome-Wide Association Studies (PheWAS) can emphasize the associations of genetic variations and diseases. The systematic analysis of these associations can highlight various important aspects of gene correlations and disease relationships. In this study, we have investigated a systematic approach to analyze associated networks of genes and diseases to explore novel scientific information. We have constructed the Associated Gene Network (AGN, n = 1769) and the Associated Disease Network (ADN, n = 503) based on common diseases and genes, respectively. We have evaluated these networks based on topological measures and compared them with a randomized null network. The comparative modular analysis based on size and quantity is a clear indication of the significance of these networks. We have found numerous novel associations of genes involved in different diseases. We have also found different diseases related to one another, which can correlate scientific evidence. We have verified our analysis through GO and KEGG enrichment for different case studies and concluded that AGN and ADN can be used as reference biological networks for various purposes such as drug design and drug repurposing.
ER  - 
TY  - JOUR
T1  - Outlier Identification using Symmetric Neighborhoods
A1  - Gogoi, Prasanta
A1  - Borah, B
A1  - Bhattacharyya, D K
A1  - Kalita, J K
Y1  - 2012///
KW  -  DR
KW  -  FNNk
KW  -  FPR
KW  -  Forward neighbor
KW  -  NNk
KW  -  Outlier
KW  - Anomaly
JF  - Procedia Technology
VL  - 6
SP  - 239
EP  - 246
DO  - https://doi.org/10.1016/j.protcy.2012.10.029
UR  - https://www.sciencedirect.com/science/article/pii/S2212017312005749
N1  - 2nd International Conference on Communication, Computing &amp; Security [ICCCS-2012]
N2  - In recent research, outlier mining has been widely used in areas such as telecommunication, health care, ﬁnance and network intrusion detection. By applying outlier mining in network anomaly detection, rarely occurring attacks can be identiﬁed. In this paper we propose an outlier mining method based on symmetric neighborhood relationships. We evaluate the method with UCI ML Repository datasets, benchmark dataset KDD Cup 1999 and real time intrusion datasets. The experimental results are compared with existing approaches and performance is excellent.
ER  - 
TY  - JOUR
T1  - Automatic detection of microaneurysms in color fundus images
A1  - Walter, Thomas
A1  - Massin, Pascale
A1  - Erginay, Ali
A1  - Ordonez, Richard
A1  - Jeulin, Clotilde
A1  - Klein, Jean-Claude
Y1  - 2007///
KW  -  Attribute opening
KW  -  Criteria based operators
KW  -  Density estimation
KW  -  Diabetic retinopathy
KW  -  Diameter opening
KW  -  Lesion detection
KW  -  Mathematical morphology
KW  -  Shade correction
KW  - Microaneurysm detection
JF  - Medical Image Analysis
VL  - 11
IS  - 6
SP  - 555
EP  - 566
DO  - https://doi.org/10.1016/j.media.2007.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S1361841507000461
N2  - This paper addresses the automatic detection of microaneurysms in color fundus images, which plays a key role in computer assisted diagnosis of diabetic retinopathy, a serious and frequent eye disease. The algorithm can be divided into four steps. The first step consists in image enhancement, shade correction and image normalization of the green channel. The second step aims at detecting candidates, i.e. all patterns possibly corresponding to MA, which is achieved by diameter closing and an automatic threshold scheme. Then, features are extracted, which are used in the last step to automatically classify candidates into real MA and other objects; the classification relies on kernel density estimation with variable bandwidth. A database of 21 annotated images has been used to train the algorithm. The algorithm was compared to manually obtained gradings of 94 images; sensitivity was 88.5% at an average number of 2.13 false positives per image.
ER  - 
TY  - JOUR
T1  - On computing critical factors based healthy behavior index for behavior assessment
A1  - Bilal, Hafiz Syed Muhammad
A1  - Amin, Muhammad Bilal
A1  - Hussain, Jamil
A1  - Ali, Syed Imran
A1  - Hussain, Shujaat
A1  - Sadiq, Muhammad
A1  - Razzaq, Muhammad Asif
A1  - Abbas, Asim
A1  - Choi, Chunho
A1  - Lee, Sungyoung
Y1  - 2020///
KW  -  Alcohol
KW  -  Behavior quantification
KW  -  Healthy behavior index
KW  -  Nutrition
KW  -  Physical activity
KW  -  Smoking
KW  - Healthy behavior
JF  - International Journal of Medical Informatics
VL  - 141
SP  - 104181
EP  - 104181
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104181
UR  - https://www.sciencedirect.com/science/article/pii/S138650561930348X
N2  - Objective
Ubiquitous computing has supported personalized health through a vast variety of wellness and healthcare self-quantification applications over the last decade. These applications provide insights for daily life activities but unable to portray the comprehensive impact of personal habits on human health. Therefore, in order to facilitate the individuals, we have correlated the lifestyle habits in an appropriate proportion to determine the overall impact of influenced behavior on the well-being of humans.
Materials and methods
To study the combined impact of personal behaviors, we have proposed a methodology to derive the comprehensive Healthy Behavior Index (HBI) consisting of two major processes: (1) Behaviors’ Weight-age Identification (BWI), and (2) Healthy Behavior Quantification and Index (HBQI) modeling. The BWI process identifies the high ranked contributing behaviors through life-expectancy based weight-age, whereas HBQI derives a mathematical model based on quantification and indexing of behavior using wellness guidelines.
Results
The contributing behaviors are identified through text mining technique and verified by seven experts with a Kappa agreement level of 0.379. A real-world user-centric statistical evaluation is applied through User Experience Questionnaire (UEQ) method to evaluate the impact of HBI service. This HBI service is developed for the Mining Minds, a wellness management application. This study involves 103 registered participants (curious about the chronic disease) for a Korean wellness management organization. They used the HBI service over 12 weeks, the results for which were evaluated through UEQ and user feedback. The service reliability for the Cronbach's alpha coefficient greater than 0.7 was achieved using HBI service whereas the stimulation coefficient of the value 0.86 revealed significant effect. We observed an overall novelty of the value 0.88 showing the potential interest of participants.
Conclusions
The comprehensive HBI has demonstrated positive user experience concerning the stimulation for adapting the healthy behaviors. The HBI service is designed independently to work as a service, so any other wellness management service-enabled platform can consume it to evaluate the healthy behavior index of the person for recommendation generation, behavior indication, and behavior adaptation.
ER  - 
TY  - JOUR
T1  - Assessment of four neural network based classifiers to automatically detect red lesions in retinal images
A1  - García, María
A1  - López, María I
A1  - Álvarez, Daniel
A1  - Hornero, Roberto
Y1  - 2010///
KW  -  Logistic regression
KW  -  Neural network
KW  -  Red lesion
KW  -  Retinal imaging
KW  - Diabetic retinopathy
JF  - Medical Engineering & Physics
VL  - 32
IS  - 10
SP  - 1085
EP  - 1093
DO  - https://doi.org/10.1016/j.medengphy.2010.07.014
UR  - https://www.sciencedirect.com/science/article/pii/S1350453310001669
N2  - Diabetic retinopathy (DR) is an important cause of visual impairment in industrialised countries. Automatic detection of DR early markers can contribute to the diagnosis and screening of the disease. The aim of this study was to automatically detect one of such early signs: red lesions (RLs), like haemorrhages and microaneurysms. To achieve this goal, we extracted a set of colour and shape features from image regions and performed feature selection using logistic regression. Four neural network (NN) based classifiers were subsequently used to obtain the final segmentation of RLs: multilayer perceptron (MLP), radial basis function (RBF), support vector machine (SVM) and a combination of these three NNs using a majority voting (MV) schema. Our database was composed of 115 images. It was divided into a training set of 50 images (with RLs) and a test set of 65 images (40 with RLs and 25 without RLs). Attending to performance and complexity criteria, the best results were obtained for RBF. Using a lesion-based criterion, a mean sensitivity of 86.01% and a mean positive predictive value of 51.99% were obtained. With an image-based criterion, a mean sensitivity of 100%, mean specificity of 56.00% and mean accuracy of 83.08% were achieved.
ER  - 
TY  - JOUR
T1  - An interpretable knowledge-based decision support system and its applications in pregnancy diagnosis
A1  - Song, Kehui
A1  - Zeng, Xianyi
A1  - Zhang, Ying
A1  - De Jonckheere, Julien
A1  - Yuan, Xiaojie
A1  - Koehl, Ludovic
Y1  - 2021///
KW  -  Decision making
KW  -  Fuzzy best–worst method
KW  -  Multi-granularity Linguistic Term Sets
KW  - Medical expert system
JF  - Knowledge-Based Systems
VL  - 221
SP  - 106835
EP  - 106835
DO  - https://doi.org/10.1016/j.knosys.2021.106835
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121000988
N2  - This paper aims to propose an interpretable knowledge-based decision support system (IKBDSS) that will assist physicians to predict the risk level of a disease. Our system enables to integrate both historical cases extracted from database and opinions provided by different experts in order to set up a medical knowledge base and provide relevant advises by inferring from the knowledge base. To present various experts’ opinions, the Multi-granularity Linguistic Term Sets (MLTS) model is used to address the ambiguity and intangibility of knowledge. Our work mainly focuses on knowledge acquisition, similarity degree calculation and consistency checking process. It is worth mentioning that a criterion weights calculation method is introduced to objectively obtain the weights based on knowledge from experts, rather than subjectively predefined. The developed system leads to a better performance in specificity, sensitivity and F1 score compared to other methods in the literature. To conclude, our work contributes to: (1) The development of a medical decision support system to combine clinical records and domain knowledge to predict diagnosis. (2) The decision-making process ensures interpretability, which increases the reliability of our system in terms of being a decision supporter. (3) The criterion weights are calculated based on the professional knowledge presented in MLTS form, and this process improves the capacity of providing diagnostic recommendations.
ER  - 
TY  - JOUR
T1  - Algorithm for registration of full Scanning Laser Ophthalmoscope video sequences
A1  - Mariño, C
A1  - Ortega, M
A1  - Barreira, N
A1  - Penedo, M G
A1  - Carreira, M J
A1  - González, F
Y1  - 2011///
KW  -  Artery–vein time
KW  -  Crest lines
KW  -  Dye–dilution curves
KW  -  Mutual information
KW  - Image registration
JF  - Computer Methods and Programs in Biomedicine
VL  - 102
IS  - 1
SP  - 1
EP  - 16
DO  - https://doi.org/10.1016/j.cmpb.2010.12.001
UR  - https://www.sciencedirect.com/science/article/pii/S0169260710002919
N2  - Abstract
Fluorescein angiography is an established technique for examining the functional integrity of the retinal microcirculation for early detection of changes due to retinopathy. This paper describes a new method for the registration of large Scanning Laser Ophthalmoscope sequences (SLO), where the patient has been injected with a fluorescent dye. This allows the measurement of parameters such as the arteriovenous passage time. Due to the long time needed to acquire these sequences, there will inevitably be eye movement, which must be corrected prior to the application of quantitative analysis. The algorithm described here combines mutual information-based registration and landmark-based registration. The former will allow the alignment of the darkest frames of the sequence, where the dye has not still arrived to the retina, because of its ability to work with images without a preprocessing or segmentation, while the latter uses relevant features (the vessels) extracted by means of a robust creaseness operator, to get a very fast and accurate registration. The algorithm only detects rigid transformations but proves to be robust against the slight alterations derived from the eye location perspective during acquisition. Results were validated by expert clinicians.
ER  - 
TY  - JOUR
T1  - IDRiD: Diabetic Retinopathy – Segmentation and Grading Challenge
A1  - Porwal, Prasanna
A1  - Pachade, Samiksha
A1  - Kokare, Manesh
A1  - Deshmukh, Girish
A1  - Son, Jaemin
A1  - Bae, Woong
A1  - Liu, Lihong
A1  - Wang, Jianzong
A1  - Liu, Xinhui
A1  - Gao, Liangxin
A1  - Wu, TianBo
A1  - Xiao, Jing
A1  - Wang, Fengyan
A1  - Yin, Baocai
A1  - Wang, Yunzhi
A1  - Danala, Gopichandh
A1  - He, Linsheng
A1  - Choi, Yoon Ho
A1  - Lee, Yeong Chan
A1  - Jung, Sang-Hyuk
A1  - Li, Zhongyu
A1  - Sui, Xiaodan
A1  - Wu, Junyan
A1  - Li, Xiaolong
A1  - Zhou, Ting
A1  - Toth, Janos
A1  - Baran, Agnes
A1  - Kori, Avinash
A1  - Chennamsetty, Sai Saketh
A1  - Safwan, Mohammed
A1  - Alex, Varghese
A1  - Lyu, Xingzheng
A1  - Cheng, Li
A1  - Chu, Qinhao
A1  - Li, Pengcheng
A1  - Ji, Xin
A1  - Zhang, Sanyuan
A1  - Shen, Yaxin
A1  - Dai, Ling
A1  - Saha, Oindrila
A1  - Sathish, Rachana
A1  - Melo, Tânia
A1  - Araújo, Teresa
A1  - Harangi, Balazs
A1  - Sheng, Bin
A1  - Fang, Ruogu
A1  - Sheet, Debdoot
A1  - Hajdu, Andras
A1  - Zheng, Yuanjie
A1  - Mendonça, Ana Maria
A1  - Zhang, Shaoting
A1  - Campilho, Aurélio
A1  - Zheng, Bin
A1  - Shen, Dinggang
A1  - Giancardo, Luca
A1  - Quellec, Gwenolé
A1  - Mériaudeau, Fabrice
Y1  - 2020///
KW  -  Challenge
KW  -  Deep learning
KW  -  Retinal image analysis
KW  - Diabetic Retinopathy
JF  - Medical Image Analysis
VL  - 59
SP  - 101561
EP  - 101561
DO  - https://doi.org/10.1016/j.media.2019.101561
UR  - https://www.sciencedirect.com/science/article/pii/S1361841519301033
N2  - Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.
ER  - 
TY  - JOUR
T1  - Blood flow quantification in dialysis access using digital subtraction angiography: A retrospective study
A1  - Koirala, Nischal
A1  - McLennan, Gordon
Y1  - 2020///
KW  -  Dialysis access
KW  -  Digital subtraction angiography
KW  -  Gamma variate curve fit
KW  -  Thermodilution
KW  - Blood flow quantification
JF  - Computer Methods and Programs in Biomedicine
VL  - 190
SP  - 105379
EP  - 105379
DO  - https://doi.org/10.1016/j.cmpb.2020.105379
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719309277
N2  - Background and objective
Vascular access is the “lifeline” of end-stage renal disease patients, which is surgically constructed to remove blood-waste and return artificially filtered blood into circulation. The arteriovenous shunting causes an abrupt change in blood flow and results in increased fluidic stress, which predisposes to access stenosis and thrombosis. While access flow is crucial to evaluate interventional endpoint, application to measure flow using digital angiogram is not yet available. The goal of this study was to determine the feasibility of flow quantification in dialysis access using a software tool and to guide the design of an imaging protocol.
Methods
173 digital subtraction angiographic (DSA) images were retrospectively analyzed to evaluate access flow in a custom-programming environment. Four bolus transit time algorithms and a distance calculation method were assessed for flow computation. Gamma variate function was applied to remove secondary flow and intensity outliers in the bolus time-intensity curves and evaluated for enhancement in computational accuracy. The percent deviations of flow rates computed from dilution of iodinated radio-contrast material were compared with in situ catheter-based flow measurement.
Results
Among the implemented bolus transit time algorithms, quantification error (mean ± standard error) of cross-correlation algorithm without and with gamma variate curve fitting was 35 ± 1% and 22 ± 1%, respectively. All other algorithms had quantification error >27%. The bias and limits of agreement of the cross-correlation algorithm with gamma variate curve fit was −94 ml/min and [−353, 165] mL/min, respectively.
Conclusions
The cross-correlation algorithm with gamma variate curve fit had the best accuracy and reproducibility for image-based blood flow computation. To further enhance accuracy, images may need to be acquired with a dedicated injection protocol with predetermined parameters such as the duration, rate and mode of bolus injection, and the acquisition frame rate.
ER  - 
TY  - JOUR
T1  - Radial artery pulse waveform analysis based on curve fitting using discrete Fourier series
A1  - Jiang, Zhixing
A1  - Zhang, David
A1  - Lu, Guangming
Y1  - 2019///
KW  -  Curve fitting
KW  -  Discrete fourier series
KW  -  Diseases diagnosis
KW  -  Pressure waveform analysis
KW  - Radial artery
JF  - Computer Methods and Programs in Biomedicine
VL  - 174
SP  - 25
EP  - 31
DO  - https://doi.org/10.1016/j.cmpb.2018.04.019
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717308258
N2  - Background and objectives: Radial artery pulse diagnosis has been playing an important role in traditional Chinese medicine (TCM). For its non-invasion and convenience, the pulse diagnosis has great significance in diseases analysis of modern medicine. The practitioners sense the pulse waveforms in patients’ wrist to make diagnoses based on their non-objective personal experience. With the researches of pulse acquisition platforms and computerized analysis methods, the objective study on pulse diagnosis can help the TCM to keep up with the development of modern medicine. Methods: In this paper, we propose a new method to extract feature from pulse waveform based on discrete Fourier series (DFS). It regards the waveform as one kind of signal that consists of a series of sub-components represented by sine and cosine (SC) signals with different frequencies and amplitudes. After the pulse signals are collected and preprocessed, we fit the average waveform for each sample using discrete Fourier series by least squares. The feature vector is comprised by the coefficients of discrete Fourier series function. Results: Compared with the fitting method using Gaussian mixture function, the fitting errors of proposed method are smaller, which indicate that our method can represent the original signal better. The classification performance of proposed feature is superior to the other features extracted from waveform, liking auto-regression model and Gaussian mixture model. Conclusions: The coefficients of optimized DFS function, who is used to fit the arterial pressure waveforms, can obtain better performance in modeling the waveforms and holds more potential information for distinguishing different psychological states.
ER  - 
TY  - JOUR
T1  - Detection of alcoholism using EEG signals and a CNN-LSTM-ATTN network
A1  - Neeraj
A1  - Singhal, Vatsal
A1  - Mathew, Jimson
A1  - Behera, Ranjan Kumar
Y1  - 2021///
KW  -  Alcoholism
KW  -  Deep learning
KW  -  EEG
KW  -  FFT
KW  - Time series
JF  - Computers in Biology and Medicine
VL  - 138
SP  - 104940
EP  - 104940
DO  - https://doi.org/10.1016/j.compbiomed.2021.104940
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521007344
N2  - Alcoholism is a serious disorder that poses a problem for modern society, but the detection of alcoholism has no widely accepted standard tests or procedures. If alcoholism goes undetected at its early stages, it can create havoc in the patient's life. An electroencephalography (EEG) is a method used to measure the brain's electrical activity and can detect alcoholism. EEG signals are complex and multi-channel and thus can be difficult to interpret manually. Several previous works have tried to classify a subject as alcoholic or control (non-alcoholic) based on EEG signals. Such works have mainly used machine learning or statistical techniques along with handcrafted features such as entropy, correlation dimension, Hurst exponent. With the growth in computational power and data volume worldwide, deep learning models have recently been gaining momentum in various fields. However, only a few studies are available on the application of deep learning models for the classification of alcoholism using EEG signals. This paper proposes a deep learning architecture that uses a combination of fast Fourier transform (FFT), a convolution neural network (CNN), long short-term memory (LSTM), and a recently proposed attention mechanism for extracting Spatio-temporal features from multi-channel EEG signals. The proposed architecture can classify a subject as an alcoholic or control with a high degree of accuracy by analyzing EEG signals of that subject and can be used for automating alcoholism detection. The analytical results using the proposed architecture show a 98.83% accuracy, making it better than most state-of-the-art algorithms.
ER  - 
TY  - JOUR
T1  - Learning and recommending treatments using electronic medical records
A1  - Hoang, Khanh Hung
A1  - Ho, Tu Bao
Y1  - 2019///
KW  -  Electronic medical records
KW  -  Treatment patterns
KW  -  Treatment recommendation
KW  -  Treatment regimen
KW  - Healthcare mining
JF  - Knowledge-Based Systems
VL  - 181
SP  - 104788
EP  - 104788
DO  - https://doi.org/10.1016/j.knosys.2019.05.031
UR  - https://www.sciencedirect.com/science/article/pii/S0950705119302436
N2  - Treatment directly affects patient health status. In recent years, the fast development electronic medical records (EMRs) has provided valuable resources for solving healthcare issues, especially learning and recommending treatments. However, most of the related studies are limited in exploiting various patient information, handling varying-length treatment records, capturing different kinds of treatment patterns and interpreting the recommendation mechanism. This research proposes three methods, one for learning and two for recommending treatments, to overcome the above drawbacks. All methods adopt a mixed-variate restrict Boltzmann machine to represent different kinds of patient records. The treatment learning method captures significant changes in prescription indication to split varying-length records flexibly and organizes sequences of prescription drugs into regimen trees to reveal many more different kinds of treatment patterns. The two treatment recommendation methods illustrate different ideas that can improve the treatment recommendation mechanism by combining the treatments derived from patient groups and neighbor patients. Our experimental evaluation was conducted on three acute disease cohorts extracted from the MIMIC III database. The obtained results show that the proposed methods are able to provide different kinds of treatment patterns and yield competitive efficacy with better interpretability as compared to relevant studies.
ER  - 
TY  - JOUR
T1  - FunVar: A systematic pipeline to unravel the convergence patterns of genetic variants in ASD, a paradigmatic complex disease
A1  - Asif, Muhammad
A1  - Vicente, Astrid M
A1  - Couto, Francisco M
Y1  - 2019///
KW  -  Complex diseases
KW  -  Genetic variants
KW  -  Semantic similarity
KW  - Functional enrichment analysis
JF  - Journal of Biomedical Informatics
VL  - 98
SP  - 103273
EP  - 103273
DO  - https://doi.org/10.1016/j.jbi.2019.103273
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419301923
N2  - In recent years, the technological advances for capturing genetic variation in large populations led to the identification of large numbers of putative or disease-causing variants. However, their mechanistic understanding is lagging far behind and has posed new challenges regarding their relevance for disease phenotypes, particularly for common complex disorders. In this study, we propose a systematic pipeline to infer biological meaning from genetic variants, namely rare Copy Number Variants (CNVs). The pipeline consists of three modules that seek to (1) improve genetic data quality by excluding low confidence CNVs, (2) identify disrupted biological processes, and (3) aggregate similar enriched biological processes terms using semantic similarity. The proposed pipeline was applied to CNVs from individuals diagnosed with Autism Spectrum Disorder (ASD). We found that rare CNVs disrupting brain expressed genes dysregulated a wide range of biological processes, such as nervous system development and protein polyubiquitination. The disrupted biological processes identified in ASD patients were in accordance with previous findings. This coherence with literature indicates the feasibility of the proposed pipeline in interpreting the biological role of genetic variants in complex disease development. The suggested pipeline is easily adjustable at each step and its independence from any specific dataset and software makes it an effective tool in analyzing existing genetic resources. The FunVar pipeline is available at https://github.com/lasigeBioTM/FunVar and includes pre and post processing steps to effectively interpret biological mechanisms of putative disease causing genetic variants.
ER  - 
TY  - JOUR
T1  - An automated knowledge-based textual summarization system for longitudinal, multivariate clinical data
A1  - Goldstein, Ayelet
A1  - Shahar, Yuval
Y1  - 2016///
KW  -  ICU
KW  -  Natural Language Generation
KW  -  Temporal abstraction
KW  -  Textual summarization
KW  - Medical informatics
JF  - Journal of Biomedical Informatics
VL  - 61
SP  - 159
EP  - 175
DO  - https://doi.org/10.1016/j.jbi.2016.03.022
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416300156
N2  - Objectives
Design and implement an intelligent free-text summarization system: The system’s input includes large numbers of longitudinal, multivariate, numeric and symbolic clinical raw data, collected over varying periods of time, and in different complex contexts, and a suitable medical knowledge base. The system then automatically generates a textual summary of the data. We aim to prove the feasibility of implementing such a system, and to demonstrate its potential benefits for clinicians and for enhancement of quality of care.
Methods
We have designed a new, domain-independent, knowledge-based system, the CliniText system, for automated summarization in free text of longitudinal medical records of any duration, in any context. The system is composed of six components: (1) A temporal abstraction module generates all possible abstractions from the patient’s raw data using a temporal-abstraction knowledge base; (2) The abductive reasoning module infers abstractions or events from the data, which were not explicitly included in the database; (3) The pruning module filters out raw or abstract data based on predefined heuristics; (4) The document structuring module organizes the remaining raw or abstract data, according to the desired format; (5) The microplanning module, groups the raw or abstract data and creates referring expressions; (6) The surface realization module, generates the text, and applies the grammar rules of the chosen language. We have performed an initial technical evaluation of the system in the cardiac intensive-care and diabetes domains. We also summarize the results of a more detailed evaluation study that we have performed in the intensive-care domain that assessed the completeness, correctness, and overall quality of the system’s generated text, and its potential benefits to clinical decision making. We assessed these measures for 31 letters originally composed by clinicians, and for the same letters when generated by the CliniText system.
Results
We have successfully implemented all of the components of the CliniText system in software. We have also been able to create a comprehensive temporal-abstraction knowledge base to support its functionality, mostly in the intensive-care domain. The initial technical evaluation of the system in the cardiac intensive-care and diabetes domains has shown great promise, proving the feasibility of constructing and operating such systems. The detailed results of the evaluation in the intensive-care domain are out of scope of the current paper, and we refer the reader to a more detailed source. In all of the letters composed by clinicians, there were at least two important items per letter missed that were included by the CliniText system. The clinicians’ letters got a significantly better grade in three out of four measured quality parameters, as judged by an expert; however, the variance in the quality was much higher in the clinicians’ letters. In addition, three clinicians answered questions based on the discharge letter 40% faster, and answered four out of the five questions equally well or significantly better, when using the CliniText-generated letters, than when using the clinician-composed letters.
Conclusions
Constructing a working system for automated summarization in free text of large numbers of varying periods of multivariate longitudinal clinical data is feasible. So is the construction of a large knowledge base, designed to support such a system, in a complex clinical domain, such as the intensive-care domain. The integration of the quality and functionality results suggests that the optimal discharge letter should exploit both human and machine, possibly by creating a machine-generated draft that will be polished by a human clinician.
ER  - 
TY  - JOUR
T1  - AIOSA: An approach to the automatic identification of obstructive sleep apnea events based on deep learning
A1  - Bernardini, Andrea
A1  - Brunello, Andrea
A1  - Gigli, Gian Luigi
A1  - Montanari, Angelo
A1  - Saccomanno, Nicola
Y1  - 2021///
KW  -  Convolutional neural networks
KW  -  Deep learning
KW  -  Obstructive sleep apnea
KW  -  Time-series
KW  - AI for healthcare
JF  - Artificial Intelligence in Medicine
VL  - 118
SP  - 102133
EP  - 102133
DO  - https://doi.org/10.1016/j.artmed.2021.102133
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721001263
N2  - Obstructive Sleep Apnea Syndrome (OSAS) is the most common sleep-related breathing disorder. It is caused by an increased upper airway resistance during sleep, which determines episodes of partial or complete interruption of airflow. The detection and treatment of OSAS is particularly important in patients who suffered a stroke, because the presence of severe OSAS is associated with higher mortality, worse neurological deficits, worse functional outcome after rehabilitation, and a higher likelihood of uncontrolled hypertension. The gold standard test for diagnosing OSAS is polysomnography (PSG). Unfortunately, performing a PSG in an electrically hostile environment, like a stroke unit, on neurologically impaired patients is a difficult task; moreover, the number of strokes per day vastly outnumbers the availability of polysomnographs and dedicated healthcare professionals. Hence, a simple and automated recognition system to identify OSAS cases among acute stroke patients, relying on routinely recorded vital signs, is highly desirable. The vast majority of the work done so far focuses on data recorded in ideal conditions and highly selected patients, and thus it is hardly exploitable in real-life circumstances, where it would be of actual use. In this paper, we propose a novel convolutional deep learning architecture able to effectively reduce the temporal resolution of raw waveform data, like physiological signals, extracting key features that can be used for further processing. We exploit models based on such an architecture to detect OSAS events in stroke unit recordings obtained from the monitoring of unselected patients. Unlike existing approaches, annotations are performed at one-second granularity, allowing physicians to better interpret the model outcome. Results are considered to be satisfactory by the domain experts. Moreover, through tests run on a widely-used public OSAS dataset, we show that the proposed approach outperforms current state-of-the-art solutions.
ER  - 
TY  - JOUR
T1  - A patient-similarity-based model for diagnostic prediction
A1  - Jia, Zheng
A1  - Zeng, Xian
A1  - Duan, Huilong
A1  - Lu, Xudong
A1  - Li, Haomin
Y1  - 2020///
KW  -  Analogy reasoning
KW  -  Diagnostic prediction
KW  -  Machine learning
KW  - Patient similarity
JF  - International Journal of Medical Informatics
VL  - 135
SP  - 104073
EP  - 104073
DO  - https://doi.org/10.1016/j.ijmedinf.2019.104073
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619310925
N2  - Objective
To simulate the clinical reasoning of doctors, retrieve analogous patients of an index patient automatically and predict diagnoses by the similar/dissimilar patients.
Methods
We proposed a novel patient-similarity-based framework for diagnostic prediction, which is inspired by the structure-mapping theory about analogy reasoning in psychology. Patient similarity is defined as the similarity between two patients’ diagnoses sets rather than a dichotomous (absence/presence of just one disease). The multilabel classification problem is converted to a single-value regression problem by integrating the pairwise patients’ clinical features into a vector and taking the vector as the input and the patient similarity as the output. In contrast to the common k-NN method which only considering the nearest neighbors, we not only utilize similar patients (positive analogy) to generate diagnostic hypotheses, but also utilize dissimilar patients (negative analogy) are used to reject diagnostic hypotheses.
Results
The patient-similarity-based models perform better than the one-vs-all baseline and traditional k-NN methods. The f-1 score of positive-analogy-based prediction is 0.698, significantly higher than the scores of baselines ranging from 0.368 to 0.661. It increases to 0.703 when the negative analogy method is applied to modify the prediction results of positive analogy. The performance of this method is highly promising for larger datasets.
Conclusion
The patient-similarity-based model provides diagnostic decision support that is more accurate, generalizable, and interpretable than those of previous methods and is based on heterogeneous and incomplete data. The model also serves as a new application for the use of clinical big data through artificial intelligence technology.
ER  - 
TY  - JOUR
T1  - Pairing conceptual modeling with machine learning
A1  - Maass, Wolfgang
A1  - Storey, Veda C
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Database management
KW  -  Framework for incorporating conceptual modeling into data science projects
KW  -  Machine learning
KW  -  Methodologies and tools
KW  -  Models
KW  - Conceptual modeling
JF  - Data & Knowledge Engineering
VL  - 134
SP  - 101909
EP  - 101909
DO  - https://doi.org/10.1016/j.datak.2021.101909
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X21000367
N2  - Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.
ER  - 
TY  - JOUR
T1  - Multi independent latent component extension of naive Bayes classifier
A1  - Alizadeh, Sasan H
A1  - Hediehloo, Alireza
A1  - Harzevili, Nima Shiri
Y1  - 2021///
KW  -  Conditional mutual dependency
KW  -  Conditional mutual information
KW  -  Latent variable
KW  - Naive Bayes classifier
JF  - Knowledge-Based Systems
VL  - 213
SP  - 106646
EP  - 106646
DO  - https://doi.org/10.1016/j.knosys.2020.106646
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120307759
N2  - Naive Bayes (NB) classifier ease of use along with its remarkable performance has led many researchers to extend the scope of its applications to real-world domains by relaxing the conditional independence assumption of features given the information about the class variable. However, fulfilling this objective, most of the generalizations, cut their own way through compromising the model’s simplicity, make more complex classifiers with a substantial deviation from the original one. Multi Independent Latent Component Naive Bayes Classifier (MILC-NB) leverages a set of latent variables to preserve the overall structure of naive Bayes classifier while rectifying its major restriction. Each latent variable is responsible for keeping a subset of conditionally dependent features d-connected within a component, and the set of features is divided into non-overlapping partitions across different components accordingly. We prove that components are conditionally independent given the information about the class variable which allows us to devise novel mathematical methods with a substantial reduction in the complexities of classification and learning. Experiments on 34 datasets obtained from the OpenML repository indicate that MILC-NB outperforms state-of-the-art classifiers in terms of area under the ROC curve (AUC) and classification accuracy (ACC).
ER  - 
TY  - JOUR
T1  - Networks of spiking neurons in modeling and problem solving
A1  - Cios, Krzysztof J
A1  - Swiercz, Waldemar
A1  - Jackson, William
Y1  - 2004///
KW  -  Brain modeling
KW  -  Clustering
KW  -  Diabetic retinopathy
KW  -  Integrate-and-fire neuron model
KW  - Networks of spiking neurons
JF  - Neurocomputing
VL  - 61
SP  - 99
EP  - 119
DO  - https://doi.org/10.1016/j.neucom.2004.03.007
UR  - https://www.sciencedirect.com/science/article/pii/S0925231204002309
N1  - Hybrid Neurocomputing: Selected Papers from the 2nd International Conference on Hybrid Intelligent Systems
N2  - In this paper, we describe the networks of spiking neurons and show their applications for modeling and problem solving. We have used integrate-and-fire neuron model that closely simulates a biological neuron's behavior. First, we model the somatosensory system with Hebbian-type spike-time-dependent plasticity and show the ability of the network to self-organize. Second, we apply a network of spiking neurons for identification, via clustering, of diabetic retinopathy images using temporal correlation learning rule. Results show that the network distinguishes the diabetic objects of interest from the image background.
ER  - 
TY  - JOUR
T1  - Learning multimorbidity patterns from electronic health records using Non-negative Matrix Factorisation
A1  - Hassaine, Abdelaali
A1  - Canoy, Dexter
A1  - Solares, Jose Roberto Ayala
A1  - Zhu, Yajie
A1  - Rao, Shishir
A1  - Li, Yikuan
A1  - Zottoli, Mariagrazia
A1  - Rahimi, Kazem
A1  - Salimi-Khorshidi, Gholamreza
Y1  - 2020///
KW  -  Disease trajectories
KW  -  Electronic health records
KW  -  Multimorbidity
KW  -  Temporal phenotyping
KW  - Non-negative Matrix Factorisation
JF  - Journal of Biomedical Informatics
VL  - 112
SP  - 103606
EP  - 103606
DO  - https://doi.org/10.1016/j.jbi.2020.103606
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420302343
N2  - Multimorbidity, or the presence of several medical conditions in the same individual, has been increasing in the population — both in absolute and relative terms. Nevertheless, multimorbidity remains poorly understood, and the evidence from existing research to describe its burden, determinants and consequences has been limited. Previous studies attempting to understand multimorbidity patterns are often cross-sectional and do not explicitly account for multimorbidity patterns’ evolution over time; some of them are based on small datasets and/or use arbitrary and narrow age ranges; and those that employed advanced models, usually lack appropriate benchmarking and validations. In this study, we (1) introduce a novel approach for using Non-negative Matrix Factorisation (NMF) for temporal phenotyping (i.e., simultaneously mining disease clusters and their trajectories); (2) provide quantitative metrics for the evaluation of these clusters and trajectories; and (3) demonstrate how the temporal characteristics of the disease clusters that result from our model can help mine multimorbidity networks and generate new hypotheses for the emergence of various multimorbidity patterns over time. We trained and evaluated our models on one of the world’s largest electronic health records (EHR) datasets, containing more than 7 million patients, from which over 2 million where relevant to, and hence included in this study.
ER  - 
TY  - JOUR
T1  - An ensemble method based multilayer dynamic system to predict cardiovascular disease using machine learning approach
A1  - Uddin, Mohammed Nasir
A1  - Halder, Rajib Kumar
Y1  - 2021///
KW  -  Cardiovascular disease
KW  -  Classification
KW  -  Ensemble model
KW  -  Feature selection
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 24
SP  - 100584
EP  - 100584
DO  - https://doi.org/10.1016/j.imu.2021.100584
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821000745
N2  - Cardiovascular disease is defined as a set of conditions related to the disorder of the heart and blood vessels. Predicting and diagnosing cardiovascular disease is significant to ensure the appropriate treatment of this disease. Machine learning approaches are generally utilized to automatically detect the hidden patterns in vast amounts of data without human intervention. In the early stage of cardiovascular disease, a machine learning model can aid physicians in making the right decision about the medication. This research aims to develop an intelligent agent to predict cardiovascular disease to investigate what steps should be taken before any untoward incident occurs. This paper proposes an ensemble method-based multilayer dynamic system (MLDS) that can improve its current knowledge in every layer. The proposed model applies Correlation Attribute Evaluator (CAE), Gain Ratio Attribute Evaluator (GRAE), Information Gain Attribute Evaluator (IGAE), Lasso, and Extra Trees classifier (ETC) for feature selection. Finally, Random Forest (RF), Naïve Bayes (NB), and Gradient Boosting (GB) classifiers combinedly construct the ensemble method for classification in the model. The K Nearest Neighbor (KNN) algorithm is applied to find the test data's neighborhood data points while the base classifiers mentioned are failed to classify correctly in any layer. To test the proposed model's efficiency, we have used a realistic dataset (70,000 instances) collected from Kaggle. The proposed model has achieved 88.84%, 89.44%, 91.56%, 92.72%, and 94.16% accuracy based on the train and test data's different splitting ratios (50:50, 60:40, 70:30, 80:20, and 87.5:12.5). Our proposed model has achieved a 0.94 AUC value. AUC = 0.94 means it has a 94% probability of correctly classifying positive and negative classes, Whereas the splitting ratio is 87.5:12.5. The Cleveland, Hungarian, and Cleveland-Hungary-Switzerland-Long Beach datasets have also been applied to train the model, and the model achieved 98.88%, 99.53%, 99.98%, 98.36%, 96.66%, 97.77%, 99.56, and 94.37% accuracy depending on the different splitting ratios of these datasets. The proposed model has been compared to five other models, indicating that the proposed model can effectively predict cardiovascular disease.
ER  - 
TY  - JOUR
T1  - Modeling the risk of structural fire incidents using a self-organizing map
A1  - Asgary, Ali
A1  - Sadeghi Naini, Ali
A1  - Levy, Jason
Y1  - 2012///
KW  -  Computer-aided dispatching
KW  -  Fire response
KW  -  Fire risk
KW  -  Self-organizing map (SOM)
KW  -  Structural fire
KW  - Neural network
JF  - Fire Safety Journal
VL  - 49
SP  - 1
EP  - 9
DO  - https://doi.org/10.1016/j.firesaf.2011.12.007
UR  - https://www.sciencedirect.com/science/article/pii/S0379711211001652
N2  - A Self-Organizing Map (SOM) is used to classify and assess the risk levels of structural fire incidents. Such an assessment can be used not only for improving fire safety and protection of existing and future structures, but also for enhancing emergency responses to future fire incidents. This can reduce the damages and injuries resulting from fires. The map has a 2D hexagonal lattice structure and was applied on a sample of structural fire incident records of Toronto which were reported between 2000 and 2006. Assessment results suggest that the SOM approach is able to successfully classify incidents with different properties into their predefined risk level classes. In summary, the proposed approach shows superior performance for predicting risk, although the quality and quantity of training samples is critical to the success of the predictions
ER  - 
TY  - JOUR
T1  - Towards a Novel and Generic Approach for OWL Ontology Weighting
A1  - Abioui, Hasna
A1  - Idarrou, Ali
A1  - Bouzit, Ali
A1  - Mammass, Driss
Y1  - 2018///
KW  -  Ontology Weighting
KW  -  Semantic Relationships
KW  -  Taxonomic Structure
KW  - Ontologies
JF  - Procedia Computer Science
VL  - 127
SP  - 426
EP  - 435
DO  - https://doi.org/10.1016/j.procs.2018.01.140
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918301522
N1  - PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017
N2  - Semantic search is qualified – by web-related enterprises as well as, academic research – as a key technology, ensuring important improvements in terms of shared data understanding, while it leads to refined and targeted interpretations. Accordingly, ontologies are the focal asset for a well-functioning semantic search approach, since their ability to share, represent and reuse explicit and semantic domain specification. Nowadays, a multitude of ontologies containing up to hundreds of thousands of concepts are proposed. Thus, our challenge as researchers exceeds conceptualizing or creating ontologies to being able to choose the fitting and suitable one, taking into account specific criteria. This paper comes within the same context as it presents a novel approach for weighting OWL ontologies, in order to choose the most appropriate one from a set of proposed ontologies. Our approach takes into account not only the taxonomic structure, but also the semantic aspect of the ontology. Furthermore, semantic relationships and specific concepts are the favored since they reflect the semantic richness of the ontology.
ER  - 
TY  - JOUR
T1  - Prediction of the number of COVID-19 confirmed cases based on K-means-LSTM
A1  - Vadyala, Shashank Reddy
A1  - Betgeri, Sai Nethra
A1  - Sherer, Eric A
A1  - Amritphale, Amod
Y1  - 2021///
KW  -  COVID-19
KW  -  Day level forecasting
KW  -  Deep learning
KW  -  Neural network
KW  -  SEIR Model
KW  - Coronavirus
JF  - Array
VL  - 11
SP  - 100085
EP  - 100085
DO  - https://doi.org/10.1016/j.array.2021.100085
UR  - https://www.sciencedirect.com/science/article/pii/S2590005621000333
N2  - COVID-19 is a pandemic disease that began to rapidly spread in the US, with the first case detected on January 19, 2020, in Washington State. March 9, 2020, and then quickly increased with total cases of 25,739 as of April 20, 2020. Although most people with coronavirus 81%, according to the U.S. Centers for Disease Control and Prevention (CDC), will have little to mild symptoms, others may rely on a ventilator to breathe or not at all. SEIR models have broad applicability in predicting the outcome of the population with a variety of diseases. However, many researchers use these models without validating the necessary hypotheses. Far too many researchers often “overfit” the data by using too many predictor variables and small sample sizes to create models. Models thus developed are unlikely to stand validity check on a separate group of population and regions. The researcher remains unaware that overfitting has occurred, without attempting such validation. In the paper, we present a combination algorithm that combines similar days features selection based on the region using Xgboost, K-Means, and long short-term memory (LSTM) neural networks to construct a prediction model (i.e., K-Means-LSTM) for short-term COVID-19 cases forecasting in Louisana state USA. The weighted k-means algorithm based on extreme gradient boosting is used to evaluate the similarity between the forecasts and past days. The results show that the method with K-Means-LSTM has a higher accuracy with an RMSE of 601.20 whereas the SEIR model with an RMSE of 3615.83.
ER  - 
TY  - JOUR
T1  - Time-series deep survival prediction for hemodialysis patients using an attention-based Bi-GRU network
A1  - Yang, Ziyue
A1  - Tian, Yu
A1  - Zhou, Tianshu
A1  - Zhu, Yilin
A1  - Zhang, Ping
A1  - Chen, Jianghua
A1  - Li, Jingsong
Y1  - 2021///
KW  -  Deep learning
KW  -  Hemodialysis (HD)
KW  -  Loss function
KW  -  Time-series
KW  - Survival analysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 212
SP  - 106458
EP  - 106458
DO  - https://doi.org/10.1016/j.cmpb.2021.106458
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721005320
N2  - Background and objective
The number of end-stage renal disease (ESRD) patients treated with hemodialysis (HD) has significantly increased, but the prognosis remains poor. Time-series features have been included in only a few studies to predict HD patient survival, and how to utilize such features effectively remains unclear. This article aims to develop a more accurate, interpretable, and clinically practical personalized survival prediction model for HD patients.
Methods
This study proposed and evaluated an attention-based Bi-GRU network using time-series features for survival prediction. A distance-based loss function was proposed to improve performance. We used data from 1232 ESRD patients who received regular hemodialysis treatment for ≥ 3 months from 2007 to 2016 at the First Affiliated Hospital of Zhejiang University. The proposed model was compared with representative sequence modeling deep learning architectures and existing survival analysis methods in terms of the C-index and IBS value. Post hoc tests were used to test statistical significance. The attention map was used to assess feature importance over time. The impact of time-series changes on survival was investigated after controlling initial values (using BMI as an example).
Results
The proposed method outperformed other sequence modeling architectures and the state-of-the-art survival analysis approaches in terms of the C-index and the integrated Brier score (IBS) value. Our method achieved a C-index of 0.7680 (95% confidence intervals [CI]: 0.7645, 0.7716) and an IBS of 0.1302 (95% confidence intervals [CI]: 0.1292, 0.1313), showing an improvement of up to 5.4% in terms of the C-index and a decrease of 3.2% in terms of the IBS value. The addition of the distance-based loss function improved the performance. The predicted risk and actual risk levels closely agreed. This study also found that even after controlling the initial body mass index (BMI) values, different 3-month BMI trends could produce different survival outcomes.
Conclusions
This study proposed a more effective and interpretable method to use time-series information in survival analysis. The proposed method may help promote personalized medicine and improve patient prognosis.
ER  - 
TY  - JOUR
T1  - Atrial fibrillation detection with and without atrial activity analysis using lead-I mobile ECG technology
A1  - Tuboly, Gergely
A1  - Kozmann, György
A1  - Kiss, Orsolya
A1  - Merkely, Béla
Y1  - 2021///
KW  -  Atrial fibrillation detection
KW  -  Heart rhythm
KW  -  Single-channel mobile electrocardiography
KW  - Atrial activity
JF  - Biomedical Signal Processing and Control
VL  - 66
SP  - 102462
EP  - 102462
DO  - https://doi.org/10.1016/j.bspc.2021.102462
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421000598
N2  - Objective
This paper presents the performance of an atrial fibrillation (AF) detection algorithm in a lead-I mobile ECG setting. Our aim is to demonstrate that false positive cases occurring due to non-AF arrhythmias can be significantly reduced by taking atrial activity into account in addition to heart rhythm.
Methods
AF detection was carried out in two ways: only by heart rhythm assessment based on Poincaré plot of RR intervals (A1), and by also performing P wave analysis on the average majority cycle (A2).
Results
The algorithm was tested on the PhysioNet MIT-BIH Atrial Fibrillation Database, Long-Term AF Database, MIT-BIH Arrhythmia Database, and MIT-BIH Normal Sinus Rhythm Database. A1 produced an average sensitivity (Se) of 97.64 % and specificity (Sp) of 93.06 %. A2 resulted Se = 96.52 % and Sp = 99.12 %. Additionally, clinical data acquisition was performed by the Sanatmetal WIWE mobile ECG system. On the latter records, Se = 100 % and Sp = 95.31 % were obtained with A1, while A2 resulted Se = 100 % and Sp = 100 %.
Conclusion
Atrial activity analysis significantly increased Sp in records with frequent premature beats (by almost 12 %) and marked sinus arrhythmia (by 7.5 %). Based on our results and data from the literature, we suggest that AF detection methods should be tested more carefully on non-AF arrhythmia cases. Applied in WIWE, the A2 version of our algorithm outperforms the AF detectors of current single-channel mobile ECG systems.
Significance
The results contribute to the significant reduction of false positive AF detections in lead-I mobile ECG technology.
ER  - 
TY  - JOUR
T1  - A novel recognition system for human activity based on wavelet packet and support vector machine optimized by improved adaptive genetic algorithm
A1  - Jiang, Jin
A1  - Jiang, Ting
A1  - Zhai, Shijun
Y1  - 2014///
KW  -  Improved adaptive genetic algorithm (IAGA)
KW  -  Support vector machine (SVM)
KW  -  Wavelet packet transform (WPT)
KW  - Human activity recognition
JF  - Physical Communication
VL  - 13
SP  - 211
EP  - 220
DO  - https://doi.org/10.1016/j.phycom.2014.04.006
UR  - https://www.sciencedirect.com/science/article/pii/S1874490714000421
N2  - A new human activities recognition system based on support vector machine (SVM) optimized by improved adaptive genetic algorithm (IAGA) and wavelet packet is proposed. Wavelet packet transform (WPT) is applied to extract the signatures from various actions. SVM is a powerful tool for solving the classification problem with small sampling, nonlinearity and high dimension. Genetic algorithm (GA) is employed to determine the two optimal parameters for SVM with highest predictive accuracy and generalization ability. Moreover, the IAGA adopts the dynamic cross rate and mutation rate according to the group fitness, thus effectively avoiding the disadvantages of the standard GA, such as premature convergence and low robustness. The average recognition accuracy rate goes up to 97.6%. In addition, the result of suggested method is also compared with other feature extraction methods which further demonstrate the superiority of WPT and generalization ability of IAGA. The aforementioned results clearly demonstrate that the proposed method is superior to the traditional method in activity recognition.
ER  - 
TY  - JOUR
T1  - Ensemble 1-D CNN diagnosis model for VRF system refrigerant charge faults under heating condition
A1  - Cheng, Hengda
A1  - Chen, Huanxin
A1  - Li, Zhengfei
A1  - Cheng, Xiangdong
Y1  - 2020///
KW  -  1-D CNN
KW  -  Ensemble learning
KW  -  Fault diagnosis
KW  -  Refrigerant charge fault
KW  - VRF system
JF  - Energy and Buildings
VL  - 224
SP  - 110256
EP  - 110256
DO  - https://doi.org/10.1016/j.enbuild.2020.110256
UR  - https://www.sciencedirect.com/science/article/pii/S0378778819338861
N2  - Variable refrigerant flow (VRF) systems are widely-adopted air conditioning systems. When system faults occur in VRF systems, the efficiency of VRF system will drop drastically. This paper presents a single 1-D CNN model and an ensemble model with parallel 1-D CNNs for diagnosing VRF system refrigerant charge faults under heating condition. From the cleaned experiment data of a commercial VRF system, 15 features are selected as the input for the proposed model with ReliefF algorithm. After training, the diagnosis accuracy of the single 1-D CNN model and ensemble 1-D CNN models is evaluated and compared with that of BPNN model and DT model. The result shows that both single 1-D CNN and ensemble 1-D CNN model can diagnose VRF system refrigerant charge fault effectively. The fault detection is also achieved in proposed models. The average diagnosis accuracy of 9-level refrigerant charge faults of the ensemble 1-D CNN model is up to 97.4%, surpassing that of BPNN model, SVM model, DT mode and DBN model. 1-D CNN based model is utilized for VRF system fault diagnosis for the first time, which lays a foundation for the expansion of the related researches.
ER  - 
TY  - JOUR
T1  - SCAI-SVSC: Smart clothing for effective interaction with a sustainable vital sign collection
A1  - Hu, Long
A1  - Yang, Jun
A1  - Chen, Min
A1  - Qian, Yongfeng
A1  - Rodrigues, Joel J P C
Y1  - 2018///
KW  -  Cloud computing
KW  -  Emotion care
KW  -  Healthcare system
KW  -  Wearable computing
KW  - Smart clothing
JF  - Future Generation Computer Systems
VL  - 86
SP  - 329
EP  - 338
DO  - https://doi.org/10.1016/j.future.2018.03.042
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X17328285
N2  - In this paper, we propose a new wearable device named smart clothing. Compared with traditional equipments smart clothing has lots of advantages in many aspects. This paper introduces the construction of smart clothing system, discusses its usage scenario and the data transmission mode with the terminal, cloud platform, and builds an efficient healthcare system. This paper also discusses the use of smart clothing for measurement of human body signs such as blood oxygen, body temperature, heartbeat, and ensures users being in good health by real-time monitoring. Finally, this paper focuses on the collection of ECG signals and the experiment of analyzing user’s feelings.
ER  - 
TY  - JOUR
T1  - Intradialytic hypotension related episodes identification based on the most effective features of photoplethysmography signal
A1  - Nafisi, Vahid Reza
A1  - Shahabi, Mina
Y1  - 2018///
KW  -  Hypovolemia
KW  -  Intradialytic hypotension (IDH)
KW  -  PPG signal morphology
KW  - Hemodialysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 157
SP  - 1
EP  - 9
DO  - https://doi.org/10.1016/j.cmpb.2018.01.012
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717313007
N2  - Background and objective
One of the most adverse conditions facing the hemodialysis patient is repetitive hypotension during their dialysis session. Different factors can be used to monitor patient conditions and prevent Intradialytic Hypotension (IDH) during hemodialysis. These factors include blood pressure, blood volume, and electrical Impedance factors. In this paper, pre-IDH and IDH episodes were recognized and classified by using the features of the finger photoplethysmography (PPG) signal. In other words, the goal of present study is to use PPG signal features to predict the risk of acute hypotension.
Methods
Since the PPG signal is non-stationary in nature, the main signal was divided in five-minute intervals with no overlap and then each interval was analyzed separately and fifteen PPG signal features in time and seven features in the frequency domain were extracted. Then different feature selection and classification methods were applied on the normalized feature matrix to select the best features and detect IDH and pre-IDH episodes in dialysis sessions.
Results
The best results were achieved from a genetic algorithm and AdaBoost. The obtained results on our developed database indicated that the mean and maximum accuracy of the proposed algorithm were 94.5 ± 1.0 and 96.6 respectively.
Conclusion
Some PPG signal features can be useful during hemodialysis session for hypotension management.
ER  - 
TY  - JOUR
T1  - Automatic detection of optic disc in color fundus retinal images using circle operator
A1  - Reza, M Nahid
Y1  - 2018///
KW  -  Circle operator
KW  -  Optic disk detection
KW  -  Retinal image
KW  - Biomedical imaging
JF  - Biomedical Signal Processing and Control
VL  - 45
SP  - 274
EP  - 283
DO  - https://doi.org/10.1016/j.bspc.2018.05.027
UR  - https://www.sciencedirect.com/science/article/pii/S1746809418301356
N2  - In the field of computer aided eye disease diagnosis, automatic optic disk detection is required. In this paper a method is proposed to detect optic disk automatically in color retinal fundus image without using background mask and blood vessels. Based on the properties of optic disk, an idea of circle operator is presented here. This method has been applied on six public databases and the promising results are obtained. The experimental results indicate that this proposed method of automatic optic disk detection has good accuracy and is also time efficient.
ER  - 
TY  - JOUR
T1  - Neurological Status Classification Using Convolutional Neural Network
A1  - Jaloli, Mehrad
A1  - Choudhary, Divya
A1  - Cescon, Marzia
Y1  - 2020///
KW  -  Cognitive control
KW  -  Deep neural network
KW  -  Neurological status assessment
KW  -  Physiological signal processing
KW  -  Potential impact of automation
KW  -  open problems
KW  - Assistive devices
JF  - IFAC-PapersOnLine
VL  - 53
IS  - 5
SP  - 409
EP  - 414
DO  - https://doi.org/10.1016/j.ifacol.2021.04.193
UR  - https://www.sciencedirect.com/science/article/pii/S2405896321003487
N1  - 3rd IFAC Workshop on Cyber-Physical &amp; Human Systems CPHS 2020
N2  - In this study we show that a Convolutional Neural Network (CNN) model is able to accurately discriminate between 4 different phases of neurological status in a non-Electroencephalogram (EEG) dataset recorded in an experiment in which subjects are exposed to physical, cognitive and emotional stress. We demonstrate that the proposed model is able to obtain 99.99% Area Under the Curve (AUC) of Receiver Operation characteristic (ROC) and 99.82% classification accuracy on the test dataset. Furthermore, for comparison, we show that our models outperforms traditional classification methods such as SVM, and RF. Finally, we show the advantage of CNN models, in comparison to other methods, in robustness to noise by 97.46% accuracy on a noisy dataset.
ER  - 
TY  - JOUR
T1  - Vessel segmentation and microaneurysm detection using discriminative dictionary learning and sparse representation
A1  - Javidi, Malihe
A1  - Pourreza, Hamid-Reza
A1  - Harati, Ahad
Y1  - 2017///
KW  -  Discriminative dictionary learning
KW  -  Microaneurysm detection
KW  -  Sparse representation
KW  - Blood vessel segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 139
SP  - 93
EP  - 108
DO  - https://doi.org/10.1016/j.cmpb.2016.10.015
UR  - https://www.sciencedirect.com/science/article/pii/S0169260716305454
N2  - Diabetic retinopathy (DR) is a major cause of visual impairment, and the analysis of retinal image can assist patients to take action earlier when it is more likely to be effective. The accurate segmentation of blood vessels in the retinal image can diagnose DR directly. In this paper, a novel scheme for blood vessel segmentation based on discriminative dictionary learning (DDL) and sparse representation has been proposed. The proposed system yields a strong representation which contains the semantic concept of the image. To extract blood vessel, two separate dictionaries, for vessel and non-vessel, capable of providing reconstructive and discriminative information of the retinal image are learned. In the test step, an unseen retinal image is divided into overlapping patches and classified to vessel and non-vessel patches. Then, a voting scheme is applied to generate the binary vessel map. The proposed vessel segmentation method can achieve the accuracy of 95% and a sensitivity of 75% in the same range of specificity 97% on two public datasets. The results show that the proposed method can achieve comparable results to existing methods and decrease false positive vessels in abnormal retinal images with pathological regions. Microaneurysm (MA) is the earliest sign of DR that appears as a small red dot on the surface of the retina. Despite several attempts to develop automated MA detection systems, it is still a challenging problem. In this paper, a method for MA detection, which is similar to our vessel segmentation approach, is proposed. In our method, a candidate detection algorithm based on the Morlet wavelet is applied to identify all possible MA candidates. In the next step, two discriminative dictionaries with the ability to distinguish MA from non-MA object are learned. These dictionaries are then used to classify the detected candidate objects. The evaluations indicate that the proposed MA detection method achieves higher average sensitivity about 2–15%, compared to existing methods.
ER  - 
TY  - JOUR
T1  - Text mining with sentiment analysis on seafarers’ medical documents
A1  - Chintalapudi, Nalini
A1  - Battineni, Gopi
A1  - Canio, Marzio Di
A1  - Sagaro, Getu Gamo
A1  - Amenta, Francesco
Y1  - 2021///
KW  -  Machine learning
KW  -  Seafarers
KW  -  Sentiment analysis
KW  -  Word clouds
KW  - Text mining
JF  - International Journal of Information Management Data Insights
VL  - 1
IS  - 1
SP  - 100005
EP  - 100005
DO  - https://doi.org/10.1016/j.jjimei.2020.100005
UR  - https://www.sciencedirect.com/science/article/pii/S2667096820300057
N2  - Digital health systems contain large amounts of patient records, doctor notes, and prescriptions in text format. This information summarized over the electronic clinical information will lead to an improved quality of healthcare, the possibility of fewer medical errors, and low costs. Besides, seafarers are more vulnerable to have accidents, and prone to health hazards because of work culture, climatic changes, and personal habits. Therefore, text mining implementation in seafarers’ medical documents can generate better knowledge of medical issues that often happened onboard. Medical records are collected from digital health systems of Centro Internazionale Radio Medico (C.I.R.M.) which is an Italian Telemedical Maritime Assistance System (TMAS). Three years (2018–2020) patient data have been used for analysis. Adoption of both lexicon and Naïve Bayes’ algorithms was done to perform sentimental analysis and experiments were conducted over R statistical tool. Visualization of symptomatic information was done through word clouds and 96% of the correlation between medical problems and diagnosis outcome has been achieved. We validate the sentiment analysis with more than 80% accuracy and precision.
ER  - 
TY  - JOUR
T1  - Pulse of the pandemic: Iterative topic filtering for clinical information extraction from social media
A1  - Wu, Julia
A1  - Sivaraman, Venkatesh
A1  - Kumar, Dheekshita
A1  - Banda, Juan M
A1  - Sontag, David
Y1  - 2021///
KW  -  Clinical concept extraction
KW  -  Information retrieval
KW  -  Public health surveillance
KW  -  Social media
KW  -  Topic modeling
KW  - Data mining
JF  - Journal of Biomedical Informatics
VL  - 120
SP  - 103844
EP  - 103844
DO  - https://doi.org/10.1016/j.jbi.2021.103844
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001738
N2  - The rapid evolution of the COVID-19 pandemic has underscored the need to quickly disseminate the latest clinical knowledge during a public-health emergency. One surprisingly effective platform for healthcare professionals (HCPs) to share knowledge and experiences from the front lines has been social media (for example, the “#medtwitter” community on Twitter). However, identifying clinically-relevant content in social media without manual labeling is a challenge because of the sheer volume of irrelevant data. We present an unsupervised, iterative approach to mine clinically relevant information from social media data, which begins by heuristically filtering for HCP-authored texts and incorporates topic modeling and concept extraction with MetaMap. This approach identifies granular topics and tweets with high clinical relevance from a set of about 52 million COVID-19-related tweets from January to mid-June 2020. We also show that because the technique does not require manual labeling, it can be used to identify emerging topics on a week-to-week basis. Our method can aid in future public-health emergencies by facilitating knowledge transfer among healthcare workers in a rapidly-changing information environment, and by providing an efficient and unsupervised way of highlighting potential areas for clinical research.
ER  - 
TY  - JOUR
T1  - Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions
A1  - Kalantari, Ali
A1  - Kamsin, Amirrudin
A1  - Shamshirband, Shahaboddin
A1  - Gani, Abdullah
A1  - Alinejad-Rokny, Hamid
A1  - Chronopoulos, Anthony T
Y1  - 2018///
KW  -  Big data
KW  -  Detection
KW  -  Ensemble algorithm
KW  -  Medical application
KW  - Computational intelligence
JF  - Neurocomputing
VL  - 276
SP  - 2
EP  - 22
DO  - https://doi.org/10.1016/j.neucom.2017.01.126
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217315436
N1  - Machine Learning and Data Mining Techniques for Medical Complex Data Analysis
N2  - The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.
ER  - 
TY  - JOUR
T1  - Skeleton avatar technology as a way to measure physical activity in healthy older adults
A1  - Lincke, Alisa
A1  - Fagerström, Cecilia
A1  - Ekstedt, Mirjam
A1  - Löwe, Welf
A1  - Backåberg, Sofia
Y1  - 2021///
KW  -  Balance
KW  -  Machine learning
KW  -  Older adults
KW  -  Physical activity
KW  -  Self-reported assessments
KW  - Accelerometer
JF  - Informatics in Medicine Unlocked
VL  - 24
SP  - 100609
EP  - 100609
DO  - https://doi.org/10.1016/j.imu.2021.100609
UR  - https://www.sciencedirect.com/science/article/pii/S235291482100099X
N2  - Background
Nowadays, self-reported assessments (SA) and accelerometer-based assessments (AC) are commonly used methods to measure daily life physical activity (PA) in older adults. SA is simple, cost-effective, and can be used in large epidemiological studies, but its reliability and validity have been questioned. Accelerometer measurement has proven valid to provide accurate and reliable measurement of everyday life physical activities regarding frequency, duration, and intensity in older populations, but is expensive and requires a long-time measurement. Here is, furthermore, a lack of well-defined and reliable accelerometer cut-off points to measure PA among older adults. Therefore, there is a need to develop a simple and reliable method to complement/replace self-assessment methods of daily life physical activity and facilitate the future development of cut-off points to measure daily life physical activities among older adults. In this study, we explore how skeleton avatar technology (SAT) can be used to measure PA among older adults.
Objectives
1. To explore the association between accelerometer data and self-reported assessment data of daily life physical activities in older adults, and 2. To explore how the SAT of a standardized functional (balance) test can be used to measure daily life physical activity among older adults.
Method
The correlation analysis was used to explore the association between response variables, and deep neural networks were used to predict the response variables (AC and SA outcomes).
Results
The results indicate that there is a moderate (r = 0.31) significant (p = 0.029) correlation between AC of PA and SA of PA. The functional balance test assessed with SAT was able to predict AC with 3.89% Mean Absolute Error (MAE), and SA with 11.07% MAE.
Conclusion
Overall, these results indicate that one functional balance test measured with SAT can be used to predict PA outcomes measured with accelerometer devices. SAT can predict PA outcomes better than SA outcomes within the same population. More research is needed to explore the ability of SAT predicting PA among older adults with various functional abilities, and how SAT can be developed using 2D recordings, such as mobile phone recordings, to predict PA efficiently.
ER  - 
TY  - JOUR
T1  - A new robust method for blood vessel segmentation in retinal fundus images based on weighted line detector and hidden Markov model
A1  - Zhou, Chao
A1  - Zhang, Xiaogang
A1  - Chen, Hua
Y1  - 2020///
KW  -  Difficult region
KW  -  Hidden Markov model
KW  -  Line detector
KW  -  Thin vessel
KW  -  Vessel segmentation
KW  - Retinal image
JF  - Computer Methods and Programs in Biomedicine
VL  - 187
SP  - 105231
EP  - 105231
DO  - https://doi.org/10.1016/j.cmpb.2019.105231
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719306169
N2  - Background and objective
Automatic vessel segmentation is a crucial preliminary processing step to facilitate ophthalmologist diagnosis in some diseases. But, due to the complexity of retinal fundus image, there are some problems on accurate segmentation of retinal vessel. In this paper, a new method for retinal vessel segmentation is proposed to handle two main problems: thin vessel missing and false detection in difficult regions.
Methods
First, an improved line detector is proposed and used to fast extract the major structures of vessels. Then, Hidden Markov model (HMM) is applied to effectively detect vessel centerlines that include thin vessels. Finally, a denoising approach is presented to remove noises and two types of vessels are unified to obtain the complete segmentation results.
Results
Our method is tested on two public databases (DRIVE and STARE databases), and five measures namely accuracy (Acc), sensitivity (Se), specificity (Sp), Dice coefficient (Dc), structural similarity index (SSIM) and feature similarity index (FSIM) are used to evaluate our segmentation performance. The respective values of the performance measures are 0.9475, 0.7262, 0.9803, 0.7781, 0.9992 and 0.9793 for DRIVE dataset and 0.9535, 0.7865, 0.9730, 0.7764, 0.9987 and 0.9742 for STARE dataset.
Conclusions
The experiment results show that our method outperforms most published state-of-the-art methods and is better the result of a human observer. Moreover, in term of specificity, our proposed algorithm can obtain the best score among the unsupervised methods. Meanwhile, there are excellent structure and feature similarities between our result and the ground truth according to achieved SSIM and FSIM. Visual inspection on the segmentation results shows that the proposed method produces more accurate segmentations on some difficult regions such as optic disc and central light reflex while detecting thin vessels effectively compared with the other methods.
ER  - 
TY  - JOUR
T1  - Swarm intelligence based adaptive gamma corrected (SIAGC) retinal image enhancement technique for early detection of diabetic retinopathy
A1  - Acharya, Upendra Kumar
A1  - Kumar, Sandeep
Y1  - 2021///
KW  -  Adaptive gamma correction (AGC)
KW  -  Diabetic retinopathy (DR)
KW  -  Image enhancement
KW  -  Particle swarm optimization (PSO)
KW  -  Retinal image
KW  - Swarm intelligence
JF  - Optik
VL  - 247
SP  - 167904
EP  - 167904
DO  - https://doi.org/10.1016/j.ijleo.2021.167904
UR  - https://www.sciencedirect.com/science/article/pii/S0030402621014819
N2  - Retinal images are low contrast, complex, suffer the problems of blurring and uneven illumination. So, it is very difficult to identify the vascular abnormalities. Some of the existing fundus sensors result noisy information, which makes scientific evaluation and diagnosis more complicated. So, for detecting the vascular abnormalities and for evaluating the early stages of Diabetic Retinopathy (DR), a robust retinal image enhancement technique (SIAGC) is proposed in this paper. To enhance the contrast of retinal images effectively and to make the detection process easier, the object of the retinal image is 1st extracted from the background. To avoid the over enhancement, plateau limit is applied separately to both regions. Then mapping function and adaptive gamma parameter have been evaluated using modified weighted probability density function (PDF) for enhancing the image quality. The plateau thresholds and exponentiation parameters used for gamma correction are automatically selected using swarm intelligence in order to maximize the proposed fitness function. It improves the adaptive-ness of the proposed technique. A multi-objective fitness function is proposed in this paper, which includes entropy, edge contents, AMBE and PSNR. Experimental results indicate the robustness of the proposed SIAGC based enhancement technique over other state of the art techniques.
ER  - 
TY  - JOUR
T1  - Interpretable deep learning to map diagnostic texts to ICD-10 codes
A1  - Atutxa, Aitziber
A1  - de Ilarraza, Arantza Díaz
A1  - Gojenola, Koldo
A1  - Oronoz, Maite
A1  - Perez-de-Viñaspre, Olatz
Y1  - 2019///
KW  -  Electronic health records
KW  -  Neural machine translation
KW  -  Sequence-to-sequence mapping
KW  - International Classification of Diseases
JF  - International Journal of Medical Informatics
VL  - 129
SP  - 49
EP  - 59
DO  - https://doi.org/10.1016/j.ijmedinf.2019.05.015
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618310670
N2  - Background
Automatic extraction of morbid disease or conditions contained in Death Certificates is a critical process, useful for billing, epidemiological studies and comparison across countries. The fact that these clinical documents are written in regular natural language makes the automatic coding process difficult because, often, spontaneous terms diverge strongly from standard reference terminology such as the International Classification of Diseases (ICD).
Objective
Our aim is to propose a general and multilingual approach to render Diagnostic Terms into the standard framework provided by the ICD. We have evaluated our proposal on a set of clinical texts written in French, Hungarian and Italian.
Methods
ICD-10 encoding is a multi-class classification problem with an extensive (thousands) number of classes. After considering several approaches, we tackle our objective as a sequence-to-sequence task. According to current trends, we opted to use neural networks. We tested different types of neural architectures on three datasets in which Diagnostic Terms (DTs) have their ICD-10 codes associated.
Results and conclusions
Our results give a new state-of-the art on multilingual ICD-10 coding, outperforming several alternative approaches, and showing the feasibility of automatic ICD-10 prediction obtaining an F-measure of 0.838, 0.963 and 0.952 for French, Hungarian and Italian, respectively. Additionally, the results are interpretable, providing experts with supporting evidence when confronted with coding decisions, as the model is able to show the alignments between the original text and each output code.
ER  - 
TY  - JOUR
T1  - Neural network based detection of hard exudates in retinal images
A1  - García, María
A1  - Sánchez, Clara I
A1  - López, María I
A1  - Abásolo, Daniel
A1  - Hornero, Roberto
Y1  - 2009///
KW  -  Multilayer perceptron
KW  -  Neural network
KW  -  Radial basis function
KW  -  Retinal imaging
KW  -  Support vector machine
KW  - Hard exudate
JF  - Computer Methods and Programs in Biomedicine
VL  - 93
IS  - 1
SP  - 9
EP  - 19
DO  - https://doi.org/10.1016/j.cmpb.2008.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0169260708001855
N2  - Diabetic retinopathy (DR) is an important cause of visual impairment in developed countries. Automatic recognition of DR lesions in fundus images can contribute to the diagnosis of the disease. The aim of this study is to automatically detect one of these lesions, hard exudates (EXs), in order to help ophthalmologists in the diagnosis and follow-up of the disease. We propose an algorithm which includes a neural network (NN) classifier for this task. Three NN classifiers were investigated: multilayer perceptron (MLP), radial basis function (RBF) and support vector machine (SVM). Our database was composed of 117 images with variable colour, brightness, and quality. 50 of them (from DR patients) were used to train the NN classifiers and 67 (40 from DR patients and 27 from healthy retinas) to test the method. Using a lesion-based criterion, we achieved a mean sensitivity (SEl) of 88.14% and a mean positive predictive value (PPVl) of 80.72% for MLP. With RBF we obtained SEl=88.49% and PPVl=77.41%, while we reached SEl=87.61% and PPVl=83.51% using SVM. With an image-based criterion, a mean sensitivity (SEi) of 100%, a mean specificity (SPi) of 92.59% and a mean accuracy (ACi) of 97.01% were obtained with MLP. Using RBF we achieved SEi=100%, SPi=81.48% and ACi=92.54%. With SVM the image-based results were SEi=100%, SPi=77.78% and ACi=91.04%.
ER  - 
TY  - JOUR
T1  - Automatic extraction of vascularity measurements using OCT-A images
A1  - Díaz, Macarena
A1  - Novo, Jorge
A1  - Penedo, Manuel G
A1  - Ortega, Marcos
Y1  - 2018///
KW  -  Image Segmentation
KW  -  Optical Coherence Tomography Angiography
KW  -  retinal imaging
KW  -  vascularity
KW  - Computer-aided diagnosis
JF  - Procedia Computer Science
VL  - 126
SP  - 273
EP  - 281
DO  - https://doi.org/10.1016/j.procs.2018.07.261
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918312377
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia
N2  - Optical Coherence Tomography Angiography (OCT-A) represents a new modality of ophthalmological imaging that stands out for being a non-invasive capture technique that facilitates the analysis of the vascular characteristics of the eye fundus. In this paper, we propose a complete automatic methodology that identifies the vascular and avascular zones in OCT-A images, quantifying each one of them for their posterior use in clinical analyses and diagnostic processes. To achieve this, we firstly intensify the vascular characteristics to facilitate the posterior extraction. Then, a set of image processing techniques are combined to differentiate both vascular and avascular regions and, finally, measure their representative parameters. The proposed methodology was tested on a set of images that were marked by an expert ophthalmologist, being used as reference in the validation of the method. The proposed approach presented satisfactory results in the validation experiments with the vascular and avascular measurements, demonstrating their utility for the diagnosis and monitoring of different vascular diseases that are frequently analysed through the retinal microcirculation.
ER  - 
TY  - JOUR
T1  - An automatic system to identify heart disease risk factors in clinical texts over time
A1  - Chen, Qingcai
A1  - Li, Haodi
A1  - Tang, Buzhou
A1  - Wang, Xiaolong
A1  - Liu, Xin
A1  - Liu, Zengjian
A1  - Liu, Shu
A1  - Wang, Weida
A1  - Deng, Qiwen
A1  - Zhu, Suisong
A1  - Chen, Yangxin
A1  - Wang, Jingfeng
Y1  - 2015///
KW  -  Clinical information extraction
KW  -  Heart disease
KW  -  Machine learning
KW  - Risk factor identification
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - S158
EP  - S163
DO  - https://doi.org/10.1016/j.jbi.2015.09.002
UR  - https://www.sciencedirect.com/science/article/pii/S153204641500194X
N1  - Supplement: Proceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data
N2  - Despite recent progress in prediction and prevention, heart disease remains a leading cause of death. One preliminary step in heart disease prediction and prevention is risk factor identification. Many studies have been proposed to identify risk factors associated with heart disease; however, none have attempted to identify all risk factors. In 2014, the National Center of Informatics for Integrating Biology and Beside (i2b2) issued a clinical natural language processing (NLP) challenge that involved a track (track 2) for identifying heart disease risk factors in clinical texts over time. This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records. Identification of tags and attributes associated with disease presence and progression, risk factors, and medications in patient medical history were required. Our participation led to development of a hybrid pipeline system based on both machine learning-based and rule-based approaches. Evaluation using the challenge corpus revealed that our system achieved an F1-score of 92.68%, making it the top-ranked system (without additional annotations) of the 2014 i2b2 clinical NLP challenge.
ER  - 
TY  - JOUR
T1  - Fast macula detection and application to retinal image quality assessment
A1  - Alais, Robin
A1  - Dokládal, Petr
A1  - Erginay, Ali
A1  - Figliuzzi, Bruno
A1  - Decencière, Etienne
Y1  - 2020///
KW  -  Convolutional neural networks
KW  -  Macula detection
KW  -  Retinal imaging
KW  - Image quality assessment
JF  - Biomedical Signal Processing and Control
VL  - 55
SP  - 101567
EP  - 101567
DO  - https://doi.org/10.1016/j.bspc.2019.101567
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419301417
N2  - In this article, we present a segmentation algorithm for assessing retinal image quality with respect to the visibility of the macular region. An image is considered of acceptable quality if the macular region is clearly visible and entirely in the field of view. Additionally, for acceptable images, the method is able to locate the fovea with a maximal error of 0.34 mm. The algorithm is based on a lightweight fully-convolutional network, several thousand times smaller than state-of-the-art networks investigated so far in preliminary studies. We obtain near-human performance for assessing macula visibility and fovea localization. The presented method can easily be embedded in tabletop or handheld retinographs, decreasing the number of ungradable images, saving both patient and physician time. It is an important step towards automatic screening of retinal pathologies, including diabetic retinopathy, which is a major global healthcare issue.
ER  - 
TY  - JOUR
T1  - Statistical atlas based exudate segmentation
A1  - Ali, Sharib
A1  - Sidibé, Désiré
A1  - Adal, Kedir M
A1  - Giancardo, Luca
A1  - Chaum, Edward
A1  - Karnowski, Thomas P
A1  - Mériaudeau, Fabrice
Y1  - 2013///
KW  -  Retinal images registration
KW  -  Statistical retinal atlas
KW  - Exudate segmentation
JF  - Computerized Medical Imaging and Graphics
VL  - 37
IS  - 5
SP  - 358
EP  - 368
DO  - https://doi.org/10.1016/j.compmedimag.2013.06.006
UR  - https://www.sciencedirect.com/science/article/pii/S0895611113001262
N1  - Retinal Image Analysis
N2  - Diabetic macular edema (DME) is characterized by hard exudates. In this article, we propose a novel statistical atlas based method for segmentation of such exudates. Any test fundus image is first warped on the atlas co-ordinate and then a distance map is obtained with the mean atlas image. This leaves behind the candidate lesions. Post-processing schemes are introduced for final segmentation of the exudate. Experiments with the publicly available HEI-MED data-set shows good performance of the method. A lesion localization fraction of 82.5% at 35% of non-lesion localization fraction on the FROC curve is obtained. The method is also compared to few most recent reference methods.
ER  - 
TY  - JOUR
T1  - Assigning clinical codes with data-driven concept representation on Dutch clinical free text
A1  - Scheurwegs, Elyne
A1  - Luyckx, Kim
A1  - Luyten, Léon
A1  - Goethals, Bart
A1  - Daelemans, Walter
Y1  - 2017///
KW  -  Data mining
KW  -  Distributional semantics
KW  -  Electronic health records
KW  -  International classification of diseases
KW  -  Text mining
KW  -  Unsupervised learning
KW  -  Word2vec
KW  - Clinical coding
JF  - Journal of Biomedical Informatics
VL  - 69
SP  - 118
EP  - 127
DO  - https://doi.org/10.1016/j.jbi.2017.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417300771
N2  - Clinical codes are used for public reporting purposes, are fundamental to determining public financing for hospitals, and form the basis for reimbursement claims to insurance providers. They are assigned to a patient stay to reflect the diagnosis and performed procedures during that stay. This paper aims to enrich algorithms for automated clinical coding by taking a data-driven approach and by using unsupervised and semi-supervised techniques for the extraction of multi-word expressions that convey a generalisable medical meaning (referred to as concepts). Several methods for extracting concepts from text are compared, two of which are constructed from a large unannotated corpus of clinical free text. A distributional semantic model (i.c. the word2vec skip-gram model) is used to generalize over concepts and retrieve relations between them. These methods are validated on three sets of patient stay data, in the disease areas of urology, cardiology, and gastroenterology. The datasets are in Dutch, which introduces a limitation on available concept definitions from expert-based ontologies (e.g. UMLS). The results show that when expert-based knowledge in ontologies is unavailable, concepts derived from raw clinical texts are a reliable alternative. Both concepts derived from raw clinical texts perform and concepts derived from expert-created dictionaries outperform a bag-of-words approach in clinical code assignment. Adding features based on tokens that appear in a semantically similar context has a positive influence for predicting diagnostic codes. Furthermore, the experiments indicate that a distributional semantics model can find relations between semantically related concepts in texts but also introduces erroneous and redundant relations, which can undermine clinical coding performance.
ER  - 
TY  - JOUR
T1  - Query bot for retrieving patients’ clinical history: A COVID-19 use-case
A1  - Wang, Yibo
A1  - Tariq, Amara
A1  - Khan, Fiza
A1  - Gichoya, Judy Wawira
A1  - Trivedi, Hari
A1  - Banerjee, Imon
Y1  - 2021///
KW  -  BERT
KW  -  Clinical notes
KW  -  Relevance feedback
KW  -  k-means
KW  - Information retrieval
JF  - Journal of Biomedical Informatics
VL  - 123
SP  - 103918
EP  - 103918
DO  - https://doi.org/10.1016/j.jbi.2021.103918
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421002471
N2  - Objective
With increasing patient complexity whose data are stored in fragmented health information systems, automated and time-efficient ways of gathering important information from the patients' medical history are needed for effective clinical decision making. Using COVID-19 as a case study, we developed a query-bot information retrieval system with user-feedback to allow clinicians to ask natural questions to retrieve data from patient notes.
Materials and methods
We applied clinicalBERT, a pre-trained contextual language model, to our dataset of patient notes to obtain sentence embeddings, using K-Means to reduce computation time for real-time interaction. Rocchio algorithm was then employed to incorporate user-feedback and improve retrieval performance.
Results
In an iterative feedback loop experiment, MAP for final iteration was 0.93/0.94 as compared to initial MAP of 0.66/0.52 for generic and 1./1. compared to 0.79/0.83 for COVID-19 specific queries confirming that contextual model handles the ambiguity in natural language queries and feedback helps to improve retrieval performance. User-in-loop experiment also outperformed the automated pseudo relevance feedback method. Moreover, the null hypothesis which assumes identical precision between initial retrieval and relevance feedback was rejected with high statistical significance (p ≪ 0.05). Compared to Word2Vec, TF-IDF and bioBERT models, clinicalBERT works optimally considering the balance between response precision and user-feedback.
Discussion
Our model works well for generic as well as COVID-19 specific queries. However, some generic queries are not answered as well as others because clustering reduces query performance and vague relations between queries and sentences are considered non-relevant. We also tested our model for queries with the same meaning but different expressions and demonstrated that these query variations yielded similar performance after incorporation of user-feedback.
Conclusion
In conclusion, we develop an NLP-based query-bot that handles synonyms and natural language ambiguity in order to retrieve relevant information from the patient chart. User-feedback is critical to improve model performance.
ER  - 
TY  - JOUR
T1  - Precision nutrition: A systematic literature review
A1  - Kirk, Daniel
A1  - Catal, Cagatay
A1  - Tekinerdogan, Bedir
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Deep learning
KW  -  Machine learning
KW  -  Personalized nutrition
KW  -  Systematic literature review
KW  - Precision nutrition
JF  - Computers in Biology and Medicine
VL  - 133
SP  - 104365
EP  - 104365
DO  - https://doi.org/10.1016/j.compbiomed.2021.104365
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521001591
N2  - Precision Nutrition research aims to use personal information about individuals or groups of individuals to deliver nutritional advice that, theoretically, would be more suitable than generic advice. Machine learning, a subbranch of Artificial Intelligence, has promise to aid in the development of predictive models that are suitable for Precision Nutrition. As such, recent research has applied machine learning algorithms, tools, and techniques in precision nutrition for different purposes. However, a systematic overview of the state-of-the-art on the use of machine learning in Precision Nutrition is lacking. Therefore, we carried out a Systematic Literature Review (SLR) to provide an overview of where and how machine learning has been used in Precision Nutrition from various aspects, what such machine learning models use as input features, what the availability status of the data used in the literature is, and how the models are evaluated. Nine research questions were defined in this study. We retrieved 4930 papers from electronic databases and 60 primary studies were selected to respond to the research questions. All of the selected primary studies were also briefly discussed in this article. Our results show that fifteen problems spread across seven domains of nutrition and health are present. Four machine learning tasks are seen in the form of regression, classification, recommendation and clustering, with most of these utilizing a supervised approach. In total, 30 algorithms were used, with 19 appearing more than once. Models were through the use of four groups of approaches and 23 evaluation metrics. Personalized approaches are promising to reduce the burden of these current problems in nutrition research, and the current review shows Machine Learning can be incorporated into Precision Nutrition research with high performance. Precision Nutrition researchers should consider incorporating Machine Learning into their methods to facilitate the integration of many complex features, allowing for the development of high-performance Precision Nutrition approaches.
ER  - 
TY  - JOUR
T1  - An investigation on magnetic imaging findings of the inner ear: A relationship between the internal auditory canal, its nerves and benign paroxysmal positional vertigo
A1  - Ergen, Burhan
A1  - Baykara, Murat
A1  - Polat, Cahit
Y1  - 2014///
KW  -  Benign paroxysmal positional vertigo
KW  -  Inner ear
KW  -  Internal auditory canal
KW  - Magnetic resonance imaging
JF  - Biomedical Signal Processing and Control
VL  - 9
SP  - 14
EP  - 18
DO  - https://doi.org/10.1016/j.bspc.2013.09.007
UR  - https://www.sciencedirect.com/science/article/pii/S1746809413001328
N2  - Visualization of the inner ear has been performed using magnetic resonance imaging (MRI) to investigate benign paroxysmal positional vertigo (BPPV). In the patients with BPPV, our recent findings indicate the thickness of some internal auditory canal (IAC) nerves narrower than the thickness of healthy subjects. The thickness of the IAC and its nerves are measured using brain MRI images. The cross sectional area (CSA) of a nerve is assumed as its thickness. Some statistical measurement and a statistical classification are performed on the CSA data to investigate any relation between IAC, the nerves and BPPV.
ER  - 
TY  - JOUR
T1  - A novel incremental Kernel Nonparametric SVM model (iKN-SVM) for data classification: An application to face detection
A1  - Soula, Arbia
A1  - Tbarki, Khaoula
A1  - Ksantini, Riadh
A1  - Said, Salma Ben
A1  - Lachiri, Zied
Y1  - 2020///
KW  -  Classification
KW  -  Face detection
KW  -  Kernel-based methods
KW  -  Nonparametric discriminant analysis
KW  -  Support Vector Machines
KW  - Incremental learning
JF  - Engineering Applications of Artificial Intelligence
VL  - 89
SP  - 103468
EP  - 103468
DO  - https://doi.org/10.1016/j.engappai.2019.103468
UR  - https://www.sciencedirect.com/science/article/pii/S0952197619303501
N2  - In this paper, we propose a novel incremental classifier to overcome problems associated with batch techniques, along with issues related to data spread that Kernel Support Vector Machines (KSVM) may encounter. Basically, we present a Kernel SVM-based model that learns incrementally, as new data is available over time, in order to handle dynamic and large data effectively and reduce the computational time. The proposed model deals with the data spread issues by introducing near-global variations, from the scatter matrices of the Kernel Nonparametric Discriminant Analysis (KNDA), into the optimization problem of incremental KSVM, while considering local characteristics of the data provided by KSVM. Besides, our model has a quadratic convex optimization problem with one global solution. Furthermore, an extensive comparison of the model with other state-of-the-art incremental and batch algorithms on various datasets, has been carried out, in order to show its advantages and effectiveness for classification tasks. Moreover, an evaluation of the proposed method on face detection is provided.
ER  - 
TY  - JOUR
T1  - AI powered electrochemical multi-component detection of insulin and glucose in serum
A1  - Zhao, Yuliang
A1  - Zhang, Hongyu
A1  - Li, Yang
A1  - Yu, Xiaodong
A1  - Cai, Yi
A1  - Sha, Xiaopeng
A1  - Wang, Shuyu
A1  - Zhan, Zhikun
A1  - Xu, Jianghong
A1  - Liu, Lianqing
Y1  - 2021///
KW  -  Concentration prediction
KW  -  Glucose
KW  -  Insulin
KW  -  Machine learning
KW  - Electrochemical
JF  - Biosensors and Bioelectronics
VL  - 186
SP  - 113291
EP  - 113291
DO  - https://doi.org/10.1016/j.bios.2021.113291
UR  - https://www.sciencedirect.com/science/article/pii/S0956566321003286
N2  - Multi-component detection of insulin and glucose in serum is of great importance and urgently needed in clinical diagnosis and treatment due to its economy and practicability. However, insulin and glucose can hardly be determined by traditional electrochemical detection methods. Their mixed oxidation currents and rare involvement in the reaction process make it difficult to decouple them. In this study, AI algorithms are introduced to power the electrochemical method to conquer this problem. First, the current curves of insulin, glucose, and their mixed solution are obtained using cyclic voltammetry. Then, seven features of the cyclic voltammetry curve are extracted as characteristic values for detecting the concentrations of insulin and glucose. Finally, after training using machine learning algorithms, insulin and glucose concentrations are decoupled and regressed accurately. The entire detection process only takes three minutes. It can detect insulin at the pmol level and glucose at the mmol level, which meets the basic clinical requirements. The average relative error in predicting insulin concentrations is around 6.515%, and that in predicting glucose concentrations is around 4.36%. To verify the performance and effectiveness of the proposed method, it is used to determine the concentrations of insulin and glucose in fetal bovine serum and real clinical serum samples. The results are satisfactory, demonstrating that the method can meet basic clinical needs. This multi-component testing system delivers acceptable detect limit and accuracy and has the merits of low cost and high efficiency, holding great potential for use in clinical diagnosis.
ER  - 
TY  - JOUR
T1  - Classification of motor imagery electroencephalogram signals by using a divergence based convolutional neural network
A1  - Dokur, Zümray
A1  - Olmez, Tamer
Y1  - 2021///
KW  -  Classification
KW  -  Convolutional neural network
KW  -  Data augmentation
KW  -  Motor imagery
KW  - EEG
JF  - Applied Soft Computing
VL  - 113
SP  - 107881
EP  - 107881
DO  - https://doi.org/10.1016/j.asoc.2021.107881
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621008036
N2  - Deep neural networks (DNNs) are observed to be successful in pattern classification. However, the high classification performances of DNNs are related to their large training sets. Unfortunately, in the literature, the datasets used to classify motor imagery (MI) electroencephalogram (EEG) signals contain a small number of samples. To achieve high performances with small-sized datasets, most of the studies have employed a transformation such as the common spatial patterns (CSP) before the classification process. However, the CSP is dependent on subjects and introduces computational load in real time applications. It is observed in the literature that data augmentation is not applied for increasing the classification performance of EEG signals. In this study, we have investigated the effect of the augmentation process on the classification performance of MI EEG signals instead of using a preceding transformation such as the CSP, and we have demonstrated that the augmentation process is able to compete with the CSP by generating high success rates for the classification of MI EEGs. In addition to the augmentation process, we have modified the DNN structure to increase the classification performance, to decrease the number of nodes in the structure, and to use less number of hyper parameters. A minimum distance network following the last layer of the convolutional neural network (CNN) was used as the classifier instead of a fully connected neural network (FCNN). By augmenting the EEG dataset and focusing solely on CNN’s training, the training algorithm of the proposed structure is strengthened without applying any transformation. We tested these improvements on brain–computer interface (BCI) competitions 2005 and 2008 databases with two and four classes, and the positive effects of the augmentation on the average accuracies are demonstrated.
ER  - 
TY  - JOUR
T1  - Detecting disease genes based on semi-supervised learning and protein–protein interaction networks
A1  - Nguyen, Thanh-Phuong
A1  - Ho, Tu-Bao
Y1  - 2012///
KW  -  Disease gene neighbours
KW  -  Disease-causing gene prediction
KW  -  Multiple data resources integration
KW  -  Protein–protein interaction network
KW  - Semi-supervised learning
JF  - Artificial Intelligence in Medicine
VL  - 54
IS  - 1
SP  - 63
EP  - 71
DO  - https://doi.org/10.1016/j.artmed.2011.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S0933365711001230
N2  - Objective
Predicting or prioritizing the human genes that cause disease, or “disease genes”, is one of the emerging tasks in biomedicine informatics. Research on network-based approach to this problem is carried out upon the key assumption of “the network-neighbour of a disease gene is likely to cause the same or a similar disease”, and mostly employs data regarding well-known disease genes, using supervised learning methods. This work aims to find an effective method to exploit the disease gene neighbourhood and the integration of several useful omics data sources, which potentially enhance disease gene predictions.
Methods
We have presented a novel method to effectively predict disease genes by exploiting, in the semi-supervised learning (SSL) scheme, data regarding both disease genes and disease gene neighbours via protein–protein interaction network. Multiple proteomic and genomic data were integrated from six biological databases, including Universal Protein Resource, Interologous Interaction Database, Reactome, Gene Ontology, Pfam, and InterDom, and a gene expression dataset.
Results
By employing a 10 times stratified 10-fold cross validation, the SSL method performs better than the k-nearest neighbour method and the support vector machines method in terms of sensitivity of 85%, specificity of 79%, precision of 81%, accuracy of 82%, and a balanced F-function of 83%. The other comparative experimental evaluations demonstrate advantages of the proposed method given a small amount of labeled data with accuracy of 78%. We have applied the proposed method to detect 572 putative disease genes, which are biologically validated by some indirect ways.
Conclusion
Semi-supervised learning improved ability to study disease genes, especially a specific disease when the known disease genes (as labeled data) are very often limited. In addition to the computational improvement, the analysis of predicted disease proteins indicates that the findings are beneficial in deciphering the pathogenic mechanisms.
ER  - 
TY  - JOUR
T1  - Prediction of venous thromboembolism using semantic and sentiment analyses of clinical narratives
A1  - Sabra, Susan
A1  - Mahmood Malik, Khalid
A1  - Alobaidi, Mazen
Y1  - 2018///
KW  -  Natural language processing
KW  -  Prediction through classification
KW  -  Risk factor assessment
KW  -  Semantic enrichment
KW  -  Sentiment analysis
KW  -  Support vector machine
KW  - Venous thromboembolism
JF  - Computers in Biology and Medicine
VL  - 94
SP  - 1
EP  - 10
DO  - https://doi.org/10.1016/j.compbiomed.2017.12.026
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517304237
N2  - Venous thromboembolism (VTE) is the third most common cardiovascular disorder. It affects people of both genders at ages as young as 20 years. The increased number of VTE cases with a high fatality rate of 25% at first occurrence makes preventive measures essential. Clinical narratives are a rich source of knowledge and should be included in the diagnosis and treatment processes, as they may contain critical information on risk factors. It is very important to make such narrative blocks of information usable for searching, health analytics, and decision-making. This paper proposes a Semantic Extraction and Sentiment Assessment of Risk Factors (SESARF) framework. Unlike traditional machine-learning approaches, SESARF, which consists of two main algorithms, namely, ExtractRiskFactor and FindSeverity, prepares a feature vector as the input to a support vector machine (SVM) classifier to make a diagnosis. SESARF matches and maps the concepts of VTE risk factors and finds adjectives and adverbs that reflect their levels of severity. SESARF uses a semantic- and sentiment-based approach to analyze clinical narratives of electronic health records (EHR) and then predict a diagnosis of VTE. We use a dataset of 150 clinical narratives, 80% of which are used to train our prediction classifier support vector machine, with the remaining 20% used for testing. Semantic extraction and sentiment analysis results yielded precisions of 81% and 70%, respectively. Using a support vector machine, prediction of patients with VTE yielded precision and recall values of 54.5% and 85.7%, respectively.
ER  - 
TY  - JOUR
T1  - SSOMaj-SMOTE-SSOMin: Three-step intelligent pruning of majority and minority samples for learning from imbalanced datasets
A1  - Susan, Seba
A1  - Kumar, Amitesh
Y1  - 2019///
KW  -  Oversampling
KW  -  Particle swarm optimization
KW  -  SMOTE
KW  -  Sample subset optimization
KW  -  Undersampling
KW  - Imbalanced learning
JF  - Applied Soft Computing
VL  - 78
SP  - 141
EP  - 149
DO  - https://doi.org/10.1016/j.asoc.2019.02.028
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619300924
N2  - Real world datasets, particularly in the current context of Big Data applications, suffer from the problem of imbalanced representation of samples from different categories. Most classifiers and learning techniques are inept to deal with this problem, with the majority of them tending to overlook the issue. Typical data balancing methods in literature resort to data sampling that constitutes of either undersampling the majority class samples or oversampling the minority class samples. An intelligent combination of undersampling the majority class and oversampling the minority class is expected to improve the learning performance. In this paper, data balancing is achieved prior to classification, through a novel three-step sequence of intelligent undersampling of the majority class followed by the oversampling of the minority class, which is further followed by the intelligent undersampling of the minority class that has now become the majority class due to the oversampling. The recently proposed Sample Subspace Optimization (SSO) that uses Particle Swarm Optimization (PSO) as an intelligent agent to find globally optimum solutions in the search space, is our choice for the intelligent undersampling technique. The oversampling in the second step is achieved through Synthetic Minority Oversampling (SMOTE) as well as intelligent variants such as Borderline SMOTE, ADASYN and MWMOTE. The increase in computational complexity is compensated by the higher performance achieved due to relevant sampling. Experiments on benchmark datasets from the UCI repository establish the efficiency of our three-step approach SSOMaj–SMOTE–SSOMin as observed from the higher AUC scores from the Receiver Operating Characteristics.
ER  - 
TY  - JOUR
T1  - A multiple combined method for rebalancing medical data with class imbalances
A1  - Wang, Yun-Chun
A1  - Cheng, Ching-Hsue
Y1  - 2021///
KW  -  MetaCost
KW  -  Particle swarm optimization
KW  -  Synthetic minority oversampling technique
KW  - Class imbalance
JF  - Computers in Biology and Medicine
VL  - 134
SP  - 104527
EP  - 104527
DO  - https://doi.org/10.1016/j.compbiomed.2021.104527
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521003218
N2  - Most classification algorithms assume that classes are in a balanced state. However, datasets with class imbalances are everywhere. The classes of actual medical datasets are imbalanced, severely impacting identification models and even sacrificing the classification accuracy of the minority class, even though it is the most influential and representative. The medical field has irreversible characteristics. Its tolerance rate for misjudgment is relatively low, and errors may cause irreparable harm to patients. Therefore, this study proposes a multiple combined method to rebalance medical data featuring class imbalances. The combined methods include (1) resampling methods (synthetic minority oversampling technique [SMOTE] and undersampling [US]), (2) particle swarm optimization (PSO), and (3) MetaCost. This study conducted two experiments with nine medical datasets to verify and compare the proposed method with the listing methods. A decision tree is used to generate decision rules for easy understanding of the research results. The results show that (1) the proposed method with ensemble learning can improve the area under a receiver operating characteristic curve (AUC), recall, precision, and F1 metrics; (2) MetaCost can increase sensitivity; (3) SMOTE can effectively enhance AUC; (4) US can improve sensitivity, F1, and misclassification costs in data with a high-class imbalance ratio; and (5) PSO-based attribute selection can increase sensitivity and reduce data dimension. Finally, we suggest that the dataset with an imbalanced ratio >9 must use the US results to make the decision. As the imbalanced ratio is < 9, the decision-maker can simultaneously consider the results of SMOTE and US to identify the best decision.
ER  - 
TY  - JOUR
T1  - A chemosensor array for the colorimetric identification of some carboxylic acids in human urine samples
A1  - Sheini, Azarmidokht
A1  - Khajehsharifi, Habibollah
A1  - Shahbazy, Mohammad
A1  - Kompany-Zareh, Mohsen
Y1  - 2017///
KW  -  Carboxylic acids
KW  -  Human urine samples
KW  -  Linear discriminant analysis
KW  -  Principal component analysis
KW  - Colorimetric chemosensor array
JF  - Sensors and Actuators B: Chemical
VL  - 242
SP  - 288
EP  - 298
DO  - https://doi.org/10.1016/j.snb.2016.11.008
UR  - https://www.sciencedirect.com/science/article/pii/S0925400516317932
N2  - A simple colorimetric sensor array containing eight chemosensors from commercial dyes and metal salts was designed to detect a number of important carboxylic acids in human urine. Some common chemometric methods, including principal component analysis (PCA), linear discriminant analysis (LDA) and k-nearest neighbor (kNN) based hierarchical cluster analysis (HCA) were used to test the discriminatory power of the array. The eight-member sensor array perfectly identified eleven carboxylic acids in water with 100% classification accuracy. In order evaluate the potential of sensor array used in biological environments (biofluids), carboxylic acids in human urine were analyzed and 100% classification accuracy was achieved. In addition, the array’s performance in the semi-quantitative identification of carboxylic acids was investigated, and the results showed that the sensor array can discriminate seven typical carboxylic acids at concentrations ranging from 100 to 1000μmolL−1. These results illustrate the potential use of the sensor array for disease diagnosis and other biomedical monitoring applications.
ER  - 
TY  - JOUR
T1  - CIBS: A biomedical text summarizer using topic-based sentence clustering
A1  - Moradi, Milad
Y1  - 2018///
KW  -  Coverage
KW  -  Domain knowledge
KW  -  Itemset mining
KW  -  Medical text mining
KW  -  Multi-document summarization
KW  - Natural Language Processing
JF  - Journal of Biomedical Informatics
VL  - 88
SP  - 53
EP  - 61
DO  - https://doi.org/10.1016/j.jbi.2018.11.006
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418302156
N2  - Automatic text summarizers can reduce the time required to read lengthy text documents by extracting the most important parts. Multi-document summarizers should produce a summary that covers the main topics of multiple related input texts to diminish the extent of redundant information. In this paper, we propose a novel summarization method named Clustering and Itemset mining based Biomedical Summarizer (CIBS). The summarizer extracts biomedical concepts from the input documents and employs an itemset mining algorithm to discover main topics. Then, it applies a clustering algorithm to put the sentences into clusters such that those in the same cluster share similar topics. Selecting sentences from all the clusters, the summarizer can produce a summary that covers a wide range of topics of the input text. Using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) toolkit, we evaluate the performance of the CIBS method against four summarizers including a state-of-the-art method. The results show that the CIBS method can improve the performance of single- and multi-document biomedical text summarization. It is shown that the topic-based sentence clustering approach can be effectively used to increase the informative content of summaries, as well as to decrease the redundant information.
ER  - 
TY  - JOUR
T1  - Comparison of QT interval variability of coronary patients without myocardial infarction with that of patients with old myocardial infarction
A1  - Yao, Lianke
A1  - Li, Peng
A1  - Liu, Changchun
A1  - Hou, Yunxiu
A1  - Yan, Chang
A1  - Li, Liping
A1  - Li, Ke
A1  - Wang, Xinpei
A1  - Deogire, Aruna
A1  - Du, Chunlei
A1  - Zhang, Huan
A1  - Wang, Jikuo
A1  - Li, Han
Y1  - 2019///
KW  -  Coronary artery disease
KW  -  Heart rate variability
KW  -  Myocardial infarction
KW  -  Ventricular repolarization
KW  - QT interval variability
JF  - Computers in Biology and Medicine
VL  - 113
SP  - 103396
EP  - 103396
DO  - https://doi.org/10.1016/j.compbiomed.2019.103396
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519302732
N2  - Background
The significant association of myocardial ischemia with elevated QT interval variability (QTV) has been reported in myocardial infarction (MI) patients. However, the influence of the time course of MI on QTV has not been investigated systematically.
Method
Short-term QT and RR interval time series were constructed from the 5 min electrocardiograms of 49 coronary patients without MI and 26 patients with old MI (OMI). The QTV, heart rate variability (HRV), and QT–RR coupling of the two groups were analyzed using various time series analysis tools in the time- and frequency-domains, as well as nonlinear dynamics.
Results
Nearly all of the tested QTV indices for coronary patients with OMI were higher than those for patients without MI. However, no significant differences were found between the two groups in any of the variables employed to assess the HRV and QT–RR coupling. All of the markers that showed statistical significances in univariate analyses still possessed the capabilities of distinguishing between the two groups even after adjusting for studied baseline characteristics, including the coronary atherosclerotic burden.
Conclusions
The results suggested that the QTV increased in coronary patients with OMI compared to those without MI, which might reflect the influence of post-MI remodeling on the beat-to-beat temporal variability of ventricular repolarization. The non-significant differences in the HRV and QT–RR couplings could indicate that there were no differences in the modulation of the autonomic nervous system and interaction of QT with the RR intervals between the two groups.
ER  - 
TY  - JOUR
T1  - Detecting time-evolving phenotypic topics via tensor factorization on electronic health records: Cardiovascular disease case study
A1  - Zhao, Juan
A1  - Zhang, Yun
A1  - Schlueter, David J
A1  - Wu, Patrick
A1  - Eric Kerchberger, Vern
A1  - Trent Rosenbloom, S
A1  - Wells, Quinn S
A1  - Feng, QiPing
A1  - Denny, Joshua C
A1  - Wei, Wei-Qi
Y1  - 2019///
KW  -  Computational phenotyping
KW  -  Tensor decomposition
KW  - Deep phenotyping
JF  - Journal of Biomedical Informatics
VL  - 98
SP  - 103270
EP  - 103270
DO  - https://doi.org/10.1016/j.jbi.2019.103270
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419301893
N2  - Objective
Discovering subphenotypes of complex diseases can help characterize disease cohorts for investigative studies aimed at developing better diagnoses and treatments. Recent advances in unsupervised machine learning on electronic health record (EHR) data have enabled researchers to discover phenotypes without input from domain experts. However, most existing studies have ignored time and modeled diseases as discrete events. Uncovering the evolution of phenotypes – how they emerge, evolve and contribute to health outcomes – is essential to define more precise phenotypes and refine the understanding of disease progression. Our objective was to assess the benefits of an unsupervised approach that incorporates time to model diseases as dynamic processes in phenotype discovery.
Methods
In this study, we applied a constrained non-negative tensor-factorization approach to characterize the complexity of cardiovascular disease (CVD) patient cohort based on longitudinal EHR data. Through tensor-factorization, we identified a set of phenotypic topics (i.e., subphenotypes) that these patients established over the 10 years prior to the diagnosis of CVD, and showed the progress pattern. For each identified subphenotype, we examined its association with the risk for adverse cardiovascular outcomes estimated by the American College of Cardiology/American Heart Association Pooled Cohort Risk Equations, a conventional CVD-risk assessment tool frequently used in clinical practice. Furthermore, we compared the subsequent myocardial infarction (MI) rates among the six most prevalent subphenotypes using survival analysis.
Results
From a cohort of 12,380 adult CVD individuals with 1068 unique PheCodes, we successfully identified 14 subphenotypes. Through the association analysis with estimated CVD risk for each subtype, we found some phenotypic topics such as Vitamin D deficiency and depression, Urinary infections cannot be explained by the conventional risk factors. Through a survival analysis, we found markedly different risks of subsequent MI following the diagnosis of CVD among the six most prevalent topics (p < 0.0001), indicating these topics may capture clinically meaningful subphenotypes of CVD.
Conclusion
This study demonstrates the potential benefits of using tensor-decomposition to model diseases as dynamic processes from longitudinal EHR data. Our results suggest that this data-driven approach may potentially help researchers identify complex and chronic disease subphenotypes in precision medicine research.
ER  - 
TY  - JOUR
T1  - Mental health ubiquitous monitoring supported by social situation awareness: A systematic review
A1  - Moura, Ivan
A1  - Teles, Ariel
A1  - Silva, Francisco
A1  - Viana, Davi
A1  - Coutinho, Luciano
A1  - Barros, Flávio
A1  - Endler, Markus
Y1  - 2020///
KW  -  Mental states
KW  -  Sociability
KW  -  Social behavior
KW  -  Social situation awareness
KW  -  Ubiquitous computing
KW  - Mental health
JF  - Journal of Biomedical Informatics
VL  - 107
SP  - 103454
EP  - 103454
DO  - https://doi.org/10.1016/j.jbi.2020.103454
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300824
N2  - Traditionally, the process of monitoring and evaluating social behavior related to mental health has based on self-reported information, which is limited by the subjective character of responses and various cognitive biases. Today, however, there is a growing amount of studies that have provided methods to objectively monitor social behavior through ubiquitous devices and have used this information to support mental health services. In this paper, we present a Systematic Literature Review (SLR) to identify, analyze and characterize the state of the art about the use of ubiquitous devices to monitor users’ social behavior focused on mental health. For this purpose, we performed an exhaustive literature search on the six main digital libraries. A screening process was conducted on 160 peer-reviewed publications by applying suitable selection criteria to define the appropriate studies to the scope of this SLR. Next, 20 selected studies were forwarded to the data extraction phase. From an analysis of the selected studies, we recognized the types of social situations identified, the process of transforming contextual data into social situations, the use of social situation awareness to support mental health monitoring, and the methods used to evaluate proposed solutions. Additionally, we identified the main trends presented by this research area, as well as open questions and perspectives for future research. Results of this SLR showed that social situation-aware ubiquitous systems represent promising assistance tools for patients and mental health professionals. However, studies still present limitations in methodological rigor and restrictions in experiments, and solutions proposed by them have limitations to be overcome.
ER  - 
TY  - JOUR
T1  - Identification of Latent Risk Clinical Attributes for Children Born Under IUGR Condition Using Machine Learning Techniques
A1  - Nguyen Van, Sau
A1  - Lobo Marques, J A
A1  - Biala, T A
A1  - Li, Ye
Y1  - 2021///
KW  -  ABPM (Ambulatory Blood Pressure Monitoring)
KW  -  HRV (Heart Rate Variability)
KW  -  Machine Learning
KW  - IUGR (Intrauterine Growth Restriction)
JF  - Computer Methods and Programs in Biomedicine
VL  - 200
SP  - 105842
EP  - 105842
DO  - https://doi.org/10.1016/j.cmpb.2020.105842
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720316758
N2  - Background and objective
Intrauterine Growth Restriction (IUGR) is a condition in which a fetus does not grow to the expected weight during pregnancy. There are several well documented causes in the literature for this issue, such as maternal disorder, and genetic influences. Nevertheless, besides the risk during pregnancy and labour periods, in a long term perspective, the impact of IUGR condition during the child development is an area of research itself. The main objective of this work is to propose a machine learning solution to identify the most significant features of importance based on physiological, clinical or socioeconomic factors correlated with previous IUGR condition after 10 years of birth.
Methods
In this work, 41 IUGR (18 male) and 34 Non-IUGR (22 male) children were followed up 9 years after the birth, in average (9.1786 ± 0.6784 years old). A group of machine learning algorithms is proposed to classify children previously identified as born under IUGR condition based on 24-hours monitoring of ECG (Holter) and blood pressure (ABPM), and other clinical and socioeconomic attributes. In additional, an algorithm of relevance determination based on the classifier is also proposed, to determine the level of importance of the considered features.
Results
The proposed classification solution achieved accuracy up to 94.73%, and better performance than seven state-of-the-art machine learning algorithms. Also, relevant latent factors related to HRV and BP monitoring are proposed, such as: day-time heart rate (day-time HR), day-night systolic blood pressure (day-night SBP), 24-hour standard deviation (SD) of SBP, dropped, morning cortisol creatinine, 24-hour mean of SDs of all NN intervals for each 5 minutes segment (24-hour SDNNi), among others.
Conclusion
With outstanding accuracy of our proposed solutions, the classification system and the indication of relevant attributes may support medical teams on the clinical monitoring of IUGR children during their childhood development.
ER  - 
TY  - JOUR
T1  - Pixel-wise body composition prediction with a multi-task conditional generative adversarial network
A1  - Wang, Qiyue
A1  - Xue, Wu
A1  - Zhang, Xiaoke
A1  - Jin, Fang
A1  - Hahn, James
Y1  - 2021///
KW  -  Conditional generative adversarial network
KW  -  Medical image processing
KW  - Body composition analysis
JF  - Journal of Biomedical Informatics
VL  - 120
SP  - 103866
EP  - 103866
DO  - https://doi.org/10.1016/j.jbi.2021.103866
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001957
N2  - The analysis of human body composition plays a critical role in health management and disease prevention. However, current medical technologies to accurately assess body composition such as dual energy X-ray absorptiometry, computed tomography, and magnetic resonance imaging have the disadvantages of prohibitive cost or ionizing radiation. Recently, body shape based techniques using body scanners and depth cameras, have brought new opportunities for improving body composition estimation by intelligently analyzing body shape descriptors. In this paper, we present a multi-task deep neural network method utilizing a conditional generative adversarial network to predict the pixel level body composition using only 3D body surfaces. The proposed method can predict 2D subcutaneous and visceral fat maps in a single network with a high accuracy. We further introduce an interpreted patch discriminator which optimizes the texture accuracy of the 2D fat maps. The validity and effectiveness of our new method are demonstrated experimentally on TCIA and LiTS datasets. Our proposed approach outperforms competitive methods by at least 41.3% for the whole body fat percentage, 33.1% for the subcutaneous and visceral fat percentage, and 4.1% for the regional fat predictions.
ER  - 
TY  - JOUR
T1  - Taxonomic classification of metagenomic sequences from Relative Abundance Index profiles using deep learning
A1  - Karagöz, Meryem Altın
A1  - Nalbantoglu, O Ufuk
Y1  - 2021///
KW  -  Metagenomics
KW  -  Sequence analysis
KW  -  Taxonomic classification
KW  - Convolutional neural networks
JF  - Biomedical Signal Processing and Control
VL  - 67
SP  - 102539
EP  - 102539
DO  - https://doi.org/10.1016/j.bspc.2021.102539
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421001361
N2  - We propose a Convolutional Neural Network approach based on k-mer representation for metagenomic fragment classification problem. The proposed model consists of two steps; the first step is representation of DNA based on k-mer frequency with Relative Abundance Index (RAI) and the second step is classification metagenomic fragments with CNN. RAI scores, as DNA fragment representations are fed to CNN classifiers (CNN-RAI). RAI consist of the over- and under abundance statistics gathered from the taxon for each k-mer. In order to compare the performances of CNN-RAI and RAIphy, which classifies metagenomic fragments using the same input attributes with an expectation-maximization based approach, databases of different metagenomic scenarios were tested. Metagenomics data that were generated (or simulated) by different Next-Generation Sequencing platforms, respectively Illumina technology and Oxford Nanopore MinION were compiled into shotgun metagenomics or 16S rRNA datasets. RAI based method and CNN models were trained on represented data with read lengths ranging between 200 and 10,000 bp, also with distinct k-mer size (3≤k≤7) at genus level. RAI score was used for the first time in the deep learning algorithm as a spectral representation with improved performance thanks to the ability of deep learning on each dataset for a range of parameters. The proposed representation was compared to the current spectral methods and shown to be competitive for all datasets used in this study.
ER  - 
TY  - JOUR
T1  - Segmentation of histological images and fibrosis identification with a convolutional neural network
A1  - Fu, Xiaohang
A1  - Liu, Tong
A1  - Xiong, Zhaohan
A1  - Smaill, Bruce H
A1  - Stiles, Martin K
A1  - Zhao, Jichao
Y1  - 2018///
KW  -  Deep learning
KW  -  Fibrosis
KW  -  Histology
KW  -  Image segmentation
KW  - Convolutional neural network
JF  - Computers in Biology and Medicine
VL  - 98
SP  - 147
EP  - 158
DO  - https://doi.org/10.1016/j.compbiomed.2018.05.015
UR  - https://www.sciencedirect.com/science/article/pii/S0010482518301288
N2  - Segmentation of histological images is one of the most crucial tasks for many biomedical analyses involving quantification of certain tissue types, such as fibrosis via Masson's trichrome staining. However, challenges are posed by the high variability and complexity of structural features in such images, in addition to imaging artifacts. Further, the conventional approach of manual thresholding is labor-intensive, and highly sensitive to inter- and intra-image intensity variations. An accurate and robust automated segmentation method is of high interest. We propose and evaluate an elegant convolutional neural network (CNN) designed for segmentation of histological images, particularly those with Masson's trichrome stain. The network comprises 11 successive convolutional – rectified linear unit – batch normalization layers. It outperformed state-of-the-art CNNs on a dataset of cardiac histological images (labeling fibrosis, myocytes, and background) with a Dice similarity coefficient of 0.947. With 100 times fewer (only 300,000) trainable parameters than the state-of-the-art, our CNN is less susceptible to overfitting, and is efficient. Additionally, it retains image resolution from input to output, captures fine-grained details, and can be trained end-to-end smoothly. To the best of our knowledge, this is the first deep CNN tailored to the problem of concern, and may potentially be extended to solve similar segmentation tasks to facilitate investigations into pathology and clinical treatment.
ER  - 
TY  - JOUR
T1  - Computer technologies to integrate medical treatments to manage multimorbidity
A1  - Riaño, David
A1  - Ortega, Wilfrido
Y1  - 2017///
KW  -  Co-morbidity
KW  -  Management of concomitant diseases
KW  -  Multimorbidity
KW  - Treatment integration techniques
JF  - Journal of Biomedical Informatics
VL  - 75
SP  - 1
EP  - 13
DO  - https://doi.org/10.1016/j.jbi.2017.09.009
UR  - https://www.sciencedirect.com/science/article/pii/S153204641730206X
N2  - The high prevalence of multimorbid cases is a challenge for Health-Care Systems today. Clinical practice guidelines are the means to register and transmit the available evidence-based medical knowledge concerning concrete diseases. Several computer languages have been defined to represent this knowledge in a way that computers could use to help physicians in the daily practice of medicine. The generation of guidelines for all possible multimorbidities entails several issues that are difficult to address. Consequently, numerous medical informatics technologies have appeared merging computer information structures in a way that the treatment knowledge about single diseases could be combined in order to deliver health-care to patients suffering from multimorbidity. This paper proposes a classification of the most promising current technologies addressing this issue and provides an analysis of their maturity, strengths, and weaknesses. We conclude with an enumeration of ten relevant issues to consider when developing such technologies.
ER  - 
TY  - JOUR
T1  - Computer-aided decision-making for predicting liver disease using PSO-based optimized SVM with feature selection
A1  - Joloudari, Javad Hassannataj
A1  - Saadatfar, Hamid
A1  - Dehzangi, Abdollah
A1  - Shamshirband, Shahaboddin
Y1  - 2019///
KW  -  Classification models
KW  -  Disease prediction
KW  -  Feature selection
KW  -  Liver disease
KW  - Data mining
JF  - Informatics in Medicine Unlocked
VL  - 17
SP  - 100255
EP  - 100255
DO  - https://doi.org/10.1016/j.imu.2019.100255
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819302539
N2  - Using medical data mining models has been considered as a significant way to predict diseases in recent years. In the field of healthcare, we face a large amount of data, and this is one of the challenges in predicting and analyzing the target disease. With the help of data mining models, one can convert this data into valuable information, and through analyzing them logically and scientifically, one can reach accurate decision-making and actual prediction. Another challenge in the field of disease prediction is selecting features that are more significant than other features. Feature subset selection is performed to improve the performance of models with the highest accuracy. The purpose of this study is to select significant features by comparing data mining models to predict liver disease based on an extraction, loading, transformation, analysis (ELTA) approach for correct diagnosis. Hence, the data mining models are compared based on the ELTA approach, such as random forest, Multi-Layer Perceptron (MLP) neural network, Bayesian networks, Support Vector Machine (SVM), and Particle Swarm Optimization (PSO)-SVM. Among these models, the PSO-SVM model has the best performance regarding the criteria of specificity, sensitivity, accuracy, Area under the Curve (AUC), F-measure, precision, and False Positive Rate (FPR). Furthermore, a 10-fold cross-validation method for evaluation of models is used so that the models were evaluated on a liver disease dataset. The average of estimated accuracy was calculated as 87.35%, 78.91%, 66.78%, 76.51% and 95.17% for Random forest, MLP Neural network, Bayesian network, SVM and PSO-SVM models, respectively. Regarding the mentioned evaluation criteria, we obtained the highest performance of accuracy with the least number of features through the hybrid PSO-SVM-based optimized model.
ER  - 
TY  - JOUR
T1  - A partition enhanced mining algorithm for distributed association rule mining systems
A1  - Ogunde, A O
A1  - Folorunso, O
A1  - Sodiya, A S
Y1  - 2015///
KW  -  Database partitioning
KW  -  Distributed association rule mining systems
KW  -  Frequent itemset
KW  -  Mobile agents
KW  - Data mining
JF  - Egyptian Informatics Journal
VL  - 16
IS  - 3
SP  - 297
EP  - 307
DO  - https://doi.org/10.1016/j.eij.2015.06.006
UR  - https://www.sciencedirect.com/science/article/pii/S1110866515000365
N2  - The extraction of patterns and rules from large distributed databases through existing Distributed Association Rule Mining (DARM) systems is still faced with enormous challenges such as high response times, high communication costs and inability to adapt to the constantly changing databases. In this work, a Partition Enhanced Mining Algorithm (PEMA) is presented to address these problems. In PEMA, the Association Rule Mining Coordinating Agent receives a request and decides the appropriate data sites, partitioning strategy and mining agents to use. The mining process is divided into two stages. In the first stage, the data agents horizontally segment the databases with small average transaction length into relatively smaller partitions based on the number of available sites and the available memory. On the other hand, databases with relatively large average transaction length were vertically partitioned. After this, Mobile Agent-Based Association Rule Mining-Agents, which are the mining agents, carry out the discovery of the local frequent itemsets. At the second stage, the local frequent itemsets were incrementally integrated by the from one data site to another to get the global frequent itemsets. This reduced the response time and communication cost in the system. Results from experiments conducted on real datasets showed that the average response time of PEMA showed an improvement over existing algorithms. Similarly, PEMA incurred lower communication costs with average size of messages exchanged lower when compared with benchmark DARM systems. This result showed that PEMA could be efficiently deployed for efficient discovery of valuable knowledge in distributed databases.
ER  - 
TY  - JOUR
T1  - Dynamically weighted evolutionary ordinal neural network for solving an imbalanced liver transplantation problem
A1  - Dorado-Moreno, Manuel
A1  - Pérez-Ortiz, María
A1  - Gutiérrez, Pedro A
A1  - Ciria, Rubén
A1  - Briceño, Javier
A1  - Hervás-Martínez, César
Y1  - 2017///
KW  -  Imbalanced classification
KW  -  Liver transplantation
KW  -  Ordinal classification
KW  -  Survival analysis
KW  - Artificial neural networks
JF  - Artificial Intelligence in Medicine
VL  - 77
SP  - 1
EP  - 11
DO  - https://doi.org/10.1016/j.artmed.2017.02.004
UR  - https://www.sciencedirect.com/science/article/pii/S0933365716302901
N2  - Objective
Create an efficient decision-support model to assist medical experts in the process of organ allocation in liver transplantation. The mathematical model proposed here uses different sources of information to predict the probability of organ survival at different thresholds for each donor–recipient pair considered. Currently, this decision is mainly based on the Model for End-stage Liver Disease, which depends only on the severity of the recipient and obviates donor–recipient compatibility. We therefore propose to use information concerning the donor, the recipient and the surgery, with the objective of allocating the organ correctly.
Methods and materials
The database consists of information concerning transplants conducted in 7 different Spanish hospitals and the King's College Hospital (United Kingdom). The state of the patients is followed up for 12 months. We propose to treat the problem as an ordinal classification one, where we predict the organ survival at different thresholds: less than 15 days, between 15 and 90 days, between 90 and 365 days and more than 365 days. This discretization is intended to produce finer-grain survival information (compared with the common binary approach). However, it results in a highly imbalanced dataset in which more than 85% of cases belong to the last class. To solve this, we combine two approaches, a cost-sensitive evolutionary ordinal artificial neural network (ANN) (in which we propose to incorporate dynamic weights to make more emphasis on the worst classified classes) and an ordinal over-sampling technique (which adds virtual patterns to the minority classes and thus alleviates the imbalanced nature of the dataset).
Results
The results obtained by our proposal are promising and satisfactory, considering the overall accuracy, the ordering of the classes and the sensitivity of minority classes. In this sense, both the dynamic costs and the over-sampling technique improve the base results of the considered ANN-based method. Comparing our model with other state-of-the-art techniques in ordinal classification, competitive results can also be appreciated. The results achieved with this proposal improve the ones obtained by other state-of-the-art models: we were able to correctly predict more than 73% of the transplantation results, with a geometric mean of the sensitivities of 31.46%, which is much higher than the one obtained by other models.
Conclusions
The combination of the proposed cost-sensitive evolutionary algorithm together with the application of an over-sampling technique improves the predictive capability of our model in a significant way (especially for minority classes), which can help the surgeons make more informed decisions about the most appropriate recipient for an specific donor organ, in order to maximize the probability of survival after the transplantation and therefore the fairness principle.
ER  - 
TY  - JOUR
T1  - Nonlinear metric learning for kNN and SVMs through geometric transformations
A1  - Shi, Bibo
A1  - Liu, Jundong
Y1  - 2018///
KW  -  Nearest neighbor
KW  -  Nonlinear transformations
KW  -  SVMs
KW  -  Thin-plate splines
KW  - Distance metric learning
JF  - Neurocomputing
VL  - 318
SP  - 18
EP  - 29
DO  - https://doi.org/10.1016/j.neucom.2018.07.074
UR  - https://www.sciencedirect.com/science/article/pii/S092523121830910X
N2  - In recent years, research on extending linear metric learning models to handle nonlinear structures has attracted great interests. In this paper, we propose a novel nonlinear solution through the utilization of deformable geometric models to learn spatially varying metrics, and apply the strategy to boost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS) are chosen as the geometric model with the consideration of their remarkable expressive power to generate high-order yet smooth deformations. Through TPS-regulated space transformations, we are able to pull same-class neighbors closer while keeping different-class samples away from each other to improve kNN classification. For SVMs, the same practice is carried out aiming to make the data samples more linearly separable, in the input space or the kernel induced feature space. Improvements in the performance of kNN and SVM classifications are demonstrated through a number of experiments on synthetic and real-world datasets, with comparisons made with several state-of-the-art metric learning solutions.
ER  - 
TY  - JOUR
T1  - Domain-driven KDD for mining functionally novel rules and linking disjoint medical hypotheses
A1  - Sebastian, Y
A1  - Then, Patrick H H
Y1  - 2011///
KW  -  Data mining methods
KW  -  Interactive data exploration and discovery
KW  -  Medical knowledge support systems
KW  -  Rule interestingness
KW  - Association rules
JF  - Knowledge-Based Systems
VL  - 24
IS  - 5
SP  - 609
EP  - 620
DO  - https://doi.org/10.1016/j.knosys.2011.01.008
UR  - https://www.sciencedirect.com/science/article/pii/S0950705111000207
N2  - Introduction
An important quality of association rules is novelty. However, evaluating rule novelty is AI-hard and has been a serious challenge for most data mining systems.
Objective
In this paper, we introduce functional novelty, a new non-pairwise approach to evaluating rule novelty. A functionally novel rule is interesting as it suggests previously unknown relations between user hypotheses.
Methods
We developed a novel domain-driven KDD framework for discovering functionally novel association rules. Association rules were mined from cardiovascular data sets. At post-processing, domain knowledge-compliant rules were discovered by applying semantic-based filtering based on UMLS ontology. Their knowledge compliance scores were computed against medical knowledge in Pubmed literature. A cardiologist explored possible relationships between several pairs of unknown hypotheses. The functional novelty of each rule was computed based on its likelihood to mediate these relationships.
Results
Highly interesting rules were successfully discovered. For instance, common rules such as diabetes mellitus⇔coronary arteriosclerosis was functionally novel as it mediated a rare association between von Willebrand factor and intracardiac thrombus.
Conclusion
The proposed post-mining domain-driven rule evaluation technique and measures proved to be useful for estimating candidate functionally novel rules with the results validated by a cardiologist.
ER  - 
TY  - JOUR
T1  - Cross-Domain Semantic Web Model for Understanding Multilingual Natural Language Queries: English/Arabic Health/Food Domain Use Case
A1  - Al-Nazer, Ahmed
A1  - Albukhitan, Saeed
A1  - Helmy, Tarek
Y1  - 2016///
KW  -  Arabic Natural Language
KW  -  Question Answering
KW  -  SPARQL
KW  -  Semantic Web Service
KW  - Semantic Web
JF  - Procedia Computer Science
VL  - 83
SP  - 607
EP  - 614
DO  - https://doi.org/10.1016/j.procs.2016.04.138
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916301685
N1  - The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops
N2  - With the growth of the Semantic Web and its applications, the need to use it in different languages, such as Arabic, is becoming more important. Two of the challenges with the Semantic Web technologies are the lack of multilingual support and the complexity of integrating multiple ontologies used by this technology. The objective of this paper is to present efforts that will help users who use the Arabic language to ask natural language questions and then get their semantic representation in SPARQL that allows them to be executed and get the relevant semantic results. This natural language interface makes more use of the cross-domain ontologies and hence improves the understanding of their inquiries, which is needed in some critical domains such as health and food where precise advice is essential. The approach we followed is multilingual and overcomes the limitations in the published relevant systems. With the proposed approach, users who speak Arabic can use the widely published ontologies in English without concern for the translation of their questions. The proposed approach will take care of matching the entered questions with the relevant ontologies to produce their semantic Web queries. The proposed approach has been implemented and empirically evaluated. The experimental results are promising, which will help in improving the awareness and usage of the Semantic Web by different lingual and cultural users.
ER  - 
TY  - JOUR
T1  - NFnetFu: A novel workflow for microbiome data fusion
A1  - Bisht, Vartika
A1  - Acharjee, Animesh
A1  - Gkoutos, Georgios V
Y1  - 2021///
KW  -  Clustering
KW  -  Fuzzy inference
KW  -  Network fusion
KW  - Microbiome
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104556
EP  - 104556
DO  - https://doi.org/10.1016/j.compbiomed.2021.104556
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521003504
N2  - Microbiome data analysis and its interpretation into meaningful biological insights remain very challenging for numerous reasons, perhaps most prominently, due to the need to account for multiple factors, including collinearity, sparsity (excessive zeros) and effect size, that the complex experimental workflow and subsequent downstream data analysis require. Moreover, a meaningful microbiome data analysis necessitates the development of interpretable models that incorporate inferences across available data as well as background biomedical knowledge. We developed a multimodal framework that considers sparsity (excessive zeros), lower effect size, intrinsically microbial correlations, i.e., collinearity, as well as background biomedical knowledge in the form of a cluster-infused enriched network architecture. Finally, our framework also provides a candidate taxa/Operational Taxonomic Unit (OTU) that can be targeted for future validation experiments. We have developed a tool, the term NFnetFU (Neuro Fuzzy network Fusion), that encompasses our framework and have made it freely available at https://github.com/VartikaBisht6197/NFnetFu.
ER  - 
TY  - JOUR
T1  - SVM-based waist circumference estimation using Kinect
A1  - Seo, Dasom
A1  - Kang, Euncheol
A1  - Kim, Yu-mi
A1  - Kim, Sun-Young
A1  - Oh, Il-Seok
A1  - Kim, Min-Gul
Y1  - 2020///
KW  -  Machine learning
KW  -  Waist measurement
KW  - Support vector machine
JF  - Computer Methods and Programs in Biomedicine
VL  - 191
SP  - 105418
EP  - 105418
DO  - https://doi.org/10.1016/j.cmpb.2020.105418
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719303207
N2  - Background and objective
Conventional anthropometric studies using Kinect depth sensors have concentrated on estimating the distances between two points such as height. This paper deals with a novel waist measurement method using SVM regression, further widening spectrum of Kinect's potential applications. Waist circumference is a key index for the diagnosis of abdominal obesity, which has been linked to metabolic syndromes and other related diseases. Yet, the existing measuring method, tape measure, requires a trained personnel and is therefore costly and time-consuming.
Methods
A dataset was constructed by recording both 30 frames of Kinect depth image and careful tape measurement of 19 volunteers by a clinical investigator. This paper proposes a new SVM regressor-based approach for estimating waist circumference. A waist curve vector is extracted from a raw depth image using joint information provided by Kinect SDK. To avoid overfitting, a data augmentation technique is devised. The 30 frontal vectors and 30 backside vectors, each sampled for 1 s per person, are combined to form 900 waist curve vectors and a total of 17,100 samples were collected from 19 individuals. On an individual basis, we performed leave-one-out validation using the SVM regressor with the tape measurement-gold standard of waist circumference measurement-values labeled as ground-truth. On an individual basis, we performed leave-one-out validation using the SVM regressor with the tape measurement-gold standard of waist circumference measurement-values labeled as ground-truth.
Results
The mean error of the SVM regressor was 4.62 cm, which was smaller than that of the geometric estimation method. Potential uses are discussed.
Conclusions
A possible method for measuring waist circumference using a depth sensor is demonstrated through experimentation. Methods for improving accuracy in the future are presented. Combined with other potential applications of Kinect in healthcare setting, the proposed method will pave the way for patient-centric approach of delivering care without laying burdens on patients.
ER  - 
TY  - JOUR
T1  - A kernel-based sparsity preserving method for semi-supervised classification
A1  - Gu, Nannan
A1  - Wang, Di
A1  - Fan, Mingyu
A1  - Meng, Deyu
Y1  - 2014///
KW  -  Feature extraction
KW  -  Manifold regularization
KW  -  Semi-supervised classification
KW  -  Semi-supervised learning
KW  - Sparse representation
JF  - Neurocomputing
VL  - 139
SP  - 345
EP  - 356
DO  - https://doi.org/10.1016/j.neucom.2014.02.022
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214004159
N2  - In this paper, we propose an effective approach to semi-supervised classification through kernel-based sparse representation. The new method computes the sparse representation of data in the feature space, and then the learner is subject to a cost function which aims to preserve the sparse representing coefficients. By mapping the data into the feature space, the so-called “l2-norm problem” that may be encountered when directly applying sparse representations to non-image data classification tasks will be naturally alleviated, and meanwhile, the label of a data point can be reconstructed more precisely by the labels of other data points using the sparse representing coefficients. Inherited from sparse representation, our method can adaptively establish the relationship between data points, and has high discriminative ability. Furthermore, the new method has a natural multi-class explicit expression for new samples. Experimental results on several benchmark data sets are provided to show the effectiveness of our method.
ER  - 
TY  - JOUR
T1  - A novel Domain Adaptive Deep Recurrent Network for multivariate time series prediction
A1  - Yang, Tao
A1  - Yu, Xia
A1  - Ma, Ning
A1  - Zhao, Yuhang
A1  - Li, Hongru
Y1  - 2021///
KW  -  Deep learning
KW  -  Domain adaptation
KW  -  Recurrent neural network
KW  -  Transfer learning
KW  - Multivariate time series prediction
JF  - Engineering Applications of Artificial Intelligence
VL  - 106
SP  - 104498
EP  - 104498
DO  - https://doi.org/10.1016/j.engappai.2021.104498
UR  - https://www.sciencedirect.com/science/article/pii/S0952197621003468
N2  - Multivariate time series prediction has attracted growing interest in many research fields. Recently, deep learning has been applied to multivariate time series prediction and has achieved encouraging results. However, in real-world scenarios, the insufficient data of multivariate time series at the beginning of the observation causes the deep learning model unable to exert its expected performance. Furthermore, there is the distribution discrepancy between different multivariate time series caused by many factors, making it unfeasible to reuse existing data or models directly. Therefore, a novel Domain Adaptive Deep Recurrent Network (DADRN) is proposed for multivariate time series prediction with insufficient data, which transferring the knowledge of the target-related time series (source domain) to the target time series (target domain) by minimizing distribution mismatch in the feature sharing space. The DADRN automatically learns the temporal dependence of predictive time series and the dynamic dependencies between multiple time variables through the deep recurrent neural network. Besides, a special transfer learning method, domain adaptation, is embedded in the constructed deep recurrent network to reduce the distribution discrepancy between different domains. The proposed domain independence strategy and domain weighted loss further enhance the DADRN’s transfer learning capability by improving the distribution estimation of the target domain and balancing the network’s learning on two domains. The reasonable combination of deep recurrent network and domain adaptation endows DADRN with favorable transfer learning capability, and its effectiveness is demonstrated by the experimental results on two real-world datasets.
ER  - 
TY  - JOUR
T1  - A Web-based Platform for Automated Diabetic Retinopathy Screening
A1  - Arenas-Cavalli, José Tomás
A1  - Ríos, Sebastián A
A1  - Pola, Mariano
A1  - Donoso, Rodrigo
Y1  - 2015///
KW  -  Digital image processing
KW  -  Extended-minima transform
KW  -  Fuzzy c-means (FCM)
KW  -  NN
KW  -  Neural networks
KW  -  Pattern recognition
KW  -  Web-based platform
KW  - Diabetic retinopathy
JF  - Procedia Computer Science
VL  - 60
SP  - 557
EP  - 563
DO  - https://doi.org/10.1016/j.procs.2015.08.179
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915023066
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings
N2  - Diabetic patients are encouraged to undergo frequent retinal examination because of diabetic retinopathy (DR): the most common cause of blindness in working population. As this population is generally too large for healthcare systems, exam reviewing must be optimized. Therefore, a web-based application is proposed. It features user interfaces for healthcare professionals, including ophthalmologists, and an automated DR detection module (by means of image processing and diverse computational intelligence techniques) that allows to relieve their work load. Overall automated system performance reaches 91.9% and 65.2% in terms of sensitivity and specificity, respectively.
ER  - 
TY  - JOUR
T1  - GCHAR: An efficient Group-based Context—aware human activity recognition on smartphone
A1  - Cao, Liang
A1  - Wang, Yufeng
A1  - Zhang, Bo
A1  - Jin, Qun
A1  - Vasilakos, Athanasios V
Y1  - 2018///
KW  -  Context awareness
KW  -  Hierarchical classifier
KW  -  Machine learning
KW  - Human Activity Recognition (HAR)
JF  - Journal of Parallel and Distributed Computing
VL  - 118
SP  - 67
EP  - 80
DO  - https://doi.org/10.1016/j.jpdc.2017.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S0743731517301582
N2  - With smartphones increasingly becoming ubiquitous and being equipped with various sensors, nowadays, there is a trend towards implementing HAR (Human Activity Recognition) algorithms and applications on smartphones, including health monitoring, self-managing system and fitness tracking. However, one of the main issues of the existing HAR schemes is that the classification accuracy is relatively low, and in order to improve the accuracy, high computation overhead is needed. In this paper, an efficient Group-based Context-aware classification method for human activity recognition on smartphones, GCHAR is proposed, which exploits hierarchical group-based scheme to improve the classification efficiency, and reduces the classification error through context awareness rather than the intensive computation. Specifically, GCHAR designs the two-level hierarchical classification structure, i.e., inter-group and inner-group, and utilizes the previous state and transition logic (so-called context awareness) to detect the transitions among activity groups. In comparison with other popular classifiers such as RandomTree, Bagging, J48, BayesNet, KNN and Decision Table, thorough experiments on the realistic dataset (UCI HAR repository) demonstrate that GCHAR achieves the best classification accuracy, reaching 94.1636%, and time consumption in training stage of GCHAR is four times shorter than the simple Decision Table and is decreased by 72.21% in classification stage in comparison with BayesNet.
ER  - 
TY  - JOUR
T1  - Similarity-based health risk prediction using Domain Fusion and electronic health records data
A1  - Guo, Jia
A1  - Yuan, Chi
A1  - Shang, Ning
A1  - Zheng, Tian
A1  - Bello, Natalie A
A1  - Kiryluk, Krzysztof
A1  - Weng, Chunhua
A1  - Wang, Shuang
Y1  - 2021///
KW  -  Clinical prediction tools
KW  -  Domain fusion
KW  -  Similarity
KW  - Patient domain
JF  - Journal of Biomedical Informatics
VL  - 116
SP  - 103711
EP  - 103711
DO  - https://doi.org/10.1016/j.jbi.2021.103711
UR  - https://www.sciencedirect.com/science/article/pii/S153204642100040X
N2  - Electronic Health Record (EHR) data represents a valuable resource for individualized prospective prediction of health conditions. Statistical methods have been developed to measure patient similarity using EHR data, mostly using clinical attributes. Only a handful of recent methods have combined clinical analytics with other forms of similarity analytics, and no unified framework exists yet to measure comprehensive patient similarity. Here, we developed a generic framework named Patient similarity based on Domain Fusion (PsDF). PsDF performs patient similarity assessment on each available domain data separately, and then integrate the affinity information over various domains into a comprehensive similarity metric. We used the integrated patient similarity to support outcome prediction by assigning a risk score to each patient. With extensive simulations, we demonstrated that PsDF outperformed existing risk prediction methods including a random forest classifier, a regression-based model, and a naïve similarity method, especially when heterogeneous signals exist across different domains. Using PsDF and EHR data extracted from the data warehouse of Columbia University Irving Medical Center, we developed two different clinical prediction tools for two different clinical outcomes: incident cases of end stage kidney disease (ESKD) and severe aortic stenosis (AS) requiring valve replacement. We demonstrated that our new prediction method is scalable to large datasets, robust to random missingness, and generalizable to diverse clinical outcomes.
ER  - 
TY  - JOUR
T1  - Geoblood: A Web Based Tool for Geo-analysis of Biological Data
A1  - Canino, Giovanni
A1  - Scarpino, Mariagrazia
A1  - Cristiano, Francesca
A1  - Mirarchi, Domenico
A1  - Tradigo, Giuseppe
A1  - Guzzi, Pietro H
A1  - Cuda, Giovanni
A1  - Veltri, Pierangelo
Y1  - 2016///
KW  -  Drg
KW  -  Geographical data ;
KW  - Elettronic medical record
JF  - Procedia Computer Science
VL  - 98
SP  - 473
EP  - 478
DO  - https://doi.org/10.1016/j.procs.2016.09.077
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916322220
N1  - The 7th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2016)/The 6th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2016)/Affiliated Workshops
N2  - Clinical records (also known as Electronic Medical Records or EMRs) have been considered as a collection of patients health information. Elettronic Citizen Helth Records store information regarding life style, activities, habits, environment (i.e. water sources or air quality), routine health screening like blood biological analysis. Patients and providers stand to benefit from Internet of Things (IoT) in healthcare. Some uses of healthcare IoT are mobile medical applications or wearable devices that allow patients to capture their health data. Smart personal devices as well as data generator devices (i.e., internet of things) allow to provide such kind of data which may be collected in unique (cloud) database. Such information can then be related to location data within a geographic context. We use clinical anonymized data extracted from a Biological Department of University Magna Graecia Hospital. We show how to include geographic operators to statistical methods and how to analyze environmental data and citizen habits to improve wellness. We report on developing and testing a geo-based system to analyze biological data.
ER  - 
TY  - JOUR
T1  - Monotonic classification: An overview on algorithms, performance measures and data sets
A1  - Cano, José-Ramón
A1  - Gutiérrez, Pedro Antonio
A1  - Krawczyk, Bartosz
A1  - Woźniak, Michał
A1  - García, Salvador
Y1  - 2019///
KW  -  Monotonic data sets
KW  -  Ordinal classification
KW  -  Performance metrics
KW  -  Software
KW  -  Taxonomy
KW  - Monotonic classification
JF  - Neurocomputing
VL  - 341
SP  - 168
EP  - 182
DO  - https://doi.org/10.1016/j.neucom.2019.02.024
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219302383
N2  - Currently, knowledge discovery in databases is an essential first step when identifying valid, novel and useful patterns for decision making. There are many real-world scenarios, such as bankruptcy prediction, option pricing or medical diagnosis, where the classification models to be learned need to fulfill restrictions of monotonicity (i.e. the target class label should not decrease when input attributes values increase). For instance, it is rational to assume that a higher debt ratio of a company should never result in a lower level of bankruptcy risk. Consequently, there is a growing interest from the data mining research community concerning monotonic predictive models. This paper aims to present an overview of the literature in the field, analyzing existing techniques and proposing a taxonomy of the algorithms based on the type of model generated. For each method, we review the quality metrics considered in the evaluation and the different data sets and monotonic problems used in the analysis. In this way, this paper serves as an overview of monotonic classification research in specialized literature and can be used as a functional guide for the field.
ER  - 
TY  - JOUR
T1  - A data-driven approach to referable diabetic retinopathy detection
A1  - Pires, Ramon
A1  - Avila, Sandra
A1  - Wainer, Jacques
A1  - Valle, Eduardo
A1  - Abramoff, Michael D
A1  - Rocha, Anderson
Y1  - 2019///
KW  -  Integrated patient-basis analysis
KW  -  Multi-resolution training
KW  -  Referral
KW  -  Robust feature-extraction augmentation
KW  -  Screening
KW  - Diabetic retinopathy
JF  - Artificial Intelligence in Medicine
VL  - 96
SP  - 93
EP  - 106
DO  - https://doi.org/10.1016/j.artmed.2019.03.009
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718306353
N2  - Prior art on automated screening of diabetic retinopathy and direct referral decision shows promising performance; yet most methods build upon complex hand-crafted features whose performance often fails to generalize.
Objective
We investigate data-driven approaches that extract powerful abstract representations directly from retinal images to provide a reliable referable diabetic retinopathy detector.
Methods
We gradually build the solution based on convolutional neural networks, adding data augmentation, multi-resolution training, robust feature-extraction augmentation, and a patient-basis analysis, testing the effectiveness of each improvement.
Results
The proposed method achieved an area under the ROC curve of 98.2% (95% CI: 97.4–98.9%) under a strict cross-dataset protocol designed to test the ability to generalize — training on the Kaggle competition dataset and testing using the Messidor-2 dataset. With a 5 × 2-fold cross-validation protocol, similar results are achieved for Messidor-2 and DR2 datasets, reducing the classification error by over 44% when compared to most published studies in existing literature.
Conclusion
Additional boost strategies can improve performance substantially, but it is important to evaluate whether the additional (computation- and implementation-) complexity of each improvement is worth its benefits. We also corroborate that novel families of data-driven methods are the state of the art for diabetic retinopathy screening. Significance: By learning powerful discriminative patterns directly from available training retinal images, it is possible to perform referral diagnostics without detecting individual lesions.
ER  - 
TY  - JOUR
T1  - MS-electronic nose performance improvement using the retention time dimension and two-way and three-way data processing methods
A1  - Burian, Cosmin
A1  - Brezmes, Jesus
A1  - Vinaixa, Maria
A1  - Cañellas, Nicolau
A1  - Llobet, Eduard
A1  - Vilanova, Xavier
A1  - Correig, Xavier
Y1  - 2010///
KW  -  Electronic nose
KW  -  Fuzzy Artmap
KW  -  GC–MS
KW  -  PARAFAC
KW  -  n-PLS
KW  - Multi-wall
JF  - Sensors and Actuators B: Chemical
VL  - 143
IS  - 2
SP  - 759
EP  - 768
DO  - https://doi.org/10.1016/j.snb.2009.10.015
UR  - https://www.sciencedirect.com/science/article/pii/S0925400509007837
N2  - In order to improve the mass spectra (MS)-based electronic nose (E-nose) performance, we have included the retention time data given by a new E-nose configuration based on a gas chromatograph–mass spectrometer (GC–MS) as a third dimension. The primary aim of this work is to show that the addition of the third dimension is useful, and brings extra information, helping in the classification of samples. By using this extra information our second goal is to optimize the chromatographic method in order to shorten the time of the chromatographic analysis to a minimum, while still having acceptable results. An experiment was designed in the form of 20 solutions with a high degree of similarity in mass spectra and chromatographic retention times. In order to optimize the system performance and reduce the time of the measurements to a minimum two different chromatographic methods were evaluated. By analyzing these solutions with two-way and three-way PCA, PARAFAC, PLS-DA and n-PLS-DA, and concatenated with supervised Fuzzy Artmap paradigms, we show that the addition of the extra information in the form of the chromatographic separation, even when using a short chromatographic separation, improves the results obtained, compared to the two-way analysis of the mass spectra or total ion chromatogram (TIC) alone. A third goal was to see which signal processing approach was the best suited. We found that when the retention time is used as a third dimension when chromatographic peaks are well resolved, two-way methods work better than their three-way counterparts, whereas in the case of a more challenging situation (a more coeluted chromatogram, with a much shorter measurement time) three-way methods perform better than classic two-way approaches.
ER  - 
TY  - JOUR
T1  - A method for the graphical modeling of relative temporal constraints
A1  - Mate, Sebastian
A1  - Bürkle, Thomas
A1  - Kapsner, Lorenz A
A1  - Toddenroth, Dennis
A1  - Kampf, Marvin O
A1  - Sedlmayr, Martin
A1  - Castellanos, Ixchel
A1  - Prokosch, Hans-Ulrich
A1  - Kraus, Stefan
Y1  - 2019///
KW  -  Data integration
KW  -  Data retrieval
KW  -  Patient cohort identification
KW  -  Phenotyping
KW  -  Phenotyping algorithms
KW  - Temporal queries
JF  - Journal of Biomedical Informatics
VL  - 100
SP  - 103314
EP  - 103314
DO  - https://doi.org/10.1016/j.jbi.2019.103314
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302333
N2  - Searching for patient cohorts in electronic patient data often requires the definition of temporal constraints between the selection criteria. However, beyond a certain degree of temporal complexity, the non-graphical, form-based approaches implemented in current translational research platforms may be limited when modeling such constraints. In our opinion, there is a need for an easily accessible and implementable, fully graphical method for creating temporal queries. We aim to respond to this challenge with a new graphical notation. Based on Allen’s time interval algebra, it allows for modeling temporal queries by arranging simple horizontal bars depicting symbolic time intervals. To make our approach applicable to complex temporal patterns, we apply two extensions: with duration intervals, we enable the inference about relative temporal distances between patient events, and with time interval modifiers, we support counting and excluding patient events, as well as constraining numeric values. We describe how to generate database queries from this notation. We provide a prototypical implementation, consisting of a temporal query modeling frontend and an experimental backend that connects to an i2b2 system. We evaluate our modeling approach on the MIMIC-III database to demonstrate that it can be used for modeling typical temporal phenotyping queries.
ER  - 
TY  - JOUR
T1  - SemBioNLQA: A semantic biomedical question answering system for retrieving exact and ideal answers to natural language questions
A1  - Sarrouti, Mourad
A1  - Ouatik El Alaoui, Said
Y1  - 2020///
KW  -  BioASQ
KW  -  Biomedical informatics
KW  -  Information retrieval
KW  -  Machine learning
KW  -  Natural language processing
KW  -  Passage retrieval
KW  - Biomedical question answering
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101767
EP  - 101767
DO  - https://doi.org/10.1016/j.artmed.2019.101767
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718302756
N2  - Background and objective
Question answering (QA), the identification of short accurate answers to users questions written in natural language expressions, is a longstanding issue widely studied over the last decades in the open-domain. However, it still remains a real challenge in the biomedical domain as the most of the existing systems support a limited amount of question and answer types as well as still require further efforts in order to improve their performance in terms of precision for the supported questions. Here, we present a semantic biomedical QA system named SemBioNLQA which has the ability to handle the kinds of yes/no, factoid, list, and summary natural language questions.
Methods
This paper describes the system architecture and an evaluation of the developed end-to-end biomedical QA system named SemBioNLQA, which consists of question classification, document retrieval, passage retrieval and answer extraction modules. It takes natural language questions as input, and outputs both short precise answers and summaries as results. The SemBioNLQA system, dealing with four types of questions, is based on (1) handcrafted lexico-syntactic patterns and a machine learning algorithm for question classification, (2) PubMed search engine and UMLS similarity for document retrieval, (3) the BM25 model, stemmed words and UMLS concepts for passage retrieval, and (4) UMLS metathesaurus, BioPortal synonyms, sentiment analysis and term frequency metric for answer extraction.
Results and conclusion
Compared with the current state-of-the-art biomedical QA systems, SemBioNLQA, a fully automated system, has the potential to deal with a large amount of question and answer types. SemBioNLQA retrieves quickly users’ information needs by returning exact answers (e.g., “yes”, “no”, a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, whereas it provides only the ideal answers for summary questions. Moreover, experimental evaluations performed on biomedical questions and answers provided by the BioASQ challenge especially in 2015, 2016 and 2017 (as part of our participation), show that SemBioNLQA achieves good performances compared with the most current state-of-the-art systems and allows a practical and competitive alternative to help information seekers find exact and ideal answers to their biomedical questions. The SemBioNLQA source code is publicly available at https://github.com/sarrouti/sembionlqa.
ER  - 
TY  - JOUR
T1  - GT-Finder: Classify the family of glucose transporters with pre-trained BERT language models
A1  - Ali Shah, Syed Muazzam
A1  - Taju, Semmy Wellem
A1  - Ho, Quang-Thai
A1  - Nguyen, Trinh-Trung-Duong
A1  - Ou, Yu-Yen
Y1  - 2021///
KW  -  Bidirectional encoder representations from transformers
KW  -  Contextualized word embedding
KW  -  Feature importance
KW  -  Glucose transporter
KW  - BERT
JF  - Computers in Biology and Medicine
VL  - 131
SP  - 104259
EP  - 104259
DO  - https://doi.org/10.1016/j.compbiomed.2021.104259
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521000536
N2  - Recently, language representation models have drawn a lot of attention in the field of natural language processing (NLP) due to their remarkable results. Among them, BERT (Bidirectional Encoder Representations from Transformers) has proven to be a simple, yet powerful language model that has achieved novel state-of-the-art performance. BERT adopted the concept of contextualized word embeddings to capture the semantics and context in which words appear. We utilized pre-trained BERT models to extract features from protein sequences for discriminating three families of glucose transporters: the major facilitator superfamily of glucose transporters (GLUTs), the sodium-glucose linked transporters (SGLTs), and the sugars will eventually be exported transporters (SWEETs). We treated protein sequences as sentences and transformed them into fixed-length meaningful vectors where a 768- or 1024-dimensional vector represents each amino acid. We observed that BERT-Base and BERT-Large models improved the performance by more than 4% in terms of average sensitivity and Matthews correlation coefficient (MCC), indicating the efficiency of this approach. We also developed a bidirectional transformer-based protein model (TransportersBERT) for comparison with existing pre-trained BERT models.
ER  - 
TY  - JOUR
T1  - Personalized mental stress detection with self-organizing map: From laboratory to the field
A1  - Tervonen, Jaakko
A1  - Puttonen, Sampsa
A1  - Sillanpää, Mikko J
A1  - Hopsu, Leila
A1  - Homorodi, Zsolt
A1  - Keränen, Janne
A1  - Pajukanta, Janne
A1  - Tolonen, Antti
A1  - Lämsä, Arttu
A1  - Mäntyjärvi, Jani
Y1  - 2020///
KW  -  Clustering
KW  -  Machine learning
KW  -  Personalization
KW  -  Stress detection
KW  -  Unsupervised learning
KW  - Behavior
JF  - Computers in Biology and Medicine
VL  - 124
SP  - 103935
EP  - 103935
DO  - https://doi.org/10.1016/j.compbiomed.2020.103935
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520302717
N2  - Stress has become a major health concern and there is a need to study and develop new digital means for real-time stress detection. Currently, the majority of stress detection research is using population based approaches that lack the capability to adapt to individual differences. They also use supervised learning methods, requiring extensive labeling of training data, and they are typically tested on data collected in a laboratory and thus do not generalize to field conditions. To address these issues, we present multiple personalized models based on an unsupervised algorithm, the Self-Organizing Map (SOM), and we propose an algorithmic pipeline to apply the method for both laboratory and field data. The performance is evaluated on a dataset of physiological measurements from a laboratory test and on a field dataset consisting of four weeks of physiological and smartphone usage data. In these tests, the performance on the field data was steady across the different personalization levels (accuracy around 60%) and a fully personalized model performed the best on the laboratory data, achieving accuracy of 92% which is comparable to state-of-the-art supervised classifiers. These results demonstrate the feasibility of SOM in personalized mental stress detection both in constrained and free-living environment.
ER  - 
TY  - JOUR
T1  - Guided Stochastic Gradient Descent Algorithm for inconsistent datasets
A1  - Sharma, Anuraganand
Y1  - 2018///
KW  -  Classification
KW  -  Greedy selection
KW  -  Guided Stochastic Gradient Descent Algorithm
KW  -  Logistic regression
KW  -  Machine learning
KW  -  Neural networks
KW  - Stochastic Gradient Descent Algorithm
JF  - Applied Soft Computing
VL  - 73
SP  - 1068
EP  - 1080
DO  - https://doi.org/10.1016/j.asoc.2018.09.038
UR  - https://www.sciencedirect.com/science/article/pii/S156849461830557X
N2  - Stochastic Gradient Descent (SGD) Algorithm, despite its simplicity, is considered an effective and default standard optimization algorithm for machine learning classification models such as neural networks and logistic regression. However, SGD’s gradient descent is biased towards the random selection of a data instance. In this paper, it has been termed as data inconsistency. The proposed variation of SGD, Guided Stochastic Gradient Descent (GSGD) Algorithm, tries to overcome this inconsistency in a given dataset through greedy selection of consistent data instances for gradient descent. The empirical test results show the efficacy of the method. Moreover, GSGD has also been incorporated and tested with other popular variations of SGD, such as Adam, Adagrad and Momentum. The guided search with GSGD achieves better convergence and classification accuracy in a limited time budget than its original counterpart of canonical and other variation of SGD. Additionally, it maintains the same efficiency when experimented on medical benchmark datasets with logistic regression for classification.
ER  - 
TY  - JOUR
T1  - Discovering Context of Labeled Text Documents Using Context Similarity Coefficient
A1  - Kulkarni, Anagha
A1  - Tokekar, Vrinda
A1  - Kulkarni, Parag
Y1  - 2015///
KW  -  context closeness value
KW  -  pattern based clustering
KW  -  text mining
KW  - Context discovery
JF  - Procedia Computer Science
VL  - 49
SP  - 118
EP  - 127
DO  - https://doi.org/10.1016/j.procs.2015.04.235
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915007449
N1  - Proceedings of 4th International Conference on Advances in Computing, Communication and Control (ICAC3'15)
N2  - To find closeness between two data points, traditional distance based closeness measurement calculates distance between two data points. However, it fails to capture behaviour of data series. Behaviour of data series can be captured by association and disassociation between patterns of data points. This can reflect closeness between them. The same concept can be applied to find association between text documents. Using this philosophy, this paper proposes a novel approach of document association based on context similarity coe_cient (CSC). CSC based document association helps to capture contextual relationship between documents. Experiments conducted on standard datasets such as Reuters-21578 and RCV1 show that CSC successfully finds closeness between the documents.
ER  - 
TY  - JOUR
T1  - Local structured feature learning with dynamic maximum entropy graph
A1  - Wang, Zheng
A1  - Nie, Feiping
A1  - Wang, Rong
A1  - Yang, Hui
A1  - Li, Xuelong
Y1  - 2021///
KW  -  Dynamic maximum entropy graph
KW  -  Local structured feature learning
KW  -  ℓ-Norm constraint optimization
KW  - Supervised dimensionality reduction
JF  - Pattern Recognition
VL  - 111
SP  - 107673
EP  - 107673
DO  - https://doi.org/10.1016/j.patcog.2020.107673
UR  - https://www.sciencedirect.com/science/article/pii/S0031320320304763
N2  - In recent years, Linear Discriminant Analysis (LDA) has seen huge adoption in data mining applications. Due to its globality, it is incompetent to handle multimodal data. Besides, most of LDA’s variants learn the projection matrix based on the pre-defined similarity matrix, which is easily affected by noisy and irrelevant features. To address above two issues, a novel local structured feature learning with Dynamic Maximum Entropy Graph (DMEG) method is developed which firstly develops a more discriminative LDA with whitening constraint that can minimize the within-class scatter while keeping the total samples scatter unchanged simultaneously. Second, for exploring the local structure of data, the ℓ0-norm constraint is imposed on similarity matrix to ensure the k connectivity on graph. More importantly, proposed model learns the similarity and projection matrix simultaneously to ensure that the neighborships can be found in the optimal subspace where the noise have been removed already. Moreover, a maximum entropy regularization is employed to reinforce the discriminability of graph and avoid the trivial solution. Last but not least, an efficient iterative optimization algorithm is provided to optimize proposed model with a NP-hard constraint. Extensive experiments conducted on synthetic and several real-world data sets demonstrate the efficiency in classification task and robustness to noise of proposed method.
ER  - 
TY  - JOUR
T1  - Multiclass magnetic resonance imaging brain tumor classification using artificial intelligence paradigm
A1  - Tandel, Gopal S
A1  - Balestrieri, Antonella
A1  - Jujaray, Tanay
A1  - Khanna, Narender N
A1  - Saba, Luca
A1  - Suri, Jasjit S
Y1  - 2020///
KW  -  Artificial intelligence
KW  -  Benchmarking
KW  -  Classification
KW  -  Convolution neural network
KW  -  Machine learning
KW  -  Performance
KW  -  Transfer learning
KW  -  Validation
KW  -  Verification
KW  - Tumour grading system
JF  - Computers in Biology and Medicine
VL  - 122
SP  - 103804
EP  - 103804
DO  - https://doi.org/10.1016/j.compbiomed.2020.103804
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520301724
N2  - Motivation
Brain or central nervous system cancer is the tenth leading cause of death in men and women. Even though brain tumour is not considered as the primary cause of mortality worldwide, 40% of other types of cancer (such as lung or breast cancers) are transformed into brain tumours due to metastasis. Although the biopsy is considered as the gold standard for cancer diagnosis, it poses several challenges such as low sensitivity/specificity, risk during the biopsy procedure, and relatively long waiting times for the biopsy results. Due to an increase in the sheer volume of patients with brain tumours, there is a need for a non-invasive, automatic computer-aided diagnosis tool that can automatically diagnose and estimate the grade of a tumour accurately within a few seconds.
Method
Five clinically relevant multiclass datasets (two-, three-, four-, five-, and six-class) were designed. A transfer-learning-based Artificial Intelligence paradigm using a Convolutional Neural Network (CCN) was proposed and led to higher performance in brain tumour grading/classification using magnetic resonance imaging (MRI) data. We benchmarked the transfer-learning-based CNN model against six different machine learning (ML) classification methods, namely Decision Tree, Linear Discrimination, Naive Bayes, Support Vector Machine, K-nearest neighbour, and Ensemble.
Results
The CNN-based deep learning (DL) model outperforms the six types of ML models when considering five types of multiclass tumour datasets. These five types of data are two-, three-, four-, five, and six-class. The CNN-based AlexNet transfer learning system yielded mean accuracies derived from three kinds of cross-validation protocols (K2, K5, and K10) of 100, 95.97, 96.65, 87.14, and 93.74%, respectively. The mean areas under the curve of DL and ML were found to be 0.99 and 0.87, respectively, for p < 0.0001, and DL showed a 12.12% improvement over ML. Multiclass datasets were benchmarked against the TT protocol (where training and testing samples are the same). The optimal model was validated using a statistical method of a tumour separation index and verified on synthetic data consisting of eight classes.
Conclusion
The transfer-learning-based AI system is useful in multiclass brain tumour grading and shows better performance than ML systems.
ER  - 
TY  - JOUR
T1  - A high performance hardware architecture for portable, low-power retinal vessel segmentation
A1  - Koukounis, Dimitris
A1  - Ttofis, Christos
A1  - Papadopoulos, Agathoklis
A1  - Theocharides, Theocharis
Y1  - 2014///
KW  -  Hardware acceleration
KW  -  On-site medical diagnostics
KW  -  Portable biometrics
KW  -  Reconfigurable parallel architectures
KW  - Retinal vessel segmentation
JF  - Integration
VL  - 47
IS  - 3
SP  - 377
EP  - 386
DO  - https://doi.org/10.1016/j.vlsi.2013.11.005
UR  - https://www.sciencedirect.com/science/article/pii/S016792601300076X
N1  - Special issue: VLSI for the new era
N2  - The retina of the human eye and more particularly the retinal blood vasculature can be used in several medical and biometric applications. The use of retinal images in such applications however, is computationally intensive, due to the high complexity of the algorithms used to extract the vessels from the retina. In addition, the emergence of portable biometric authentication applications, as well as onsite biomedical diagnostics raises the need for real-time, power-efficient implementations of such algorithms that can also satisfy the performance and accuracy requirements of portable systems that use retinal images. In an attempt to meet those requirements, this work presents a VLSI implementation of a retina vessel segmentation system while exploring various parameters that affect the power consumption, the accuracy and performance of the system. The proposed design implements an unsupervised vessel segmentation algorithm which utilizes matched filtering with signed integers to enhance the difference between the blood vessels and the rest of the retina. The design accelerates the process of obtaining a binary map of the vessels tree by using parallel processing and efficient resource sharing, achieving real-time performance. The design has been verified on a commercial FPGA platform and exhibits significant performance improvements (up to 90×) when compared to other existing hardware and software implementations, with an overall accuracy of 92.4%. Furthermore, the low power consumption of the proposed VLSI implementation enables the proposed architecture to be used in portable systems, as it achieves an efficient balance between performance, power consumption and accuracy.
ER  - 
TY  - JOUR
T1  - Retinal vessel segmentation in colour fundus images using Extreme Learning Machine
A1  - Zhu, Chengzhang
A1  - Zou, Beiji
A1  - Zhao, Rongchang
A1  - Cui, Jinkai
A1  - Duan, Xuanchu
A1  - Chen, Zailiang
A1  - Liang, Yixiong
Y1  - 2017///
KW  -  Computer-aided diagnosis
KW  -  Feature extraction
KW  -  Retinal vessel segmentation
KW  -  Supervised learning
KW  - Colour fundus image
JF  - Computerized Medical Imaging and Graphics
VL  - 55
SP  - 68
EP  - 77
DO  - https://doi.org/10.1016/j.compmedimag.2016.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S0895611116300416
N1  - Special Issue on Ophthalmic Medical Image Analysis
N2  - Attributes of the retinal vessel play important role in systemic conditions and ophthalmic diagnosis. In this paper, a supervised method based on Extreme Learning Machine (ELM) is proposed to segment retinal vessel. Firstly, a set of 39-D discriminative feature vectors, consisting of local features, morphological features, phase congruency, Hessian and divergence of vector fields, is extracted for each pixel of the fundus image. Then a matrix is constructed for pixel of the training set based on the feature vector and the manual labels, and acts as the input of the ELM classifier. The output of classifier is the binary retinal vascular segmentation. Finally, an optimization processing is implemented to remove the region less than 30 pixels which is isolated from the retinal vascilar. The experimental results testing on the public Digital Retinal Images for Vessel Extraction (DRIVE) database demonstrate that the proposed method is much faster than the other methods in segmenting the retinal vessels. Meanwhile the average accuracy, sensitivity, and specificity are 0.9607, 0.7140 and 0.9868, respectively. Moreover the proposed method exhibits high speed and robustness on a new Retinal Images for Screening (RIS) database. Therefore it has potential applications for real-time computer-aided diagnosis and disease screening.
ER  - 
TY  - JOUR
T1  - Using classification techniques for statistical analysis of Anemia
A1  - Meena, Kanak
A1  - Tayal, Devendra K
A1  - Gupta, Vaidehi
A1  - Fatima, Aiman
Y1  - 2019///
KW  -  Anemia
KW  -  Associative classification
KW  -  Decision tree
KW  -  Healthcare
KW  - Data mining
JF  - Artificial Intelligence in Medicine
VL  - 94
SP  - 138
EP  - 152
DO  - https://doi.org/10.1016/j.artmed.2019.02.005
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718301313
N2  - Anemia in children is becoming a worldwide problem owing to the unawareness among people regarding the disease, its causes and preventive measures. This study develops a decision support system using data mining techniques that are applied to a database containing data about nutritional factors for children. The data set was taken from NFHS-4, a survey conducted by the Government of India in 2015–16. The work attempts to predict anemia among children and establish a relation between mother’s health and diet during pregnancy and its effects on anemic status of her child. It aims to help parents and clinicians to understand the influence of an infant’s feeding practices and diet on his/her health and provide guidelines regarding diet to prevent anemia. Earlier, systems were built on computer using medical experts’ advicewhich was then translated into algorithms for use. However, this method was time consuming thus, artificial intelligence came into play utilizing knowledge discovery and data mining tools for predictive modeling. The two techniques, decision tree and association rule mining has been applied and compared to select more appropriate technique for this particular task and a model is proposed in the healthcare domain with the aim to reduce the risk of the blood-related disease anemia.
ER  - 
TY  - JOUR
T1  - A decision support system for cost-effective diagnosis
A1  - Chi, Chih-Lin
A1  - Street, W Nick
A1  - Katz, David A
Y1  - 2010///
KW  -  Cost-effective diagnosis
KW  -  Feature selection
KW  -  Machine learning
KW  -  Optimization
KW  -  Utility-based data mining
KW  - Decision support systems
JF  - Artificial Intelligence in Medicine
VL  - 50
IS  - 3
SP  - 149
EP  - 161
DO  - https://doi.org/10.1016/j.artmed.2010.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365710001053
N2  - Objective
Speed, cost, and accuracy are three important goals in disease diagnosis. This paper proposes a machine learning-based expert system algorithm to optimize these goals and assist diagnostic decisions in a sequential decision-making setting.
Methods
The algorithm consists of three components that work together to identify the sequence of diagnostic tests that attains the treatment or no test threshold probability for a query case with adequate certainty: lazy-learning classifiers, confident diagnosis, and locally sequential feature selection (LSFS). Speed-based and cost-based objective functions can be used as criteria to select tests.
Results
Results of four different datasets are consistent. All LSFS functions significantly reduce tests and costs. Average cost savings for heart disease, thyroid disease, diabetes, and hepatitis datasets are 50%, 57%, 22%, and 34%, respectively. Average test savings are 55%, 73%, 24%, and 39%, respectively. Accuracies are similar to or better than the baseline (the classifier that uses all available tests in the dataset).
Conclusion
We have demonstrated a new approach that dynamically estimates and determines the optimal sequence of tests that provides the most information (or disease probability) based on a patient's available information.
ER  - 
TY  - JOUR
T1  - Class proximity measures – Dissimilarity-based classification and display of high-dimensional data
A1  - Somorjai, R L
A1  - Dolenko, B
A1  - Nikulin, A
A1  - Roberson, W
A1  - Thiessen, N
Y1  - 2011///
KW  -  Class-proximity planes
KW  -  Classification
KW  -  Distance/dissimilarity measures
KW  -  High-dimensional data
KW  -  Projections
KW  -  Proximity measures
KW  -  Visualization
KW  - Mappings
JF  - Journal of Biomedical Informatics
VL  - 44
IS  - 5
SP  - 775
EP  - 788
DO  - https://doi.org/10.1016/j.jbi.2011.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S1532046411000682
N2  - For two-class problems, we introduce and construct mappings of high-dimensional instances into dissimilarity (distance)-based Class-Proximity Planes. The Class Proximity Projections are extensions of our earlier relative distance plane mapping, and thus provide a more general and unified approach to the simultaneous classification and visualization of many-feature datasets. The mappings display all L-dimensional instances in two-dimensional coordinate systems, whose two axes represent the two distances of the instances to various pre-defined proximity measures of the two classes. The Class Proximity mappings provide a variety of different perspectives of the dataset to be classified and visualized. We report and compare the classification and visualization results obtained with various Class Proximity Projections and their combinations on four datasets from the UCI data base, as well as on a particular high-dimensional biomedical dataset.
ER  - 
TY  - JOUR
T1  - Machine learning models for synthesizing actionable care decisions on lower extremity wounds
A1  - Nguyen, Holly
A1  - Agu, Emmanuel
A1  - Tulu, Bengisu
A1  - Strong, Diane
A1  - Mombini, Haadi
A1  - Pedersen, Peder
A1  - Lindsay, Clifford
A1  - Dunn, Raymond
A1  - Loretz, Lorraine
Y1  - 2020///
KW  -  Chronic wounds
KW  -  Lower extremity ulcers
KW  -  Machine learning
KW  - Classification
JF  - Smart Health
VL  - 18
SP  - 100139
EP  - 100139
DO  - https://doi.org/10.1016/j.smhl.2020.100139
UR  - https://www.sciencedirect.com/science/article/pii/S2352648320300313
N2  - Lower extremity chronic wounds affect 4.5 million Americans annually. Due to inadequate access to wound experts in underserved areas, many patients receive non-uniform, non-standard wound care, resulting in increased costs and lower quality of life. We explored machine learning classifiers to generate actionable wound care decisions about four chronic wound types (diabetic foot, pressure, venous, and arterial ulcers). These decisions (target classes) were: (1) Continue current treatment, (2) Request non-urgent change in treatment from a wound specialist, (3) Refer patient to a wound specialist. We compare classification methods (single classifiers, bagged & boosted ensembles, and a deep learning network) to investigate (1) whether visual wound features are sufficient for generating a decision and (2) whether adding unstructured text from wound experts increases classifier accuracy. Using 205 wound images, the Gradient Boosted Machine (XGBoost) outperformed other methods when using both visual and textual wound features, achieving 81% accuracy. Using only visual features decreased the accuracy to 76%, achieved by a Support Vector Machine classifier. We conclude that machine learning classifiers can generate accurate wound care decisions on lower extremity chronic wounds, an important step toward objective, standardized wound care. Higher decision-making accuracy was achieved by leveraging clinical comments from wound experts.
ER  - 
TY  - JOUR
T1  - Performance Improvement Algorithms in Big Data Analysis
A1  - Metsker, Oleg
A1  - Efimov, Egor
A1  - Trofimov, Egor
A1  - Kopanitsa, Georgy
A1  - Bolgova, Ekaterina
A1  - Yakovlev, Alexey
Y1  - 2020///
KW  -  CUDA
KW  -  machine learning
KW  -  performance improvement
KW  - Legal tech
JF  - Procedia Computer Science
VL  - 178
SP  - 386
EP  - 393
DO  - https://doi.org/10.1016/j.procs.2020.11.040
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920324194
N1  - 9th International Young Scientists Conference in Computational Science, YSC2020, 05-12 September 2020
N2  - This article describes the study results of development methods and algorithms for performance improvement in the big data analysis and training of machine learning models. The data growing and complexity problem describing complex processes require new approaches and tools for the scientific and technical community. In the course of research, the algorithms for analysis of heterogeneous medical and law records were performed. The performance improvement in classification, clustering and graph calculation problems were solved. With using CUDA it was possible to get more than 95 times performance. The usage of high-performance technologies is important in the analysis of electronic records because it provides an adequate response to the process of analysis of large scale of data from information systems. This study shows how to speed up the calculations on the example of most basic and widespread machine learning tasks. The results of the study can be used to develop a new generation of decision support systems, interactive data analysis systems and methods for eScience
ER  - 
TY  - JOUR
T1  - Patient Specific Machine Learning Models for ECG Signal Classification
A1  - Pandey, Saroj Kumar
A1  - Janghel, Rekh Ram
A1  - Vani, Vyom
Y1  - 2020///
KW  -  Electrocardiogram
KW  -  Support vector machine
KW  -  ensemble
KW  - Arrhythmia
JF  - Procedia Computer Science
VL  - 167
SP  - 2181
EP  - 2190
DO  - https://doi.org/10.1016/j.procs.2020.03.269
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920307353
N1  - International Conference on Computational Intelligence and Data Science
N2  - Arrhythmia is one of the major cause of deaths across the globe. Almost 17.9 million deaths are caused due to cardiovascular diseases. In order to reduce this much mortality rate, the cardiovascular disease should be properly identified and the proper treatment for the same should be immediately provided to the patients. In this study, a new ensemble based support vector machine (SVM) classifier was proposed to classify heartbeat into four classes from MIT-BIH arrhythmia database. The results were compared with other classifiers that are SVM, Random Forest (RF), K-Nearest Neighbours (KNN), and Long Short Term Memory network. The four features were extracted from the ECG signals that were used by the classifiers are Wavelets, high order statistics, R-R intervals and morphological features. An ensemble of SVMs obtained the best result with an overall accuracy of 94.4%.
ER  - 
TY  - JOUR
T1  - Noise processing in exercise ECG stress test for the analysis and the clinical characterization of QRS and T wave alternans
A1  - Bortolan, G
A1  - Christov, I
A1  - Simova, I
A1  - Dotsinsky, I
Y1  - 2015///
KW  -  Alternans
KW  -  ECG stress test
KW  -  Noise suppression
KW  -  Principal Component Analysis
KW  - Signal filtering
JF  - Biomedical Signal Processing and Control
VL  - 18
SP  - 378
EP  - 385
DO  - https://doi.org/10.1016/j.bspc.2015.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S1746809415000154
N2  - The aim of this study is to analyze different sources of noise in ECG recordings from stress tests in order to obtain reliable parameters and measurements for the analysis of T-wave and QRS-complex alternans (TWA & QRSA). Simple methods for eliminating common sources of noise like power-line interference, baseline drift and electromyographic noise were used. The pre-processing phase considered the detection of steep slope/spike, low amplitude signal, and flat line or missing lead artefacts. The detection of TWA and QRSA was based on Principal Component Analysis indices and wave amplitudes considering all the leads. A particular database of 106 ECG records during stress testing was considered. The signal quality analysis performed in this study has permitted to obtain reliable and noise-tolerant measurements of TWA and QRSA indices. The different diagnostic groups were used for the evaluation of the clinical significance of the alternans. Men have significantly higher values of QRSA than women. Smokers have significantly higher TWA values as compared with non-smokers. Significant negative correlation was obtained between age and both TWA and QRSA. Correlations between TWA and QRSA and the double product of arterial hypertension and the maximal heart rate during the stress test were statistically significant, positive and relatively strong.
ER  - 
TY  - JOUR
T1  - Prognosis Analysis of Heart Failure Based on Recurrent Attention Model
A1  - Gong, J
A1  - Bai, X
A1  - Li, D.-a.
A1  - Zhao, J
A1  - Li, X
Y1  - 2020///
KW  -  Prognosis
KW  -  Recurrent attention model
KW  - Heart failure
JF  - IRBM
VL  - 41
IS  - 2
SP  - 71
EP  - 79
DO  - https://doi.org/10.1016/j.irbm.2019.08.002
UR  - https://www.sciencedirect.com/science/article/pii/S1959031819300661
N2  - Objectives
Heart failure is a group of complex clinical syndromes that lead to ventricular filling or impaired ejection ability due to abnormal heart structure or function. Difficult treatment, poor prognosis and high mortality are the main characteristics of heart failure. According to admission data and past medical use, the 30-day mortality rate of patients with heart failure was obtained and the main characteristics affecting the 30-day mortality of patients with heart failure were determined.
Material and methods
Based on the data of April 2016 to July 2018 of Shanxi Acadeny of Medical Sciences, and we chose 4,682 information on heart failure patients, of which 539 died in the hospital by screening. We built a 30-day mortality prediction model for patients with heart failure. The model can fuse clinical data and text data through multiple kernel learning, and input the fused data into the recurrent attention model. It can not only predict the 30-day mortality of patients with heart failure, but also the influencing factors of prognosis of patients with heart failure were also obtained.
Results
The prediction accuracy of the recurrent attention network is obviously higher than that of other machine learning models, and the accuracy rate reaches 93.4%. The AUC value of the area under the ROC curve of the model reaches 87%, which is obviously higher than that of the traditional machine learning models such as decision tree, naive Bayesian and support vector machine. In addition, the model can also reach a conclusion that New York heart function classification, age, NT—ProBNP, LVEF, β-blockers, ventricular arrhythmia, high blood pressure, coronary heart disease (CHD) and bronchitis were independent risk factors for death. And patients with revascularization, ACEI/ARB drugs, β-blockers, spironolactone have a better prognosis than non-users. This provides an important reference for doctors to better treat and manage patients with heart failure.
Conclusion
Experiments show that the prognostic effect of the recurrent attention model is significantly higher than that of other traditional machine learning models. Because the model increases the attention mechanism, the important features affecting the prognostic results are obtained, which enables doctors to prescribe drugs according to the symptoms, take timely precautions and help patients to treat in time.
ER  - 
TY  - JOUR
T1  - Lightweight deep neural networks for cholelithiasis and cholecystitis detection by point-of-care ultrasound
A1  - Yu, Chih-Jui
A1  - Yeh, Hsing-Jung
A1  - Chang, Chun-Chao
A1  - Tang, Jui-Hsiang
A1  - Kao, Wei-Yu
A1  - Chen, Wen-Chao
A1  - Huang, Yi-Jin
A1  - Li, Chien-Hung
A1  - Chang, Wei-Hao
A1  - Lin, Yun-Ting
A1  - Sufriyana, Herdiantri
A1  - Su, Emily Chia-Yu
Y1  - 2021///
KW  -  Abdomen
KW  -  Computer-aided diagnosis
KW  -  Machine learning
KW  -  Neural network
KW  -  Pattern recognition
KW  - Ultrasound
JF  - Computer Methods and Programs in Biomedicine
VL  - 211
SP  - 106382
EP  - 106382
DO  - https://doi.org/10.1016/j.cmpb.2021.106382
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721004569
N2  - Background and objective
Emergency physicians (EPs) frequently deal with abdominal pain, including that is caused by either gallstones or acute cholecystitis. Easy access and low cost justify point-of-care ultrasound (POCUS) use as a first-line test to detect these diseases; yet, the detection performance of POCUS by EPs is unreliable, causing misdiagnoses with serious impacts. This study aimed to develop a machine learning system to detect and localize gallstones and to detect acute cholecystitis by ultrasound (US) still images taken by physicians or technicians for preliminary diagnoses.
Methods
Abdominal US images (> 89,000) were collected from 2386 patients in a hospital database. We constructed training sets for gallstones with or without cholecystitis (N = 10,971) and cholecystitis with or without gallstones (N = 7348) as positives. Validation sets were also constructed for gallstones (N = 2664) and cholecystitis (N = 1919). We applied a single-shot multibox detector (SSD) and a feature pyramid network (FPN) to classify and localize objects using image features extracted by ResNet-50 for gallstones, and MobileNet V2 to classify cholecystitis. The deep learning models were pretrained using the COCO-2017 and ILSVRC-2012 datasets.
Results
Using the validation sets, the SSD-FPN-ResNet-50 and MobileNet V2 achieved areas under the receiver operating characteristics curve of 0.92 and 0.94, respectively. The inference speeds were 21 (47.6 frames per second, fps) and 7 ms (142.9 fps).
Conclusions
A machine learning system was developed to detect and localize gallstones, and to detect cholecystitis, with acceptable discrimination and speed. This is the first study to develop this system for either gallstone or cholecystitis detection with absence or presence of each one. After clinical trials, this system may be used to assist EPs, including those in remote areas, for detecting these diseases.
ER  - 
TY  - JOUR
T1  - DENSE-INception U-net for medical image segmentation
A1  - Zhang, Ziang
A1  - Wu, Chengdong
A1  - Coleman, Sonya
A1  - Kerr, Dermot
Y1  - 2020///
KW  -  DenseNet
KW  -  GoogLeNet
KW  -  Medical image segmentation
KW  -  U-net
KW  - Deep learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 192
SP  - 105395
EP  - 105395
DO  - https://doi.org/10.1016/j.cmpb.2020.105395
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719307904
N2  - Background and objective
Convolutional neural networks (CNNs) play an important role in the field of medical image segmentation. Among many kinds of CNNs, the U-net architecture is one of the most famous fully convolutional network architectures for medical semantic segmentation tasks. Recent work shows that the U-net network can be substantially deeper thus resulting in improved performance on segmentation tasks. Though adding more layers directly into network is a popular way to make a network deeper, it may lead to gradient vanishing or redundant computation during training.
Methods
A novel CNN architecture is proposed that integrates the Inception-Res module and densely connecting convolutional module into the U-net architecture. The proposed network model consists of the following parts: firstly, the Inception-Res block is designed to increase the width of the network by replacing the standard convolutional layers; secondly, the Dense-Inception block is designed to extract features and make the network more deep without additional parameters; thirdly, the down-sampling block is adopted to reduce the size of feature maps to accelerate learning and the up-sampling block is used to resize the feature maps.
Results
The proposed model is tested on images of blood vessel segmentations from retina images, the lung segmentation of CT Data from the benchmark Kaggle datasets and the MRI scan brain tumor segmentation datasets from MICCAI BraTS 2017. The experimental results show that the proposed method can provide better performance on these two tasks compared with the state-of-the-art algorithms. The results reach an average Dice score of 0.9857 in the lung segmentation. For the blood vessel segmentation, the results reach an average Dice score of 0.9582. For the brain tumor segmentation, the results reach an average Dice score of 0.9867.
Conclusions
The experiments highlighted that combining the inception module with dense connections in the U-Net architecture is a promising approach for semantic medical image segmentation.
ER  - 
TY  - JOUR
T1  - Performance of machine learning approaches on prediction of esophageal varices for Egyptian chronic hepatitis C patients
A1  - Abd El-Salam, Shimaa M
A1  - Ezz, Mohamed M
A1  - Hashem, Somaya
A1  - Elakel, Wafaa
A1  - Salama, Rabab
A1  - ElMakhzangy, Hesham
A1  - ElHefnawi, Mahmoud
Y1  - 2019///
KW  -  Esophageal varices
KW  -  Hepatitis C virus
KW  -  Medical diagnosis
KW  -  Prediction algorithms
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 17
SP  - 100267
EP  - 100267
DO  - https://doi.org/10.1016/j.imu.2019.100267
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819302643
N2  - Esophageal Varices is one of the most common side-effects of liver cirrhosis diseases which is detected by Upper endoscopy. Screening all patients implies many endoscopies will be needed, which increases the workload of endoscopy units. The aim of this study is to find solutions to diagnose the disease, by analyzing the patterns found in the data through classification analysis, using machine learning techniques for early prediction in cirrhotic patients based on their clinical examination. This research study attempts to propose a quicker and more efficient technique for disease diagnosis, leading to timely patient treatment. Our method analyzed 4962 patients with chronic hepatitis C from fifteen different centers in Egypt between 2006 and 2017. The dataset included twenty-four individual clinical laboratory variables. Esophageal Varices was present in 2218 patients and absent in 2,744 patients. Different types of feature selection (Filter-Wrapper) Approaches were applied to select the most significant features. The proposed model used six common algorithms including Neural Networks, Naïve Bayes, Decision Tree, Support Vector Machine, Random Forest and Bayesian Network to achieve our objective. The results showed that correlation and (p-value) based on filter method and Bayesian Network algorithm are well-suited for this analysis. Only nine variables: Gender, Platelet, Albumin, Total Bilirubin, Baseline_PCR, Liver, Spleen, Stiffness, and prothrombin concentration were the most significant predictors for Esophageal Varices. The Bayesian Network algorithm showed the highest performance; it achieved 74.8% and 68.9% for Area Under Receiver Operating Characteristic curves and accuracy, respectively. To conclude, machine learning techniques were able to predict Esophageal Varices in cirrhotic patients. The experimental results show that the Bayesian Network achieved better results than the other approaches.
ER  - 
TY  - JOUR
T1  - Hybrid System for Information Extraction from Social Media Text: Drug Abuse Case Study
A1  - Jenhani, Ferdaous
A1  - Gouider, Mohamed Salah
A1  - Said, Lamjed Ben
Y1  - 2019///
KW  -  Classification
KW  -  Hybrid Approach
KW  -  Information Extraction
KW  -  Natural Language Processing
KW  -  ODIN
KW  -  Twitter
KW  - Social Media
JF  - Procedia Computer Science
VL  - 159
SP  - 688
EP  - 697
DO  - https://doi.org/10.1016/j.procs.2019.09.224
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919314103
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - Social media are becoming widely used in the healthcare field as a patients-caregivers communication tool giving birth to new sources of information rich with the knowledge that may improve this field. Therefore, social media data analysis becomes a real business requirement for healthcare industrials and data scientists. However, regarding their complexity and unstructured character, existing natural language processing tools cannot succeed their exploitation. In the literature, a wide range of approaches appeared based on dictionaries, linguistic patterns and machine learning having their strengths and weaknesses. In this work, we propose a hybrid system combining the above approaches by taking the advantage of each of them to extract structured and salient drug abuse information from health-related tweets. We improve the system accuracy by real time update of the domain dictionary. We collected 1000000 tweets and we conducted different experiments showing the advantage of hybridization on efficient information extraction from social media data.
ER  - 
TY  - JOUR
T1  - Multiscale segmentation of exudates in retinal images using contextual cues and ensemble classification
A1  - Fraz, M Moazam
A1  - Jahangir, Waqas
A1  - Zahid, Saqib
A1  - Hamayun, Mian M
A1  - Barman, Sarah A
Y1  - 2017///
KW  -  Diabetic retinopathy
KW  -  Ensemble classification
KW  -  Exudate segmentation
KW  -  Feature extraction
KW  -  Machine learning
KW  - Medical image analysis
JF  - Biomedical Signal Processing and Control
VL  - 35
SP  - 50
EP  - 62
DO  - https://doi.org/10.1016/j.bspc.2017.02.012
UR  - https://www.sciencedirect.com/science/article/pii/S1746809417300381
N2  - Diabetic Retinopathy (DR) is the one among other main reasons of blindness in the adult population. Early discovery of DR through screening programs and successive treatment is critical in order to avoid visual blindness. The early signs of DR as manifested in retinal images include micro-aneurysms, hemorrhages and exudates. In this paper, we have presented an ensemble classifier of bootstrapped decision trees for multiscale localization and segmentation of exudates in retinal fundus images. The candidate exudates are extracted at fine grain and coarse grain levels using morphological reconstruction and Gabor filter respectively. The contextual cues are applied to the candidate exudates, which greatly reduces false positives in exudate segmentation. Several region based features are computed from candidate regions to train the ensemble classifier for classification of pixel as exudate and non-exudate region. The method has been evaluated on four publically available databases; DIARETDB1, e-Ophtha EX, HEI-MED and Messidor. The method has achieved the segmentation accuracy as (0.8772, 0.8925, 0.9577, and 0.9836) and area under ROC as (0.9310, 0.9403, 0.9842, and 0.9961) for each of the dataset respectively. The algorithm appears to be an efficient tool for automated detection of exudates in large population based DR screening programs, due to the attained accuracy, robustness, simplicity and speed.
ER  - 
TY  - JOUR
T1  - Mobile cloud computing for ECG telemonitoring and real-time coronary heart disease risk detection
A1  - Venkatesan, C
A1  - Karthigaikumar, P
A1  - Satheeskumaran, S
Y1  - 2018///
KW  -  Adaptive neuro fuzzy inference system (ANFIS)
KW  -  Coronary heart disease (CHD)
KW  -  Electrocardiogram (ECG)
KW  -  Heart rate variability (HRV)
KW  - Mobile cloud computing
JF  - Biomedical Signal Processing and Control
VL  - 44
SP  - 138
EP  - 145
DO  - https://doi.org/10.1016/j.bspc.2018.04.013
UR  - https://www.sciencedirect.com/science/article/pii/S1746809418300983
N2  - Advancement in healthcare technologies and biomedical equipment leads to accurate diagnosis of heart related diseases. The major challenges associated with telehealthcare technologies are complex computational requirement and large amount of data processing in continuous monitoring. Mobile cloud computing approach is presented in this work to overcome the issues involved in ECG telemonitoring. Mobile cloud approach is superior to telehealth monitoring techniques due to the access to centralized cloud data and report delivery to mobile phones. In this work, ECG telemonitoring and coronary heart disease (CHD) risk assessment are combined using mobile cloud computing approach. CHD risk is identified using feature extraction and adaptive neuro fuzzy inference system (ANFIS) based classification. In feature extraction process, R-peaks are detected using wavelet transform to find heart rate variability (HRV) of the ECG signal. Various HRV parameters are extracted and applied to ANFIS classifier which employs adaptive feature selection to evaluate CHD risk. Since the mobile cloud approach deals with large amount of data, 160 files of MIT–BIH arrhythmia database has been used in this work for the assessment of CHD risk. ECG signal data are classified into two categories (normal and CHD risky) using ANFIS classifier. The classifier performance is evaluated and comparison is established with other similar classifiers.
ER  - 
TY  - JOUR
T1  - Reinforcement learning for rule extraction from a labeled dataset
A1  - Vogiatzis, D
A1  - Stafylopatis, A
Y1  - 2002///
KW  -  Reinforcement learning
KW  - Rule extraction
JF  - Cognitive Systems Research
VL  - 3
IS  - 2
SP  - 237
EP  - 253
DO  - https://doi.org/10.1016/S1389-0417(01)00060-2
UR  - https://www.sciencedirect.com/science/article/pii/S1389041701000602
N1  - Integration of Symbolic and Connectionist Systems
N2  - The article introduces a method, which is based on reinforcement learning, for extracting rules of the form if-then-else from a labeled data-set. The constituent parts of a rule are the input dimensions of the labeled data-set, each accompanied by an appropriate interval of activation, and a label which stands for class membership. Initially, the input space is partitioned using tiles. The algorithm tries to compose the largest possible orthogonal intervals out of tiles. After the creation of intervals for each dimension the rule receives credit for its classification ability. This credit with the aid of reinforcement will be used to improve its constituent parts. The effectiveness of the proposed method has been tested on five different classification problems: the Iris data set, the Concentric data, the 4 Gaussians, the Pima-Indians set and the Image Segmentation data set.
ER  - 
TY  - JOUR
T1  - OpinionMine: A Bayesian-based framework for opinion mining using Twitter Data
A1  - Zervoudakis, Stefanos
A1  - Marakakis, Emmanouil
A1  - Kondylakis, Haridimos
A1  - Goumas, Stefanos
Y1  - 2021///
KW  -  Bayesian reasoning
KW  -  Incremental learning
KW  -  Probabilistic rules
KW  -  Twitter Data
KW  - Statistical Relational Learning
JF  - Machine Learning with Applications
VL  - 3
SP  - 100018
EP  - 100018
DO  - https://doi.org/10.1016/j.mlwa.2020.100018
UR  - https://www.sciencedirect.com/science/article/pii/S2666827020300189
N2  - This article studies opinion mining from social media with probabilistic logic reasoning. As it is known, Twitter is one of the most active social networks, with millions of tweets sent daily, where multiple users express their opinion about traveling, economic issues, political decisions etc. As such, it offers a valuable source of information for opinion mining. In this paper we present OpinionMine, a Bayesian-based framework for opinion mining, exploiting Twitter Data. Initially, our framework imports Tweets massively by using Twitter’s API. Next, the imported Tweets are further processed automatically for constructing a set of untrained rules and random variables. Then, a Bayesian Network is derived by using the set of untrained rules, the random variables and an evidence set. After that, the trained model can be used for the evaluation of new Tweets. Finally, the constructed model can be retrained incrementally, thus becoming more robust. As application domain for the development of our methodology we have selected tourism because it is one of the most popular topics in social media. Our framework can predict users’ intention to visit a place. Among the advantages of our framework is that it follows an incremental learning strategy. That is, the derived model can be retrained incrementally with new training sets thus becoming more robust. Further, our framework can be easily adapted to opinion mining from social media on other topics, whereas the rules of the derived model are constructed in an efficient way and automatically.
ER  - 
TY  - JOUR
T1  - Feature Selection Using Improved Teaching Learning Based Algorithm on Chronic Kidney Disease Dataset
A1  - M, Manonmani.
A1  - Balakrishnan, Sarojini
Y1  - 2020///
KW  -  Chronic Kidney Disease
KW  -  Convolution Neural Networks
KW  -  Gradient Boosting
KW  -  Improved Teacher Learner Based Optimization
KW  -  Optimization techniques
KW  -  Support Vector Machine
KW  - Teacher Learner Based Optimization
JF  - Procedia Computer Science
VL  - 171
SP  - 1660
EP  - 1669
DO  - https://doi.org/10.1016/j.procs.2020.04.178
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920311595
N1  - Third International Conference on Computing and Network Communications (CoCoNet'19)
N2  - Feature selection plays an important role in almost any data mining application especially in medical data mining to solve the problem of ‘curse of dimensionality’ and provide early diagnosis with relevant features and high accuracy. Innumerable feature selection methods have been presented in state-of-arts literature to tackle the problems of high dimensional data. Many evolutionary and swarm intelligence algorithms find solutions based on algorithm-specific control parameters. However, it is a challenging task to identify the optimal feature subset using a feature selection algorithm that is not dependent on the controlling parameters of an algorithm that is specific to a particular problem in hand. Hence, the present research work is based on the working principle of the original TLBO algorithm which does not require any algorithm-specific parameters. The proposed research work is known as Improved Teacher Learner Based Optimization (ITLBO) algorithm which aims to select the best feature subset based on Chebyshev distance formula in the evaluation of the fitness function and common control parameters viz., population size and number of generations to find the optimal feature subset for early diagnosis of chronic diseases. The proposed feature selection technique was applied to Chronic Kidney Disease (CKD) dataset and has achieved a significant feature reduction of 36% compared to the feature reduction of 25 % obtained by applying the original TLBO algorithm. The derived optimal feature subset obtained from TLBO algorithm and feature subset obtained from ITLBO algorithm is validated by evaluating the accuracy of Support Vector Machine (SVM), Convolution Neural Networks (CNN) and Gradient Boosting classification algorithms. Experimental results reveal that there is an overall improvement of classification accuracy for the three algorithms for the derived feature subset from the proposed feature selection algorithm compared to the original TLBO algorithm.
ER  - 
TY  - JOUR
T1  - Cost-sensitive KNN classification
A1  - Zhang, Shichao
Y1  - 2020///
KW  -  KNN
KW  - Cost-sensitive
JF  - Neurocomputing
VL  - 391
SP  - 234
EP  - 242
DO  - https://doi.org/10.1016/j.neucom.2018.11.101
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219304151
N2  - KNN (K Nearest Neighbors) classification is one of top-10 data mining algorithms. It is significant to extend KNN classifiers sensitive to costs for imbalanced data classification applications. This paper designs two efficient cost-sensitive KNN classification models, referred to Direct-CS-KNN classifier and Distance-CS-KNN classifier. The two CS-KNN classifiers are further improved with extant strategies, such as smoothing, minimum-cost k-value selection, feature selection and ensemble selection. We evaluate our methods with real data sets, to show that our CS-KNN classifiers can significantly reduce misclassification cost.
ER  - 
TY  - JOUR
T1  - Application of artificial intelligence methods in vital signs analysis of hospitalized patients: A systematic literature review
A1  - Kaieski, Naira
A1  - da Costa, Cristiano André
A1  - da Rosa Righi, Rodrigo
A1  - Lora, Priscila Schmidt
A1  - Eskofier, Björn
Y1  - 2020///
KW  -  Health informatics
KW  -  Machine learning
KW  -  Vital signs
KW  - Artificial intelligence
JF  - Applied Soft Computing
VL  - 96
SP  - 106612
EP  - 106612
DO  - https://doi.org/10.1016/j.asoc.2020.106612
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620305500
N2  - In a hospital environment, patients are monitored continuously by electronic devices and health professionals. Therefore, a large amount of data is collected and stored in electronic health records systems for each patient. Among such data, vital signs are one of the most common and relevant types of information monitored to assess a patient’s health status. Artificial intelligence techniques can be used to analyze and learn useful standards from clinical datasets to provide better evidence to support the decisions of health professionals and thus help to improve patient health outcomes in hospitals. This systematic literature review aims to provide an updated computational perspective of how artificial intelligence has been applied to analyze the vital signs of adult hospitalized patients and the outcomes obtained. To this end, we reviewed 2899 scientific articles published between 2008 and 2018 and selected 78 articles that met our inclusion criteria to answer the research questions. Moreover, we used the information found in the reviewed articles to propose a taxonomy and identified the main concerns, challenges, and opportunities in this field. Our findings demonstrate that many researchers are exploring the use of artificial intelligence methods in tasks related to improving the health outcomes of hospitalized patients in distinct units. Additionally, although vital signs are significant predictors of clinical deterioration, they are not analyzed in isolation to predict or identify a clinical outcome. Our taxonomy and discussion contribute to the achievement of a significant degree of coverage regarding the aspects related to using machine learning to improve health outcomes in hospital environments, while highlighting gaps in the literature for future research.
ER  - 
TY  - JOUR
T1  - Spam filtering using a logistic regression model trained by an artificial bee colony algorithm
A1  - Dedeturk, Bilge Kagan
A1  - Akay, Bahriye
Y1  - 2020///
KW  -  Artificial bee colony
KW  -  Logistic regression
KW  -  Naive Bayes
KW  -  Support vector machines
KW  -  Turkish emails
KW  - Spam filtering
JF  - Applied Soft Computing
VL  - 91
SP  - 106229
EP  - 106229
DO  - https://doi.org/10.1016/j.asoc.2020.106229
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620301691
N2  - Email spam is a serious problem that annoys recipients and wastes their time. Machine-learning methods have been prevalent in spam detection systems owing to their efficiency in classifying mail as solicited or unsolicited. However, existing spam detection techniques usually suffer from low detection rates and cannot efficiently handle high-dimensional data. Therefore, we propose a novel spam detection method that combines the artificial bee colony algorithm with a logistic regression classification model. The empirical results on three publicly available datasets (Enron, CSDMC2010, and TurkishEmail) show that the proposed model can handle high-dimensional data thanks to its highly effective local and global search abilities. We compare the proposed model’s spam detection performance to those of support vector machine, logistic regression, and naive Bayes classifiers, in addition to the performance of the state-of-the-art methods reported by previous studies. We observe that the proposed method outperforms other spam detection techniques considered in this study in terms of classification accuracy.
ER  - 
TY  - JOUR
T1  - Correlation method versus enhanced modified moving average method for automatic detection of T-wave alternans
A1  - Burattini, Laura
A1  - Bini, Silvia
A1  - Burattini, Roberto
Y1  - 2010///
KW  -  Correlation method
KW  -  Enhanced modified moving average method
KW  - T-wave alternans
JF  - Computer Methods and Programs in Biomedicine
VL  - 98
IS  - 1
SP  - 94
EP  - 102
DO  - https://doi.org/10.1016/j.cmpb.2010.01.008
UR  - https://www.sciencedirect.com/science/article/pii/S0169260710000210
N2  - Enhanced modified moving average method (EMMAM) and correlation method (CM) for microvolt TWA identification are compared by aid of simulated ECG tracings (cases of absence of TWA and presence of stationary or time-varying TWA) and ECG recordings from healthy subjects (H-group) and patients who survived an acute myocardial infarction (AMI-group). The two competing methods were found to be equivalent when analyzing clean ECGs affected by stationary TWA. Non-stationary TWA is correctly tracked by the CM, whereas it is identified as stationary by the EMMAM. Moreover, the EMMAM suffers for its tendency to identify as TWA noise and other kinds of repolarization variability. Such limitation is most likely the cause of its false-positive TWA production. Finally, only the CM incorporates a local threshold criterion in the TWA detection algorithm which allows better discrimination between H and AMI groups, who are well known to be at increased risk to develop TWA.
ER  - 
TY  - JOUR
T1  - Directed disease networks to facilitate multiple-disease risk assessment modeling
A1  - Wang, Tingyan
A1  - Qiu, Robin G
A1  - Yu, Ming
A1  - Zhang, Runtong
Y1  - 2020///
KW  -  Directed network
KW  -  Disability adjusted life year
KW  -  Disease temporal relations
KW  -  Health risk assessment
KW  - Multiple-disease risk prediction
JF  - Decision Support Systems
VL  - 129
SP  - 113171
EP  - 113171
DO  - https://doi.org/10.1016/j.dss.2019.113171
UR  - https://www.sciencedirect.com/science/article/pii/S0167923619302003
N2  - We investigate multiple disease risk prediction modeling, aimed at assessing future disease risks for an individual who is ready for discharge after hospitalization. We propose a novel framework that combines directed disease network and recommendation system techniques to substantially enhance multiple disease risk predictive modeling. Firstly, a directed disease network considering temporal information is developed. Then based on this directed disease network, we look into different disease risk score computing approaches. We validate the proposed approaches with two real-world datasets from two independent hospitals. The predicted results can be promisingly utilized as a reference for medical experts to offer effective healthcare guidance for both inpatients and outpatients. The proposed framework can also be utilized for developing an innovative tool that helps individuals create and maintain a better healthcare plan over time.
ER  - 
TY  - JOUR
T1  - Early detection and risk assessment for chronic disease with irregular longitudinal data analysis
A1  - He, Kai
A1  - Huang, Shuai
A1  - Qian, Xiaoning
Y1  - 2019///
KW  -  Longitudinal measurements
KW  -  Machine learning
KW  -  Risk monitoring
KW  -  Structured output
KW  -  Support Vector Machine
KW  - Early diagnosis
JF  - Journal of Biomedical Informatics
VL  - 96
SP  - 103231
EP  - 103231
DO  - https://doi.org/10.1016/j.jbi.2019.103231
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419301509
N2  - Early detection and risk assessment of complex chronic disease based on longitudinal clinical data is helpful for doctors to make early diagnosis and monitor the disease progression. Disease diagnosis with computer-aided methods has been extensively studied. However, early detection and contemporaneous risk assessment based on partially labeled irregular longitudinal measurements is relatively unexplored. In this paper, we propose a flexible mixed-kernel framework for training a contemporaneous disease risk detector to predict the onset of disease and monitor the disease progression. Moreover, we address the label insufficiency problem by identifying the pattern of disease-induced progression over time with longitudinal data. Our method is based on a Structured Output Support Vector Machine (SOSVM), extended to longitudinal data analysis. Extensive experiments are conducted on several datasets of varying complexity, including the contemporaneous risk assessment with simulated irregular longitudinal data; the identification of the onset of Type 1 Diabetes (T1D) with irregularly sampled longitudinal RNA-Seq gene expression dataset; as well as the monitoring of the drug long-term effects on patients using longitudinal RNA-Seq dataset containing missing time points, demonstrating that our method enhances the accuracy in both early diagnosis and risk estimation with partially labeled irregular longitudinal clinical data.
ER  - 
TY  - JOUR
T1  - MICE vs PPCA: Missing data imputation in healthcare
A1  - Hegde, Harshad
A1  - Shimpi, Neel
A1  - Panny, Aloksagar
A1  - Glurich, Ingrid
A1  - Christie, Pamela
A1  - Acharya, Amit
Y1  - 2019///
KW  -  Dental informatics
KW  -  MICE
KW  -  Medical dental data
KW  -  Multiple imputations using chained equations
KW  -  PPCA
KW  -  Probabilistic principal component analysis
KW  - Imputation
JF  - Informatics in Medicine Unlocked
VL  - 17
SP  - 100275
EP  - 100275
DO  - https://doi.org/10.1016/j.imu.2019.100275
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819302783
N2  - Retrospective analyses of real-world clinical data face challenges owing to the absence of some data elements. Historically, missing data was addressed by first classifying its presence into one of three categories: missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR). Imputation techniques continue to be developed and tested to gauge their capacity to mitigate the negative impact of missing data types on analyses and their results. This study undertook a comparison of two techniques of data imputation: probabilistic principal component analysis (PPCA) and multiple imputation using chained equations (MICE). Retrospective data from 41,543 unique patients including both medical and dental variables (n = 116) were mined from the institutional research data warehouse, which captures data through an integrated medical and dental electronic health record (iEHR). A subset with complete data on all variables of interest was sampled. “Missing data” were artificially created by randomly removing data elements to create the missing data problem. Applying PPCA and MICE, the capacity of the two techniques to create an accurate imputed dataset was tested. Comparisons were drawn between imputed dataset and sampled subset, to investigate which technique more closely simulated the true data. PPCA outperformed MICE with an overall correct imputation percentage (accuracy) and root mean square error (RMSE) of approximately 65% and 0.29, respectively, compared to MICE, which yielded approximately 38% accuracy with a RMSE of 0.83. Overall, this study concluded that PPCA demonstrated higher capacity to impute MCAR data than MICE.
ER  - 
TY  - JOUR
T1  - Artificial neural networks based Internet hypertension prediction tool development and validation
A1  - Polak, Sebastian
A1  - Mendyk, Aleksander
Y1  - 2008///
KW  -  Artificial neural networks
KW  -  Internet
KW  - Hypertension risk prediction
JF  - Applied Soft Computing
VL  - 8
IS  - 1
SP  - 734
EP  - 739
DO  - https://doi.org/10.1016/j.asoc.2007.06.001
UR  - https://www.sciencedirect.com/science/article/pii/S1568494607000579
N2  - The objective of the study was to improve and validate previously prepared, artificial neural networks (ANNs) based system of high blood pressure risk prediction. Based on simple demographic data system is able to estimate the risk of hypertension. Database for the model construction was obtained from Centers for Disease Control-National Center for Health Statistics (CDC-NCHS). Best generalization results were found to be close to 75%—expressed as the total classification rate. Java applet was designed to be the interface between ANN system and end user. The validation procedure (years 1987–2000) acknowledged above 70% effectiveness of the system.
ER  - 
TY  - JOUR
T1  - A K-anonymous clustering algorithm based on the analytic hierarchy process
A1  - Wang, Kun
A1  - Zhao, Wei
A1  - Cui, Junjie
A1  - Cui, Yanpeng
A1  - Hu, Jianwei
Y1  - 2019///
KW  -  Analytic hierarchy process
KW  - Clustering algorithm
JF  - Journal of Visual Communication and Image Representation
VL  - 59
SP  - 76
EP  - 83
DO  - https://doi.org/10.1016/j.jvcir.2018.12.052
UR  - https://www.sciencedirect.com/science/article/pii/S1047320318303791
N2  - To protect the privacy of users, tables generally must be anonymized before publication. All existing anonymous methods have deficiencies. They do not consider the differences in attributes, or the optimization of information loss and time efficiency. his paper proposes a new method called KACM to realize k-anonymity. This method is mainly used for hybrid tables. The calculation of the distance between records considers the connection between quasi-identifier attributes and sensitive attributes, their effect on the sensitive privacy, and the information loss during the anonymity process. In the clustering process, the records with the minimum distance are always selected to add, and the clustering is individually controlled according to k to realize the equalization division of the equivalence class and reduce the total amount of distance calculation. Finally, the validity and practicability of the method are proved using theory and experiment.
ER  - 
TY  - JOUR
T1  - A review of machine learning in hypertension detection and blood pressure estimation based on clinical and physiological data
A1  - Martinez-Ríos, Erick
A1  - Montesinos, Luis
A1  - Alfaro-Ponce, Mariel
A1  - Pecchia, Leandro
Y1  - 2021///
KW  -  Clinical data
KW  -  Machine learning
KW  -  Physiological data
KW  - Hypertension
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102813
EP  - 102813
DO  - https://doi.org/10.1016/j.bspc.2021.102813
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421004109
N2  - The use of machine learning techniques in medicine has increased in recent years due to a rise in publicly available datasets. These techniques have been applied in high blood pressure studies following two approaches: hypertension stage classification based on clinical data and blood pressure estimation based on related physiological signals. This paper presents a literature review on such studies. We aimed to identify the best practices, challenges, and opportunities in developing machine learning models to detect hypertension or estimate blood pressure using clinical data and physiological signals. Hence, we identified and examined the machine learning techniques, publicly available datasets, and predictors used in previous studies. The feature selection techniques used to reduce model complexity are also reviewed. We found a lack of studies combining socio-demographic or clinical data with physiological signals, despite the correlation of blood pressure with photoplethysmography waveforms and variables such as age, gender, body mass index, and heart rate. Therefore, there is an opportunity to increase model performance by using both types of data for hypertension detection or blood pressure monitoring.
ER  - 
TY  - JOUR
T1  - A robust multiobjective Harris’ Hawks Optimization algorithm for the binary classification problem
A1  - Dokeroglu, Tansel
A1  - Deniz, Ayça
A1  - Kiziloz, Hakan Ezgi
Y1  - 2021///
KW  -  Feature selection
KW  -  Harris’ Hawks optimization
KW  -  Multiobjective optimization
KW  - Binary classification
JF  - Knowledge-Based Systems
VL  - 227
SP  - 107219
EP  - 107219
DO  - https://doi.org/10.1016/j.knosys.2021.107219
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121004810
N2  - The Harris’ Hawks Optimization (HHO) is a recent metaheuristic inspired by the cooperative behavior of the hawks. These avians apply many intelligent techniques like surprise pounce (seven kills) while they are catching their prey according to the escaping patterns of the target. The HHO simulates these hunting patterns of the hawks to obtain the best/optimal solutions to the problems. In this study, we propose a new multiobjective HHO algorithm for the solution of the well-known binary classification problem. In this multiobjective problem, we reduce the number of selected features and try to keep the accuracy prediction as maximum as possible at the same time. We propose new discrete exploration (perching) and exploitation (besiege) operators for the hunting patterns of the hawks. We calculate the prediction accuracy of the selected features with four machine learning techniques, namely, Logistic Regression, Support Vector Machines, Extreme Learning Machines, and Decision Trees. To verify the performance of the proposed algorithm, we conduct comprehensive experiments on many benchmark datasets retrieved from the University of California, Irvine (UCI) Machine Learning Repository. Moreover, we apply it to a recent real-world dataset, i.e., a Coronavirus disease (COVID-19) dataset. Significant improvements are observed during the comparisons with state-of-the-art metaheuristic algorithms.
ER  - 
TY  - JOUR
T1  - Retinal blood vessel extraction employing effective image features and combination of supervised and unsupervised machine learning methods
A1  - Hashemzadeh, Mahdi
A1  - Adlpour Azar, Baharak
Y1  - 2019///
KW  -  Blood vessel
KW  -  Classification
KW  -  Clustering
KW  -  Image processing
KW  -  Vessel extraction
KW  - Retina
JF  - Artificial Intelligence in Medicine
VL  - 95
SP  - 1
EP  - 15
DO  - https://doi.org/10.1016/j.artmed.2019.03.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718304457
N2  - In medicine, retinal vessel analysis of fundus images is a prominent task for the screening and diagnosis of various ophthalmological and cardiovascular diseases. In this research, a method is proposed for extracting the retinal blood vessels employing a set of effective image features and combination of supervised and unsupervised machine learning techniques. Further to the common features used in extracting blood vessels, three strong features having a significant influence on the accuracy of the vessel extraction are utilized. The selected combination of the different types of individually efficient features results in a rich local information with better discrimination for vessel and non-vessel pixels. The proposed method first extracts the thick and clear vessels in an unsupervised manner, and then, it extracts the thin vessels in a supervised way. The goal of the combination of the supervised and unsupervised methods is to deal with the problem of intra-class high variance of image features calculated from various vessel pixels. The proposed method is evaluated on three publicly available databases DRIVE, STARE and CHASE_DB1. The obtained results (DRIVE: Acc = 0.9531, AUC = 0.9752; STARE: Acc = 0.9691, AUC = 0.9853; CHASE_DB1: Acc = 0.9623, AUC = 0.9789) demonstrate the better performance of the proposed method compared to the state-of-the-art methods.
ER  - 
TY  - JOUR
T1  - Tree-based generational feature selection in medical applications
A1  - Wiesław, Paja
Y1  - 2019///
KW  -  dimensionality reduction
KW  -  feature ranking
KW  -  generational feature selection
KW  -  irrelevance
KW  -  relevance
KW  - feature selection
JF  - Procedia Computer Science
VL  - 159
SP  - 2172
EP  - 2178
DO  - https://doi.org/10.1016/j.procs.2019.09.391
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919315947
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - In many knowledge discovery experiments feature selection is obvious initial part. In the paper, some attempt to tree-based generational feature selection applications in medical data analysis is presented. This approach devotes to application of classification tree algorithm to estimate importance of attributes extracted from structure of the tree with recursive application of generational feature selection. This method apply removing of selected features from dataset and then creates next generation of important feature set. The process goes until the most important feature will be a random value. Implemented method were applied on three artificial and real-world medical datasets and the results of selection and classification are presented. They were mostly more efficient after selection than using original datasets.
ER  - 
TY  - JOUR
T1  - DMET-Miner: Efficient discovery of association rules from pharmacogenomic data
A1  - Agapito, Giuseppe
A1  - Guzzi, Pietro H
A1  - Cannataro, Mario
Y1  - 2015///
KW  -  Association rules
KW  -  Frequent itemset mining
KW  -  Single nucleotide polymorphism
KW  - Personalized medicine
JF  - Journal of Biomedical Informatics
VL  - 56
SP  - 273
EP  - 283
DO  - https://doi.org/10.1016/j.jbi.2015.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S153204641500115X
N2  - Microarray platforms enable the investigation of allelic variants that may be correlated to phenotypes. Among those, the Affymetrix DMET (Drug Metabolism Enzymes and Transporters) platform enables the simultaneous investigation of all the genes that are related to drug absorption, distribution, metabolism and excretion (ADME). Although recent studies demonstrated the effectiveness of the use of DMET data for studying drug response or toxicity in clinical studies, there is a lack of tools for the automatic analysis of DMET data. In a previous work we developed DMET-Analyzer, a methodology and a supporting platform able to automatize the statistical study of allelic variants, that has been validated in several clinical studies. Although DMET-Analyzer is able to correlate a single variant for each probe (related to a portion of a gene) through the use of the Fisher test, it is unable to discover multiple associations among allelic variants, due to its underlying statistic analysis strategy that focuses on a single variant for each time. To overcome those limitations, here we propose a new analysis methodology for DMET data based on Association Rules mining, and an efficient implementation of this methodology, named DMET-Miner. DMET-Miner extends the DMET-Analyzer tool with data mining capabilities and correlates the presence of a set of allelic variants with the conditions of patient’s samples by exploiting association rules. To face the high number of frequent itemsets generated when considering large clinical studies based on DMET data, DMET-Miner uses an efficient data structure and implements an optimized search strategy that reduces the search space and the execution time. Preliminary experiments on synthetic DMET datasets, show how DMET-Miner outperforms off-the-shelf data mining suites such as the FP-Growth algorithms available in Weka and RapidMiner. To demonstrate the biological relevance of the extracted association rules and the effectiveness of the proposed approach from a medical point of view, some preliminary studies on a real clinical dataset are currently under medical investigation.
ER  - 
TY  - JOUR
T1  - Thickness related textural properties of retinal nerve fiber layer in color fundus images
A1  - Odstrcilik, Jan
A1  - Kolar, Radim
A1  - Tornow, Ralf-Peter
A1  - Jan, Jiri
A1  - Budai, Attila
A1  - Mayer, Markus
A1  - Vodakova, Martina
A1  - Laemmer, Robert
A1  - Lamos, Martin
A1  - Kuna, Zdenek
A1  - Gazarek, Jiri
A1  - Kubena, Tomas
A1  - Cernosek, Pavel
A1  - Ronzhina, Marina
Y1  - 2014///
KW  -  Fundus images
KW  -  Local binary patterns
KW  -  Markov random fields
KW  -  Retinal nerve fiber layer
KW  -  Texture analysis
KW  - Glaucoma
JF  - Computerized Medical Imaging and Graphics
VL  - 38
IS  - 6
SP  - 508
EP  - 516
DO  - https://doi.org/10.1016/j.compmedimag.2014.05.005
UR  - https://www.sciencedirect.com/science/article/pii/S0895611114000652
N2  - Images of ocular fundus are routinely utilized in ophthalmology. Since an examination using fundus camera is relatively fast and cheap procedure, it can be used as a proper diagnostic tool for screening of retinal diseases such as the glaucoma. One of the glaucoma symptoms is progressive atrophy of the retinal nerve fiber layer (RNFL) resulting in variations of the RNFL thickness. Here, we introduce a novel approach to capture these variations using computer-aided analysis of the RNFL textural appearance in standard and easily available color fundus images. The proposed method uses the features based on Gaussian Markov random fields and local binary patterns, together with various regression models for prediction of the RNFL thickness. The approach allows description of the changes in RNFL texture, directly reflecting variations in the RNFL thickness. Evaluation of the method is carried out on 16 normal (“healthy”) and 8 glaucomatous eyes. We achieved significant correlation (normals: ρ=0.72±0.14; p≪0.05, glaucomatous: ρ=0.58±0.10; p≪0.05) between values of the model predicted output and the RNFL thickness measured by optical coherence tomography, which is currently regarded as a standard glaucoma assessment device. The evaluation thus revealed good applicability of the proposed approach to measure possible RNFL thinning.
ER  - 
TY  - JOUR
T1  - Prediction performance of improved decision tree-based algorithms: a review
A1  - Mienye, Ibomoiye Domor
A1  - Sun, Yanxia
A1  - Wang, Zenghui
Y1  - 2019///
KW  -  Algorithm
KW  -  C4.5
KW  -  Classification
KW  -  Data mining
KW  -  Decision tree
KW  -  ID3
KW  -  Information gain ratio
KW  -  entropy
KW  - Machine learning
JF  - Procedia Manufacturing
VL  - 35
SP  - 698
EP  - 703
DO  - https://doi.org/10.1016/j.promfg.2019.06.011
UR  - https://www.sciencedirect.com/science/article/pii/S235197891930736X
N1  - The 2nd International Conference on Sustainable Materials Processing and Manufacturing, SMPM 2019, 8-10 March 2019, Sun City, South Africa
N2  - Applications of machine learning can be found in retail, banking, education, health sectors etc. To process the large data emanating from the various sectors, researchers are developing different algorithms using expertise from several fields and knowledge of existing algorithms. Machine learning decision tree algorithms which includes ID3, C4.5, C5.0, and CART (Classification and Regression Trees) are quite powerful. ID3 and C4.5 are mostly used in classification problems, and they are the focus of this research. C4.5 is an improved version of ID3 developed by Ross Quinlan. The prediction performance of these algorithms is very important. In this paper, the prediction performance of decision tree algorithms will be studied, an in-depth review will be conducted on relevant researches that attempted to improve the performance of the algorithms and the various methods used. Comparison will also be done between the various tree based algorithms. The major contribution of this review is to provide researchers with the progress made so far, as there is no available literature that has put together relevant improvements of decision tree based algorithms, and lastly lay the foundation for future research and improvements.
ER  - 
TY  - JOUR
T1  - Hard exudate detection based on deep model learned information and multi-feature joint representation for diabetic retinopathy screening
A1  - Wang, Hui
A1  - Yuan, Guohui
A1  - Zhao, Xuegong
A1  - Peng, Lingbing
A1  - Wang, Zhuoran
A1  - He, Yanmin
A1  - Qu, Chao
A1  - Peng, Zhenming
Y1  - 2020///
KW  -  Convolutional neural network
KW  -  Fundus images
KW  -  Multi-feature joint representation
KW  - Hard exudate detection
JF  - Computer Methods and Programs in Biomedicine
VL  - 191
SP  - 105398
EP  - 105398
DO  - https://doi.org/10.1016/j.cmpb.2020.105398
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719302081
N2  - Background and objective: Diabetic retinopathy (DR), which is generally diagnosed by the presence of hemorrhages and hard exudates, is one of the most prevalent causes of visual impairment and blindness. Early detection of hard exudates (HEs) in color fundus photographs can help in preventing such destructive damage. However, this is a challenging task due to high intra-class diversity and high similarity with other structures in the fundus images. Most of the existing methods for detecting HEs are based on characterizing HEs using hand crafted features (HCFs) only, which can not characterize HEs accurately. Deep learning methods are scarce in this domain because they require large-scale sample sets for training which are not generally available for most routine medical imaging research. Methods: To address these challenges, we propose a novel methodology for HE detection using deep convolutional neural network (DCNN) and multi-feature joint representation. Specifically, we present a new optimized mathematical morphological approach that first segments HE candidates accurately. Then, each candidate is characterized using combined features based on deep features with HCFs incorporated, which is implemented by a ridge regression-based feature fusion. This method employs multi-space-based intensity features, geometric features, a gray-level co-occurrence matrix (GLCM)-based texture descriptor, a gray-level size zone matrix (GLSZM)-based texture descriptor to construct HCFs, and a DCNN to automatically learn the deep information of HE. Finally, a random forest is employed to identify the true HEs among candidates. Results: The proposed method is evaluated on two benchmark databases. It obtains an F-score of 0.8929 with an area under curve (AUC) of 0.9644 on the e-optha database and an F-score of 0.9326 with an AUC of 0.9323 on the HEI-MED database. These results demonstrate that our approach outperforms state-of-the-art methods. Our model also proves to be suitable for clinical applications based on private clinical images from a local hospital. Conclusions: This newly proposed method integrates the traditional HCFs and deep features learned from DCNN for detecting HEs. It achieves a new state-of-the-art in both detecting HEs and DR screening. Furthermore, the proposed feature selection and fusion strategy reduces feature dimension and improves HE detection performance.
ER  - 
TY  - JOUR
T1  - Effective multilayer hybrid classification approach for automatic bridge health assessment on large-scale uncertain data
A1  - Yang, Yun
A1  - Nan, Fengtao
A1  - Yang, Po
Y1  - 2021///
KW  -  Bridge health evaluation
KW  -  Hybrid classification model
KW  -  Penalty function
KW  - Large-scale uncertain label
JF  - Journal of Industrial Information Integration
VL  - 24
SP  - 100234
EP  - 100234
DO  - https://doi.org/10.1016/j.jii.2021.100234
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X21000340
N2  - The health level of the bridge is critical to the safety and maintainability of the bridge. However, with the rapid increase of bridge data with complex information, manual evaluation of bridge health requires high labor and time costs and a lot of related knowledge. Meanwhile, due to the error of sensors and subjective judgment of experts, as well as the influence of the external environment of the bridge, there is great uncertainty in the evaluation of the bridge data. Thence, how to use a large amount of bridge data with large-scale uncertain labels to build a robust classification model for efficient and high-quality bridge health assessment has become an urgent task. In order to better assess the health of the bridge, we adopt a multi-layer hybrid method to iteratively determine the uncertain labels of the target dataset, evaluate the confidence of the large-scale uncertain labels, add high-confidence data to the training set, and correct the low-confidence data. Finally, we get an effective classification model with the optimized training dataset. This paper studies the learning problem of classification model on labeled data with large-scale uncertain labels, and proposes an effective hybrid classification model (HCM), which can establish a supervised classifier under the condition of detecting uncertain labels and realize error label correction. In order to measure the HCM label assignment problem, we introduce a new penalty function, which can evaluate the label consistency problem of two basic classifiers. Meanwhile, we apply the model to bridge data with uncertain labels for bridge health evaluation. Experiments conducted on synthetic data, benchmark data and real bridge datasets show that the proposed method is superior to other methods and provides an effective and convenient solution for bridge health assessment. At the same time, this method can also be used in other research fields where there are large-scale uncertain labels.
ER  - 
TY  - JOUR
T1  - S3Mining: A model-driven engineering approach for supporting novice data miners in selecting suitable classifiers
A1  - Espinosa, Roberto
A1  - García-Saiz, Diego
A1  - Zorrilla, Marta
A1  - Zubcoff, José Jacobo
A1  - Mazón, Jose-Norberto
Y1  - 2019///
KW  -  Knowledge base
KW  -  Meta-learning
KW  -  Model-driven
KW  -  Model-driven engineering
KW  -  Novice data miners
KW  - Data mining
JF  - Computer Standards & Interfaces
VL  - 65
SP  - 143
EP  - 158
DO  - https://doi.org/10.1016/j.csi.2019.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S0920548918303325
N2  - Data mining has proven to be very useful in order to extract information from data in many different contexts. However, due to the complexity of data mining techniques, it is required the know-how of an expert in this field to select and use them. Actually, adequately applying data mining is out of the reach of novice users which have expertise in their area of work, but lack skills to employ these techniques. In this paper, we use both model-driven engineering and scientific workflow standards and tools in order to develop named S3Mining framework, which supports novice users in the process of selecting the data mining classification algorithm that better fits with their data and goal. To this aim, this selection process uses the past experiences of expert data miners with the application of classification techniques over their own datasets. The contributions of our S3Mining framework are as follows: (i) an approach to create a knowledge base which stores the past experiences of experts users, (ii) a process that provides the expert users with utilities for the construction of classifiers’ recommenders based on the existing knowledge base, (iii) a system that allows novice data miners to use these recommenders for discovering the classifiers that better fit for solving their problem at hand, and (iv) a public implementation of the framework’s workflows. Finally, an experimental evaluation has been conducted to shown the feasibility of our framework.
ER  - 
TY  - JOUR
T1  - Computational methods for automated analysis of corneal nerve images: Lessons learned from retinal fundus image analysis
A1  - Salahuddin, Tooba
A1  - Qidwai, Uvais
Y1  - 2020///
KW  -  Automated image analysis
KW  -  Deep learning
KW  -  Neuropathy
KW  -  Retinal fundoscopy
KW  - Corneal confocal microscopy
JF  - Computers in Biology and Medicine
VL  - 119
SP  - 103666
EP  - 103666
DO  - https://doi.org/10.1016/j.compbiomed.2020.103666
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520300597
N2  - Corneal and retinal imaging provide a descriptive view of the nerve and vessel structure present inside the human eye, in a non-invasive manner. This helps in ocular, or other, disease identification and diagnosis. However, analyzing these images is a laborious task and requires expertise in the field. Therefore, there is a dire need for process automation. Although a large body of literature is available for automated analysis of retinal images, research on corneal nerve image analysis has lagged due to several reasons. In this article, we cover the recent research trends in automated analysis of corneal and retinal images, highlighting the requirements for automation of corneal nerve image analysis, and the possible reasons impeding its research progress. We also present a comparative analysis of segmentation algorithms versus their processing power derived from the studies included in the survey. Due to the advancement in retinal image analysis and the implicit similarities in retinal and corneal images, we extract lessons from the former and suggest ways to apply them to the latter. This is presented as future research directions for automatic detection of neuropathy using corneal nerve images. We believe that this article will be extremely informative for computer scientists and medical professionals alike, as the former would be informed regarding the different research problems waiting to be addressed in the field, while the latter would be enlightened to what is required from them so as to facilitate computer scientists in their path towards finding effective solutions to the problems.
ER  - 
TY  - JOUR
T1  - A query expansion method based on topic modeling and DBpedia features
A1  - Dahir, Sarah
A1  - Qadi, Abderrahim El
Y1  - 2021///
KW  -  DBpedia
KW  -  Language model
KW  -  Query expansion
KW  -  Term distribution
KW  -  Topic modeling
KW  - Information retrieval
JF  - International Journal of Information Management Data Insights
VL  - 1
IS  - 2
SP  - 100043
EP  - 100043
DO  - https://doi.org/10.1016/j.jjimei.2021.100043
UR  - https://www.sciencedirect.com/science/article/pii/S2667096821000367
N2  - Query Expansion (QE) is a method used for improving Information Retrieval (IR) by adding the terms that are almost selected from feedback documents, and similar to the user query terms. But, due to the very small average number of query keywords; it is sometimes difficult to detect the context around the user query, and expand the query accordingly, especially when it contains ambiguous terms(i.e. polysemy terms). To this end, Linked Open Data (LOD) sources may be exploited. Yet, most attributes from linked data are multi-valued which makes a system unable to determine the right one(s) to use for expansion. And few other attributes are single-valued but too long and noisy to use directly. To deal with the previous issues, integration of the topic modeling process has been proposed to predict the latent semantic attribute-topics to use for expansion. This approach reconstructs candidate documents for a given query using distribution technique Bose-Einstein statistics (Bo1) and DBpedia attributes. The Latent Dirichlet Allocation(LDA) based topic models are then generated by considering these documents and the relevant expansion terms are then determined. The proposed method has been evaluated using the AP dataset collection, and the experiments revealed significant improvements according to the retrieval results using the distribution technique Bo1. Also, the proposed “LDA-LinkedBo1” approach outperformed DBpedia association based approaches in terms of MRR@N.
ER  - 
TY  - JOUR
T1  - Unsupervised entity and relation extraction from clinical records in Italian
A1  - Alicante, Anita
A1  - Corazza, Anna
A1  - Isgrò, Francesco
A1  - Silvestri, Stefano
Y1  - 2016///
KW  -  Entities relation discovery
KW  -  Entity extraction
KW  -  Medical information extraction
KW  -  Relation clustering
KW  - Unsupervised learning
JF  - Computers in Biology and Medicine
VL  - 72
SP  - 263
EP  - 275
DO  - https://doi.org/10.1016/j.compbiomed.2016.01.014
UR  - https://www.sciencedirect.com/science/article/pii/S001048251630004X
N2  - This paper proposes and discusses the use of text mining techniques for the extraction of information from clinical records written in Italian. However, as it is very difficult and expensive to obtain annotated material for languages different from English, we only consider unsupervised approaches, where no annotated training set is necessary. We therefore propose a complete system that is structured in two steps. In the first one domain entities are extracted from the clinical records by means of a metathesaurus and standard natural language processing tools. The second step attempts to discover relations between the entity pairs extracted from the whole set of clinical records. For this last step we investigate the performance of unsupervised methods such as clustering in the space of entity pairs, represented by an ad hoc feature vector. The resulting clusters are then automatically labelled by using the most significant features. The system has been tested on a fairly large data set of clinical records in Italian, investigating the variation in the performance adopting different similarity measures in the feature space. The results of our experiments show that the unsupervised approach proposed is promising and well suited for a semi-automatic labelling of the extracted relations.
ER  - 
TY  - JOUR
T1  - Feature selection and analysis on correlated gas sensor data with recursive feature elimination
A1  - Yan, Ke
A1  - Zhang, David
Y1  - 2015///
KW  -  Breath analysis
KW  -  Correlation bias
KW  -  Feature ranking
KW  -  SVM-RFE
KW  -  Transient feature
KW  - Feature selection
JF  - Sensors and Actuators B: Chemical
VL  - 212
SP  - 353
EP  - 363
DO  - https://doi.org/10.1016/j.snb.2015.02.025
UR  - https://www.sciencedirect.com/science/article/pii/S0925400515001872
N2  - Support vector machine recursive feature elimination (SVM-RFE) is a powerful feature selection algorithm. However, when the candidate feature set contains highly correlated features, the ranking criterion of SVM-RFE will be biased, which would hinder the application of SVM-RFE on gas sensor data. In this paper, the linear and nonlinear SVM-RFE algorithms are studied. After investigating the correlation bias, an improved algorithm SVM-RFE+CBR is proposed by incorporating the correlation bias reduction (CBR) strategy into the feature elimination procedure. Experiments are conducted on a synthetic dataset and two breath analysis datasets, one of which contains temperature modulated sensors. Large and comprehensive sets of transient features are extracted from the sensor responses. The classification accuracy with feature selection proves the efficacy of the proposed SVM-RFE+CBR. It outperforms the original SVM-RFE and other typical algorithms. An ensemble method is further studied to improve the stability of the proposed method. By statistically analyzing the features’ rankings, some knowledge is obtained, which can guide future design of e-noses and feature extraction algorithms.
ER  - 
TY  - JOUR
T1  - Incorporating expert knowledge when learning Bayesian network structure: A medical case study
A1  - Julia Flores, M
A1  - Nicholson, Ann E
A1  - Brunskill, Andrew
A1  - Korb, Kevin B
A1  - Mascaro, Steven
Y1  - 2011///
KW  -  Causal discovery
KW  -  Expert priors
KW  -  Heart failure
KW  -  Medical datasets
KW  -  Structure learning
KW  - Bayesian networks
JF  - Artificial Intelligence in Medicine
VL  - 53
IS  - 3
SP  - 181
EP  - 204
DO  - https://doi.org/10.1016/j.artmed.2011.08.004
UR  - https://www.sciencedirect.com/science/article/pii/S0933365711001084
N2  - Objectives
Bayesian networks (BNs) are rapidly becoming a leading technology in applied Artificial Intelligence, with many applications in medicine. Both automated learning of BNs and expert elicitation have been used to build these networks, but the potentially more useful combination of these two methods remains underexplored. In this paper we examine a number of approaches to their combination when learning structure and present new techniques for assessing their results.
Methods and materials
Using public-domain medical data, we run an automated causal discovery system, CaMML, which allows the incorporation of multiple kinds of prior expert knowledge into its search, to test and compare unbiased discovery with discovery biased with different kinds of expert opinion. We use adjacency matrices enhanced with numerical and colour labels to assist with the interpretation of the results. We present an algorithm for generating a single BN from a set of learned BNs that incorporates user preferences regarding complexity vs completeness. These techniques are presented as part of the first detailed workflow for hybrid structure learning within the broader knowledge engineering process.
Results
The detailed knowledge engineering workflow is shown to be useful for structuring a complex iterative BN development process. The adjacency matrices make it clear that for our medical case study using the IOWA dataset, the simplest kind of prior information (partially sorting variables into tiers) was more effective in aiding model discovery than either using no prior information or using more sophisticated and detailed expert priors. The method for generating a single BN captures relationships that would be overlooked by other approaches in the literature.
Conclusion
Hybrid causal learning of BNs is an important emerging technology. We present methods for incorporating it into the knowledge engineering process, including visualisation and analysis of the learned networks.
ER  - 
TY  - JOUR
T1  - Impact of clay minerals on bacterial diversity during the fermentation process of kimchi
A1  - Kang, Jisu
A1  - Chung, Won-Hyong
A1  - Nam, Young-Do
A1  - Kim, Daeyoung
A1  - Seo, Sung Man
A1  - Lim, Seong-Il
A1  - Lee, So-Young
Y1  - 2018///
KW  -  Bacterial diversity
KW  -  Fermentation
KW  -  Kimchi
KW  -  Next-generation sequencing
KW  - Clay mineral
JF  - Applied Clay Science
VL  - 154
SP  - 64
EP  - 72
DO  - https://doi.org/10.1016/j.clay.2017.12.018
UR  - https://www.sciencedirect.com/science/article/pii/S0169131717305677
N2  - Kimchi is a popular traditional Korean food and has various beneficial health properties, e.g., it has preventative effects against cancer, obesity, diabetes, and constipation. The characteristics of kimchi, including its texture, flavor, and functionality, are affected by the major and minor ingredients. Bentonite, a clay mineral, is used as a component in functional foods, cosmetics, and pharmaceuticals owing to its physical and chemical characteristics. In this study, different types of cation-substituted forms of bentonite (Na+, K+ and Mg2+) were used as kimchi ingredients to analyze their effect on the microbial community during fermentation for 10days. Amplicon sequencing targeting the bacterial 16S rRNA gene was conducted using the ion torrent PGM system and sequencing data were analyzed with Qiime. At the initial stage, kimchi samples exhibited diverse microflora. Lactic acid bacteria (LAB), such as Weissella, Lactobacillus, and Lactococcus, dominated after 10days of fermentation. Seven core LAB species, i.e., Leuconostoc gelidum, Leuconostoc gasicomitatum, Weissella koreensis, Leuconostoc citreum, Leuconostoc lactis, Lactobacillus sakei, and Weissella cibaria, showed differences in abundance among samples treated with different types of bentonite.
ER  - 
TY  - JOUR
T1  - Heart Disease Prediction using Exploratory Data Analysis
A1  - Indrakumari, R
A1  - Poongodi, T
A1  - Jena, Soumya Ranjan
Y1  - 2020///
KW  -  Exploratory Data Analysis
KW  -  K-means Clustering Algorithm
KW  - Tableau
JF  - Procedia Computer Science
VL  - 173
SP  - 130
EP  - 139
DO  - https://doi.org/10.1016/j.procs.2020.06.017
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920315210
N1  - International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020
N2  - Healthcare industries generate enormous amount of data, so called big data that accommodates hidden knowledge or pattern for decision making. The huge volume of data is used to make decision which is more accurate than intuition. Exploratory Data Analysis (EDA) detects mistakes, finds appropriate data, checks assumptions and determines the correlation among the explanatory variables. In the context, EDA is considered as analysing data that excludes inferences and statistical modelling. Analytics is an essential technique for any profession as it forecast the future and hidden pattern. Data analytics is considered as a cost effective technology in the recent past and it plays an essential role in healthcare which includes new research findings, emergency situations and outbreaks of disease. The use of analytics in healthcare improves care by facilitating preventive care and EDA is a vital step while analysing data. In this paper, the risk factors that causes heart disease is considered and predicted using K-means algorithm and the analysis is carried out using a publicly available data for heart disease. The dataset holds 209 records with 8 attributes such as age, chest pain type, blood pressure, blood glucose level, ECG in rest, heart rate and four types of chest pain. To predict the heart disease, K-means clustering algorithm is used along with data analytics and visualization tool. The paper discusses the pre-processing methods, classifier performances and evaluation metrics. In the result section, the visualized data shows that the prediction is accurate.
ER  - 
TY  - JOUR
T1  - Iterative fusion convolutional neural networks for classification of optical coherence tomography images
A1  - Fang, Leyuan
A1  - Jin, Yuxuan
A1  - Huang, Laifeng
A1  - Guo, Siyu
A1  - Zhao, Guangzhe
A1  - Chen, Xiangdong
Y1  - 2019///
KW  -  Convolutional neural network (CNN)
KW  -  Deep learning
KW  -  Optical coherence tomography (OCT)
KW  -  Retinal
KW  - Classification
JF  - Journal of Visual Communication and Image Representation
VL  - 59
SP  - 327
EP  - 333
DO  - https://doi.org/10.1016/j.jvcir.2019.01.022
UR  - https://www.sciencedirect.com/science/article/pii/S1047320319300306
N2  - Optical coherence tomography (OCT) can achieve the high-resolution 3D tomography imaging of the retina, which is crucial for the diagnosis of retinal diseases. Currently, the classification of retinal OCT images is mainly conducted by ophthalmologists, which is time consuming and subjective. In this paper, we propose an iterative fusion convolutional neural network (IFCNN) method for the automatic classification of retinal OCT image. In the convolutional neural network (CNN), different convolutional layers contain feature information from different scales. Therefore, the proposed network adopts an iterative fusion strategy, which iteratively combines features in current convolutional layer with those of all previous layers in the CNN network, and thus can jointly utilize the features of different convolutional layers to achieve accurate classification of OCT images. Experimental results on a real retinal OCT dataset and a musculoskeletal radiographs dataset demonstrate the superiority of the proposed method over the traditional CNN and several well-known OCT classification methods.
ER  - 
TY  - JOUR
T1  - Predicting and mapping neighborhood-scale health outcomes: A machine learning approach
A1  - Feng, Chen
A1  - Jiao, Junfeng
Y1  - 2021///
KW  -  311 service
KW  -  Crowdsourced data
KW  -  Machine learning
KW  -  Neighborhood
KW  - Urban health
JF  - Computers, Environment and Urban Systems
VL  - 85
SP  - 101562
EP  - 101562
DO  - https://doi.org/10.1016/j.compenvurbsys.2020.101562
UR  - https://www.sciencedirect.com/science/article/pii/S0198971520302957
N2  - Estimating health outcomes at a neighborhood scale is important for promoting urban health, yet costly and time-consuming. In this paper, we present a machine-learning-enabled approach to predicting the prevalence of six common non-communicable chronic diseases at the census tract level. We apply our approach to the City of Austin and show that our method can yield fairly accurate predictions. In searching for the best predictive models, we experiment with eight different machine learning algorithms and 60 predictor variables that characterize the social environment, the physical environment, and the aspects and degrees of neighborhood disorder. Our analysis suggests that (a) the sociodemographic and socioeconomic variables are the strongest predictors for tract-level health outcomes and (b) the historical records of 311 service requests can be a useful complementary data source as the information distilled from the 311 data often helps improve the models' performance. The machine learning models yielded from this study can help the public and city officials evaluate future scenarios and understand how changes in the neighborhood conditions can lead to changes in the health outcomes. By analyzing where the most significant discrepancies between the predicted and the actual values are, we will also be ready to identify areas of best practice and areas in need of greater investment or policy intervention.
ER  - 
TY  - JOUR
T1  - Diverse lesion detection from retinal images by subspace learning over normal samples
A1  - Chen, Benzhi
A1  - Wang, Lisheng
A1  - Sun, Jian
A1  - Chen, Huai
A1  - Fu, Yinghua
A1  - Lan, Shouren
A1  - Huang, Yijie
A1  - Xu, Zongben
Y1  - 2018///
KW  -  Lesion detection
KW  -  Luminance difference
KW  -  Normal fundus images
KW  -  Segmentation
KW  -  Subspace learning
KW  - Diverse types of lesions
JF  - Neurocomputing
VL  - 297
SP  - 59
EP  - 70
DO  - https://doi.org/10.1016/j.neucom.2018.03.023
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218303096
N2  - Lesion detection from retinal images is an important topic in the retinal image analysis. Many computer-aided detection techniques have been developed for detecting retinal lesions. However, these techniques are mainly used to detect specific lesion types from retinal images. They cannot be applied to detect diverse types of lesions from retinal images, which is a challenging task because lesion number and types in retinal images are generally unknown in advance, and different lesions may exhibit diverse properties in shapes, sizes, colors, textures and positions. Inspired by the doctors’ visual diagnostic mode, this paper proposes a novel computational framework to detect various types of lesions from retinal images. In this framework, many healthy fundus images are collected to act as ”doctors’ detection experience”, and local visual properties of lesions are used to distinguish true positives from false positives. A specific subspace is learned from the collected normal set and acts as a specific structural filter, by which various lesions in a retinal image can be filtered out while other normal regions keep little changes. By computing the difference image between a target image and its filtered image, different types of lesion candidates can be separated from the image. Furthermore, based on local visual context properties of lesions, the true lesions are identified from the lesion candidates. Extensive experiments have shown that the proposed method can more effectively detect diverse lesions from retinal images compared with related methods.
ER  - 
TY  - JOUR
T1  - Prediction of fatty liver disease using machine learning algorithms
A1  - Wu, Chieh-Chen
A1  - Yeh, Wen-Chun
A1  - Hsu, Wen-Ding
A1  - Islam, Md. Mohaimenul
A1  - Nguyen, Phung Anh (Alex)
A1  - Poly, Tahmina Nasrin
A1  - Wang, Yao-Chin
A1  - Yang, Hsuan-Chia
A1  - (Jack) Li, Yu-Chuan
Y1  - 2019///
KW  -  Classification model
KW  -  Machine learning
KW  -  Random forest
KW  - Fatty liver disease
JF  - Computer Methods and Programs in Biomedicine
VL  - 170
SP  - 23
EP  - 29
DO  - https://doi.org/10.1016/j.cmpb.2018.12.032
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718315724
N2  - Background and objective
Fatty liver disease (FLD) is a common clinical complication; it is associated with high morbidity and mortality. However, an early prediction of FLD patients provides an opportunity to make an appropriate strategy for prevention, early diagnosis and treatment. We aimed to develop a machine learning model to predict FLD that could assist physicians in classifying high-risk patients and make a novel diagnosis, prevent and manage FLD.
Methods
We included all patients who had an initial fatty liver screening at the New Taipei City Hospital between 1st and 31st December 2009. Classification models such as random forest (RF), Naïve Bayes (NB), artificial neural networks (ANN), and logistic regression (LR) were developed to predict FLD. The area under the receiver operating characteristic curve (ROC) was used to evaluate performances among the four models.
Results
A total of 577 patients were included in this study; of those 377 patients had fatty liver. The area under the receiver operating characteristic (AUROC) of RF, NB, ANN, and LR with 10 fold-cross validation was 0.925, 0.888, 0.895, and 0.854 respectively. Additionally, The accuracy of RF, NB, ANN, and LR 87.48, 82.65, 81.85, and 76.96%.
Conclusion
In this study, we developed and compared the four classification models to predict fatty liver disease accurately. However, the random forest model showed higher performance than other classification models. Implementation of a random forest model in the clinical setting could help physicians to stratify fatty liver patients for primary prevention, surveillance, early treatment, and management.
ER  - 
TY  - JOUR
T1  - Fog-centric IoT based smart healthcare support service for monitoring and controlling an epidemic of Swine Flu virus
A1  - Singh, Prabh Deep
A1  - Kaur, Rajbir
A1  - Singh, Kiran Deep
A1  - Dhiman, Gaurav
A1  - Soni, Mukesh
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  COVID-19
KW  -  Corona virus
KW  -  Ensemble classifier
KW  -  Quality of service
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100636
EP  - 100636
DO  - https://doi.org/10.1016/j.imu.2021.100636
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821001258
N2  - Disease detection is a time-consuming and essential task in the medical diagnosis system. Machine learning plays a vital role in predicting and identifying diseases at various stages. It is a very random and timely method for analyzing disease using clinical and laboratory signs and assists medical representatives in developing a more effective diagnostic strategy for such diseases. For example, swine flu, a contagious illness caused by influenza viruses, including the H1N1 virus, infects the respiratory tract of pigs, causing a barking cough, decreased appetite, nasal secretions, and uncontrollable behaviour. Cloud computing and the Internet of things help the medical sector by processing health information in ultra-low delay so that effective decisions can be taken timely. In this paper, a fog-centric IoT-based smart healthcare support service for monitoring and controlling the Swine Flu virus epidemic is proposed. The proposed framework utilizes the concept of fog computing for delay-sensitive applications. Furthermore, a hybrid classifier is used to classify the swine flu patient at an early stage and generate alerts to the health officials and patients' guardians. In the experimental setup, the iFogSim simulator is used to mimic the IoT devices and fog nodes for evaluating various parameters such as accuracy, energy, and Latency, whereas WEKA is used for developing a hybrid classifier. Results demonstrate the benefits of combining fog and cloud computing services to achieve higher network bandwidth reliability, a higher level of operation, and a shorter response time while generating real-time notifications, as compared to an existing cloud-only model.
ER  - 
TY  - JOUR
T1  - Drug repurposing for COVID-19 via knowledge graph completion
A1  - Zhang, Rui
A1  - Hristovski, Dimitar
A1  - Schutte, Dalton
A1  - Kastrin, Andrej
A1  - Fiszman, Marcelo
A1  - Kilicoglu, Halil
Y1  - 2021///
KW  -  Drug repurposing
KW  -  Knowledge graph completion
KW  -  Literature-based discovery
KW  -  Text mining
KW  - COVID-19
JF  - Journal of Biomedical Informatics
VL  - 115
SP  - 103696
EP  - 103696
DO  - https://doi.org/10.1016/j.jbi.2021.103696
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000253
N2  - Objective
To discover candidate drugs to repurpose for COVID-19 using literature-derived knowledge and knowledge graph completion methods.
Methods
We propose a novel, integrative, and neural network-based literature-based discovery (LBD) approach to identify drug candidates from PubMed and other COVID-19-focused research literature. Our approach relies on semantic triples extracted using SemRep (via SemMedDB). We identified an informative and accurate subset of semantic triples using filtering rules and an accuracy classifier developed on a BERT variant. We used this subset to construct a knowledge graph, and applied five state-of-the-art, neural knowledge graph completion algorithms (i.e., TransE, RotatE, DistMult, ComplEx, and STELP) to predict drug repurposing candidates. The models were trained and assessed using a time slicing approach and the predicted drugs were compared with a list of drugs reported in the literature and evaluated in clinical trials. These models were complemented by a discovery pattern-based approach.
Results
Accuracy classifier based on PubMedBERT achieved the best performance (F1 = 0.854) in identifying accurate semantic predications. Among five knowledge graph completion models, TransE outperformed others (MR = 0.923, Hits@1 = 0.417). Some known drugs linked to COVID-19 in the literature were identified, as well as others that have not yet been studied. Discovery patterns enabled identification of additional candidate drugs and generation of plausible hypotheses regarding the links between the candidate drugs and COVID-19. Among them, five highly ranked and novel drugs (i.e., paclitaxel, SB 203580, alpha 2-antiplasmin, metoclopramide, and oxymatrine) and the mechanistic explanations for their potential use are further discussed.
Conclusion
We showed that a LBD approach can be feasible not only for discovering drug candidates for COVID-19, but also for generating mechanistic explanations. Our approach can be generalized to other diseases as well as to other clinical questions. Source code and data are available at https://github.com/kilicogluh/lbd-covid.
ER  - 
TY  - JOUR
T1  - Real-time apnea-hypopnea event detection during sleep by convolutional neural networks
A1  - Choi, Sang Ho
A1  - Yoon, Heenam
A1  - Kim, Hyun Seok
A1  - Kim, Han Byul
A1  - Kwon, Hyun Bin
A1  - Oh, Sung Min
A1  - Lee, Yu Jin
A1  - Park, Kwang Suk
Y1  - 2018///
KW  -  Convolutional neural networks
KW  -  Nasal pressure signal
KW  -  Real-time monitoring
KW  -  Sleep apnea and hypopnea syndrome diagnosis
KW  - Apnea-hypopnea event detection
JF  - Computers in Biology and Medicine
VL  - 100
SP  - 123
EP  - 131
DO  - https://doi.org/10.1016/j.compbiomed.2018.06.028
UR  - https://www.sciencedirect.com/science/article/pii/S0010482518301768
N2  - Sleep apnea-hypopnea event detection has been widely studied using various biosignals and algorithms. However, most minute-by-minute analysis techniques have difficulty detecting accurate event start/end positions. Furthermore, they require hand-engineered feature extraction and selection processes. In this paper, we propose a new approach for real-time apnea-hypopnea event detection using convolutional neural networks and a single-channel nasal pressure signal. From 179 polysomnographic recordings, 50 were used for training, 25 for validation, and 104 for testing. Nasal pressure signals were adaptively normalized, and then segmented by sliding a 10-s window at 1-s intervals. The convolutional neural networks were trained with the data, which consisted of class-balanced segments, and were then tested to evaluate their event detection performance. According to a segment-by-segment analysis, the proposed method exhibited performance results with a Cohen's kappa coefficient of 0.82, a sensitivity of 81.1%, a specificity of 98.5%, and an accuracy of 96.6%. In addition, the Pearson's correlation coefficient between estimated apnea-hypopnea index (AHI) and reference AHI was 0.99, and the average accuracy of sleep apnea and hypopnea syndrome (SAHS) diagnosis was 94.9% for AHI cutoff values of ≥5, 15, and 30 events/h. Our approach could potentially be used as a supportive method to reduce event detection time in sleep laboratories. In addition, it can be applied to screen SAHS severity before polysomnography.
ER  - 
TY  - JOUR
T1  - Machine Learning approach applied to Human Activity Recognition – An application to the VanKasteren dataset
A1  - Paola, Ariza-Colpas
A1  - Alvaro Agustín, Oñate-Bowen
A1  - Eydy del Carmen, Suarez-Brieva
A1  - Ana, Oviedo-Carrascal
A1  - Miguel, Urina Triana
A1  - Marlon, Piñeres-Melo
A1  - Shariq Aziz, Butt
A1  - Collazos Morales, Carlos Andrés
A1  - Ramón Enrique, Ramayo González
Y1  - 2021///
KW  -  ADL
KW  -  Activity Daily Living
KW  -  HAR
KW  -  Human Activity Recognition
KW  -  VanKasteren Dataset
KW  - Machine learning
JF  - Procedia Computer Science
VL  - 191
SP  - 367
EP  - 372
DO  - https://doi.org/10.1016/j.procs.2021.07.070
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921014733
N1  - The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology
N2  - Reminders are a core component of many assistive technology systems and are aimed specifically at helping people with dementia function more independently by compensating for cognitive deficits. These technologies are often utilized for prospective reminding, reminiscence, or within coaching-based systems. Traditionally, reminders have taken the form of nontechnology based aids, such as diaries, notebooks, cue cards and white boards. This article is based on the use of machine learning algorithms for the detection of Alzheimer’s disease. In the experimentation, the LWL, SimpleLogistic, Logistic, MultiLayerPercepton and HiperPipes algorithms were used. The result showed that the LWL algorithm produced the following results: Accuracy 98.81%, Precission 100%, Recall 97.62% and F- measure 98.80%
ER  - 
TY  - JOUR
T1  - Semen quality predictive model using Feed Forwarded Neural Network trained by Learning-Based Artificial Algae Algorithm
A1  - Yibre, Abdulkerim M
A1  - Koçer, Barış
Y1  - 2021///
KW  -  Feed Forwarded Neural Network
KW  -  Imbalanced data classification
KW  -  Machine learning
KW  -  Seminal quality
KW  - Artificial Algae Algorithm
JF  - Engineering Science and Technology, an International Journal
VL  - 24
IS  - 2
SP  - 310
EP  - 318
DO  - https://doi.org/10.1016/j.jestch.2020.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S2215098620342099
N2  - Recent scientific studies have noted that the seminal quality of males is significantly decreasing due to lifestyle and environmental factors. Clinical diagnosis of sperm quality is one important aspect of identifying the potential of semen for the occurrence of pregnancy. Due to the advances in machine learning algorithms, especially the reliable and high classification accuracy of neural network in health related problems, it is becoming possible to predict seminal quality from lifestyle data. In this respect, a few attempts were made in predicting seminal quality. These studies were conducted using imbalanced datasets, where the performance outcomes tend to be biased towards the majority class. Other studies implemented the gradient descent technique for training the neural network. The gradient descent is a local training technique that is prone to get stuck to local minima. On the contrary, meta-heuristic algorithms enable searching solutions both locally and globally. Therefore, in this study, Artificial Algae Algorithm that is improved using a Learning-Based fitness evaluation method is proposed for training Feed Forward Neural Network (FFNN). In addition, the SMOTE data balancing method was employed to balance normal and abnormal instances. Experimental analyses were carried out to evaluate the predictive accuracy of the FFNN trained using Learning-Based Artificial Algae Algorithm (FFNN-LBAAA). The results were compared with well-known machine learning algorithms, namely: Multi-layer Perceptron Neural Network (MLP), Naïve Bayes (NB), Support Vector Machine (SVM), K-Nearest Neighbor (KNN) and Random Forest (RF) algorithms. The proposed approach showed superior performance in discriminating normal and abnormal semen quality instances over the other compared algorithms.
ER  - 
TY  - JOUR
T1  - Simple methods for segmentation and measurement of diabetic retinopathy lesions in retinal fundus images
A1  - Köse, Cemal
A1  - Şevik, Uğur
A1  - İkibaş, Cevat
A1  - Erdöl, Hidayet
Y1  - 2012///
KW  -  Automatic segmentation
KW  -  Background image extraction
KW  -  DR
KW  -  Medical image analysis
KW  -  Optic disc
KW  -  Retinal images
KW  - Automatic diagnosis of DR
JF  - Computer Methods and Programs in Biomedicine
VL  - 107
IS  - 2
SP  - 274
EP  - 293
DO  - https://doi.org/10.1016/j.cmpb.2011.06.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260711001726
N2  - Diabetic retinopathy (DR) is one of the most important complications of diabetes mellitus, which causes serious damages in the retina, consequently visual loss and sometimes blindness if necessary medical treatment is not applied on time. One of the difficulties in this illness is that the patient with diabetes mellitus requires a continuous screening for early detection. So far, numerous methods have been proposed by researchers to automate the detection process of DR in retinal fundus images. In this paper, we developed an alternative simple approach to detect DR. This method was built on the inverse segmentation method, which we suggested before to detect Age Related Macular Degeneration (ARMDs). Background image approach along with inverse segmentation is employed to measure and follow up the degenerations in retinal fundus images. Direct segmentation techniques generate unsatisfactory results in some cases. This is because of the fact that the texture of unhealthy areas such as DR is not homogenous. The inverse method is proposed to exploit the homogeneity of healthy areas rather than dealing with varying structure of unhealthy areas for segmenting bright lesions (hard exudates and cotton wool spots). On the other hand, the background image, dividing the retinal image into high and low intensity areas, is exploited in segmentation of hard exudates and cotton wool spots, and microaneurysms (MAs) and hemorrhages (HEMs), separately. Therefore, a complete segmentation system is developed for segmenting DR, including hard exudates, cotton wool spots, MAs, and HEMs. This application is able to measure total changes across the whole retinal image. Hence, retinal images that belong to the same patients are examined in order to monitor the trend of the illness. To make a comparison with other methods, a Naïve Bayes method is applied for segmentation of DR. The performance of the system, tested on different data sets including various qualities of retinal fundus images, is over 95% in detection of the optic disc (OD), and 90% in segmentation of the DR.
ER  - 
TY  - JOUR
T1  - Retinal vessel segmentation using multiwavelet kernels and multiscale hierarchical decomposition
A1  - Wang, Yangfan
A1  - Ji, Guangrong
A1  - Lin, Ping
A1  - Trucco, Emanuele
Y1  - 2013///
KW  -  Matched filter
KW  -  Multiscale hierarchical decomposition
KW  -  Multiwavelet
KW  -  Retinal images
KW  -  Segmentation
KW  - Vessel detection
JF  - Pattern Recognition
VL  - 46
IS  - 8
SP  - 2117
EP  - 2133
DO  - https://doi.org/10.1016/j.patcog.2012.12.014
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313000241
N2  - We propose a comprehensive method for segmenting the retinal vasculature in fundus camera images. Our method does not require preprocessing and training and can therefore be used directly on different images sets. We enhance the vessels using matched filtering with multiwavelet kernels (MFMK), separating vessels from clutter and bright, localized features. Noise removal and vessel localization are achieved by a multiscale hierarchical decomposition of the normalized enhanced image. We show a necessary condition to achieve the optimal decomposition and derive the associated value of the scale parameter controlling the amount of details captured. Finally, we obtain a binary map of the vasculature by locally adaptive thresholding, generating a threshold surface based on the vessel edge information extracted by the previous processes. We report experimental results on two public retinal data sets, DRIVE and STARE, demonstrating an excellent performance in comparison with retinal vessel segmentation methods reported recently.
ER  - 
TY  - JOUR
T1  - Thyroid Disease Treatment prediction with machine learning approaches
A1  - Aversano, Lerina
A1  - Bernardi, Mario Luca
A1  - Cimitile, Marta
A1  - Iammarino, Martina
A1  - Macchia, Paolo Emidio
A1  - Nettore, Immacolata Cristina
A1  - Verdone, Chiara
Y1  - 2021///
KW  -  Classifiers
KW  -  K-Cross Validation
KW  -  Prediction
KW  -  Thyroid treatment
KW  -  machine learning
KW  - Thyroid diseases
JF  - Procedia Computer Science
VL  - 192
SP  - 1031
EP  - 1040
DO  - https://doi.org/10.1016/j.procs.2021.08.106
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921015945
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 25th International Conference KES2021
N2  - The thyroid is an endocrine gland located in the anterior region of the neck: its main task is to produce thyroid hormones, which are functional to our entire body. Its possible dysfunction can lead to the production of an insufficient or excessive amount of thyroid hormone. Therefore, the thyroid can become inflamed or swollen due to one or more swellings forming inside it. Some of these nodules can be the site of malignant tumors. One of the most used treatments is sodium levothyroxine, also known as LT4, a synthetic thyroid hormone used in the treatment of thyroid disorders and diseases. Predictions about the treatment can be important for supporting endocrinologists’ activities and improve the quality of the patients’ life. To date, there are numerous studies in the literature that focus on the prediction of thyroid diseases on the trend of the hormonal parameters of people. This work, differently, aims to predict the LT4 treatment trend for patients suffering from hypothyroidism. To this end, a dedicated dataset was built that includes medical information related to patients being treated in the ”AOU Federico II” hospital of Naples. For each patient, the clinical history is available over time, and therefore on the basis of the trend of the hormonal parameters and other attributes considered it was possible to predict the course of each patient’s treatment in order to understand if this should be increased or decreased. To conduct this study, we used different machine learning algorithms. In particular, we compared the results of 10 different classifiers. The performances of the different algorithms show good results, especially in the case of the Extra-Tree Classifier, where the accuracy reaches 84%.
ER  - 
TY  - JOUR
T1  - Adverse drug reaction detection on social media with deep linguistic features
A1  - Zhang, Ying
A1  - Cui, Shaoze
A1  - Gao, Huiying
Y1  - 2020///
KW  -  Deep linguistic features
KW  -  Feature-based method
KW  -  Pharmacovigilance
KW  -  Social media
KW  - Adverse drug reactions
JF  - Journal of Biomedical Informatics
VL  - 106
SP  - 103437
EP  - 103437
DO  - https://doi.org/10.1016/j.jbi.2020.103437
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300654
N2  - Adverse reactions caused by drugs are one of the most important public health problems. Social media has encouraged more patients to share their drug use experiences and has become a major source for the detection of professionally unreported adverse drug reactions (ADRs). Since a large number of user posts do not mention any ADR, accurate detection of the presence of ADRs in each user post is necessary before further research can be conducted. Previous feature-based methods focus on extracting more shallow linguistic features that are unable to capture deep and subtle information in the context, ultimately failing to provide satisfactory accuracy. To overcome the limitations of previous studies, this paper proposes a novel method that can extract deep linguistic features and then combine them with shallow linguistic features for ADR detection. We first extract predicate-ADR pairs under the guidance of extended syntactic dependencies and ADR lexicon. Then, we extract semantic and part-of-speech (POS) features for each pair and pool the features of different pairs to generate a holistic representation of deep linguistic features. Finally, we use the collection of deep features and several shallow features to train the predictive models. A series of experiments are performed on data sets collected from DailyStrength and Twitter. Our approach can achieve AUCs of 94.44% and 88.97% on the two data sets, respectively, outperforming other state-of-the-art methods. The results demonstrate the potential benefits of deep linguistic features for ADR detection on social data. This method can be applied to multiple other healthcare and text analysis tasks and can be used to support pharmacovigilance research.
ER  - 
TY  - JOUR
T1  - Customer relationship management analysis of outpatients in a Chinese infectious disease hospital using drug-proportion recency-frequency-monetary model
A1  - Li, Min
A1  - Wang, Qunwei
A1  - Shen, Yinzhong
A1  - Zhu, TongYu
Y1  - 2021///
KW  -  Hospitals for infectious diseases
KW  -  K-means
KW  -  Patient types
KW  -  dRFM model
KW  - CRM
JF  - International Journal of Medical Informatics
VL  - 147
SP  - 104373
EP  - 104373
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104373
UR  - https://www.sciencedirect.com/science/article/pii/S1386505620319092
N2  - Background
Identifying the patient types with different economic values can be useful for hospital development.
Objective
This work uses the theory of customer relationship management (CRM) to analyze the outpatients in the hospital for infectious diseases in Shanghai, China.
Methods
A total of 2,271,020 data elements of outpatients in the research unit between August 2009 and December 2019 were extracted, analyzed and cleaned to obtain 171,107 valid data elements (1 element per person). The main diseases were viral hepatitis B (VHB) and acquired immunodeficiency syndrome (AIDS), and the average percentage of drug expenditure was 80.39 %. We innovatively expanded the classic RFM (R: recency, F: frequency, M: monetary) model in CRM to the dRFM (d: percentage of drug expenditure) model. We selected the best clustering algorithm from the K-means, Kohonen and two-step clustering methods to find the optimal model to distinguish the types of patients with different economic values and the best decision-making algorithm from the C5.0, CART classification regression tree, CHAID and QUEST algorithms to verify the model.
Results
After performing two rounds of K-means clustering analysis on three models: RFM, RFM + dRFM and dRFM, and 97,855 data elements were retained. The RFM + dRFM model was the optimal model, clustering the patients into 3 types: potential patients (24.2 %) to be retained, with a high drug expenditure and the last visit in more than 19.06 months, high-value patients (24.5 %) to be attracted, with the last visit in about 6.66 months; basal patients (51.3 %) to be kept, with the last visit in about 3.7 months. The model was then verified using the C5.0 decision tree algorithm with an accuracy rate of 99.97 %.
Conclusion
This objective CRM analysis of the patients in the hospital for infectious diseases using the dRFM model accurately identified different types of patients, providing an objective and effective basis for hospital management.
ER  - 
TY  - JOUR
T1  - Morphological classifiers
A1  - Rodrigues, É O
A1  - Conci, A
A1  - Liatsis, P
Y1  - 2018///
KW  -  Mathematical morphology
KW  -  Set theory
KW  -  Supervised learning
KW  - Morphological classifier
JF  - Pattern Recognition
VL  - 84
SP  - 82
EP  - 96
DO  - https://doi.org/10.1016/j.patcog.2018.06.010
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318302206
N2  - This work proposes a new type of classifier called Morphological Classifier (MC). MCs aggregate concepts from mathematical morphology and supervised learning. The outcomes of this aggregation are classifiers that may preserve shape characteristics of classes, subject to the choice of a stopping criterion and structuring element. MCs are fundamentally based on set theory, and their classification model can be a mathematical set itself. Two types of morphological classifiers are proposed in the current work, namely, Morphological k-NN (MkNN) and Morphological Dilation Classifier (MDC), which demonstrate the feasibility of the approach. This work provides evidence regarding the advantages of MCs, e.g., very fast classification times as well as competitive accuracy rates. The performance of MkNN and MDC was tested using p-dimensional datasets. MCs tied or outperformed 14 well established classifiers in 5 out of 8 datasets. In all occasions, the obtained accuracies were higher than the average accuracy obtained with all classifiers. Moreover, the proposed implementations utilize the power of the Graphics Processing Units (GPUs) to speed up processing.
ER  - 
TY  - JOUR
T1  - Detection of blood vessels in ophthalmoscope images using MF/ant (matched filter/ant colony) algorithm
A1  - Cinsdikici, Muhammed Gökhan
A1  - Aydın, Doğan
Y1  - 2009///
KW  -  Ant algorithm
KW  -  DRIVE database
KW  -  Matched filter
KW  - Retinal (Ophthalmoscope) image blood vessel segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 96
IS  - 2
SP  - 85
EP  - 95
DO  - https://doi.org/10.1016/j.cmpb.2009.04.005
UR  - https://www.sciencedirect.com/science/article/pii/S0169260709001230
N2  - Blood vessels in ophthalmoscope images play an important role in diagnosis of some serious pathologies on retinal images. Hence, accurate extraction of vessels is becoming a main topic of this research area. Matched filter (MF) implementation for blood vessel detection is one of the methods giving more accurate results. Using this filter alone might not recover all the vessels (especially the capillaries). In this paper, a novel approach (MF/ant algorithm) is proposed to overcome the deficiency of the MF. The proposed method is a hybrid model of matched filter and ant colony algorithm. In this work, the accuracy and parameters of the hybrid algorithm are also discussed. The proposed method shows its success using the well known reference ophthalmoscope images of DRIVE database.
ER  - 
TY  - JOUR
T1  - On the evolutionary weighting of neighbours and features in the k-nearest neighbour rule
A1  - Mateos-García, Daniel
A1  - García-Gutiérrez, Jorge
A1  - Riquelme-Santos, José C
Y1  - 2019///
KW  -  Feature weighting
KW  -  Neighbours weighting
KW  - Evolutionary computation
JF  - Neurocomputing
VL  - 326-327
SP  - 54
EP  - 60
DO  - https://doi.org/10.1016/j.neucom.2016.08.159
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217315333
N2  - This paper presents an evolutionary method for modifying the behaviour of the k-Nearest-Neighbour classifier (kNN) called Simultaneous Weighting of Attributes and Neighbours (SWAN). Unlike other weighting methods, SWAN presents the ability of adjusting the contribution of the neighbours and the significance of the features of the data. The optimization process focuses on the search of two real-valued vectors. One of them represents the votes of neighbours, and the other one represents the weight of each feature. The synergy between the two sets of weights found in the optimization process helps to improve significantly, the classification accuracy. The results on 35 datasets from the UCI repository suggest that SWAN statistically outperforms the other weighted kNN methods.
ER  - 
TY  - JOUR
T1  - R-HEFS: Rough set based heterogeneous ensemble feature selection method for medical data classification
A1  - Bania, Rubul Kumar
A1  - Halder, Anindya
Y1  - 2021///
KW  -  Classification
KW  -  Medical data
KW  -  Rough set
KW  -  Stability
KW  - Ensemble feature selection
JF  - Artificial Intelligence in Medicine
VL  - 114
SP  - 102049
EP  - 102049
DO  - https://doi.org/10.1016/j.artmed.2021.102049
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721000427
N2  - Feature selection is one of the trustworthy processes of dimensionality reduction technique to select a subset of relevant and non-redundant features from large datasets. Ensemble feature selection (EFS) approach is a recent technique aiming at accumulating diversity in the subset of selected features. It improves the performance of learning algorithms and obtains more stable and robust results. In this paper, a novel rough set theory (RST) based heterogeneous EFS method (R-HEFS) is proposed for selecting the less redundant and highly relevant features during the aggregation of diverse feature subsets by applying the feature-class, feature-feature rough dependency and feature-significance measures. In R-HEFS five state-of-the-art RST based filter methods are used as a base feature selectors. Experiments are carried out on 10 benchmark medical datasets collected from the UCI repository. For the imputation of the missing values and discretization of the continuous features, k nearest neighbor (kNN) imputation method and RST based discretization techniques are applied. The effectiveness of the proposed R-HEFS method is evaluated and analyzed by using four benchmark classifiers viz., Naïve Bayes (NB), random forest (RF), support vector machine (SVM), and AdaBoost. The proposed R-HEFS method turns out to be effective by removing the non-relevant and redundant features during the process of aggregation of base feature selectors and it assists to increase the classification accuracy. Out of 10 different medical datasets, on 7 datasets, R-HEFS has achieved better average classification accuracy. So, the overall results strongly suggest that the proposed R-HEFS method can reduce the dimension of large medical datasets and may help the physicians or medical experts to diagnose (classify) different diseases with lesser computational complexities.
ER  - 
TY  - JOUR
T1  - Determining the direction of the local search in topological ordering space for Bayesian network structure learning
A1  - Wang, Zidong
A1  - Gao, Xiaoguang
A1  - Tan, Xiangyuan
A1  - Liu, Xiaohan
Y1  - 2021///
KW  -  Local search
KW  -  Structure learning
KW  - Bayesian network
JF  - Knowledge-Based Systems
VL  - 234
SP  - 107566
EP  - 107566
DO  - https://doi.org/10.1016/j.knosys.2021.107566
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121008285
N2  - Local search in a topological ordering space is an efficient way of learning Bayesian network structures in large-scale problems. However, existing algorithms typically focus on stochastically developing the neighborhood of an ordering without a specific direction and can quickly stop at a local optimum. In this study, a novel approach is proposed to improve the capability of a local search by determining the search direction. The direction of the search step is identified with respect to the priority in a score cache. We also design robust terminal conditions and insertion methods based on the proposed operator. The direction of escaping from local optima is identified by transferring the ordering to a new restart, which implies an equivalent class of the optimal structure. Moreover, we adopt a breadth-first search method for the conversion between structures and orderings. The identification of the direction can accelerate the convergence of the local search and acquire the higher quality structure. Furthermore, our experimental results demonstrated that the proposed methods highly improve the accuracy and efficiency of learning the optimal ordering from the training dataset compared with state-of-the-art algorithms.
ER  - 
TY  - JOUR
T1  - Simple hybrid method for fine microaneurysm detection from non-dilated diabetic retinopathy retinal images
A1  - Sopharak, Akara
A1  - Uyyanonvara, Bunyarit
A1  - Barman, Sarah
Y1  - 2013///
KW  -  Microaneurysms
KW  -  Naive Bayes classifier
KW  - Diabetic retinopathy
JF  - Computerized Medical Imaging and Graphics
VL  - 37
IS  - 5
SP  - 394
EP  - 402
DO  - https://doi.org/10.1016/j.compmedimag.2013.05.005
UR  - https://www.sciencedirect.com/science/article/pii/S0895611113001006
N1  - Retinal Image Analysis
N2  - Microaneurysms detection is an important task in computer aided diagnosis of diabetic retinopathy. Microaneurysms are the first clinical sign of diabetic retinopathy, a major cause of vision loss in diabetic patients. Early microaneurysm detection can help reduce the incidence of blindness. Automatic detection of microaneurysms is still an open problem due to their tiny sizes, low contrast and also similarity with blood vessels. It is particularly very difficult to detect fine microaneurysms, especially from non-dilated pupils and that is the goal of this paper. Simple yet effective methods are used. They are coarse segmentation using mathematic morphology and fine segmentation using naive Bayes classifier. A total of 18 microaneurysms features are proposed in this paper and they are extracted for naive Bayes classifier. The detected microaneurysms are validated by comparing at pixel level with ophthalmologists’ hand-drawn ground-truth. The sensitivity, specificity, precision and accuracy are 85.68, 99.99, 83.34 and 99.99%, respectively.
ER  - 
TY  - JOUR
T1  - Cost-sensitive case-based reasoning using a genetic algorithm: Application to medical diagnosis
A1  - Park, Yoon-Joo
A1  - Chun, Se-Hak
A1  - Kim, Byung-Chun
Y1  - 2011///
KW  -  Breast cancer
KW  -  Diabetes
KW  -  Genetic algorithm
KW  -  Heart disease
KW  -  Hepatitis
KW  -  Medical diagnosis
KW  -  Misclassification cost
KW  - Cost-sensitive case-based reasoning
JF  - Artificial Intelligence in Medicine
VL  - 51
IS  - 2
SP  - 133
EP  - 145
DO  - https://doi.org/10.1016/j.artmed.2010.12.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365710001387
N1  - Advances in Case-Based Reasoning in the Health Sciences
N2  - Objective
The paper studies the new learning technique called cost-sensitive case-based reasoning (CSCBR) incorporating unequal misclassification cost into CBR model. Conventional CBR is now considered as a suitable technique for diagnosis, prognosis and prescription in medicine. However it lacks the ability to reflect asymmetric misclassification and often assumes that the cost of a positive diagnosis (an illness) as a negative one (no illness) is the same with that of the opposite situation. Thus, the objective of this research is to overcome the limitation of conventional CBR and encourage applying CBR to many real world medical cases associated with costs of asymmetric misclassification errors.
Methods
The main idea involves adjusting the optimal cut-off classification point for classifying the absence or presence of diseases and the cut-off distance point for selecting optimal neighbors within search spaces based on similarity distribution. These steps are dynamically adapted to new target cases using a genetic algorithm. We apply this proposed method to five real medical datasets and compare the results with two other cost-sensitive learning methods—C5.0 and CART.
Results
Our finding shows that the total misclassification cost of CSCBR is lower than other cost-sensitive methods in many cases. Even though the genetic algorithm has limitations in terms of unstable results and over-fitting training data, CSCBR results with GA are better overall than those of other methods. Also the paired t-test results indicate that the total misclassification cost of CSCBR is significantly less than C5.0 and CART for several datasets.
Conclusion
We have proposed a new CBR method called cost-sensitive case-based reasoning (CSCBR) that can incorporate unequal misclassification costs into CBR and optimize the number of neighbors dynamically using a genetic algorithm. It is meaningful not only for introducing the concept of cost-sensitive learning to CBR, but also for encouraging the use of CBR in the medical area. The result shows that the total misclassification costs of CSCBR do not increase in arithmetic progression as the cost of false absence increases arithmetically, thus it is cost-sensitive. We also show that total misclassification costs of CSCBR are the lowest among all methods in four datasets out of five and the result is statistically significant in many cases. The limitation of our proposed CSCBR is confined to classify binary cases for minimizing misclassification cost because our proposed CSCBR is originally designed to classify binary case. Our future work extends this method for multi-classification which can classify more than two groups.
ER  - 
TY  - JOUR
T1  - Identifying Disease -Treatment Relations Using Machine Learning Approach
A1  - Keerrthega, M C
A1  - Thenmozhi, D
Y1  - 2016///
KW  -  Machine Learning
KW  -  SMO Classifcation
KW  - Natural Language Processing
JF  - Procedia Computer Science
VL  - 87
SP  - 306
EP  - 315
DO  - https://doi.org/10.1016/j.procs.2016.05.166
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916305063
N1  - Fourth International Conference on Recent Trends in Computer Science &amp; Engineering (ICRTCSE 2016)
N2  - Identifying the disease treatment relation enables to find what disease a person suffers from and what appropriate treatment can be given to that person. The semantic relation tags namely Cure, Prevent and Sideeffects helps to find out the relationship between disease and treatment. Many methodologies like co-occurrence analysis, rule based methodologies and statistical methods are used in disease treatment relation. However, machine learning is widely used in many applications like protein-protein interaction, extraction of medical knowledge and in health care field. we propose a machine learning approach termed as SMO classification, which uses several features namely medical papers, medical abstracts. Our approach identifies the features namely disease-treat, cure, prevent and sideeffects. The performance can be measured by Accuracy, Precision, F-measure and Recall.
ER  - 
TY  - JOUR
T1  - Predictive analysis in outpatients assisted by the Internet of Medical Things
A1  - Jin, Yue
A1  - Yu, Han
A1  - Zhang, Yin
A1  - Pan, Ning
A1  - Guizani, Mohsen
Y1  - 2019///
KW  -  BP neural network
KW  -  Multidimensional features
KW  -  Outpatient number prediction
KW  - IoMT
JF  - Future Generation Computer Systems
VL  - 98
SP  - 219
EP  - 226
DO  - https://doi.org/10.1016/j.future.2019.01.019
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18327717
N2  - With the rise of the Internet of Things era, data resources can be acquired in real time through intelligent sensing technology of IoMT (Internet of Medical Things), and it is very helpful for the analysis and prediction of medical data. Through the analysis and study of the data of the department of respiration combined with the data of the air quality dimension, meteorological dimension, and time dimension, this paper studies its related features and establishes a multidimensional features prediction model based on a BP neural network. The comparative experiments prove that the model prediction has a good effect. At the same time, this paper clarifies the validity of the multidimensional prediction model by comparing the relevant research. The results of the comparative experiments in this paper show that the outpatient quantity prediction is not a simple time-series problem, and it contains a variety of nonlinear influencing factors. In the contrast experiments, the multi-dimensional prediction model including air quality feature is better than others, and the experimental results prove that air quality indicators play an important role in the prediction of respiratory clinic outpatient visits. In addition, this paper discusses the significant lag effect on the prediction of respiratory consultations, and the four-day lag prediction has the best effect in the relatively short-term study. It indirectly proves that air has a lagging effect on respiratory diseases, and provides a reference for future research, no matter medical or modeling. The research has important reference value for the management and distribution of medical resources and the formulation of medical policies as well as important significance for the formulation of policies for disease prevention and the prevention and control of environmental pollution.
ER  - 
TY  - JOUR
T1  - Segmentation of optic disc and blood vessels in retinal images using wavelets, mathematical morphology and Hessian-based multi-scale filtering
A1  - Rodrigues, Luiz Carlos
A1  - Marengoni, Maurício
Y1  - 2017///
KW  -  Graphs
KW  -  Mathematical morphology
KW  -  Wavelets
KW  - Retinal fundus images
JF  - Biomedical Signal Processing and Control
VL  - 36
SP  - 39
EP  - 49
DO  - https://doi.org/10.1016/j.bspc.2017.03.014
UR  - https://www.sciencedirect.com/science/article/pii/S1746809417300629
N2  - The high importance of the accurate and early diagnostic has motivated the development of computer vision techniques of image processing and segmentation required for an completely automated assessment system for the retinal conditions. In this study we present a new algorithm built on wavelets transforms and mathematical morphology for detecting the optic disc and we explore the tubular characteristic of the blood vessels to segment the retinal veins and arteries. Both, optic disc and vascular structure, are landmarks for image registration and are essential for the retinal image analysis. Instead of a manual try and error method to choose the best parameters for detecting vessels as accurately as possible, we used a genetic algorithm and its sequence of generations and crossovers. However the technique of exploring the tubular characteristic of the vessels reaches its limits when the vessels are represented by, sometimes not continuous, winding lines of 1 pixel. To overcome this limitation we adopted a graph based approach using Dijkstra's shortest path algorithm to track the segments and a statistic method of Student t distribution to assess whether or not the identified segment is part of the vascular structure. The proposed method was developed and tested on the Digital Retinal Images for Vessel Extraction (DRIVE) freely available database, which contains 40 annotated color eye fundus image.
ER  - 
TY  - JOUR
T1  - A size-invariant convolutional network with dense connectivity applied to retinal vessel segmentation measured by a unique index
A1  - Zhuo, Zhongshuo
A1  - Huang, Jianping
A1  - Lu, Ke
A1  - Pan, Daru
A1  - Feng, Shouting
Y1  - 2020///
KW  -  Convolutional network
KW  -  Dense connectivity
KW  -  Evaluation metrics
KW  - Retinal vessel
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105508
EP  - 105508
DO  - https://doi.org/10.1016/j.cmpb.2020.105508
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719305292
N2  - Background and objectives: Retinal vessel segmentation (RVS) helps in diagnosing diseases such as hypertension, cardiovascular diseases, and others. Convolutional neural networks are widely used in RVS tasks. However, how to comprehensively evaluate the segmentation results and how to improve the networks’ learning ability are two great challenges. Methods: In this paper, we proposed an ingenious index: fusion score (FS), which provides an overall measure for those binary images. The FS converts multiple metrics into a single target, and therefore facilitates the optimal threshold’s selection and models’ comparison. In addition, We simultaneously combined size-invariant feature maps and dense connectivity together to improve the traditional CNN’s learning ability. Therefore, a size-invariant convolutional network with dense connectivity is designed for RVS. The size-invariant skill helps the deep layers create feature maps with high resolution. The dense connectivity technique is utilized to integrate those hierarchical features and reuse characteristic maps to enhance the network’s learning ability. Finally, an optimized threshold is used on the output image to obtain a binary image. Results: The results of experiments conducted on two shared retinal image databases, DRIVE and STARE, demonstrate that our approach outperforms other techniques when evaluated in terms of F1-score, Matthews correlation coefficient (MCC), G-mean and FS. In addition, the cross training reveals that our method has stronger robustness with respect to training sets. Segmenting a 565 × 584 image only takes 39 ms with a single GPU (graphics processing unit). Conclusions: Compared with those traditional metrics, the FS is a better indicator to measure the results of RVS tasks. The experimental results revealed that the proposed method is more suitable for real-world applications.
ER  - 
TY  - JOUR
T1  - A novel approach to diagnose sleep apnea using enhanced frequency extraction network
A1  - Wu, Yitao
A1  - Pang, Xiongwen
A1  - Zhao, Gansen
A1  - Yue, Huijun
A1  - Lei, Wenbin
A1  - Wang, Yongquan
Y1  - 2021///
KW  -  Frequency decomposition
KW  -  Frequency extraction network
KW  -  Nasal airflow
KW  - Sleep apnea-hypopnea syndrome
JF  - Computer Methods and Programs in Biomedicine
VL  - 206
SP  - 106119
EP  - 106119
DO  - https://doi.org/10.1016/j.cmpb.2021.106119
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721001942
N2  - ABSTRACT
Sleep apnea-hypopnea syndrome (SAHS), as a widespread respiratory sleep disorder, if left untreated, can lead to a series of pathological changes. By using Polysomnography (PSG), traditional SAHS diagnosis tends to be complex and costly. Nasal airflow (NA) is the most direct reflection of the severity of SAHS. Therefore, we try to take advantage of NA signals that can be easily recorded by wearable devices. In this paper, we present an automatic detection approach of SAH events based on single-channel signal. Through this approach, an enhanced frequency extraction network is designed, which factorizes the mixed feature maps by their frequencies. And the spatial resolution of low-frequency components is reduced so as to save spending. Besides, in our research, the vanilla convolution block of the high-frequency components are replaced by residual blocks and smaller groups of filters with bigger size kernels. And we use the spatial attention module to facilitate feature extraction. Compared with state-of-the-art networks in this field, the promising results reveal that the proposed network for SAH events multiclass classification shows outstanding performance with accuracy of 91.23%, sensitivity of 90.81% and specificity of 90.59%. Thus, we believe that our approach, as a low-cost and high-efficiency solution, shows a great potential for detecting SAH events.
ER  - 
TY  - JOUR
T1  - Multistage fusion approaches based on a generative model and multivariate exponentially weighted moving average for diagnosis of cardiovascular autonomic nerve dysfunction
A1  - Hassan, Mohammad Mehedi
A1  - Huda, Shamsul
A1  - Yearwood, John
A1  - Jelinek, Herbert F
A1  - Almogren, Ahmad
Y1  - 2018///
KW  -  Autonomic nerve dysfunction classification
KW  -  Blind source separation
KW  -  Fusion of features and decisions
KW  -  Multivariate exponentially weighted moving average
KW  - Fusion of multiple statistical process control techniques
JF  - Information Fusion
VL  - 41
SP  - 105
EP  - 118
DO  - https://doi.org/10.1016/j.inffus.2017.08.004
UR  - https://www.sciencedirect.com/science/article/pii/S1566253517304797
N2  - Like many medical diagnoses, clinical decision support system (CDSS) is essential to diagnose the cardiovascular autonomic neuropathy (CAN). However, diagnosis of CAN using the traditional ‘Ewing battery test’ becomes very difficult due to the inherent imbalanced and incompleteness condition in the collected clinical data. This influences the health professionals to investigate other related diagnostic reports of patients, including Electrocardiogram (ECG) data from ECG sensors, blood chemistry, podiatry and endocrinology features. However, additional components increase the dimensionality of the feature set as well as its heterogeneity and modality in the clinical data which may limit the applications of traditional data mining approaches for an accurate diagnosis of CAN in the CDSS. To address the aforementioned problem, in this paper, we have proposed, a novel multistage fusion approach based on a generative model and a statistical process control (SPC) technique to diagnose CAN more accurately. The proposed approach develops two different generative models by using a shared and a separated Independent Component Analysis (ICA) to overcome the incompleteness and modality of the data. Due to the heterogeneous and non-normality features, statistical correlations and multivariate control limits in relation to the CAN diagnosis parameters are determined by fusioning of a series of exponentially weighted moving average (MEWMA) control processes. Fusioned features from both component analyses and SPC are applied in an ensemble classification system. The proposed multistage fusion approach is experimentally verified to justify its performance by using a large dataset collected from the diabetes screening research initiative (DiScRi) project at Charles Sturt University, NSW, Australia. Our comprehensive experimental results show that the proposed fusion approach performs better than the standard classifier for both ‘Ewing’ feature set and ‘Ewing and additional feature set’ with significant improvement in accuracy.
ER  - 
TY  - JOUR
T1  - Detection of neovascularization in retinal images using multivariate m-Mediods based classifier
A1  - Usman Akram, M
A1  - Khalid, Shehzad
A1  - Tariq, Anam
A1  - Younus Javed, M
Y1  - 2013///
KW  -  Blood vessels
KW  -  Mediods
KW  -  Neovascularization
KW  -  Proliferative diabetic retinopathy
KW  - Retinal image analysis
JF  - Computerized Medical Imaging and Graphics
VL  - 37
IS  - 5
SP  - 346
EP  - 357
DO  - https://doi.org/10.1016/j.compmedimag.2013.06.008
UR  - https://www.sciencedirect.com/science/article/pii/S0895611113001304
N1  - Retinal Image Analysis
N2  - Diabetic retinopathy is a progressive eye disease and one of the leading causes of blindness all over the world. New blood vessels (neovascularization) start growing at advance stage of diabetic retinopathy known as proliferative diabetic retinopathy. Early and accurate detection of proliferative diabetic retinopathy is very important and crucial for protection of patient's vision. Automated systems for detection of proliferative diabetic retinopathy should identify between normal and abnormal vessels present in digital retinal image. In this paper, we proposed a new method for detection of abnormal blood vessels and grading of proliferative diabetic retinopathy using multivariate m-Mediods based classifier. The system extracts the vascular pattern and optic disc using a multilayered thresholding technique and Hough transform respectively. It grades the fundus image in different categories of proliferative diabetic retinopathy using classification and optic disc coordinates. The proposed method is evaluated using publicly available retinal image databases and results show that the proposed system detects and grades proliferative diabetic retinopathy with high accuracy.
ER  - 
TY  - JOUR
T1  - Predicting existence of Mycobacterium tuberculosis on patients using data mining approaches
A1  - Uçar, Tamer
A1  - Karahoca, Adem
Y1  - 2011///
KW  -  ANFIS
KW  -  Data mining
KW  -  Multilayer perceptron
KW  -  PART
KW  - Tuberculosis
JF  - Procedia Computer Science
VL  - 3
SP  - 1404
EP  - 1411
DO  - https://doi.org/10.1016/j.procs.2011.01.022
UR  - https://www.sciencedirect.com/science/article/pii/S1877050911000238
N1  - World Conference on Information Technology
N2  - A correct diagnosis of tuberculosis (TB) can be only stated by applying a medical test to patient’s phlegm. The result of this test is obtained after a time period of about 45 days. The purpose of this study is to develop a data mining(DM) solution which makes diagnosis of tuberculosis as accurate as possible and helps deciding if it is reasonable to start tuberculosis treatment on suspected patients without waiting the exact medical test results or not. In this research, we proposed the use of Sugeno-type “adaptive-network-based fuzzy inference system” (ANFIS) to predict the existence of mycobacterium tuberculosis. 667 different patient records which are obtained from a clinic are used in the entire process of this research. Each of the patient records consist of 30 separate input parameters. ANFIS model is generated by using 500 of those records. We also implemented a multilayer perceptron and PART model using the same data set. The ANFIS model classifies the instances with an RMSE of 18% whereas Multilayer Perceptron does the same classification with an RMSE of % 19 and PART algorithm with an RMSE of % 20. ANFIS is an accurate and reliable method when compared with Multilayer Perceptron and PART algorithms for classification of tuberculosis patients. This study has contribution on forecasting patients before the medical tests.
ER  - 
TY  - JOUR
T1  - Feature selection and classification systems for chronic disease prediction: A review
A1  - Jain, Divya
A1  - Singh, Vijendra
Y1  - 2018///
KW  -  Adaptive classification systems
KW  -  Disease diagnosis
KW  -  Feature selection
KW  -  Parallel classification systems
KW  -  Traditional systems
KW  - Chronic disease
JF  - Egyptian Informatics Journal
VL  - 19
IS  - 3
SP  - 179
EP  - 189
DO  - https://doi.org/10.1016/j.eij.2018.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S1110866517300294
N2  - Chronic Disease Prediction plays a pivotal role in healthcare informatics. It is crucial to diagnose the disease at an early stage. This paper presents a survey on the utilization of feature selection and classification techniques for the diagnosis and prediction of chronic diseases. Adequate selection of features plays a significant role for enhancing accuracy of classification systems. Dimensionality reduction helps in improving overall performance of machine learning algorithm. The application of classification algorithms on disease datasets yields promising results by developing adaptive, automated and intelligent diagnostic systems for chronic diseases. Parallel classification systems can be used to expedite the process and to enhance the computational efficiency of results. This work presents a comprehensive overview of various feature selection methods and their inherent pros and cons. We then analyze adaptive classification systems and parallel classification systems for chronic disease prediction.
ER  - 
TY  - JOUR
T1  - Identifying high-cost patients using data mining techniques and a small set of non-trivial attributes
A1  - Izad Shenas, Seyed Abdolmotalleb
A1  - Raahemi, Bijan
A1  - Hossein Tekieh, Mohammad
A1  - Kuziemsky, Craig
Y1  - 2014///
KW  -  Data mining
KW  -  Decision tree
KW  -  Medical Expenditure Panel Survey
KW  -  Predictive models
KW  - Healthcare expenditures
JF  - Computers in Biology and Medicine
VL  - 53
SP  - 9
EP  - 18
DO  - https://doi.org/10.1016/j.compbiomed.2014.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S001048251400170X
N2  - In this paper, we use data mining techniques, namely neural networks and decision trees, to build predictive models to identify very high-cost patients in the top 5 percentile among the general population. A large empirical dataset from the Medical Expenditure Panel Survey with 98,175 records was used in our study. After pre-processing, partitioning and balancing the data, the refined dataset of 31,704 records was modeled by Decision Trees (including C5.0 and CHAID), and Neural Networks. The performances of the models are analyzed using various measures including accuracy, G-mean, and Area under ROC curve. We concluded that the CHAID classifier returns the best G-mean and AUC measures for top performing predictive models ranging from 76% to 85%, and 0.812 to 0.942 units, respectively. We also identify a small set of 5 non-trivial attributes among a primary set of 66 attributes to identify the top 5% of the high cost population. The attributes are the individual׳s overall health perception, age, history of blood cholesterol check, history of physical/sensory/mental limitations, and history of colonic prevention measures. The small set of attributes are what we call non-trivial and does not include visits to care providers, doctors or hospitals, which are highly correlated with expenditures and does not offer new insight to the data. The results of this study can be used by healthcare data analysts, policy makers, insurer, and healthcare planners to improve the delivery of health services.
ER  - 
TY  - JOUR
T1  - Ensemble Risk Model of Emergency Admissions (ERMER)
A1  - Mesgarpour, Mohsen
A1  - Chaussalet, Thierry
A1  - Chahed, Salma
Y1  - 2017///
KW  -  Bayesian
KW  -  Ensemble
KW  -  Framework
KW  -  Inpatient
KW  -  Readmission
KW  - Hospital Episode Statistics
JF  - International Journal of Medical Informatics
VL  - 103
SP  - 65
EP  - 77
DO  - https://doi.org/10.1016/j.ijmedinf.2017.04.010
UR  - https://www.sciencedirect.com/science/article/pii/S1386505617300886
N2  - Introduction
About half of hospital readmissions can be avoided with preventive interventions. Developing decision support tools for identification of patients’ emergency readmission risk is an important area of research. Because, it remains unclear how to design features and develop predictive models that can adjust continuously to a fast-changing healthcare system and population characteristics. The objective of this study was to develop a generic ensemble Bayesian risk model of emergency readmission.
Methods
We produced a decision support tool that predicts risk of emergency readmission using England's Hospital Episode Statistics inpatient database. Firstly, we used a framework to develop an optimal set of features. Then, a combination of Bayes Point Machine (BPM) models for different cohorts was considered to create an optimised ensemble model, which is stronger than the individual generative and non-linear classifications. The developed Ensemble Risk Model of Emergency Admissions (ERMER) was trained and tested using three time-frames: 1999–2004, 2000–05 and 2004–09, each of which includes about 20% of patients in England during the trigger year.
Results
Comparisons are made for different time-frames, sub-populations, risk cut-offs, risk bands and top risk segments. The precision was 71.6–73.9%, the specificity was 88.3–91.7% and the sensitivity was 42.1–49.2% across different time-frames. Moreover, the Area Under the Curve was 75.9–77.1%.
Conclusions
The decision support tool performed considerably better than the previous modelling approaches, and it was robust and stable with high precision. Moreover, the framework and the Bayesian model allow the model to continuously adjust it to new significant features, different population characteristics and changes in the system.
ER  - 
TY  - JOUR
T1  - Examining the predictability and prognostication of multimorbidity among older Delayed-Discharge Patients: A Machine learning analytics
A1  - Ghazalbash, Somayeh
A1  - Zargoush, Manaf
A1  - Mowbray, Fabrice
A1  - Papaioannou, Alexandra
Y1  - 2021///
KW  -  Delayed discharge
KW  -  Machine learning prediction
KW  -  Older adults
KW  -  Patient complexity
KW  -  Prognostication
KW  - Multimorbidity
JF  - International Journal of Medical Informatics
VL  - 156
SP  - 104597
EP  - 104597
DO  - https://doi.org/10.1016/j.ijmedinf.2021.104597
UR  - https://www.sciencedirect.com/science/article/pii/S1386505621002239
N2  - Background
Patient complexity among older delayed-discharge patients complicates discharge planning, resulting in a higher rate of adverse outcomes, such as readmission and mortality. Early prediction of multimorbidity, as a common indicator of patient complexity, can support proactive discharge planning by prioritizing complex patients and reducing healthcare inefficiencies.
Objective
We set out to accomplish the following two objectives: 1) to examine the predictability of three common multimorbidity indices, including Charlson–Deyo Comorbidity Index (CDCI), the Elixhauser Comorbidity Index (ECI), and the Functional Comorbidity Index (FCI) using machine learning (ML), and 2) to assess the prognostic power of these indices in predicting 30-day readmission and mortality.
Materials and Methods
We used data including 163,983 observations of patients aged 65 and older who experienced discharge delay in Ontario, Canada, during 2004 – 2017. First, we utilized various classification ML algorithms, including classification and regression trees, random forests, bagging trees, extreme gradient boosting, and logistic regression, to predict the multimorbidity status based on CDCI, ECI, and FCI. Second, we used adjusted multinomial logistic regression to assess the association between multimorbidity indices and the patient-important outcomes, including 30-day mortality and readmission.
Results
For all ML algorithms and regardless of the predictive performance criteria, better predictions were established for the CDCI compared with the ECI and FCI. Remarkably, the most predictable multimorbidity index (i.e., CDCI with Area Under the Receiver Operating Characteristic Curve = 0.80, 95% CI = 0.79 – 0.81) also offered the highest prognostications regarding adverse events (RRRmortality = 3.44, 95% CI = 3.21 – 3.68 and RRRreadmission = 1.36, 95% CI = 1.31 – 1.40).
Conclusions
Our findings highlight the feasibility and utility of predicting multimorbidity status using ML algorithms, resulting in the early detection of patients at risk of mortality and readmission. This can support proactive triage and decision-making about staffing and resource allocation, with the goal of optimizing patient outcomes and facilitating an upstream and informed discharge process through prioritizing complex patients for discharge and providing patient-centered care.
ER  - 
TY  - JOUR
T1  - A minimum centre distance rule activation method for extended belief rule-based classification systems
A1  - Zhu, Haizhen
A1  - Xiao, Mingqing
A1  - Yang, Longhao
A1  - Tang, Xilang
A1  - Liang, Yajun
A1  - Li, Jianfeng
Y1  - 2020///
KW  -  Classification
KW  -  Filtering and selecting
KW  -  Minimum centre distance
KW  -  Rule activation
KW  - Extended belief-rule-based system
JF  - Applied Soft Computing
VL  - 91
SP  - 106214
EP  - 106214
DO  - https://doi.org/10.1016/j.asoc.2020.106214
UR  - https://www.sciencedirect.com/science/article/pii/S156849462030154X
N2  - Originating from the belief-rule-based (BRB) system, the extended belief rule-based (EBRB) system combined the advantages of the rule-based method and those of data-driven methods. By transforming the data set into extended belief rules and using evidential reasoning (ER), the EBRB system has expanded the application of BRB systems and demonstrated their capability in addressing classification problems. Nevertheless, the problem of activating nearly the entire rule base in every classification process is embedded in the EBRB scheme. There have been advances in rule activation for the EBRB system; however, the introduction of subjective information into the classification, high computational costs and long response times are common problems facing existing rule activation methods. To solve the problems facing rule activation for EBRB systems, a minimum centre distance rule activation (MCDRA) method for EBRB systems is proposed. In MCDRA, no subjective information is required, and no time-consuming iteration procedure is necessary. Two components of the proposed MCDRA, i.e., the filtering procedure and the selection procedure, are designed to eliminate unrelated samples of input query data and to select and activate the highly related samples to the input query data. A total of 12 benchmark data sets are used to test the performance of EBRB with MCDRA (M-EBRB). The experimental results show that compared with other rule activation methods, the proposed method obtains satisfactory rule activation ratios, accuracies and response times. Additionally, M-EBRB performs well on noisy data and comparatively with both the fuzzy-rule-based classification system (FRBCS) and several machine learning classification algorithms. In addition, MCDRA can be utilized as a generic rule activation method and can be used to optimize other rule-based classification systems.
ER  - 
TY  - JOUR
T1  - Ischemic stroke: Process perspective, clinical and profile characteristics, and external factors
A1  - Sato, Denise M V
A1  - Mantovani, Letícia K
A1  - Safanelli, Juliana
A1  - Guesser, Vanessa
A1  - Nagel, Vivian
A1  - Moro, Carla H C
A1  - Cabral, Norberto L
A1  - Scalabrin, Edson E
A1  - Moro, Claudia
A1  - Santos, Eduardo A P
Y1  - 2020///
KW  -  Data analysis
KW  -  Process mining
KW  -  Stroke care process
KW  -  Stroke prevention
KW  - Ischemic stroke
JF  - Journal of Biomedical Informatics
VL  - 111
SP  - 103582
EP  - 103582
DO  - https://doi.org/10.1016/j.jbi.2020.103582
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420302100
N2  - Objective
To describe a method of analysis for understanding the health care process, enriched with information on the clinical and profile characteristics of the patients. To apply the proposed technique to analyze an ischemic stroke dataset.
Materials and methods
We analyzed 4,830 electronic health records (EHRs) from patients with ischemic stroke (2010–2017), containing information about events realized during treatment and clinical and profile information of the patients. The proposed method combined process mining techniques with data analysis, grouping the data by primary care units (PCU — units responsible for the primary care of patients residing in a geographical area).
Results
A novel method, named process, data, and management (PDM) analysis method was used for ischemic stroke data and it provided the following outcomes: health care process for patients with ischemic stroke with time statistics; analysis of potential factors for slow hospital admission indicating an increase in the time to hospital admission of 3.4 h (mean value) for patients with an origin at the urgent care center (UCC) – 30% of patients; analysis of PCUs with distinct secondary stroke rates indicating that the social class of patients is the main difference between them; and the visualization of risk factors (before the stroke) by the PCU to inform the health manager about the potential of prevention.
Discussion
PDM analysis describes a step-by-step method for combining process analysis with data analysis considering a management focus. The results obtained on the stroke context can support the definition of more refined action plans by the health manager, improving the stroke health care process and preventing new events.
Conclusion
When a patient is diagnosed with ischemic stroke, immediate treatment is needed. Moreover, it is possible to prevent new events to some degree by monitoring and treating risk factors. PDM analysis provides an overview of the health care process with time, combining elements that affect the treatment flow and factors, which can indicate a potential for preventing new events. We also can apply PDM analysis in different scenarios, when there is information about activities from treatment flow and other characteristics related to the treatment or the prevention of the analyzed disease. The management focus of the results aids in the formulation of service policies, action plans, and resource allocation.
ER  - 
TY  - JOUR
T1  - Detection of distributed denial of service attacks using an ensemble of adaptive and hybrid neuro-fuzzy systems
A1  - Arun Raj Kumar, P
A1  - Selvakumar, S
Y1  - 2013///
KW  -  Ensemble of classifiers
KW  -  Machine learning
KW  -  Neural networks
KW  -  Neuro-fuzzy
KW  - DDoS
JF  - Computer Communications
VL  - 36
IS  - 3
SP  - 303
EP  - 319
DO  - https://doi.org/10.1016/j.comcom.2012.09.010
UR  - https://www.sciencedirect.com/science/article/pii/S0140366412003222
N2  - A DDoS attack is the most prevalent threat, viz., flooding the computing and communication resources in order to make the service unavailable for legitimate users, since a decade and continues to be threatening till date. Therefore, these critical resources must be protected against the DDoS attacks. The detection of DDoS attacks requires adaptive and incremental learning classifier, less computational complexity, and accurate decision making from uncertain information. Hence, the DDoS attacks could be detected using existing soft computing techniques such as fuzzy logic, neural networks, and genetic algorithms. Fuzzy logic has the advantage of interpreting the rules well but it suffers from the disadvantage of not able to acquire the rules automatically. The neural networks generalize the network well but they cannot interpret the rules. Genetic algorithm provides optimal solutions but the time complexity is high. Hybrid methods, Neuro-fuzzy and genetic fuzzy have been proposed to overcome the drawbacks of interpretability and manual rules acquisition. In this paper, adaptive and hybrid neuro-fuzzy systems were proposed as subsystems of the ensemble. Sugeno type Adaptive Neuro-Fuzzy Inference System (ANFIS) has been chosen as a base classifier for our research as Mamdani type ANFIS is not suitable for real time due to its high computational complexity and non-adaptiveness to extract exact knowledge from the dataset. Single classifier makes error on different training samples. So, by creating an ensemble of classifiers and combining their outputs, the total error can be reduced and the detection accuracy can be increased. Improvement in the performance of ANFIS ensemble is the focus of this paper. Our proposed DDoS classification algorithm, NFBoost, differs from the existing methods in weight update distribution strategy, error cost minimization, and ensemble output combination method, but resembles similar in classifier weight assignment and error computation. Our proposed NFBoost algorithm is achieved by combining ensemble of classifier outputs and Neyman Pearson cost minimization strategy, for final classification decision. Publicly available datasets such as KDD Cup, CAIDA DDOS Attack 2007, CONFICKER worm, UNINA traffic traces, and UCI Datasets were used for the simulation experiments. NFBoost was trained and tested with the publicly available datasets and our own SSE Lab1All hosts in SSE Lab are connected through a backbone bandwidth of 1Gbps and to different sites through a 2Mbps MPLS VPN cloud.1 SSENET 2011 datasets. Detection accuracy and Cost per sample were the two metrics used to analyze the performance of the NFBoost classification algorithm and were compared with bagging, boosting, and AdaBoost algorithms. From the simulation results, it is evident that NFBoost algorithm achieves high detection accuracy (99.2%) with fewer false alarms. Cost per instance is also very less for the NFBoost algorithm compared to the existing algorithms. NFBoost algorithm outperforms the existing ensemble algorithms with a maximum gain of 8.4% and a minimum gain of 1.1%.
ER  - 
TY  - JOUR
T1  - An improved arteriovenous classification method for the early diagnostics of various diseases in retinal image
A1  - Xu, Xiayu
A1  - Ding, Wenxiang
A1  - Abràmoff, Michael D
A1  - Cao, Ruofan
Y1  - 2017///
KW  -  Arteriovenous classification
KW  -  Computer-aided diagnostics
KW  -  Image analysis
KW  - Retinal image
JF  - Computer Methods and Programs in Biomedicine
VL  - 141
SP  - 3
EP  - 9
DO  - https://doi.org/10.1016/j.cmpb.2017.01.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260716309002
N2  - (Background and objectives)
Retinal artery and vein classification is an important task for the automatic computer-aided diagnosis of various eye diseases and systemic diseases. This paper presents an improved supervised artery and vein classification method in retinal image.
(Methods)
Intra-image regularization and inter-subject normalization is applied to reduce the differences in feature space. Novel features, including first-order and second-order texture features, are utilized to capture the discriminating characteristics of arteries and veins.
(Results)
The proposed method was tested on the DRIVE dataset and achieved an overall accuracy of 0.923.
(Conclusion)
This retinal artery and vein classification algorithm serves as a potentially important tool for the early diagnosis of various diseases, including diabetic retinopathy and cardiovascular diseases.
ER  - 
TY  - JOUR
T1  - Automated Atrial Fibrillation Detection using a Hybrid CNN-LSTM Network on Imbalanced ECG Datasets
A1  - Petmezas, Georgios
A1  - Haris, Kostas
A1  - Stefanopoulos, Leandros
A1  - Kilintzis, Vassilis
A1  - Tzavelis, Andreas
A1  - Rogers, John A
A1  - Katsaggelos, Aggelos K
A1  - Maglaveras, Nicos
Y1  - 2021///
KW  -  CNN
KW  -  LSTM
KW  -  arrhythmia detection
KW  -  focal loss
KW  - atrial fibrillation
JF  - Biomedical Signal Processing and Control
VL  - 63
SP  - 102194
EP  - 102194
DO  - https://doi.org/10.1016/j.bspc.2020.102194
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420303323
N2  - Atrial fibrillation is a heart arrhythmia strongly associated with other heart-related complications that can increase the risk of strokes and heart failure. Manual electrocardiogram (ECG) interpretation for its diagnosis is tedious, time-consuming, requires high expertise, and suffers from inter- and intra-observer variability. Deep learning techniques could be exploited in order for robust arrhythmia detection models to be designed. In this paper, we propose a novel hybrid neural model utilizing focal loss, an improved version of cross-entropy loss, to deal with training data imbalance. ECG features initially extracted via a Convolutional Neural Network (CNN) are input to a Long Short-Term Memory (LSTM) model for temporal dynamics memorization and thus, more accurate classification into the four ECG rhythm types, namely normal (N), atrial fibrillation (AFIB), atrial flutter (AFL) and AV junctional rhythm (J). The model was trained on the MIT-BIH Atrial Fibrillation Database and achieved a sensitivity of 97.87%, and specificity of 99.29% using a ten-fold cross-validation strategy. The proposed model can aid clinicians to detect common atrial fibrillation in real-time on routine screening ECG.
ER  - 
TY  - JOUR
T1  - Optic disc localization algorithm based on improved corner detection
A1  - Gui, Bin
A1  - Shuai, Ren-Jun
A1  - Chen, Ping
Y1  - 2018///
KW  -  CLAHE
KW  -  Corner detection
KW  -  Fast corner detection
KW  -  Harris
KW  - Optic disc localization
JF  - Procedia Computer Science
VL  - 131
SP  - 311
EP  - 319
DO  - https://doi.org/10.1016/j.procs.2018.04.169
UR  - https://www.sciencedirect.com/science/article/pii/S187705091830543X
N1  - Recent Advancement in Information and Communication Technology:
N2  - In the computer-aided diagnosis of retinal images, the location of the optic disc plays an important supporting role in the location of the macula, the segmentation of the blood vessel and the determination of the location of the lesions. In order to avoid the failure of the location of optic discs due to the misjudgment and segmentation of the blood vessels and the changes of brightness caused by the lesions, this paper proposes an algorithm of optic disc localization based on an improved corner detection algorithm by using the feature that the distribution of vessels in the optic disc area is complicated and the corner distribution is the densest. The localization of the optic disc location by using this method in the 3 databases shows that this method can effectively locate the optic disc and has higher accuracy and real-time performance. This method provides a new idea for the localization of the optic disc.
ER  - 
TY  - JOUR
T1  - An empirical evaluation of deep learning for ICD-9 code assignment using MIMIC-III clinical notes
A1  - Huang, Jinmiao
A1  - Osorio, Cesar
A1  - Sy, Luke Wicent
Y1  - 2019///
KW  -  CNNs
KW  -  Clinical notes
KW  -  Code assignment
KW  -  ICD-9
KW  -  MIMIC-III
KW  -  Machine learning
KW  -  Medical codes
KW  -  RNNs
KW  - Deep learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 177
SP  - 141
EP  - 153
DO  - https://doi.org/10.1016/j.cmpb.2019.05.024
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718309945
N2  - Background and Objective
Code assignment is of paramount importance in many levels in modern hospitals, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious and subjective, and it requires medical coders with extensive training. This study aims to evaluate the performance of deep-learning-based systems to automatically map clinical notes to ICD-9 medical codes.
Methods
The evaluations of this research are focused on end-to-end learning methods without manually defined rules. Traditional machine learning algorithms, as well as state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks, were applied to the Medical Information Mart for Intensive Care (MIMIC-III) dataset. An extensive number of experiments was applied to different settings of the tested algorithm.
Results
Findings showed that the deep learning-based methods outperformed other conventional machine learning methods. From our assessment, the best models could predict the top 10 ICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10 ICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also outperformed existing work under certain evaluation metrics.
Conclusion
A set of standard metrics was utilized in assessing the performance of ICD-9 code assignment on MIMIC-III dataset. All the developed evaluation tools and resources are available online, which can be used as a baseline for further research.
ER  - 
TY  - JOUR
T1  - Reliability of heart-rate-variability features derived from ultra-short ECG recordings and their validity in the assessment of cardiac autonomic neuropathy
A1  - Wehler, D
A1  - Jelinek, H F
A1  - Gronau, A
A1  - Wessel, N
A1  - Kraemer, J F
A1  - Krones, R
A1  - Penzel, T
Y1  - 2021///
KW  -  Cardiac autonomic neuropathy
KW  -  Diagnostics
KW  -  Electrocardiography
KW  -  Reliability
KW  -  Ultra-short
KW  - Heart Rate Variability
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102651
EP  - 102651
DO  - https://doi.org/10.1016/j.bspc.2021.102651
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421002482
N2  - Objective
Our research aimed at providing methodologically strong evidence for the reliability of HRV analysis of ultra-short (10–150 s) ECGs compared to the benchmark of 300 s. Furthermore, we evaluated the retainment of HRV differences between participants with definite cardiac autonomic neuropathy (dCAN) and controls as ECG recording length was reduced to 10 s.
Methods
Pre-processed ECG recordings from 11 individuals with dCAN and 76 healthy controls were used to determine SDNN, RMSSD, Low Frequency (LF) and High Frequency (HF) and assess whether the reliability of HRV features remains comparable for ultra-short ECG time series. For frequency domain analysis, the Lomb-Scargle-Periodogram (LSP) and Fast-Fourier-Transform (FFT) approaches were compared. Nonparametric areas under the curve (AUCs) were estimated to examine the overall ability of ultra-short HRV to classify dCAN from ECGs of 10–300 s.
Results
RMSSD and HF were reliable and retained significant differences between control and dCAN from ECGs down to 10 s. SDNN and LF could be considered reliable in recordings of at least 30 s. LSP and FFT yielded comparable results. Most prevalent group differences were found for LF between 60 and 300 s.
Conclusion
RMSSD and HF were reliable HRV features showing excellent potential for the classification of dCAN from 10-s ECG strips. For recordings longer than one minute, LF proved to be reliable and yielded best discrimination results.
Significance
The current research indicated that ultra-short HRV analysis has the potential to become a time-efficient and accurate diagnostic tool for dCAN in clinical practice.
ER  - 
TY  - JOUR
T1  - Enhancing Decision Tree Classification Accuracy through Genetically Programmed Attributes for Wart Treatment Method Identification
A1  - Khatri, Sabita
A1  - Arora, Deepak
A1  - Kumar, Anil
Y1  - 2018///
KW  -  Decision Tree
KW  -  Genetic Programming
KW  -  Immunotherapy
KW  -  Machine Learning
KW  - Warts
JF  - Procedia Computer Science
VL  - 132
SP  - 1685
EP  - 1694
DO  - https://doi.org/10.1016/j.procs.2018.05.141
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918308731
N1  - International Conference on Computational Intelligence and Data Science
N2  - Origin: Warts are produced and developed on the human body due to infection induced by Human Papillomavirus. The most influenced zone of warts are hands and feet particularly, which is bit irritating and difficult to recoup in later stages. The major challenge in treating warts is the diversity of treatment method applicable on different patients, so it becomes difficult to recognize specific treatment method to be adopted in order to treat this infection. Ramifications of machine learning techniques in the medical domain have become crucial nowadays for early disease detection and developing expert systems. Objective: This research work focuses on enhancing predictive accuracy of J48, which is a binary decision tree based classifier by adding attributes based on genetic programming. These genetically tuned attribute construction not only just upgrades the classification capabilities of J48 classifier but also additionally expand the information space, intending J48 for giving more exact predictions for wart treatment method identification. Method: For their experimental setup, authors have chosen immunotherapy and cryotherapy datasets from UCI machine learning repositories, which includes instances of patients responses against treated with immunotherapy and cryotherapy methods for both plantar and common warts. The investigation has been led with the help of WEKA tool, which is an open source for performing data mining operations. Finding: After experimentation, it is found after inclusion of attributes generated through genetic programming, the classification accuracy of J48 can be increased by a substantial amount with less error rate. The result shows significant performance improvements in classification accuracy of J48 by 82.22% to 96.66% and 93.33% to 98.88% for immunotherapy and cryotherapy datasets, implemented with J48 and J48+GA respectively.
ER  - 
TY  - JOUR
T1  - Prognostic Bayesian networks: II: An application in the domain of cardiac surgery
A1  - Verduijn, Marion
A1  - Rosseel, Peter M J
A1  - Peek, Niels
A1  - de Jonge, Evert
A1  - de Mol, Bas A J M
Y1  - 2007///
KW  -  Bayesian network
KW  -  Cardiac surgery
KW  -  Health care process
KW  - Prognostic model
JF  - Journal of Biomedical Informatics
VL  - 40
IS  - 6
SP  - 619
EP  - 630
DO  - https://doi.org/10.1016/j.jbi.2007.07.004
UR  - https://www.sciencedirect.com/science/article/pii/S1532046407000639
N1  - Intelligent Data Analysis in Biomedicine
N2  - A prognostic Bayesian network (PBN) is new type of prognostic model that implements a dynamic, process-oriented view on prognosis. In a companion article, the rationale of the PBN is described, and a dedicated learning procedure is presented. This article presents an application hereof in the domain of cardiac surgery. A PBN is induced from clinical data of cardiac surgical patients using the proposed learning procedure; hospital mortality is used as outcome variable. The predictive performance of the PBN is evaluated on an independent test set, and results were compared to the performance of a network that was induced using a standard algorithm where candidate networks are selected using the minimal description length principle. The PBN is embedded in the prognostic system ProCarSur; a prototype of this system is presented. This application shows PBNs as a useful prognostic tool in medical processes. In addition, the article shows the added value of the PBN learning procedure.
ER  - 
TY  - JOUR
T1  - Automatic analysis of diabetic peripheral neuropathy using multi-scale quantitative morphology of nerve fibres in corneal confocal microscopy imaging
A1  - Dabbah, M A
A1  - Graham, J
A1  - Petropoulos, I N
A1  - Tavakoli, M
A1  - Malik, R A
Y1  - 2011///
KW  -  Curvilinear structures
KW  -  Diabetic neuropathy
KW  -  Image quantification
KW  - Corneal confocal microscopy
JF  - Medical Image Analysis
VL  - 15
IS  - 5
SP  - 738
EP  - 747
DO  - https://doi.org/10.1016/j.media.2011.05.016
UR  - https://www.sciencedirect.com/science/article/pii/S1361841511000806
N1  - Special Issue on the 2010 Conference on Medical Image Computing and Computer-Assisted Intervention
N2  - Diabetic peripheral neuropathy (DPN) is one of the most common long term complications of diabetes. Corneal confocal microscopy (CCM) image analysis is a novel non-invasive technique which quantifies corneal nerve fibre damage and enables diagnosis of DPN. This paper presents an automatic analysis and classification system for detecting nerve fibres in CCM images based on a multi-scale adaptive dual-model detection algorithm. The algorithm exploits the curvilinear structure of the nerve fibres and adapts itself to the local image information. Detected nerve fibres are then quantified and used as feature vectors for classification using random forest (RF) and neural networks (NNT) classifiers. We show, in a comparative study with other well known curvilinear detectors, that the best performance is achieved by the multi-scale dual model in conjunction with the NNT classifier. An evaluation of clinical effectiveness shows that the performance of the automated system matches that of ground-truth defined by expert manual annotation.
ER  - 
TY  - JOUR
T1  - Co-occurrence of medical conditions: Exposing patterns through probabilistic topic modeling of snomed codes
A1  - Bhattacharya, Moumita
A1  - Jurkovitz, Claudine
A1  - Shatkay, Hagit
Y1  - 2018///
KW  -  Co-occurring medical conditions
KW  -  Electronic health records
KW  -  SNOMED-CT codes
KW  - Topic modeling
JF  - Journal of Biomedical Informatics
VL  - 82
SP  - 31
EP  - 40
DO  - https://doi.org/10.1016/j.jbi.2018.04.008
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300728
N2  - Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13% of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMED-CT codes assigned and recorded in the EHRs of >13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.
ER  - 
TY  - JOUR
T1  - Ontology driven decision support for the diagnosis of mild cognitive impairment
A1  - Zhang, Xiaowei
A1  - Hu, Bin
A1  - Ma, Xu
A1  - Moore, Philip
A1  - Chen, Jing
Y1  - 2014///
KW  -  Decision tree
KW  -  Diagnosis
KW  -  Magnetic resonance imaging (MRI)
KW  -  Mild cognitive impairment
KW  -  Ontology
KW  - Alzheimer dementia
JF  - Computer Methods and Programs in Biomedicine
VL  - 113
IS  - 3
SP  - 781
EP  - 791
DO  - https://doi.org/10.1016/j.cmpb.2013.12.023
UR  - https://www.sciencedirect.com/science/article/pii/S0169260713004136
N2  - In recent years, mild cognitive impairment (MCI) has attracted significant attention as an indicator of high risk for Alzheimer's disease (AD), and the diagnosis of MCI can alert patient to carry out appropriate strategies to prevent AD. To avoid subjectivity in diagnosis, we propose an ontology driven decision support method which is an automated procedure for diagnosing MCI through magnetic resonance imaging (MRI). In this approach, we encode specialized MRI knowledge into an ontology and construct a rule set using machine learning algorithms. Then we apply these two parts in conjunction with reasoning engine to automatically distinguish MCI patients from normal controls (NC). The rule set is trained by MRI data of 187 MCI patients and 177 normal controls selected from Alzheimer's Disease Neuroimaging Initiative (ADNI) using C4.5 algorithm. By using a 10-fold cross validation, we prove that the performance of C4.5 with 80.2% sensitivity is better than other algorithms, such as support vector machine (SVM), Bayesian network (BN) and back propagation (BP) neural networks, and C4.5 is suitable for the construction of reasoning rules. Meanwhile, the evaluation results suggest that our approach would be useful to assist physicians efficiently in real clinical diagnosis for the disease of MCI.
ER  - 
TY  - JOUR
T1  - Semantic Classification of Diseases in Discharge Summaries Using a Context-aware Rule-based Classifier
A1  - Solt, Illés
A1  - Tikk, Domonkos
A1  - Gál, Viktor
A1  - Kardkovács, Zsolt T
Y1  - 2009///
JF  - Journal of the American Medical Informatics Association
VL  - 16
IS  - 4
SP  - 580
EP  - 584
DO  - https://doi.org/10.1197/jamia.M3087
UR  - https://www.sciencedirect.com/science/article/pii/S1067502709000875
N2  - Objective
Automated and disease-specific classification of textual clinical discharge summaries is of great importance in human life science, as it helps physicians to make medical studies by providing statistically relevant data for analysis. This can be further facilitated if, at the labeling of discharge summaries, semantic labels are also extracted from text, such as whether a given disease is present, absent, questionable in a patient, or is unmentioned in the document. The authors present a classification technique that successfully solves the semantic classification task.
Design
The authors introduce a context-aware rule-based semantic classification technique for use on clinical discharge summaries. The classification is performed in subsequent steps. First, some misleading parts are removed from the text; then the text is partitioned into positive, negative, and uncertain context segments, then a sequence of binary classifiers is applied to assign the appropriate semantic labels.
Measurement
For evaluation the authors used the documents of the i2b2 Obesity Challenge and adopted its evaluation measures: F1-macro and F1-micro for measurements.
Results
On the two subtasks of the Obesity Challenge (textual and intuitive classification) the system performed very well, and achieved a F1-macro = 0.80 for the textual and F1-macro = 0.67 for the intuitive tasks, and obtained second place at the textual and first place at the intuitive subtasks of the challenge.
Conclusions
The authors show in the paper that a simple rule-based classifier can tackle the semantic classification task more successfully than machine learning techniques, if the training data are limited and some semantic labels are very sparse.
ER  - 
TY  - JOUR
T1  - Leveraging syntax to better capture the semantics of elliptical coordinated compound noun phrases
A1  - Blake, Catherine
A1  - Rindflesch, Tom
Y1  - 2017///
KW  -  Elliptical coordinated compound noun phrases
KW  -  Entity recognition
KW  -  Natural language processing
KW  -  Noun phrase semantics
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 72
SP  - 120
EP  - 131
DO  - https://doi.org/10.1016/j.jbi.2017.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417301570
N2  - Full-text scientific articles are increasingly available, but capturing the meaning conveyed within an article automatically remains a bottleneck for semantic search and reasoning systems. In this paper we consider elliptical coordinated compound noun phrases that authors use to save space in an article. Systems that do not attend to coordination would incorrectly interpret “breast or lung cancer” as a body part (breast) and a disease (lung cancer) rather than two diseases. The algorithmic approach introduced in this paper uses a generate-and-test strategy where candidate expansions for forward, backward and complex ellipses are generated from syntactic dependencies. Dependencies are also used to create a dictionary of non-coordinated noun phrases that is used during the test phrase. Experiments on 21,280 full-text articles show that more than a million noun phrases were impacted by coordinated ellipses. The system achieves 73.07% precision, 75.38% recall, 74.23% F-score and 94.72% accuracy for new noun phrases in the development set. The precision was higher for backward (82.62 vs. 78.63) and forward expansions (64.82 vs. 60.17) and lower for complex expansions (63.41 vs. 72.59) in a test set. On average 10.79% of all noun phrases would be missed if coordination were not resolved, which corresponds to 48 new noun phrases per article in the journal Carcinogenesis, 52 new phrases per article in Diabetes, and 56 new phrases per article in Endocrinology. Results also show coordinated ellipses are more prevalent in abstracts (12.31% of all noun phrases) than in the body of an article (10.70%). To further test the generalizability of this approach the system (without modification) was used on two new collections.
ER  - 
TY  - JOUR
T1  - Wearable sensor-based evaluation of psychosocial stress in patients with metabolic syndrome
A1  - Patlar Akbulut, Fatma
A1  - Ikitimur, Baris
A1  - Akan, Aydin
Y1  - 2020///
KW  -  Affective Computing
KW  -  HRV
KW  -  Metabolic Syndrome
KW  -  Neural Networks
KW  -  e-Health
KW  - Wearable System
JF  - Artificial Intelligence in Medicine
VL  - 104
SP  - 101824
EP  - 101824
DO  - https://doi.org/10.1016/j.artmed.2020.101824
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719311777
N2  - The prevalence of metabolic disorders has increased rapidly as such they become a major health issue recently. Despite the definition of genetic associations with obesity and cardiovascular diseases, they constitute only a small part of the incidence of disease. Environmental and physiological effects such as stress, behavioral and metabolic disturbances, infections, and nutritional deficiencies have now revealed as contributing factors to develop metabolic diseases. This study presents a multivariate methodology for the modeling of stress on metabolic syndrome (MES) patients. We have developed a supporting system to cope with MES patients’ anxiety and stress by means of several biosignals such as ECG, GSR, body temperature, SpO2, glucose level, and blood pressure that are measured by a wearable device. We employed a neural network model to classify emotions with HRV analysis in the detection of stressor moments. We have accurately recognized the stressful situations using physiological responses to stimuli by utilizing our proposed affective state detection algorithm. We evaluated our system with a dataset of 312 biosignal records from 30 participants and the results showed that our proposed method achieved an average accuracy of 92% and 89% in distinguishing stress level in MES and other groups respectively. Both being the focus of an MES group and others proved to be highly arousing experiences which were significantly reflected in the physiological signal. Exposure to the stress in MES and cardiovascular heart disease patients increases the chronic symptoms. An early stage of comprehensive intervention may reduce the risk of general cardiovascular events in these particular groups. In this context, the use of e-health applications such as our proposed system facilitates these processes.
ER  - 
TY  - JOUR
T1  - Automated detection of congestive heart failure from electrocardiogram signal using Stockwell transform and hybrid classification scheme
A1  - Tripathy, R K
A1  - Paternina, Mario R A
A1  - Arrieta, Juan G
A1  - Zamora-Méndez, Alejandro
A1  - Naik, Ganesh R
Y1  - 2019///
KW  -  ECG
KW  -  Hybrid classifier
KW  -  Performance evaluation
KW  -  Stockwell transform
KW  -  Time-Frequency entropy
KW  - Congestive heart failure
JF  - Computer Methods and Programs in Biomedicine
VL  - 173
SP  - 53
EP  - 65
DO  - https://doi.org/10.1016/j.cmpb.2019.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718302980
N2  - Background and Objective
The congestive heart failure (CHF) is a life-threatening cardiac disease which arises when the pumping action of the heart is less than that of the normal case. This paper proposes a novel approach to design a classifier-based system for the automated detection of CHF.
Methods
The approach is founded on the use of the Stockwell (S)-transform and frequency division to analyze the time-frequency sub-band matrices stemming from electrocardiogram (ECG) signals. Then, the entropy features are evaluated from the sub-band matrices of ECG. A hybrid classification scheme is adopted taking the sparse representation classifier and the average of the distances from the nearest neighbors into account for the detection of CHF. The proposition is validated using ECG signals from CHF subjects and normal sinus rhythm from public databases.
Results
The results reveal that the proposed system is successful for the detection of CHF with an accuracy, a sensitivity and a specificity values of 98.78%, 98.48%, and 99.09%, respectively. A comparison with the existing approaches for the detection of CHF is accomplished.
Conclusions
The time-frequency entropy features of the ECG signal in the frequency range from 11 Hz to 30 Hz have higher performance for the detection of CHF using a hybrid classifier. The approach can be used for the automated detection of CHF in tele-healthcare monitoring systems.
ER  - 
TY  - JOUR
T1  - Diagnoses Detection in Short Snippets of Narrative Medical Texts
A1  - Dudchenko, Aleksei
A1  - Ganzinger, Matthias
A1  - Kopanitsa, Georgy
Y1  - 2019///
KW  -  medical records
KW  - natural language processing
JF  - Procedia Computer Science
VL  - 156
SP  - 150
EP  - 157
DO  - https://doi.org/10.1016/j.procs.2019.08.190
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919311093
N1  - 8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece
N2  - Data extraction from narrative medical texts is a significant task to enable secondary use of medical data. Supervised learning algorithms show good results in natural language processing (NLP) tasks. We have developed a NLP framework based on supervised machine learning for entity extraction from medical texts. The framework is language independent and entities independent as long as an appropriately labeled dataset is given. The framework is based on vector representation of words and a neural network as a classifier. We have trained and evaluated the framework on two different text corpuses: diagnoses paragraphs written in German and medical records written in Russian. The neural network hyperparameters were adjusted for every dataset to get better classification results. Finally, accuracy, standard deviation, and standard error were calculated for both network models engaging 10-folds cross-validation. The obtained accuracy is 97,64% for Russian texts and 96,81% for German ones.
ER  - 
TY  - JOUR
T1  - Intelligent knowledge consolidation: From data to wisdom
A1  - Hussain, Musarrat
A1  - Satti, Fahad Ahmed
A1  - Ali, Syed Imran
A1  - Hussain, Jamil
A1  - Ali, Taqdir
A1  - Kim, Hun-Sung
A1  - Yoon, Kun-Ho
A1  - Chung, TaeChoong
A1  - Lee, Sungyoung
Y1  - 2021///
KW  -  Incremental knowledge model
KW  -  Knowledge based system
KW  -  Ripple down rules
KW  - Knowledge consolidation
JF  - Knowledge-Based Systems
VL  - 234
SP  - 107578
EP  - 107578
DO  - https://doi.org/10.1016/j.knosys.2021.107578
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121008406
N2  - Knowledge based systems have accomplished remarkable achievements in assisting evidence based decision making for complex problems. However, machine learning-driven, intelligent systems of today are dependent on the underlying knowledge model, which is acquired from domain experts, or the available datasets in a structured or unstructured format. Most of the existing literature utilized a single modal, while very few have combined multi-modalities (mainly two) for knowledge acquisition. In order to achieve a strong Artificial Intelligence, multi-domain and multi-modal knowledge acquisition, and consolidation is required. This paper presents the research work, driving the realization of such a comprehensive framework, in the field of healthcare. Using area specific, state-of-the-art machine learning techniques, we first extract knowledge from structured and unstructured data, which is consolidated with expert knowledge and managed through ripple down rules. Our presented technique shows an accuracy of 92.05%, which is much higher than single modal deep learning at 78.20%, naive bayes at 69.70%, logistic regression at 61.20%, expert driven knowledge at 86.02%, and naive knowledge combination at 70.86%. Thus, through the application of our proposed technique, we provide the foundations for an accurate and evolvable knowledge-base, that can greatly enhance decision making in the healthcare domain.
ER  - 
TY  - JOUR
T1  - Prediction of blood pressure variability using deep neural networks
A1  - Koshimizu, Hiroshi
A1  - Kojima, Ryosuke
A1  - Kario, Kazuomi
A1  - Okuno, Yasushi
Y1  - 2020///
KW  -  Blood pressure prediction
KW  -  Deep neural networks
KW  -  Telemedicine
KW  -  Time-series analysis
KW  - Blood pressure variability
JF  - International Journal of Medical Informatics
VL  - 136
SP  - 104067
EP  - 104067
DO  - https://doi.org/10.1016/j.ijmedinf.2019.104067
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619311578
N2  - Purpose
The purpose of our study was to predict blood pressure variability from time-series data of blood pressure measured at home and data obtained through medical examination at a hospital. Previous studies have reported the blood pressure variability is a significant independent risk factor for cardiovascular disease.
Methods
We adopted standard deviation for a certain period and predicted variabilities and mean values of blood pressure for 4 weeks using multi-input multi-output deep neural networks. In designing the prediction model, we prepared a dataset from a clinical study. The dataset included past time-series data for blood pressure and medical examination data such as gender, age, and others. As evaluation metrics, we used the standard deviation ratio (SR) and the root-mean-square error (RMSE). Moreover, we used cross-validation as the evaluation method.
Results
The prediction performances of blood pressure variability and mean value after 1–4 weeks showed the SRs were “0.67” to “0.70”, the RMSEs were “5.04” to “6.65” mmHg, respectively. Additionally, our models were able to work for a participant with high variability in blood pressure values due to its multi-output nature.
Conclusion
The results of this study show that our models can predict blood pressure over 4 weeks. Our models work for an individual with high variability of blood pressure. Therefore, we consider that our prediction models are valuable for blood pressure management.
ER  - 
TY  - JOUR
T1  - Integrating cluster analysis with granular computing for imbalanced data classification problem – A case study on prostate cancer prognosis
A1  - Kuo, R J
A1  - Su, P Y
A1  - Zulvia, Ferani E
A1  - Lin, C C
Y1  - 2018///
KW  -  Class imbalance
KW  -  Classification
KW  -  Granular computing
KW  -  Prostate cancer
KW  - Prognosis
JF  - Computers & Industrial Engineering
VL  - 125
SP  - 319
EP  - 332
DO  - https://doi.org/10.1016/j.cie.2018.08.031
UR  - https://www.sciencedirect.com/science/article/pii/S036083521830411X
N2  - Analyzing imbalanced dataset is a critical and challenging task in data mining, since it requires special treatment for clusters with different sizes. Imbalance dataset commonly exists in some domains like medical problems. This study intends to propose a classification algorithm based on information granulation (IG) concept for handling imbalanced dataset. The proposed algorithm assembles data from majority classes into granules to balance the class ratio within the data. The proposed algorithm works in two stages. First stage generates a set of IGs using metaheuristics approaches which is a kind of automatic clustering algorithm including dynamic clustering using particle swarm optimization (DCPSO), genetic algorithm K-means (GA K-means), and artificial bee colony K-means (ABC K-means). The next stage applies classification algorithm to classify the data. In this study, the proposed algorithm is verified using both balance and imbalanced benchmark datasets. Simulation results show that the proposed algorithms have promising classification results. Furthermore, this study also applies the proposed algorithms to prostate cancer prognosis classification problem. The algorithm is employed to predict survival rate of prostate cancer patients based on some medical data. The result shows that the proposed algorithms have lower error rate.
ER  - 
TY  - JOUR
T1  - Symptom-based network classification identifies distinct clinical subgroups of liver diseases with common molecular pathways
A1  - Shu, Zixin
A1  - Liu, Wenwen
A1  - Wu, Huikun
A1  - Xiao, Mingzhong
A1  - Wu, Deng
A1  - Cao, Ting
A1  - Ren, Meng
A1  - Tao, Junxiu
A1  - Zhang, Chuhua
A1  - He, Tangqing
A1  - Li, Xiaodong
A1  - Zhang, Runshun
A1  - Zhou, Xuezhong
Y1  - 2019///
KW  -  Community detection
KW  -  Electronic medical records
KW  -  Liver diseases
KW  -  Network medicine
KW  -  Traditional Chinese medicine
KW  - Disease subtypes
JF  - Computer Methods and Programs in Biomedicine
VL  - 174
SP  - 41
EP  - 50
DO  - https://doi.org/10.1016/j.cmpb.2018.02.014
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717307800
N2  - Background and objective
Liver disease is a multifactorial complex disease with high global prevalence and poor long-term clinical efficacy and liver disease patients with different comorbidities often incorporate multiple phenotypes in the clinic. Thus, there is a pressing need to improve understanding of the complexity of clinical liver population to help gain more accurate disease subtypes for personalized treatment.
Methods
Individualized treatment of the traditional Chinese medicine (TCM) provides a theoretical basis to the study of personalized classification of complex diseases. Utilizing the TCM clinical electronic medical records (EMRs) of 6475 liver inpatient cases, we built a liver disease comorbidity network (LDCN) to show the complicated associations between liver diseases and their comorbidities, and then constructed a patient similarity network with shared symptoms (PSN). Finally, we identified liver patient subgroups using community detection methods and performed enrichment analyses to find both distinct clinical and molecular characteristics (with the phenotype-genotype associations and interactome networks) of these patient subgroups.
Results
From the comorbidity network, we found that clinical liver patients have a wide range of disease comorbidities, in which the basic liver diseases (e.g. hepatitis b, decompensated liver cirrhosis), and the common chronic diseases (e.g. hypertension, type 2 diabetes), have high degree of disease comorbidities. In addition, we identified 303 patient modules (representing the liver patient subgroups) from the PSN, in which the top 6 modules with large number of cases include 51.68% of the whole cases and 251 modules contain only 10 or fewer cases, which indicates the manifestation diversity of liver diseases. Finally, we found that the patient subgroups actually have distinct symptom phenotypes, disease comorbidity characteristics and their underlying molecular pathways, which could be used for understanding the novel disease subtypes of liver conditions. For example, three patient subgroups, namely Module 6 (M6, n = 638), M2 (n = 623) and M1 (n = 488) were associated to common chronic liver disease conditions (hepatitis, cirrhosis, hepatocellular carcinoma). Meanwhile, patient subgroups of M30 (n = 36) and M36 (n = 37) were mostly related to acute gastroenteritis and upper respiratory infection, respectively, which reflected the individual comorbidity characteristics of liver subgroups. Furthermore, we identified the distinct genes and pathways of patient subgroups and the basic liver diseases (hepatitis b and cirrhosis), respectively. The high degree of overlapping pathways between them (e.g. M36 with 93.33% shared enriched pathways) indicates the underlying molecular network mechanisms of each patient subgroup.
Conclusions
Our results demonstrate the utility and comprehensiveness of disease classification study based on community detection of patient network using shared TCM symptom phenotypes and it can be used to other more complex diseases.
ER  - 
TY  - JOUR
T1  - Predicting healthy older adult's brain age based on structural connectivity networks using artificial neural networks
A1  - Lin, Lan
A1  - Jin, Cong
A1  - Fu, Zhenrong
A1  - Zhang, Baiwen
A1  - Bin, Guangyu
A1  - Wu, Shuicai
Y1  - 2016///
KW  -  Connectome
KW  -  DTI
KW  -  Healthy brain ageing
KW  -  White matter
KW  - MRI
JF  - Computer Methods and Programs in Biomedicine
VL  - 125
SP  - 8
EP  - 17
DO  - https://doi.org/10.1016/j.cmpb.2015.11.012
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715003235
N2  - Brain ageing is followed by changes of the connectivity of white matter (WM) and changes of the grey matter (GM) concentration. Neurodegenerative disease is more vulnerable to an accelerated brain ageing, which is associated with prospective cognitive decline and disease severity. Accurate detection of accelerated ageing based on brain network analysis has a great potential for early interventions designed to hinder atypical brain changes. To capture the brain ageing, we proposed a novel computational approach for modeling the 112 normal older subjects (aged 50–79 years) brain age by connectivity analyses of networks of the brain. Our proposed method applied principal component analysis (PCA) to reduce the redundancy in network topological parameters. Back propagation artificial neural network (BPANN) improved by hybrid genetic algorithm (GA) and Levenberg–Marquardt (LM) algorithm is established to model the relation among principal components (PCs) and brain age. The predicted brain age is strongly correlated with chronological age (r=0.8). The model has mean absolute error (MAE) of 4.29 years. Therefore, we believe the method can provide a possible way to quantitatively describe the typical and atypical network organization of human brain and serve as a biomarker for presymptomatic detection of neurodegenerative diseases in the future.
ER  - 
TY  - JOUR
T1  - Retrieval of pathological retina images using Bag of Visual Words and pLSA model
A1  - Sreejini, K S
A1  - Govindan, V K
Y1  - 2019///
KW  -  Bag of Visual Words
KW  -  Topic modelling
KW  -  pLSA
KW  - Diabetic retinopathy
JF  - Engineering Science and Technology, an International Journal
VL  - 22
IS  - 3
SP  - 777
EP  - 785
DO  - https://doi.org/10.1016/j.jestch.2019.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S2215098617314994
N2  - Automated identification of pathological retina images is very important in retinopathy. Conventional methods of retrieval in the domain of retinopathy are based on the manual observation of different components of retina. However, manual observation becomes difficult due to the large diversity of images and the varying symptoms of diseases present in pathological images. For example, varying features such as color, shape and structure of lesions complicate the manual observations and subsequent assessment of the state of the disease. As a solution to these issues, this paper proposes an unsupervised technique known as probabilistic Latent Semantic Analysis (pLSA) along with Bag of Visual Words to discriminate diseased images from normal ones. The method was tested with images from publicly available standard retina fundus image databases and achieved better performance measures compared to the existing methods.
ER  - 
TY  - JOUR
T1  - Multi-focus cluster labeling
A1  - Eikvil, Line
A1  - Jenssen, Tor-Kristian
A1  - Holden, Marit
Y1  - 2015///
KW  -  Cluster labeling
KW  -  Multi focus
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 55
SP  - 116
EP  - 123
DO  - https://doi.org/10.1016/j.jbi.2015.03.012
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415000556
N2  - Document collections resulting from searches in the biomedical literature, for instance, in PubMed, are often so large that some organization of the returned information is necessary. Clustering is an efficient tool for organizing search results. To help the user to decide how to continue the search for relevant documents, the content of each cluster can be characterized by a set of representative keywords or cluster labels. As different users may have different interests, it can be desirable with solutions that make it possible to produce labels from a selection of different topical categories. We therefore introduce the concept of multi-focus cluster labeling to give users the possibility to get an overview of the contents through labels from multiple viewpoints. The concept for multi-focus cluster labeling has been established and has been demonstrated on three different document collections. We illustrate that multi-focus visualizations can give an overview of clusters along axes that general labels are not able to convey. The approach is generic and should be applicable to any biomedical (or other) domain with any selection of foci where appropriate focus vocabularies can be established. A user evaluation also indicates that such a multi-focus concept is useful.
ER  - 
TY  - JOUR
T1  - An ensemble method for extracting adverse drug events from social media
A1  - Liu, Jing
A1  - Zhao, Songzheng
A1  - Zhang, Xiaodi
Y1  - 2016///
KW  -  Adverse drug event extraction
KW  -  Feature selection
KW  -  Feature-based approach
KW  -  Kernel-based approaches
KW  -  Social media
KW  - Relation extraction
JF  - Artificial Intelligence in Medicine
VL  - 70
SP  - 62
EP  - 76
DO  - https://doi.org/10.1016/j.artmed.2016.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S0933365715300373
N2  - Objective
Because adverse drug events (ADEs) are a serious health problem and a leading cause of death, it is of vital importance to identify them correctly and in a timely manner. With the development of Web 2.0, social media has become a large data source for information on ADEs. The objective of this study is to develop a relation extraction system that uses natural language processing techniques to effectively distinguish between ADEs and non-ADEs in informal text on social media.
Methods and materials
We develop a feature-based approach that utilizes various lexical, syntactic, and semantic features. Information-gain-based feature selection is performed to address high-dimensional features. Then, we evaluate the effectiveness of four well-known kernel-based approaches (i.e., subset tree kernel, tree kernel, shortest dependency path kernel, and all-paths graph kernel) and several ensembles that are generated by adopting different combination methods (i.e., majority voting, weighted averaging, and stacked generalization). All of the approaches are tested using three data sets: two health-related discussion forums and one general social networking site (i.e., Twitter).
Results
When investigating the contribution of each feature subset, the feature-based approach attains the best area under the receiver operating characteristics curve (AUC) values, which are 78.6%, 72.2%, and 79.2% on the three data sets. When individual methods are used, we attain the best AUC values of 82.1%, 73.2%, and 77.0% using the subset tree kernel, shortest dependency path kernel, and feature-based approach on the three data sets, respectively. When using classifier ensembles, we achieve the best AUC values of 84.5%, 77.3%, and 84.5% on the three data sets, outperforming the baselines.
Conclusions
Our experimental results indicate that ADE extraction from social media can benefit from feature selection. With respect to the effectiveness of different feature subsets, lexical features and semantic features can enhance the ADE extraction capability. Kernel-based approaches, which can stay away from the feature sparsity issue, are qualified to address the ADE extraction problem. Combining different individual classifiers using suitable combination methods can further enhance the ADE extraction effectiveness.
ER  - 
TY  - JOUR
T1  - NLP based congestive heart failure case finding: A prospective analysis on statewide electronic medical records
A1  - Wang, Yue
A1  - Luo, Jin
A1  - Hao, Shiying
A1  - Xu, Haihua
A1  - Shin, Andrew Young
A1  - Jin, Bo
A1  - Liu, Rui
A1  - Deng, Xiaohong
A1  - Wang, Lijuan
A1  - Zheng, Le
A1  - Zhao, Yifan
A1  - Zhu, Chunqing
A1  - Hu, Zhongkai
A1  - Fu, Changlin
A1  - Hao, Yanpeng
A1  - Zhao, Yingzhen
A1  - Jiang, Yunliang
A1  - Dai, Dorothy
A1  - Culver, Devore S
A1  - Alfreds, Shaun T
A1  - Todd, Rogow
A1  - Stearns, Frank
A1  - Sylvester, Karl G
A1  - Widen, Eric
A1  - Ling, Xuefeng B
Y1  - 2015///
KW  -  Electronic Medical record
KW  -  Natural language processing
KW  -  Prospective validation
KW  -  Random forests
KW  - Congestive heart failure
JF  - International Journal of Medical Informatics
VL  - 84
IS  - 12
SP  - 1039
EP  - 1047
DO  - https://doi.org/10.1016/j.ijmedinf.2015.06.007
UR  - https://www.sciencedirect.com/science/article/pii/S1386505615300137
N2  - Background
In order to proactively manage congestive heart failure (CHF) patients, an effective CHF case finding algorithm is required to process both structured and unstructured electronic medical records (EMR) to allow complementary and cost-efficient identification of CHF patients.
Methods and results
We set to identify CHF cases from both EMR codified and natural language processing (NLP) found cases. Using narrative clinical notes from all Maine Health Information Exchange (HIE) patients, the NLP case finding algorithm was retrospectively (July 1, 2012–June 30, 2013) developed with a random subset of HIE associated facilities, and blind-tested with the remaining facilities. The NLP based method was integrated into a live HIE population exploration system and validated prospectively (July 1, 2013–June 30, 2014). Total of 18,295 codified CHF patients were included in Maine HIE. Among the 253,803 subjects without CHF codings, our case finding algorithm prospectively identified 2411 uncodified CHF cases. The positive predictive value (PPV) is 0.914, and 70.1% of these 2411 cases were found to be with CHF histories in the clinical notes.
Conclusions
A CHF case finding algorithm was developed, tested and prospectively validated. The successful integration of the CHF case findings algorithm into the Maine HIE live system is expected to improve the Maine CHF care.
ER  - 
TY  - JOUR
T1  - Laplacian regularized kernel minimum squared error and its application to face recognition
A1  - Gan, Haitao
Y1  - 2014///
KW  -  Face recognition
KW  -  Laplacian regularization
KW  -  Semi-supervised learning
KW  - Kernel minimum squared error
JF  - Optik
VL  - 125
IS  - 14
SP  - 3524
EP  - 3529
DO  - https://doi.org/10.1016/j.ijleo.2014.01.058
UR  - https://www.sciencedirect.com/science/article/pii/S0030402614002149
N2  - Kernel minimum squared error (KMSE) has been receiving much attention in data mining and pattern recognition in recent years. Generally speaking, training a KMSE classifier, which is a kind of supervised learning, needs sufficient labeled examples. However, labeled examples are usually insufficient and unlabeled examples are abundant in real-world applications. In this paper, we introduce a semi-supervised KMSE algorithm, called Laplacian regularized KMSE (LapKMSE), which explicitly exploits the manifold structure. We construct a p nearest neighbor graph to model the manifold structure of labeled and unlabeled examples. Then, LapKMSE incorporates the structure information of labeled and unlabeled examples in the objective function of KMSE by adding a Laplacian regularization term. As a result, the labels of labeled and unlabeled examples vary smoothly along the geodesics on the manifold. Experimental results on several synthetic and real-world datasets illustrate the effectiveness of our algorithm. Finally our algorithm is applied to face recognition and achieves the comparable results compared to the other supervised and semi-supervised methods.
ER  - 
TY  - JOUR
T1  - Efficient sampling-based energy function evaluation for ensemble optimization using simulated annealing
A1  - Tóth, János
A1  - Tomán, Henrietta
A1  - Hajdu, András
Y1  - 2020///
KW  -  Ensemble
KW  -  Microaneurysm detection
KW  -  Parameter optimization
KW  -  Sampling-based evaluation
KW  -  Simulated annealing
KW  - Diabetic retinopathy
JF  - Pattern Recognition
VL  - 107
SP  - 107510
EP  - 107510
DO  - https://doi.org/10.1016/j.patcog.2020.107510
UR  - https://www.sciencedirect.com/science/article/pii/S0031320320303137
N2  - In this study, we attempted to develop a method for accelerating parameter optimization of an object detector ensemble over large image datasets by using simulated annealing. We propose a novel sampling-based evaluation method that considers the minimum portion of the dataset required in each iteration to maintain solution quality. This approach can be considered a noisy evaluation of the energy. The sample sizes required during the search process are theoretically determined by adapting the convergence results for noisy evaluation. To determine applicability, we prepared and optimized two ensembles for diabetic retinopathy pre-screening based on microaneurysm detection with convolutional neural network-based and traditional object detectors. Our experimental results indicate that the proposed sampling-based evaluation method substantially reduced the computational time required for optimizing the parameters of the ensembles while preserving solution quality.
ER  - 
TY  - JOUR
T1  - A smart feature selection technique for object localization in ocular fundus images with the aid of color subspaces
A1  - Ilyasova, Nataly
A1  - Paringer, Rustam
A1  - Shirokanev, Alexandr
A1  - Kupriyanov, Alexandr
A1  - Ushakova, Natalya
Y1  - 2017///
KW  -  diagnostic features
KW  -  image processing
KW  -  laser coagulation
KW  -  texture analysis
KW  - fundus images
JF  - Procedia Engineering
VL  - 201
SP  - 736
EP  - 745
DO  - https://doi.org/10.1016/j.proeng.2017.09.599
UR  - https://www.sciencedirect.com/science/article/pii/S1877705817341292
N1  - 3rd International Conference “Information Technology and Nanotechnology&quot;, ITNT-2017, 25-27 April 2017, Samara, Russia
N2  - We propose a technique for selecting effective features for object localization in ocular fundus images. The technique has made it possible to conduct smart feature analysis with the aid of color subspaces when solving a problem of selecting the areas of interest. The relevance of the problem is associated with enhancing the effectiveness of laser coagulation surgery. The proposed technique enables not only the informative features to be extracted in particular color spaces but also the most informative color subspace to be identified. The technique allows an effective feature for separating two particular classes to be identified at a definite size of the fragmentation block thanks to the use of various feature selection rules. The technique also makes it possible to find a universal feature using which two particular initial classes can be separated with a minimal clustering error in all color subspaces, also finding a color-specific in-formative feature enabling the majority of classes under study to be separated. The most informative color subspace was defined.
ER  - 
TY  - JOUR
T1  - A review of significant researches on prediction of preterm birth using uterine electromyogram signal
A1  - P., Shaniba Asmi
A1  - Subramaniam, Kamalraj
A1  - Iqbal, Nisheena V
Y1  - 2019///
KW  -  Classifier
KW  -  Electrohysterogram (EHG)
KW  -  Feature extraction
KW  -  Feature subset selection
KW  -  Pre-processing
KW  -  Uterine electromyogram (UEMG)
KW  - Preterm
JF  - Future Generation Computer Systems
VL  - 98
SP  - 135
EP  - 143
DO  - https://doi.org/10.1016/j.future.2018.10.033
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18322349
N2  - Early diagnose for the prevention of preterm birth is one of the important perinatal challenges. The neonatal care and early treatment for preterm babies are increasing the chance of survival, but anyways it affects the respiratory distress, immature brains, cerebral palsy, mental retardation, visual and hearing impairments, and poor health and growth. If preterm labor is diagnosed in the early period of gestation, then it is easy to give an appropriate treatment to the pregnant woman. The uterine electrical activity assessment is a suitable method for monitoring the labor process especially for the prediction of preterm labor. Electrohysterography is a non-invasive technique to monitor the contraction. The electrohysterogram (EHG) or uterine electromyogram (Uterine EMG) is considered as a biomarker for the prediction or preterm labor. A number of studies in this field by various researchers have been reviewed. On the basis of such reviews, this paper provides the different steps such as pre-processing , feature extraction, classifiers and feature subset selection methods for the detection and prediction of preterm birth.
ER  - 
TY  - JOUR
T1  - A lightweight CNN for Diabetic Retinopathy classification from fundus images
A1  - S., Gayathri
A1  - Gopi, Varun P
A1  - Palanisamy, P
Y1  - 2020///
KW  -  10-fold cross-validation
KW  -  CNN feature extraction
KW  -  Classifiers
KW  -  Retinal fundus images
KW  - DR binary and multi class classification
JF  - Biomedical Signal Processing and Control
VL  - 62
SP  - 102115
EP  - 102115
DO  - https://doi.org/10.1016/j.bspc.2020.102115
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420302676
N2  - Diabetic Retinopathy (DR) is a complication of diabetes mellitus that damages blood vessel networks in the retina. This is a serious vision-threatening issue in most diabetic subjects. The DR diagnosis by color fundus images involves skilled clinicians to recognize the presence of lesions in the image that can be used to detect the disease properly, making it a time-consuming process. Effective automated detection of DR is a challenging task. The feature extraction plays an excellent role in effective automated disease detection. Convolutional Neural Networks (CNN) have superior image classification efficiency in the present scenario compared to earlier handcrafted feature-based image classification techniques. This work presents a novel CNN model to extract features from retinal fundus images for better classification performance. The CNN output features are used as input for different machine learning classifiers in the suggested system. The model is evaluated through various classifiers (Support Vector Machine, AdaBoost, Naive Bayes, Random Forest, and J48) by using images from generic IDRiD, MESSIDOR, and KAGGLE datasets. The efficacy of the classifier is evaluated by comparing the specificity, precision, recall, False Positive Rate (FPR), Kappa-score, and accuracy values for each classifier. The evaluation results indicate that the proposed feature extraction technique along with the J48 classifier outperforms all the other classifiers for MESSIDOR, IDRiD, and KAGGLE datasets with an average accuracy of 99.89% for binary classification and 99.59% for multiclass classification. Furthermore, for the J48 classifier, the average Kappa-score (K-score) is 0.994 for binary classification and 0.994 for multi-class classification.
ER  - 
TY  - JOUR
T1  - An improved rule induction based denial of service attacks classification model
A1  - Mohammad, Rami Mustafa A
A1  - Alsmadi, Mutasem K
A1  - Almarashdeh, Ibrahim
A1  - Alzaqebah, Malek
Y1  - 2020///
KW  -  Classification
KW  -  Cloud computing
KW  -  Intrusion detection
KW  -  Quality of service
KW  -  Rule induction
KW  - Denial of service
JF  - Computers & Security
VL  - 99
SP  - 102008
EP  - 102008
DO  - https://doi.org/10.1016/j.cose.2020.102008
UR  - https://www.sciencedirect.com/science/article/pii/S0167404820302819
N2  - For assessing the quality of any internet and cloud computing services; accessibility is presusmed a significant factor among other Quality of Service (QoS) factors. Distributed Denial of Service attack (DDoS) is considered a significant threat pertaining to all contemporary and emerging online-based services. Intelligent solutions centered on the utilization of data mining methods are looming on the horizon as possible solutions to counter this kind of attacks. Rule Induction (RI), which is a well-known data mining method is regarded as a possible approach for developing an intelligent DDoS detection system. The current article offers an “Improved RI algorithm” (IRI) which decreases the searching space for generating classification rules by removing all unimportant candidate rule-items along the way of creating the classification model. The main advantage of IRI is producing a group of rules that can be described as concise, easy to understand, and easy-to-implement. In addition, the classifiers generated by IRI are more compact in size which is heavily weighted when producing any classification system. The proposed algorithm is then applied for detecting DDoS attacks (IRIDOS). Empirical evaluations using the UNSW-NB15 dataset that has been obtained from the University of New South Wales confirmed the robustness of IRIDOS.
ER  - 
TY  - JOUR
T1  - Sequential valuation networks for asymmetric decision problems
A1  - Demirer, Rıza
A1  - Shenoy, Prakash P
Y1  - 2006///
KW  -  Asymmetric decision problems
KW  -  Influence diagrams
KW  -  Sequential decision diagrams
KW  -  Valuation networks
KW  - Decision analysis
JF  - European Journal of Operational Research
VL  - 169
IS  - 1
SP  - 286
EP  - 309
DO  - https://doi.org/10.1016/j.ejor.2004.06.009
UR  - https://www.sciencedirect.com/science/article/pii/S0377221704004394
N2  - This paper deals with representation and solution of asymmetric decision problems. We describe a new representation called sequential valuation networks that is a hybrid of Covaliu and Oliver’s sequential decision diagrams and Shenoy’s valuation networks. The solution algorithm is based on the idea of decomposing a large asymmetric problem into smaller sub-problems and then using the fusion algorithm of valuation networks to solve the sub-problems. Sequential valuation networks inherit many of the strengths of sequential decision diagrams and valuation networks while overcoming many of their shortcomings. We illustrate our technique by representing and solving a modified version of Covaliu and Oliver’s [Manage. Sci. 41(12) (1995) 1860] Reactor problem in complete detail.
ER  - 
TY  - JOUR
T1  - Accurate prediction of glucose concentration and identification of major contributing features from hardly distinguishable near-infrared spectroscopy
A1  - Mekonnen, Bitewulign Kassa
A1  - Yang, Webb
A1  - Hsieh, Tung-Han
A1  - Liaw, Shien-Kuei
A1  - Yang, Fu-Liang
Y1  - 2020///
KW  -  Extra trees regression
KW  -  Neural network
KW  -  Partial least squares regression
KW  -  Principle component analysis
KW  -  Random forest
KW  -  Support vector machine regression
KW  -  Xgboost
KW  - Near-infrared spectroscopy
JF  - Biomedical Signal Processing and Control
VL  - 59
SP  - 101923
EP  - 101923
DO  - https://doi.org/10.1016/j.bspc.2020.101923
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420300793
N2  - In this study, we tackle the accurate prediction of glucose aqueous concentration from hardly distinguishable near-infrared (NIR) spectroscopy. We adopted several machine learning approaches for the spectral analyses and identified important features learned by each model. The models we investigated include Partial Least Squares Regression (PLSR), Support Vector Machine Regression (SVMR), Random Forest Regression (RF), Extra Trees Regression (ETR), eXtreme Gradient Boosting (Xgboost), and hybrid Principal Component Analysis-Neural Network (PCA-NN) methods. From 47 different glucose aqueous concentrations which cover the range of 40–500 mg/dl, we measured 564 near-infrared (NIR) absorbance spectra samples with wavelength range 900 nm–2200 nm. Then the spectra samples were randomly split into 80% for the training set and 20% for the testing set. In our test, we found that the models SVMR, ETR, and PCA-NN reach extremely good performance, which had correlation coefficient R > 0.99 and determination of coefficient R2 > 0.985. To explore the robustness of each machine learning approach, we extracted their high-weighting features and examine their distribution. We found that having large overlapping of the high-weighting features learned by the model when trained by different data sets may be an indication of model stability. In addition, our analysis came up with the essential region of features to disentangle the hardly distinguishable signal. Our study demonstrates a robust machine learning models for the prediction of glucose aqueous concentration in an in-vitro setup using near-infrared spectroscopy.
ER  - 
TY  - JOUR
T1  - Stratifying no-show patients into multiple risk groups via a holistic data analytics-based framework
A1  - Simsek, Serhat
A1  - Tiahrt, Thomas
A1  - Dag, Ali
Y1  - 2020///
KW  -  Healthcare informatics
KW  -  Medical decision making
KW  -  Patient no-shows
KW  - Data mining
JF  - Decision Support Systems
VL  - 132
SP  - 113269
EP  - 113269
DO  - https://doi.org/10.1016/j.dss.2020.113269
UR  - https://www.sciencedirect.com/science/article/pii/S0167923620300245
N2  - Accurate prediction of no-show patients plays a crucial role as it enables researchers to increase the efficiency of their scheduling systems. The purpose of the current study is to formulate a novel hybrid data mining-based methodology to a) accurately predict the no-show patients, b) build a parsimonious model by employing a comprehensive variable selection procedure, c) build a model that does not suffer due to data imbalance, and d) provide healthcare agencies with a patient-specific risk level. Our study suggests that an Artificial Neural Network (ANN) model should be employed as a classification algorithm in predicting patient no-shows by using the variable set that is commonly selected by a Genetic Algorithm (GA) and Simulated Annealing (SA). In addition, we used Random Under Sampling (RUS) to improve the performance of the model in predicting the minority group (no-show) patients. The patient-specific risk scores were justified by applying a threshold sensitivity analysis. Also, the web-based decision support tool that can be adopted by clinics is developed. The clinics can incorporate their own intuition/incentive to make the final decision on the cases where the model is not confident enough (i.e. when the estimated probabilities fall near the decision boundary). These insights enable health care professionals to improve clinic utilization and patient outcomes.
ER  - 
TY  - JOUR
T1  - Data-driven approach for spatiotemporal distribution prediction of fault events in power transmission systems
A1  - Sun, Chenhao
A1  - Wang, Xin
A1  - Zheng, Yihui
Y1  - 2019///
KW  -  Component importance measurement
KW  -  Fault event
KW  -  Power transmission system
KW  -  Rare association rule mining
KW  - Spatiotemporal distribution prediction
JF  - International Journal of Electrical Power & Energy Systems
VL  - 113
SP  - 726
EP  - 738
DO  - https://doi.org/10.1016/j.ijepes.2019.06.023
UR  - https://www.sciencedirect.com/science/article/pii/S0142061519305356
N2  - The spatiotemporal distribution of future fault events in a power transmission system assists in operational planning and maintenance scheduling. To this end, this paper proposes an environmental attributes-based framework for the spatiotemporal distribution prediction of potential fault events in the system. In this framework, the distribution of future fault events is predicted via the forecasted information of the environmental attributes rather than the electrical attributes. An extensive investigation covering all environmental attributes including the fault causes is presented, and the underlying fault-attribute relationships are explored. Notably, the rare association rule mining is employed to cope with the rare occurred elements in each environmental attribute through five new significance measurements. Next, to distinguish the diverse influence of each environmental element on the reliability of the whole system, the relative weights are developed. Also, the impact of the latent erroneous predictions of the events caused by one fault cause on the overall prediction performance is assessed via an extended definition of the component importance measurement. Ultimately, the efficiency of the modified significance measurements, the prediction performance in the two test cases, and the impact of each single fault cause are validated by an empirical study. The flexibility and the robustness of this framework in real applications are therefore demonstrated.
ER  - 
TY  - JOUR
T1  - Computational pathology: Challenges and promises for tissue analysis
A1  - Fuchs, Thomas J
A1  - Buhmann, Joachim M
Y1  - 2011///
KW  -  Cancer research
KW  -  Machine learning
KW  -  Medical imaging
KW  -  Survival statistics
KW  -  Whole slide imaging
KW  - Computational pathology
JF  - Computerized Medical Imaging and Graphics
VL  - 35
IS  - 7
SP  - 515
EP  - 530
DO  - https://doi.org/10.1016/j.compmedimag.2011.02.006
UR  - https://www.sciencedirect.com/science/article/pii/S0895611111000383
N1  - Whole Slide Image Process
N2  - Abstract
The histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer. A plethora of different data sources ranging from tissue microarray data to gene expression, proteomics or metabolomics data provide a detailed overview of the health status of a patient. Medical doctors need to assess these information sources and they rely on data driven automatic analysis tools. Methods for classification, grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages. This paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning.
ER  - 
TY  - JOUR
T1  - Cohort selection for clinical trials using multiple instance learning
A1  - Dai, Hong-Jie
A1  - Wang, Feng-Duo
A1  - Chen, Chih-Wei
A1  - Su, Chu-Hsien
A1  - Wu, Chi-Shin
A1  - Jonnagaddala, Jitendra
Y1  - 2020///
KW  -  Clinical natural language processing
KW  -  Electronic health records
KW  -  Multiple instance learning
KW  - Cohort selection
JF  - Journal of Biomedical Informatics
VL  - 107
SP  - 103438
EP  - 103438
DO  - https://doi.org/10.1016/j.jbi.2020.103438
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300666
N2  - Identifying patients eligible for clinical trials using electronic health records (EHRs) is a challenging task usually requiring a comprehensive analysis of information stored in multiple EHRs of a patient. The goal of this study is to investigate different methods and their effectiveness in identifying patients that meet specific eligibility selection criteria based on patients’ longitudinal records. An unstructured dataset released by the n2c2 cohort selection for clinical trials track was used, each of which included 2–5 records manually annotated to thirteen pre-defined selection criteria. Unlike the other studies, we formulated the problem as a multiple instance learning (MIL) task and compared the performance with that of the rule-based and the single instance-based classifiers. Our official best run achieved an average micro-F score of 0.8765 which was ranked as one of the top ten results in the track. Further experiments demonstrated that the performance of the MIL-based classifiers consistently yield better performance than their single-instance counterparts in the criteria that require the overall comprehension of the information distributed among all of the patient’s EHRs. Rule-based and single instance learning approaches exhibited better performance in criteria that don’t require a consideration of several factors across records. This study demonstrated that cohort selection using longitudinal patient records can be formulated as a MIL problem. Our results exhibit that the MIL-based classifiers supplement the rule-based methods and provide better results in comparison to the single instance learning approaches.
ER  - 
TY  - JOUR
T1  - Meal-time and duration monitoring using wearable sensors
A1  - Dong, Bo
A1  - Biswas, Subir
Y1  - 2017///
KW  -  Food intake detection
KW  -  Hidden markov model
KW  -  Wearable sensors
KW  - Diet monitoring
JF  - Biomedical Signal Processing and Control
VL  - 32
SP  - 97
EP  - 109
DO  - https://doi.org/10.1016/j.bspc.2016.09.018
UR  - https://www.sciencedirect.com/science/article/pii/S1746809416301446
N2  - In many recent studies it has been shown that the frequency and duration of meals taken during a day can have significant impacts on human health. Poor meal habits such as skipping breakfast, nighttime eating, and too many eating episodes can increase the risk of obesity. Researchers have often used self-reported questionnaires for analyzing such meal-time and dietary behaviors. Questionnaire-based data collection, however, often suffers from high errors due to reporting subjectivity. This paper presents a wearable sensor system that can monitor breathing and hand movement for estimating the time and duration of a meal. The system combines swallowing signatures from breathing signal with hand movement signatures from hand acceleration to train a hierarchical Support Vector Machine (SVM) classifier and a Hidden Markov Model (HMM) for mealtime and duration estimation. Algorithms are developed for detecting various types of swallowing events including for solid and liquid in the presence of artifacts such as spontaneous swallows, laughing, coughing, and throat clearance. The experiments were carried out on 14 healthy subjects wearing the proposed system. In each experiment session, the subjects were asked to have lunch, drink water, rest and talk. The subjects were asked to press a button for each swallow, and the whole experiment process was video recorded. The push button and video information were used as a ground truth for verification purposes. Through extensive experimentation in a semi-controlled setting, it was shown that the system is able to detect mealtime with high accuracy.
ER  - 
TY  - JOUR
T1  - A non-parametric segmentation methodology for oral videocapillaroscopic images
A1  - Bellavia, Fabio
A1  - Cacioppo, Antonino
A1  - Lupaşcu, Carmen Alina
A1  - Messina, Pietro
A1  - Scardina, Giuseppe
A1  - Tegolo, Domenico
A1  - Valenti, Cesare
Y1  - 2014///
KW  -  Leave-one-out cross-validation
KW  -  Mathematical morphology
KW  -  Oral videocapillaroscopy
KW  -  Wavelet analysis
KW  - Non-parametric image segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 114
IS  - 3
SP  - 240
EP  - 246
DO  - https://doi.org/10.1016/j.cmpb.2014.02.009
UR  - https://www.sciencedirect.com/science/article/pii/S0169260714000649
N2  - We aim to describe a new non-parametric methodology to support the clinician during the diagnostic process of oral videocapillaroscopy to evaluate peripheral microcirculation. Our methodology, mainly based on wavelet analysis and mathematical morphology to preprocess the images, segments them by minimizing the within-class luminosity variance of both capillaries and background. Experiments were carried out on a set of real microphotographs to validate this approach versus handmade segmentations provided by physicians. By using a leave-one-patient-out approach, we pointed out that our methodology is robust, according to precision–recall criteria (average precision and recall are equal to 0.924 and 0.923, respectively) and it acts as a physician in terms of the Jaccard index (mean and standard deviation equal to 0.858 and 0.064, respectively).
ER  - 
TY  - JOUR
T1  - Automated screening system for retinal health using bi-dimensional empirical mode decomposition and integrated index
A1  - Acharya, U Rajendra
A1  - Mookiah, Muthu Rama Krishnan
A1  - Koh, Joel E W
A1  - Tan, Jen Hong
A1  - Bhandary, Sulatha V
A1  - Rao, A Krishna
A1  - Fujita, Hamido
A1  - Hagiwara, Yuki
A1  - Chua, Chua Kuang
A1  - Laude, Augustinus
Y1  - 2016///
KW  -  Bi-dimensional empirical mode decomposition
KW  -  Computer aided diagnosis
KW  -  Energy
KW  -  Entropy
KW  -  Fundus imaging
KW  -  Posterior segment eye diseases
KW  - Retina
JF  - Computers in Biology and Medicine
VL  - 75
SP  - 54
EP  - 62
DO  - https://doi.org/10.1016/j.compbiomed.2016.04.015
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516301044
N2  - Posterior Segment Eye Diseases (PSED) namely Diabetic Retinopathy (DR), glaucoma and Age-related Macular Degeneration (AMD) are the prime causes of vision loss globally. Vision loss can be prevented, if these diseases are detected at an early stage. Structural abnormalities such as changes in cup-to-disc ratio, Hard Exudates (HE), drusen, Microaneurysms (MA), Cotton Wool Spots (CWS), Haemorrhages (HA), Geographic Atrophy (GA) and Choroidal Neovascularization (CNV) in PSED can be identified by manual examination of fundus images by clinicians. However, manual screening is labour-intensive, tiresome and time consuming. Hence, there is a need to automate the eye screening. In this work Bi-dimensional Empirical Mode Decomposition (BEMD) technique is used to decompose fundus images into 2D Intrinsic Mode Functions (IMFs) to capture variations in the pixels due to morphological changes. Further, various entropy namely Renyi, Fuzzy, Shannon, Vajda, Kapur and Yager and energy features are extracted from IMFs. These extracted features are ranked using Chernoff Bound and Bhattacharyya Distance (CBBD), Kullback–Leibler Divergence (KLD), Fuzzy-minimum Redundancy Maximum Relevance (FmRMR), Wilcoxon, Receiver Operating Characteristics Curve (ROC) and t-test methods. Further, these ranked features are fed to Support Vector Machine (SVM) classifier to classify normal and abnormal (DR, AMD and glaucoma) classes. The performance of the proposed eye screening system is evaluated using 800 (Normal=400 and Abnormal=400) digital fundus images and 10-fold cross validation method. Our proposed system automatically identifies normal and abnormal classes with an average accuracy of 88.63%, sensitivity of 86.25% and specificity of 91% using 17 optimal features ranked using CBBD and SVM-Radial Basis Function (RBF) classifier. Moreover, a novel Retinal Risk Index (RRI) is developed using two significant features to distinguish two classes using single number. Such a system helps to reduce eye screening time in polyclinics or community-based mass screening. They will refer the patients to main hospitals only if the diagnosis belong to the abnormal class. Hence, the main hospitals will not be unnecessarily crowded and doctors can devote their time for other urgent cases.
ER  - 
TY  - JOUR
T1  - A supervised machine learning-based methodology for analyzing dysregulation in splicing machinery: An application in cancer diagnosis
A1  - Reyes, Oscar
A1  - Pérez, Eduardo
A1  - Luque, Raúl M
A1  - Castaño, Justo
A1  - Ventura, Sebastián
Y1  - 2020///
KW  -  Alternative Splicing
KW  -  Classification methods
KW  -  Explaining classifier’s predictions
KW  -  Feature weighting methods
KW  - Transcript-based analysis
JF  - Artificial Intelligence in Medicine
VL  - 108
SP  - 101950
EP  - 101950
DO  - https://doi.org/10.1016/j.artmed.2020.101950
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719312187
N2  - Deregulated splicing machinery components have shown to be associated with the development of several types of cancer and, therefore, the determination of such alterations can help the development of tumor-specific molecular targets for early prognosis and therapy. Determining such splicing components, however, is not a straightforward task mainly due to the heterogeneity of tumors, the variability across samples, and the fat-short characteristic of genomic datasets. In this work, a supervised machine learning-based methodology is proposed, allowing the determination of subsets of relevant splicing components that best discriminate samples. The methodology comprises three main phases: first, a ranking of features is determined by means of applying feature weighting algorithms that compute the importance of each splicing component; second, the best subset of features that allows the induction of an accurate classifier is determined by means of conducting an effective heuristic search; then the confidence over the induced classifier is assessed by means of explaining the individual predictions and its global behavior. At the end, an extensive experimental study was conducted on a large collection of transcript-based datasets, illustrating the utility and benefit of the proposed methodology for analyzing dysregulation in splicing machinery.
ER  - 
TY  - JOUR
T1  - Machine learning in oral squamous cell carcinoma: Current status, clinical concerns and prospects for future—A systematic review
A1  - Alabi, Rasheed Omobolaji
A1  - Youssef, Omar
A1  - Pirinen, Matti
A1  - Elmusrati, Mohammed
A1  - Mäkitie, Antti A
A1  - Leivo, Ilmo
A1  - Almangush, Alhadi
Y1  - 2021///
KW  -  Explainable AI
KW  -  Oral squamous cell carcinoma
KW  -  Systematic review
KW  - Machine learning
JF  - Artificial Intelligence in Medicine
VL  - 115
SP  - 102060
EP  - 102060
DO  - https://doi.org/10.1016/j.artmed.2021.102060
UR  - https://www.sciencedirect.com/science/article/pii/S0933365721000531
N2  - Background
Oral cancer can show heterogenous patterns of behavior. For proper and effective management of oral cancer, early diagnosis and accurate prediction of prognosis are important. To achieve this, artificial intelligence (AI) or its subfield, machine learning, has been touted for its potential to revolutionize cancer management through improved diagnostic precision and prediction of outcomes. Yet, to date, it has made only few contributions to actual medical practice or patient care.
Objectives
This study provides a systematic review of diagnostic and prognostic application of machine learning in oral squamous cell carcinoma (OSCC) and also highlights some of the limitations and concerns of clinicians towards the implementation of machine learning-based models for daily clinical practice.
Data sources
We searched OvidMedline, PubMed, Scopus, Web of Science, and Institute of Electrical and Electronics Engineers (IEEE) databases from inception until February 2020 for articles that used machine learning for diagnostic or prognostic purposes of OSCC.
Eligibility criteria
Only original studies that examined the application of machine learning models for prognostic and/or diagnostic purposes were considered.
Data extraction
Independent extraction of articles was done by two researchers (A.R. & O.Y) using predefine study selection criteria. We used the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) in the searching and screening processes. We also used Prediction model Risk of Bias Assessment Tool (PROBAST) for assessing the risk of bias (ROB) and quality of included studies.
Results
A total of 41 studies were published to have used machine learning to aid in the diagnosis/or prognosis of OSCC. The majority of these studies used the support vector machine (SVM) and artificial neural network (ANN) algorithms as machine learning techniques. Their specificity ranged from 0.57 to 1.00, sensitivity from 0.70 to 1.00, and accuracy from 63.4 % to 100.0 % in these studies. The main limitations and concerns can be grouped as either the challenges inherent to the science of machine learning or relating to the clinical implementations.
Conclusion
Machine learning models have been reported to show promising performances for diagnostic and prognostic analyses in studies of oral cancer. These models should be developed to further enhance explainability, interpretability, and externally validated for generalizability in order to be safely integrated into daily clinical practices. Also, regulatory frameworks for the adoption of these models in clinical practices are necessary.
ER  - 
TY  - JOUR
T1  - Constructing biomarker for early diagnosis of aMCI based on combination of multiscale fuzzy entropy and functional brain connectivity
A1  - Su, Rui
A1  - Li, Xin
A1  - Li, Zhenyang
A1  - Han, Ying
A1  - Cui, Wei
A1  - Xie, Ping
A1  - Liu, Yi
Y1  - 2021///
KW  -  Amnestic mild cognitive impairment
KW  -  Extreme Learning Machine
KW  -  Multiscale Fuzzy entropy
KW  -  Phase locking value
KW  - Electroencephalography
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103000
EP  - 103000
DO  - https://doi.org/10.1016/j.bspc.2021.103000
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421005978
N2  - Objective
To achieve the early diagnosis of amnestic mild cognitive impairment (aMCI), this paper proposes a multi-dimensional index, which combines the advantages of the multiscale fuzzy entropy (FuzzyEn) and phase locking value (PLV) based on electroencephalography (EEG).
Methods
The complexity and synchronization of the EEG were characterized using FuzzyEn and PLV in five frequency bands, respectively. By combining the two methods, the changes in the health of brain function were comprehensively analyzed. The extreme learning machine (ELM) method was used to classify aMCI patients based on a multi-dimensional index.
Results
Compared with aMCI patients, the multiscale FuzzyEnand PLV of normal controls (NC) were higher and statistically significant (P < 0.05) in the Fp1 and Fp2channels.Moreover,significant correlation existed between the multiscale FuzzyEnor PLV and the MoCA scores in the Fp1 and Fp2 channels. The classification accuracy and running time based on ELM in the prefrontal lobe were 83.34% and 0.003 s, respectively.
Concludes
The multi-dimensional index based on prefrontal lobe could diagnosis cognitive decline of aMCI patients.
Significance
The results showed that features integrated multiscale FuzzyEn and PLV could be used as a biomarker of cognitive decline and help realize the early diagnosis of aMCI patients.
ER  - 
TY  - JOUR
T1  - Optimal segmentation of pupillometric images for estimating pupil shape parameters
A1  - De Santis, A
A1  - Iacoviello, D
Y1  - 2006///
KW  -  Level sets
KW  -  Optimal segmentation
KW  -  Pupil shape
KW  - Pupillometry
JF  - Computer Methods and Programs in Biomedicine
VL  - 84
IS  - 2
SP  - 174
EP  - 187
DO  - https://doi.org/10.1016/j.cmpb.2006.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S0169260706001659
N1  - Medical Image Segmentation Special Issue
N2  - The problem of determining the pupil morphological parameters from pupillometric data is considered. These characteristics are of great interest for non-invasive early diagnosis of the central nervous system response to environmental stimuli of different nature, in subjects suffering some typical diseases such as diabetes, Alzheimer disease, schizophrenia, drug and alcohol addiction. Pupil geometrical features such as diameter, area, centroid coordinates, are estimated by a procedure based on an image segmentation algorithm. It exploits the level set formulation of the variational problem related to the segmentation. A discrete set up of this problem that admits a unique optimal solution is proposed: an arbitrary initial curve is evolved towards the optimal segmentation boundary by a difference equation; therefore no numerical approximation schemes are needed, as required in the equivalent continuum formulation usually adopted in the relevant literature.
ER  - 
TY  - JOUR
T1  - Which risk predictors are more likely to indicate severe AKI in hospitalized patients?
A1  - Wu, Lijuan
A1  - Hu, Yong
A1  - Yuan, Borong
A1  - Zhang, Xiangzhou
A1  - Chen, Weiqi
A1  - Liu, Kang
A1  - Liu, Mei
Y1  - 2020///
KW  -  Electronic medical records
KW  -  Knowledge mining model
KW  -  Risk predictors
KW  -  Severe AKI
KW  - Acute kidney injury (AKI)
JF  - International Journal of Medical Informatics
VL  - 143
SP  - 104270
EP  - 104270
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104270
UR  - https://www.sciencedirect.com/science/article/pii/S1386505620307280
N2  - Objectives
Acute kidney injury (AKI) is a sudden episode of kidney failure or damage and the risk of AKI is determined by the complex interactions of patient factors. In this study, we aimed to find out which risk factors in hospitalized patients are more likely to indicate severe AKI.
Methods
We constructed a retrospective cohort of adult patients from all inpatient units of a tertiary care academic hospital between November 2007 and December 2016. AKI predictors included demographic information, admission and discharge dates, medications, laboratory values, past medical diagnoses and admission diagnosis. We developed a machine learning-based knowledge mining model and a screening framework to analyze which risk predictors are more likely to imply severe AKI in hospitalized populations.
Results
Among the final analysis cohort of 76,957 hospital admissions, AKI occurred in 7,259 (9.43 %) with 6,396 (8.31 %) at stage 1, 678 (0.88 %) at stage 2, and 185 (0.24 %) at stage 3. We compared the non-AKI (without AKI) vs any AKI (stages 1–3), and mild AKI (stage 1) vs severe AKI (stages 2 and 3), where the best cross-validated area under the receiver operator characteristic curve (AUC) were 0.81 (95 % CI, 0.79−0.82) and 0.66 (95 % CI, 0.62−0.71), respectively. Using the developed knowledge mining model and screening framework, we identified 33 risk predictors indicating that severe AKI may occur.
Conclusions
This study screened out 33 risk predictors that are more likely to indicate severe AKI in hospitalized patients, which would help strengthen the early care and prevention of patients.
ER  - 
TY  - JOUR
T1  - Hybrid Nature Inspired SMO-GBM Classifier for Exudate Classification on Fundus Retinal Images
A1  - Badgujar, R D
A1  - Deore, P J
Y1  - 2019///
KW  -  Fundus retinal images
KW  -  Gradient boosting machines
KW  -  Hybrid algorithm
KW  -  Optical disc
KW  - Spider monkey optimization
JF  - IRBM
VL  - 40
IS  - 2
SP  - 69
EP  - 77
DO  - https://doi.org/10.1016/j.irbm.2019.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S1959031818300587
N2  - Background: The diabetic retinopathy can result in loss of vision if not detected in the earlier stages. Exudates are the lesions which play a crucial role in early diagnosis of diabetic retinopathy. The localization of exudates lesions with high values of performance metrics is complicated due to presence of blood vessels and other noisy artifacts. Method: We present computer aided system for classification of retinal fundus images using a novel nature inspired spider monkey optimization for parameter tuning of gradient boosting machines classifier. The image enhancement has been performed with histogram equalization and contourlet transform. The pixels belonging to optic disc region are detected and eliminated using circular Hough transform and Otsu's segmentation method. We have employed Kirsch's matrices for blood vessel detection. The GLCM based feature vector extraction has been employed for textural features. The classification has been performed with hybrid SMO-GBM classifier. Result: We have utilized the STARE database for validation of proposed technique. The proposed system can effectively classify entire image set from test data. The SMO-GBM classifier can further sub-segregate into sub classes with an average accuracy of 97.5%. Conclusion: The proposed approach provides detection and grading of diabetic retinopathy. The abnormality is further categories as soft, moderate and severe. The hybrid SMO-GBM classifier yields a better statistical metrics than the existing exudates classification approaches.
ER  - 
TY  - JOUR
T1  - A CNN-based novel solution for determining the survival status of heart failure patients with clinical record data: numeric to image
A1  - Aslan, Muhammet Fatih
A1  - Sabanci, Kadir
A1  - Durdu, Akif
Y1  - 2021///
KW  -  Deep learning
KW  -  Heart failure
KW  -  Numeric-to-image
KW  - Convolutional neural network
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102716
EP  - 102716
DO  - https://doi.org/10.1016/j.bspc.2021.102716
UR  - https://www.sciencedirect.com/science/article/pii/S174680942100313X
N2  - The aim of this study is to effectively evaluate numerical data, which are frequently encountered in the medical field, with popular deep learning-based Convolutional Neural Network (CNN) models. Heart failure is a common disease worldwide and it is very important to identify patients with a high survival rate and whose condition will deteriorate. A heart failure dataset consisting of numerical values only, needs to be converted into image data for analysis using the advantages of CNN. For this, first all raw data are normalized, then each normalized feature is placed in a region in the grid image. Thus, images with different brightness regions are obtained according to the numerical value of each feature. After the data augmentation step, these images are trained with five different CNN models (GoogleNet, MobileNet v2, ResNet18, ResNet50 and ResNet101) and classified. The highest accuracy of 95.13 % is obtained with the ResNet18 model and this accuracy is superior to studies using previous numerical raw data. The success proves the applicability of the proposed method and shows that numerical data in different fields can be easily classified with CNN models.
ER  - 
TY  - JOUR
T1  - Retinal image assessment using bi-level adaptive morphological component analysis
A1  - Javidi, Malihe
A1  - Harati, Ahad
A1  - Pourreza, HamidReza
Y1  - 2019///
KW  -  Diabetic retinopathy image assessment
KW  -  Dictionary learning
KW  - Bi-level adaptive morphological component analysis
JF  - Artificial Intelligence in Medicine
VL  - 99
SP  - 101702
EP  - 101702
DO  - https://doi.org/10.1016/j.artmed.2019.07.010
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718303373
N2  - The automated analysis of retinal images is a widely researched area which can help to diagnose several diseases like diabetic retinopathy in early stages of the disease. More specifically, separation of vessels and lesions is very critical as features of these structures are directly related to the diagnosis and treatment process of diabetic retinopathy. The complexity of the retinal image contents especially in images with severe diabetic retinopathy makes detection of vascular structure and lesions difficult. In this paper, a novel framework based on morphological component analysis (MCA) is presented which benefits from the adaptive representations obtained via dictionary learning. In the proposed Bi-level Adaptive MCA (BAMCA), MCA is extended to locally deal with sparse representation of the retinal images at patch level whereas the decomposition process occurs globally at the image level. BAMCA method with appropriately offline learnt dictionaries is adopted to work on retinal images with severe diabetic retinopathy in order to simultaneously separate vessels and exudate lesions as diagnostically useful morphological components. To obtain the appropriate dictionaries, K-SVD dictionary learning algorithm is modified to use a gated error which guides the process toward learning the main structures of the retinal images using vessel or lesion maps. Computational efficiency of the proposed framework is also increased significantly through some improvement leading to noticeable reduction in run time. We experimentally show how effective dictionaries can be learnt which help BAMCA to successfully separate exudate and vessel components from retinal images even in severe cases of diabetic retinopathy. In this paper, in addition to visual qualitative assessment, the performance of the proposed method is quantitatively measured in the framework of vessel and exudate segmentation. The reported experimental results on public datasets demonstrate that the obtained components can be used to achieve competitive results with regard to the state-of-the-art vessel and exudate segmentation methods.
ER  - 
TY  - JOUR
T1  - A refined equilibrium generative adversarial network for retinal vessel segmentation
A1  - Zhou, Yukun
A1  - Chen, Zailiang
A1  - Shen, Hailan
A1  - Zheng, Xianxian
A1  - Zhao, Rongchang
A1  - Duan, Xuanchu
Y1  - 2021///
KW  -  Attention mechanism
KW  -  Refine blocks
KW  -  Symmetric adversarial architecture
KW  - Retinal vessel segmentation
JF  - Neurocomputing
VL  - 437
SP  - 118
EP  - 130
DO  - https://doi.org/10.1016/j.neucom.2020.06.143
UR  - https://www.sciencedirect.com/science/article/pii/S0925231221001004
N2  - Objective
Retinal vessel morphological parameters are vital indicator for early diagnosis of ophthalmological diseases and cardiovascular events. However, segmentation performance is highly influenced by elusive vessels, especially in low-contrast background and lesion regions. In this work, we present an end-to-end synthetic neural network to strengthen elusive vessels segmentation capability, containing a symmetric equilibrium generative adversarial network (SEGAN), multi-scale features refine blocks (MSFRB), and attention mechanism (AM).
Method
The proposed network is superior in detail information extraction by maximizing multi-scale features representation. First, SEGAN constructs a symmetric adversarial architecture in which generator is forced to produce more realistic images with local details. Second, MSFRB are devised to optimize the feature merging process, thereby maximally maintaining high resolution information. Finally, the AM is employed to encourage the network to concentrate on discriminative features.
Results
On public dataset DRIVE, STARE, CHASEDB1, and HRF, we evaluate our network quantitatively and compare it with state-of-the-art works. The ablation experiment shows that SEGAN, MSFRB, and AM both contribute to the desirable performance. Conclusion: The proposed network outperforms the existing methods and effectively functions in elusive vessels segmentation, achieving highest scores in Sensitivity, G-Mean, Precision, and F1-Score while maintaining the top level in other metrics. Significance: The satisfactory performance and computational efficiency offer great potential in clinical retinal vessel segmentation application. Meanwhile, the network could be utilized to extract detail information in other biomedical image computing.
ER  - 
TY  - JOUR
T1  - Visual instance-based recommendation system for medical data mining
A1  - Falip, Joris
A1  - Aït-Younes, Amine
A1  - Blanchard, Frédéric
A1  - Delemer, Brigitte
A1  - Diallo, Alpha
A1  - Herbin, Michel
Y1  - 2017///
KW  -  Instance-based learning
KW  -  Medical Data
KW  -  Recommendation
KW  - Exploration
JF  - Procedia Computer Science
VL  - 112
SP  - 1747
EP  - 1754
DO  - https://doi.org/10.1016/j.procs.2017.08.205
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917316009
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France
N2  - This paper presents an instance-based algorithm allowing exploration of large medical dataset by making pairwise connection between patients. In our metric-free method, each individual in a dataset ranks every member of the dataset. By aggregating these ranks, it is then possible to visualize data according to typical individuals representing subsets of closely-related patients. The paper also describes a visualization tool allowing exploration of a database of diabetic patients. This prototype of a recommendation system implements the aforementioned algorithm to enrich data, structure patients, create associations between individuals and provide recommendations.
ER  - 
TY  - JOUR
T1  - FPGA-based implementation of classification techniques: A survey
A1  - Saidi, Afef
A1  - Ben Othman, Slim
A1  - Dhouibi, Meriam
A1  - Ben Saoud, Slim
Y1  - 2021///
KW  -  Challenges
KW  -  Classification
KW  -  Deep learning
KW  -  Implementation
KW  -  Optimizations
KW  - Machine learning
JF  - Integration
VL  - 81
SP  - 280
EP  - 299
DO  - https://doi.org/10.1016/j.vlsi.2021.08.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167926021000894
N2  - Recently, a number of classification techniques have been introduced. However, processing large dataset in a reasonable time has become a major challenge. This made classification task more complex and expensive in calculation. Thus, the need for solutions to overcome these constraints such as field programmable gate arrays (FPGAs). In this paper, we give an overview of the various classification techniques. Then, we present the existing FPGA based implementation of these classification methods. After that, we investigate the confronted challenges and the optimizations strategies. Finally, we highlight the hardware accelerator architectures and tools for hardware design suggested to improve the FPGA implementation of classification methods.
ER  - 
TY  - JOUR
T1  - Inferring new drug indications using the complementarity between clinical disease signatures and drug effects
A1  - Jang, Dongjin
A1  - Lee, Sejoon
A1  - Lee, Jaehyun
A1  - Kim, Kiseong
A1  - Lee, Doheon
Y1  - 2016///
KW  -  Clinical disease signatures
KW  -  Clinical drug effects
KW  -  Electronic clinical information
KW  - Drug repositioning
JF  - Journal of Biomedical Informatics
VL  - 59
SP  - 248
EP  - 257
DO  - https://doi.org/10.1016/j.jbi.2015.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S153204641500283X
N2  - Background
Drug repositioning is the process of finding new indications for existing drugs. Its importance has been dramatically increasing recently due to the enormous increase in new drug discovery cost. However, most of the previous molecular-centered drug repositioning work is not able to reflect the end-point physiological activities of drugs because of the inherent complexity of human physiological systems.
Methods
Here, we suggest a novel computational framework to make inferences for alternative indications of marketed drugs by using electronic clinical information which reflects the end-point physiological results of drug’s effects on the biological activities of humans. In this work, we use the concept of complementarity between clinical disease signatures and clinical drug effects. With this framework, we establish disease-related clinical variable vectors (clinical disease signature vectors) and drug-related clinical variable vectors (clinical drug effect vectors) by applying two methodologies (i.e., statistical analysis and literature mining). Finally, we assign a repositioning possibility score to each disease–drug pair by the calculation of complementarity (anti-correlation) and association between clinical states (“up” or “down”) of disease signatures and clinical effects (“up”, “down” or “association”) of drugs. A total of 717 clinical variables in the electronic clinical dataset (NHANES), are considered in this study.
Results
The statistical significance of our prediction results is supported through two benchmark datasets (Comparative Toxicogenomics Database and Clinical Trials). We discovered not only lots of known relationships between diseases and drugs, but also many hidden disease–drug relationships. For example, glutathione and edetic-acid may be investigated as candidate drugs for asthma treatment. We examined prediction results by using statistical experiments (enrichment verification, hyper-geometric and permutation test P<0.009 in Comparative Toxicogenomics Database and Clinical Trials) and presented evidences for those with already published literature.
Conclusion
The results show that electronic clinical information is a feasible data resource and utilizing the complementarity (anti-correlated relationships) between clinical signatures of disease and clinical effects of drugs is a potentially predictive concept in drug repositioning research. It makes the proposed approach useful to identity novel relationships between diseases and drugs that have a high probability of being biologically valid.
ER  - 
TY  - JOUR
T1  - Mixture of latent multinomial naive Bayes classifier
A1  - Shiri Harzevili, Nima
A1  - Alizadeh, Sasan H
Y1  - 2018///
KW  -  Latent variable
KW  -  Mixture model
KW  -  Multinomial distribution
KW  - Naive Bayes
JF  - Applied Soft Computing
VL  - 69
SP  - 516
EP  - 527
DO  - https://doi.org/10.1016/j.asoc.2018.04.020
UR  - https://www.sciencedirect.com/science/article/pii/S1568494618302114
N2  - Naive Bayes classifier has been extensively applied in various domains in the past few decades due to its simple structure and remarkable predictive performance. However, it is based on a strong assumption which confines its usage for many real-world applications; conditional independence of attributes given class information. In this paper, we propose mixture of latent multinomial naive Bayes (MLMNB) classifier as an extension of naive Bayes to relax the independence assumption. MLMNB incorporates a latent variable in a predefined Bayesian network structure to model the dependencies among attributes, yet avoids burden complexities of structural learning approaches. We theoretically prove that MLMNB automatically shrinks to naive Bayes classifier whenever conditional independence assumption holds. Expectation-maximization (EM) algorithm is modified for the parameter estimation. The experimental results on 36 datasets from the University of California, Irvine (UCI) machine learning repository show that MLMNB achieves a substantial predictive performance as compared with the state-of-the-art modifications of naive Bayes classifier, in terms of classification accuracy (ACC), conditional log-likelihood (CLL), and area under the ROC curve (AUC).
ER  - 
TY  - JOUR
T1  - A new local search based hybrid genetic algorithm for feature selection
A1  - Kabir, Md. Monirul
A1  - Shahjahan, Md.
A1  - Murase, Kazuyuki
Y1  - 2011///
KW  -  Correlation information
KW  -  Fitness value
KW  -  Genetic algorithm
KW  -  Local search operation
KW  -  Neural network
KW  - Feature selection
JF  - Neurocomputing
VL  - 74
IS  - 17
SP  - 2914
EP  - 2928
DO  - https://doi.org/10.1016/j.neucom.2011.03.034
UR  - https://www.sciencedirect.com/science/article/pii/S0925231211002748
N2  - This paper presents a new hybrid genetic algorithm (HGA) for feature selection (FS), called as HGAFS. The vital aspect of this algorithm is the selection of salient feature subset within a reduced size. HGAFS incorporates a new local search operation that is devised and embedded in HGA to fine-tune the search in FS process. The local search technique works on basis of the distinct and informative nature of input features that is computed by their correlation information. The aim is to guide the search process so that the newly generated offsprings can be adjusted by the less correlated (distinct) features consisting of general and special characteristics of a given dataset. Thus, the proposed HGAFS receives the reduced redundancy of information among the selected features. On the other hand, HGAFS emphasizes on selecting a subset of salient features with reduced number using a subset size determination scheme. We have tested our HGAFS on 11 real-world classification datasets having dimensions varying from 8 to 7129. The performances of HGAFS have been compared with the results of other existing ten well-known FS algorithms. It is found that, HGAFS produces consistently better performances on selecting the subsets of salient features with resulting better classification accuracies.
ER  - 
TY  - JOUR
T1  - Discriminant document embeddings with an extreme learning machine for classifying clinical narratives
A1  - Lauren, Paula
A1  - Qu, Guangzhi
A1  - Zhang, Feng
A1  - Lendasse, Amaury
Y1  - 2018///
KW  -  Clinical narratives
KW  -  Document embeddings
KW  -  Extreme learning machines
KW  -  Feature learning
KW  -  Multiple discriminant analysis
KW  -  PV-DBOW
KW  -  Skip-gram
KW  -  Word embeddings
KW  - Document classification
JF  - Neurocomputing
VL  - 277
SP  - 129
EP  - 138
DO  - https://doi.org/10.1016/j.neucom.2017.01.117
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217314157
N1  - Hierarchical Extreme Learning Machines
N2  - The unstructured nature of clinical narratives makes them complex for automatically extracting information. Feature learning is an important precursor to document classification, a sub-discipline of natural language processing (NLP). In NLP, word and document embeddings are an effective approach for generating word and document representations (vectors) in a low-dimensional space. This paper uses skip-gram and paragraph vectors-distributed bag of words (PV-DBOW) with multiple discriminant analysis (MDA) to arrive at discriminant document embeddings. A kernel-based extreme learning machine (ELM) is used to map the clinical texts to the medical code. Experimental results on clinical texts indicate overall improvement especially for the minority classes.
ER  - 
TY  - JOUR
T1  - Predicting Adverse Events After Surgery
A1  - Roy, Senjuti Basu
A1  - Maria, Moushumi
A1  - Wang, Tina
A1  - Ehlers, Anne
A1  - Flum, David
Y1  - 2018///
JF  - Big Data Research
VL  - 13
SP  - 29
EP  - 37
DO  - https://doi.org/10.1016/j.bdr.2018.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S2214579617302186
N1  - Big Medical/Healthcare Data Analytics
N2  - Predicting risk of adverse events (AEs) following surgical procedure is of significant interest, as that may guide in better resource utilization and an improved quality of care. Currently available comorbidity indices are largely inaccurate to predict adverse events other than death, as well as off-the-shelf machine learning models do not typically account for the temporal sequence of events to enable predictive analytics. We propose a study to improve the current techniques for assessing and predicting the risk of adverse events (AEs) associated with multiple chronic conditions by designing machine learning models that account for and incorporate the temporal sequence and timing of conditions. We formalize the task as a binary classification problem. Our technical contributions include devising novel sequence based feature discovery techniques to augment existing supervised classification algorithms, as well as formalizing the classification task as a Markov Chain Model (MCM) that captures the temporal sequence of prior chronic conditions/events. Finally, we design a hybrid or multi-classifier that combines prediction from the aforementioned classification models to finally predict AE. Our experimental results, conducted using the Truven Health MarketScan Research Databases with more than 27 million of claim records on two different surgery types, discover interesting insights that can guide patient-centered decision-making and can direct healthcare teams to adjust techniques and interventions. We also extensively compare the performance of our solutions to appropriate baselines.
ER  - 
TY  - JOUR
T1  - Annoyance modeling using personal and situational variables for construction site noise in urban areas
A1  - Lee, Jae Kwan
A1  - Jang, Jaewoong
A1  - Chang, Seo Il
A1  - Lee, Soo Il
Y1  - 2021///
KW  -  Construction site noise
KW  -  Health condition
KW  -  Highly annoyed
KW  - Annoyance
JF  - Applied Acoustics
VL  - 182
SP  - 108256
EP  - 108256
DO  - https://doi.org/10.1016/j.apacoust.2021.108256
UR  - https://www.sciencedirect.com/science/article/pii/S0003682X21003509
N2  - Construction site noise is the most significant cause of noise disputes in South Korea. We conducted an annoyance evaluation to produce annoyance prediction models for four kinds of construction noise that cause the most complaints, namely construction machinery noise from pile drivers, excavators, and concrete pumping vehicles and noise from concrete mold removal working. We adjusted these four noises recorded at construction sites to noise stimuli with 35–80 dB(A), and subjects evaluated annoyance with a score for these stimuli. Participants in the experiment listened to one or two noises at the same time and evaluated their annoyance. We created multiple linear regression models and logistic regression models for annoyance scores of the subjects using acoustic features of noise stimuli and personal and situational variables. Acoustic features included traditional indicators such as Leq, Lmax, and sound quality indices. We calculated mean, maximum, and percentile values of sound quality indices such as loudness, sharpness, roughness, and fluctuation strength. The personal variables were results from a demographic survey and attitudes toward noise, while the situational variables were results from a survey on the living environment and past experiences about noise. Also, we considered the health condition and residence environment of the subjects. Using multiple linear regression, we confirmed that the acoustic features and personal and situational variables influence annoyance from construction noise. Also, we calculated the importance of each variable to the annoyance. Finally, to confirm that the percentage of highly annoyed persons (%HA) varies with personal and situational variables, we classified subjects in accordance with the variables and created a %HA curve for each group. From the result, we confirmed that there is a large difference in %HA in accordance with the health condition of the subjects.
ER  - 
TY  - JOUR
T1  - Chronic wounds multimodal image database
A1  - Kręcichwost, Michał
A1  - Czajkowska, Joanna
A1  - Wijata, Agata
A1  - Juszczyk, Jan
A1  - Pyciński, Bartłomiej
A1  - Biesok, Marta
A1  - Rudzki, Marcin
A1  - Majewski, Jakub
A1  - Kostecki, Jacek
A1  - Pietka, Ewa
Y1  - 2021///
KW  -  Colour photography
KW  -  Depth map
KW  -  Manual outlines
KW  -  Multimodal dataset
KW  -  Segmentation and registration
KW  -  Thermal imaging
KW  - Chronic wounds
JF  - Computerized Medical Imaging and Graphics
VL  - 88
SP  - 101844
EP  - 101844
DO  - https://doi.org/10.1016/j.compmedimag.2020.101844
UR  - https://www.sciencedirect.com/science/article/pii/S0895611120301397
N2  - A multimodal wound image database was created to allow fast development of computer-aided approaches for wound healing monitoring. The developed system with parallel camera optical axes enables multimodal images: photo, thermal, stereo, and depth map of the wound area to be acquired. As a result of using this system a multimodal database of chronic wound images is introduced. It contains 188 image sets of photographs, thermal images, and 3D meshes of the surfaces of chronic wounds acquired during 79 patient visits. Manual wound outlines delineated by an expert are also included in the dataset. All images of each case are additionally coregistered, and both numerical registration parameters and the transformed images are covered in the database. The presented database is publicly available for the research community at https://chronicwounddatabase.eu. That is the first publicly available database for evaluation and comparison of new image-based algorithms in the wound healing monitoring process with coregistered photographs, thermal maps, and 3D models of the wound area. Easily available database of coregistered multimodal data with the raw data set allows faster development of algorithms devoted to wound healing analysis and monitoring.
ER  - 
TY  - JOUR
T1  - Multidimensional medical data modeling based on fuzzy cognitive maps and k-means clustering
A1  - Poczeta, Katarzyna
A1  - Kubuś, Łukasz
A1  - Yastrebov, Alexander
Y1  - 2020///
KW  -  evolutionary algorithms
KW  -  multidimensional medical data
KW  - Fuzzy cognitive maps
JF  - Procedia Computer Science
VL  - 176
SP  - 118
EP  - 127
DO  - https://doi.org/10.1016/j.procs.2020.08.013
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920318378
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 24th International Conference KES2020
N2  - The paper concerns the use of fuzzy cognitive maps and k-means clustering to solve the problem of modeling multidimensional medical data. A fuzzy cognitive map is a recurrent neural network that describes the analyzed phenomenon in the form of key concepts and causal relationships between them. It is an effective tool for modeling decision support systems and is widely used in medicine. The aim of this paper is to analyze the use of fuzzy cognitive maps with k-means clustering to model decision support systems based on multidimensional data related to Parkinson’s disease. K-means method was applied to group the data, and then a separate fuzzy cognitive map was built for each cluster to increase forecasting accuracy. The learning process was realized with the use of the previously developed Individually Directional Evolutionary Algorithm. The obtained results confirm that the analyzed approach provides much better forecasting accuracy than the standard approach based on one model.
ER  - 
TY  - JOUR
T1  - Variation of P-wave indices in paroxysmal atrial fibrillation patients before and after catheter ablation
A1  - Ortigosa, Nuria
A1  - Ayala, Guillermo
A1  - Cano, Óscar
Y1  - 2021///
KW  -  Atrial fibrillation
KW  -  Catheter ablation
KW  -  Pulmonary vein isolation
KW  - Electrocardiogram (ECG)
JF  - Biomedical Signal Processing and Control
VL  - 66
SP  - 102500
EP  - 102500
DO  - https://doi.org/10.1016/j.bspc.2021.102500
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421000975
N2  - The effects of pulmonary vein isolation in the surface electrocardiogram of patients with paroxysmal atrial fibrillation are analyzed in this paper using non-invasive markers for early detection of the arrhythmia recurrences. Several features have been extracted on P-waves of V1 lead for paroxysmal atrial fibrillation patients who underwent catheter ablation of pulmonary veins for restoring sinus rhythm permanently. Surface ECG was simultaneously recorded along with intracardiac recordings starting from the beginning of the intervention until half an hour after the catheter ablation successfully ended. Significant difference between the means before and after catheter ablation have been observed for the cross-correlation index, kurtosis and amplitude dispersion of P-waves. A logistic regression has been applied to all the descriptors and pointed to the amplitude dispersion index, as well as the minimum gradient joint with kurtosis of P-waves prior to catheter ablation as good predictors of recurrence of atrial fibrillation (78% accuracy). It is important to note how using a few descriptors good classification results are achieved. This study opens a door to early detection of atrial fibrillation recurrences using markers obtained by non-invasive methods.
ER  - 
TY  - JOUR
T1  - Interference-less neural network training
A1  - Hua Ang, Ji
A1  - Guan, Sheng-Uei
A1  - Tan, Kay Chen
A1  - Mamun, Abdullah Al
Y1  - 2008///
KW  -  Attribute interference
KW  -  Input space partitioning
KW  -  Neural networks
KW  - Classification
JF  - Neurocomputing
VL  - 71
IS  - 16
SP  - 3509
EP  - 3524
DO  - https://doi.org/10.1016/j.neucom.2007.10.012
UR  - https://www.sciencedirect.com/science/article/pii/S0925231207003578
N1  - Advances in Neural Information Processing (ICONIP 2006) / Brazilian Symposium on Neural Networks (SBRN 2006)
N2  - The lack of segregation of input space for conventional neural networks (NNs) training often causes interference within the network. The interference-less neural network training (ILNNT) method employed in this paper reduces interference among input attributes by identifying those attributes that interfere with one another and separating them, while attributes that are mutually beneficial are grouped together. Separated attributes in different batches do not share the same hidden neurons while attributes within a batch are connected to the same hidden neurons. ILNNT is applied to widely used benchmark binary and multi-class classification problems and experimental results from K-fold cross validation show that there exist varying degrees of interference among the attributes for the datasets used and the classification accuracy produced by NNs with reduced interference is high.
ER  - 
TY  - JOUR
T1  - A new approach to early diagnosis of congestive heart failure disease by using Hilbert–Huang transform
A1  - Altan, Gokhan
A1  - Kutlu, Yakup
A1  - Allahverdi, Novruz
Y1  - 2016///
KW  -  Coronary artery disease
KW  -  ECG
KW  -  HRV
KW  -  Hilbert–Huang transform
KW  -  Multilayer perceptron
KW  - Congestive heart failure
JF  - Computer Methods and Programs in Biomedicine
VL  - 137
SP  - 23
EP  - 34
DO  - https://doi.org/10.1016/j.cmpb.2016.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715301899
N2  - Congestive heart failure (CHF) is a degree of cardiac disease occurring as a result of the heart's inability to pump enough blood for the human body. In recent studies, coronary artery disease (CAD) is accepted as the most important cause of CHF. This study focuses on the diagnosis of both the CHF and the CAD. The Hilbert–Huang transform (HHT), which is effective on non-linear and non-stationary signals, is used to extract the features from R-R intervals obtained from the raw electrocardiogram data. The statistical features are extracted from instinct mode functions that are obtained applying the HHT to R-R intervals. Classification performance is examined with extracted statistical features using a multilayer perceptron neural network. The designed model classified the CHF, the CAD patients and a normal control group with rates of 97.83%, 93.79% and 100%, accuracy, specificity and sensitivity, respectively. Also, early diagnosis of the CHF was performed by interpretation of the CAD with a classification accuracy rate of 97.53%, specificity of 98.18% and sensitivity of 97.13%. As a result, a single system having the ability of both diagnosis and early diagnosis of CHF is performed by integrating the CAD diagnosis method to the CHF diagnosis method.
ER  - 
TY  - JOUR
T1  - Dunuen: A User-Friendly Formal Verification Tool
A1  - Capobianco, Giovanni
A1  - Giacomo, Umberto Di
A1  - Mercaldo, Francesco
A1  - Santone, Antonella
Y1  - 2019///
KW  -  Automatic Tool
KW  -  Model Checking
KW  - Formal verification
JF  - Procedia Computer Science
VL  - 159
SP  - 1431
EP  - 1438
DO  - https://doi.org/10.1016/j.procs.2019.09.313
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919315133
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - Formal verification allows checking the design and the behaviour of a system. One of the main limitations to the adoption of formal verification techniques is the process of model creation using specification languages. For this reason a tool supporting this activity is necessary. Actually, there are several tools allowing analysts to verify models expressed into specification languages. These tools provide support for automatically checking whether a system satisfies a property. However, to use such tools it is important to deeply know a precise notation for defining a system, i.e., the Calculus of Communicating Systems. Since systems are often expressed as time-series, to overcome this problem, we provide an user-friendly tool able to automatically generate a system model starting from the CSV - Comma-Separated Values format (the most widespread format considered to release dataset). In this way we hide the details about the model construction form the analyst, which can only focus immediately on the properties to verify. We introduce Dunuen, a tool allowing the user to firstly perform a kind of pre-processing operation starting from a CSV file, as discretization or removing attributes; subsequently it automatically creates a formal model from the pre-processed CSV file and, by invoking the model checker we embedded in Dunuen, it finally verifies whether the generated model satisfies a property expressed in temporal logic through a graphic interface, proposing formal methods as an alternative to machine learning for classification tasks.
ER  - 
TY  - JOUR
T1  - The opportunities, challenges and obligations of Fitness Data Analytics
A1  - Bhargava, Yesoda
A1  - Nabi, Javaid
Y1  - 2020///
KW  -  Big Data
KW  -  Biostatistics
KW  -  Data Science
KW  -  Health data privacy
KW  -  Public Health
KW  -  Smartphone Fitness
KW  - Fitness data analysis
JF  - Procedia Computer Science
VL  - 167
SP  - 1354
EP  - 1362
DO  - https://doi.org/10.1016/j.procs.2020.03.346
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920308127
N1  - International Conference on Computational Intelligence and Data Science
N2  - Smart phone based fitness applications have become widely prevalent and extremely popular among fitness enthusiasts and common people. Variety of fitness applications can be found on app-stores, free and premium both. Data logged in these apps have proven to be useful in managing daily fitness and monitoring health-goal progress through workout summaries and insights. However, these insights are mostly limited to weekly summaries or monthly aggregate of activities performed by the user. Long-term health and wellness assessment of individuals or a group based on longitudinal fitness data is still in its infancy. Such analysis may be useful for developing personalised health recommendations for an individual and potentially contribute towards creating healthier communities. In this work, results of analysis of six-month walking data of 335 users extracted from health data server of Samsung Health are presented and discussed. The relationship between blood glucose with brisk walking is also explored. Additionally, a case study based on a single user’s fitness data analysis is presented to demonstrate the significance of longitudinal fitness data analysis. Finally the technical challenges and data privacy issues associated with fitness data analysis are discussed. The work can be mainly seen as an empirical introduction to fitness data analytics.
ER  - 
TY  - JOUR
T1  - On the neural network classification of medical data and an endeavour to balance non-uniform data sets with artificial data extension
A1  - Autio, Lassi
A1  - Juhola, Martti
A1  - Laurikkala, Jorma
Y1  - 2007///
KW  -  Artificial data extension
KW  -  Classification
KW  -  Medical decision-making
KW  - Artificial neural networks
JF  - Computers in Biology and Medicine
VL  - 37
IS  - 3
SP  - 388
EP  - 397
DO  - https://doi.org/10.1016/j.compbiomed.2006.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S0010482506000679
N2  - We studied the efficiency of multilayer perceptron networks to classify eight different medical data sets with typical problems connected to their strongly non-uniform distributions between output classes and relatively small sizes of training sets. We studied especially the possibility mentioned in the literature of balancing a class distribution by artificially extending small classes of a data set. The results obtained supported our hypothesis that principally this does somewhat improve the classification accuracy of small classes, but is also inclined to impair the classification accuracy of majority classes.
ER  - 
TY  - JOUR
T1  - Environmental benefits of bike sharing: A big data-based analysis
A1  - Zhang, Yongping
A1  - Mi, Zhifu
Y1  - 2018///
KW  -  Air pollution
KW  -  Big data
KW  -  Carbon emissions
KW  -  Energy consumption
KW  -  Sharing economy
KW  - Bike sharing
JF  - Applied Energy
VL  - 220
SP  - 296
EP  - 301
DO  - https://doi.org/10.1016/j.apenergy.2018.03.101
UR  - https://www.sciencedirect.com/science/article/pii/S0306261918304392
N2  - Bike sharing is a new form of transport and is becoming increasingly popular in cities around the world. This study aims to quantitatively estimate the environmental benefits of bike sharing. Using big data techniques, we estimate the impacts of bike sharing on energy use and carbon dioxide (CO2) and nitrogen oxide (NOX) emissions in Shanghai from a spatiotemporal perspective. In 2016, bike sharing in Shanghai saved 8358 tonnes of petrol and decreased CO2 and NOX emissions by 25,240 and 64 tonnes, respectively. From a spatial perspective, environmental benefits are much higher in more developed districts in Shanghai where population density is usually higher. From a temporal perspective, there are obvious morning and evening peaks of the environmental benefits of bike sharing, and evening peaks are higher than morning peaks. Bike sharing has great potential to reduce energy consumption and emissions based on its rapid development.
ER  - 
TY  - JOUR
T1  - Strategies for handling missing clinical data for automated surgical site infection detection from the electronic health record
A1  - Hu, Zhen
A1  - Melton, Genevieve B
A1  - Arsoniadis, Elliot G
A1  - Wang, Yan
A1  - Kwaan, Mary R
A1  - Simon, Gyorgy J
Y1  - 2017///
KW  -  Missing data
KW  -  Surgical site infections
KW  - Electronic health records
JF  - Journal of Biomedical Informatics
VL  - 68
SP  - 112
EP  - 120
DO  - https://doi.org/10.1016/j.jbi.2017.03.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417300588
N2  - Proper handling of missing data is important for many secondary uses of electronic health record (EHR) data. Data imputation methods can be used to handle missing data, but their use for analyzing EHR data is limited and specific efficacy for postoperative complication detection is unclear. Several data imputation methods were used to develop data models for automated detection of three types (i.e., superficial, deep, and organ space) of surgical site infection (SSI) and overall SSI using American College of Surgeons National Surgical Quality Improvement Project (NSQIP) Registry 30-day SSI occurrence data as a reference standard. Overall, models with missing data imputation almost always outperformed reference models without imputation that included only cases with complete data for detection of SSI overall achieving very good average area under the curve values. Missing data imputation appears to be an effective means for improving postoperative SSI detection using EHR clinical data.
ER  - 
TY  - JOUR
T1  - Hierarchical retinal blood vessel segmentation based on feature and ensemble learning
A1  - Wang, Shuangling
A1  - Yin, Yilong
A1  - Cao, Guibao
A1  - Wei, Benzheng
A1  - Zheng, Yuanjie
A1  - Yang, Gongping
Y1  - 2015///
KW  -  Ensemble learning
KW  -  Feature learning
KW  -  Random forest
KW  -  Retinal blood vessel segmentation
KW  - Convolutional neural network
JF  - Neurocomputing
VL  - 149
SP  - 708
EP  - 717
DO  - https://doi.org/10.1016/j.neucom.2014.07.059
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214010169
N2  - Segmentation of retinal blood vessels is of substantial clinical importance for diagnoses of many diseases, such as diabetic retinopathy, hypertension and cardiovascular diseases. In this paper, the supervised method is presented to tackle the problem of retinal blood vessel segmentation, which combines two superior classifiers: Convolutional Neural Network (CNN) and Random Forest (RF). In this method, CNN performs as a trainable hierarchical feature extractor and ensemble RFs work as a trainable classifier. By integrating the merits of feature learning and traditional classifier, the proposed method is able to automatically learn features from the raw images and predict the patterns. Extensive experiments have been conducted on two public retinal images databases (DRIVE and STARE), and comparisons with other major studies on the same database demonstrate the promising performance and effectiveness of the proposed method.
ER  - 
TY  - JOUR
T1  - Cloud enabled data analytics and visualization framework for health-shocks prediction
A1  - Mahmud, Shahid
A1  - Iqbal, Rahat
A1  - Doctor, Faiyaz
Y1  - 2016///
KW  -  Cloud computing
KW  -  Data analytics and Visualization
KW  -  Development process of big data application
KW  -  Healthcare demonstration
KW  -  Scientific overflow of big data
KW  - Technology integration
JF  - Future Generation Computer Systems
VL  - 65
SP  - 169
EP  - 181
DO  - https://doi.org/10.1016/j.future.2015.10.014
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X15003271
N1  - Special Issue on Big Data in the Cloud
N2  - In this paper, we present a data analytics and visualization framework for health-shocks prediction based on large-scale health informatics dataset. The framework is developed using cloud computing services based on Amazon web services (AWS) integrated with geographical information systems (GIS) to facilitate big data capture, storage, index and visualization of data through smart devices for different stakeholders. In order to develop a predictive model for health-shocks, we have collected a unique data from 1000 households, in rural and remotely accessible regions of Pakistan, focusing on factors like health, social, economic, environment and accessibility to healthcare facilities. We have used the collected data to generate a predictive model of health-shock using a fuzzy rule summarization technique, which can provide stakeholders with interpretable linguistic rules to explain the causal factors affecting health-shocks. The evaluation of the proposed system in terms of the interpret-ability and accuracy of the generated data models for classifying health-shock shows promising results. The prediction accuracy of the fuzzy model based on a k-fold cross-validation of the data samples shows above 89% performance in predicting health-shocks based on the given factors.
ER  - 
TY  - JOUR
T1  - A novel Neuro-fuzzy classification technique for data mining
A1  - Ghosh, Soumadip
A1  - Biswas, Sushanta
A1  - Sarkar, Debasree
A1  - Sarkar, Partha Pratim
Y1  - 2014///
KW  -  Data mining
KW  -  Neural network
KW  -  Neuro-fuzzy
KW  - Classification
JF  - Egyptian Informatics Journal
VL  - 15
IS  - 3
SP  - 129
EP  - 147
DO  - https://doi.org/10.1016/j.eij.2014.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S1110866514000292
N2  - In our study, we proposed a novel Neuro-fuzzy classification technique for data mining. The inputs to the Neuro-fuzzy classification system were fuzzified by applying generalized bell-shaped membership function. The proposed method utilized a fuzzification matrix in which the input patterns were associated with a degree of membership to different classes. Based on the value of degree of membership a pattern would be attributed to a specific category or class. We applied our method to ten benchmark data sets from the UCI machine learning repository for classification. Our objective was to analyze the proposed method and, therefore compare its performance with two powerful supervised classification algorithms Radial Basis Function Neural Network (RBFNN) and Adaptive Neuro-fuzzy Inference System (ANFIS). We assessed the performance of these classification methods in terms of different performance measures such as accuracy, root-mean-square error, kappa statistic, true positive rate, false positive rate, precision, recall, and f-measure. In every aspect the proposed method proved to be superior to RBFNN and ANFIS algorithms.
ER  - 
TY  - JOUR
T1  - Automated ultrasound assessment of amniotic fluid index using deep learning
A1  - Cho, Hyun Cheol
A1  - Sun, Siyu
A1  - Min Hyun, Chang
A1  - Kwon, Ja-Young
A1  - Kim, Bukweon
A1  - Park, Yejin
A1  - Seo, Jin Keun
Y1  - 2021///
KW  -  Deep learning
KW  -  Image segmentation
KW  -  Ultrasound image
KW  - Amniotic fluid index
JF  - Medical Image Analysis
VL  - 69
SP  - 101951
EP  - 101951
DO  - https://doi.org/10.1016/j.media.2020.101951
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520303157
N2  - The estimation of antenatal amniotic fluid (AF) volume (AFV) is important as it offers crucial information about fetal development, fetal well-being, and perinatal prognosis. However, AFV measurement is cumbersome and patient specific. Moreover, it is heavily sonographer-dependent, with measurement accuracy varying greatly depending on the sonographer’s experience. Therefore, the development of accurate, robust, and adoptable methods to evaluate AFV is highly desirable. In this regard, automation is expected to reduce user-based variability and workload of sonographers. However, automating AFV measurement is very challenging, because accurate detection of AF pockets is difficult owing to various confusing factors, such as reverberation artifact, AF mimicking region and floating matter. Furthermore, AF pocket exhibits an unspecified variety of shapes and sizes, and ultrasound images often show missing or incomplete structural boundaries. To overcome the abovementioned difficulties, we develop a hierarchical deep-learning-based method, which consider clinicians’ anatomical-knowledge-based approaches. The key step is the segmentation of the AF pocket using our proposed deep learning network, AF-net. AF-net is a variation of U-net combined with three complementary concepts - atrous convolution, multi-scale side-input layer, and side-output layer. The experimental results demonstrate that the proposed method provides a measurement of the amniotic fluid index (AFI) that is as robust and precise as the results from clinicians. The proposed method achieved a Dice similarity of 0.877±0.086 for AF segmentation and achieved a mean absolute error of 2.666±2.986 and mean relative error of 0.018±0.023 for AFI value. To the best of our knowledge, our method, for the first time, provides an automated measurement of AFI.
ER  - 
TY  - JOUR
T1  - Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets
A1  - McAllister, Patrick
A1  - Zheng, Huiru
A1  - Bond, Raymond
A1  - Moorhead, Anne
Y1  - 2018///
KW  -  Convolutional neural networks
KW  -  Deep learning
KW  -  Feature extraction
KW  -  Food logging
KW  - Obesity
JF  - Computers in Biology and Medicine
VL  - 95
SP  - 217
EP  - 233
DO  - https://doi.org/10.1016/j.compbiomed.2018.02.008
UR  - https://www.sciencedirect.com/science/article/pii/S0010482518300386
N2  - Obesity is increasing worldwide and can cause many chronic conditions such as type-2 diabetes, heart disease, sleep apnea, and some cancers. Monitoring dietary intake through food logging is a key method to maintain a healthy lifestyle to prevent and manage obesity. Computer vision methods have been applied to food logging to automate image classification for monitoring dietary intake. In this work we applied pretrained ResNet-152 and GoogleNet convolutional neural networks (CNNs), initially trained using ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset with MatConvNet package, to extract features from food image datasets; Food 5K, Food-11, RawFooT-DB, and Food-101. Deep features were extracted from CNNs and used to train machine learning classifiers including artificial neural network (ANN), support vector machine (SVM), Random Forest, and Naive Bayes. Results show that using ResNet-152 deep features with SVM with RBF kernel can accurately detect food items with 99.4% accuracy using Food-5K validation food image dataset and 98.8% with Food-5K evaluation dataset using ANN, SVM-RBF, and Random Forest classifiers. Trained with ResNet-152 features, ANN can achieve 91.34%, 99.28% when applied to Food-11 and RawFooT-DB food image datasets respectively and SVM with RBF kernel can achieve 64.98% with Food-101 image dataset. From this research it is clear that using deep CNN features can be used efficiently for diverse food item image classification. The work presented in this research shows that pretrained ResNet-152 features provide sufficient generalisation power when applied to a range of food image classification tasks.
ER  - 
TY  - JOUR
T1  - Assessment of fetal maturation age by heart rate variability measures using random forest methodology
A1  - Tetschke, F
A1  - Schneider, U
A1  - Schleussner, E
A1  - Witte, O W
A1  - Hoyer, D
Y1  - 2016///
KW  -  Heart rate variability
KW  -  Multiscale complexity
KW  -  Random forest
KW  - Fetal maturation
JF  - Computers in Biology and Medicine
VL  - 70
SP  - 157
EP  - 162
DO  - https://doi.org/10.1016/j.compbiomed.2016.01.020
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516300105
N2  - Fetal maturation age assessment based on heart rate variability (HRV) is a predestinated tool in prenatal diagnosis. To date, almost linear maturation characteristic curves are used in univariate and multivariate models. Models using complex multivariate maturation characteristic curves are pending. To address this problem, we use Random Forest (RF) to assess fetal maturation age and compare RF with linear, multivariate age regression. We include previously developed HRV indices such as traditional time and frequency domain indices and complexity indices of multiple scales. We found that fetal maturation was best assessed by complexity indices of short scales and skewness in state-dependent datasets (quiet sleep, active sleep) as well as in state-independent recordings. Additionally, increasing fluctuation amplitude contributed to the model in the active sleep state. None of the traditional linear HRV parameters contributed to the RF models. Compared to linear, multivariate regression, the mean prediction of gestational age (GA) is more accurate with RF than in linear, multivariate regression (quiet state: R2=0,617 vs. R2=0,461, active state: R2=0,521 vs. R2=0,436, state independent: R2=0,583 vs. R2=0,548). We conclude that classification and regression tree models such as RF methodology are appropriate for the evaluation of fetal maturation age. The decisive role of adjustments between different time scales of complexity may essentially extend previous analysis concepts mainly based on rhythms and univariate complexity indices. Those system characteristics may have implication for better understanding and accessibility of the maturating complex autonomic control and its disturbance.
ER  - 
TY  - JOUR
T1  - Artificial intelligence-based approaches for COVID-19 patient management
A1  - Lan, Lan
A1  - Sun, Wenbo
A1  - Xu, Dan
A1  - Yu, Minhua
A1  - Xiao, Feng
A1  - Hu, Huijuan
A1  - Xu, Haibo
A1  - Wang, Xinghuan
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  COVID-19(Coronavirus disease 2019)
KW  - Coronavirus disease 2019
JF  - Intelligent Medicine
VL  - 1
IS  - 1
SP  - 10
EP  - 15
DO  - https://doi.org/10.1016/j.imed.2021.05.005
UR  - https://www.sciencedirect.com/science/article/pii/S2667102621000140
N2  - During the highly infectious pandemic of coronavirus disease 2019 (COVID-19), artificial intelligence (AI) has provided support in addressing challenges and accelerating achievements in controlling this public health crisis. It has been applied in fields varying from outbreak forecasting to patient management and drug/vaccine development. In this paper, we specifically review the current status of AI-based approaches for patient management. Limitations and challenges still exist, and further needs are highlighted.
ER  - 
TY  - JOUR
T1  - Scalable Detection of Concept Drift: A Learning Technique Based on Support Vector Machines
A1  - Altendeitering, Marcel
A1  - Dübler, Stephan
Y1  - 2020///
KW  -  FLORA3
KW  -  SVM
KW  -  energy data
KW  -  machine learning
KW  - concept drift
JF  - Procedia Manufacturing
VL  - 51
SP  - 400
EP  - 407
DO  - https://doi.org/10.1016/j.promfg.2020.10.057
UR  - https://www.sciencedirect.com/science/article/pii/S2351978920319120
N1  - 30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)
N2  - The issue of concept drift describes how static machine-learning models build on historical data can become unreliable over time and pose a significant challenge to many applications. Although, there is a growing body of literature investigating concept drift existing solutions are often limited to a small number of samples or features and do not work well in Industry 4.0 scenarios. We are proposing a novel algorithm that extends the existing concept drift algorithm FLORA3 by utilizing support vector machines for the classification process. Through this combination of dynamic and static approaches the algorithm is capable of effectively analyzing data streams of high volume. For evaluation, we tested our algorithm on the publicly available data set ‘elec2’, which is based on the energy market in Australia. Our results show that the proposed algorithm needs less computational resources compared to other algorithms while maintaining a high level of accuracy.
ER  - 
TY  - JOUR
T1  - Mutual information based weight initialization method for sigmoidal feedforward neural networks
A1  - Qiao, Junfei
A1  - Li, Sanyi
A1  - Li, Wenjing
Y1  - 2016///
KW  -  Mutual information
KW  -  Weight initialization
KW  - Sigmoidal feedforward neural network
JF  - Neurocomputing
VL  - 207
SP  - 676
EP  - 683
DO  - https://doi.org/10.1016/j.neucom.2016.05.054
UR  - https://www.sciencedirect.com/science/article/pii/S092523121630491X
N2  - When a sigmoidal feedforward neural network (SFNN) is trained by the gradient-based algorithms, the quality of the overall learning process strongly depends on the initial weights. To improve the algorithm stability and avoid local minima, a Mutual Information based weight initialization (MIWI) method is proposed for SFNN. The useful information contained in input variables is measured with the mutual information (MI) between input variables and output variables. The initial distribution of weights is consistent with the information distribution in the input variables. The lower and upper bounds of the weights range are calculated to ensure the neurons inputs are within the active region of sigmoid function. The MIWI method makes the initial weights close to the global optimal point with a higher probability and avoids premature saturation. The efficiency of the MIWI method is evaluated based on several benchmark problems. The experimental results show that the stability and accuracy of the proposed method are better than some other weight initialization methods.
ER  - 
TY  - JOUR
T1  - Clinical decision-making framework against over-testing based on modeling implicit evaluation criteria
A1  - Yang, Yang
A1  - Huo, Hongxing
A1  - Jiang, Jingchi
A1  - Sun, Xuemei
A1  - Guan, Yi
A1  - Guo, Xitong
A1  - Wan, Xiang
A1  - Liu, Shengping
Y1  - 2021///
KW  -  Clinical decision-making
KW  -  Interpretability
KW  -  Multi-label classification
KW  - Over-testing
JF  - Journal of Biomedical Informatics
VL  - 119
SP  - 103823
EP  - 103823
DO  - https://doi.org/10.1016/j.jbi.2021.103823
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001520
N2  - Different statistical methods include various subjective criteria that can prevent over-testing. However, no unified framework that defines generalized objective criteria for various diseases is available to determine the appropriateness of diagnostic tests recommended by doctors. We present the clinical decision-making framework against over-testing based on modeling the implicit evaluation criteria (CDFO-MIEC). The CDFO-MIEC quantifies the subjective evaluation process using statistics-based methods to identify over-testing. Furthermore, it determines the test’s appropriateness with extracted entities obtained via named entity recognition and entity alignment. More specifically, implicit evaluation criteria are defined—namely, the correlation among the diagnostic tests, symptoms, and diseases, confirmation function, and exclusion function. Additionally, four evaluation strategies are implemented by applying statistical methods, including the multi-label k-nearest neighbor and the conditional probability algorithms, to model the implicit evaluation criteria. Finally, they are combined into a classification and regression tree to make the final decision. The CDFO-MIEC also provides interpretability by decision conditions for supporting each clinical decision of over-testing. We tested the CDFO-MIEC on 2,860 clinical texts obtained from a single respiratory medicine department in China with the appropriate confirmation by physicians. The dataset was supplemented with random inappropriate tests. The proposed framework excelled against the best competing text classification methods with a Mean_F1 of 0.9167. This determined whether the appropriate and inappropriate tests were properly classified. The four evaluation strategies captured the features effectively, and they were imperative. Therefore, the proposed CDFO-MIEC is feasible because it exhibits high performance and can prevent over-testing.
ER  - 
TY  - JOUR
T1  - Automatic exudate detection by fusing multiple active contours and regionwise classification
A1  - Harangi, Balazs
A1  - Hajdu, Andras
Y1  - 2014///
KW  -  Active contour method
KW  -  Contours combination
KW  -  Diabetic retinopathy screening
KW  -  Multiple pre-processing
KW  -  Region-wise classification
KW  - Exudate detection
JF  - Computers in Biology and Medicine
VL  - 54
SP  - 156
EP  - 171
DO  - https://doi.org/10.1016/j.compbiomed.2014.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S0010482514002327
N2  - In this paper, we propose a method for the automatic detection of exudates in digital fundus images. Our approach can be divided into three stages: candidate extraction, precise contour segmentation and the labeling of candidates as true or false exudates. For candidate detection, we borrow a grayscale morphology-based method to identify possible regions containing these bright lesions. Then, to extract the precise boundary of the candidates, we introduce a complex active contour-based method. Namely, to increase the accuracy of segmentation, we extract additional possible contours by taking advantage of the diverse behavior of different pre-processing methods. After selecting an appropriate combination of the extracted contours, a region-wise classifier is applied to remove the false exudate candidates. For this task, we consider several region-based features, and extract an appropriate feature subset to train a Naïve–Bayes classifier optimized further by an adaptive boosting technique. Regarding experimental studies, the method was tested on publicly available databases both to measure the accuracy of the segmentation of exudate regions and to recognize their presence at image-level. In a proper quantitative evaluation on publicly available datasets the proposed approach outperformed several state-of-the-art exudate detector algorithms.
ER  - 
TY  - JOUR
T1  - Applying density-based outlier identifications using multiple datasets for validation of stroke clinical outcomes
A1  - Lin, Ching-Heng
A1  - Hsu, Kai-Cheng
A1  - Johnson, Kory R
A1  - Luby, Marie
A1  - Fann, Yang C
Y1  - 2019///
KW  -  Barthel Index
KW  -  Outlier detection
KW  -  modified Rankin Scale
KW  - Stroke outcome
JF  - International Journal of Medical Informatics
VL  - 132
SP  - 103988
EP  - 103988
DO  - https://doi.org/10.1016/j.ijmedinf.2019.103988
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619306732
N2  - Introduction
Clinicians commonly use the modified Rankin Scale (mRS) and the Barthel Index (BI) to measure clinical outcome after stroke. These are potential targets in machine learning models for stroke outcome prediction. Therefore, the quality of the measurements is crucial for training and validation of these models. The objective of this study was to apply and evaluate density-based outlier detection methods for identifying potentially incorrect measurements in multiple large stroke datasets to assess the measurement quality.
Method
We applied three density-based outlier detection methods including density-based spatial clustering of applications (DBSCAN), hierarchical DBSCAN (HDBSCAN) and local outlier factor (LOF) based on a large dataset obtained from a nationwide prospective stroke registry in Taiwan. The testing of each method was done by using four different NINDS funded stroke datasets.
Result
The DBSCAN achieved a high performance across all mRS values where the highest average accuracy was 99.2 ± 0.7 at mRS of 4 and the lowest average accuracy was 92.0 ± 4.6 at mRS of 3. The LOF also achieved similar performance, however, the HDBSCAN with default parameters setting required further tuning improvement.
Conclusion
The density-based outlier detection methods were proven to be promising for validation of stroke outcome measures. The outlier detection algorithm developed from a large prospective registry dataset was effectively applied in four different NINDS stroke datasets with high performance results. The tool developed from this detection algorithm can be further applied to real world datasets to increase the data quality in stroke outcome measures.
ER  - 
TY  - JOUR
T1  - Similarity-based machine learning framework for predicting safety signals of adverse drug–drug interactions
A1  - Ibrahim, Heba
A1  - El Kerdawy, Ahmed M
A1  - Abdo, A
A1  - Sharaf Eldin, A
Y1  - 2021///
KW  -  Drug-drug interaction
KW  -  Pharmacovigilance
KW  -  Prediction
KW  -  Signal detection
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 26
SP  - 100699
EP  - 100699
DO  - https://doi.org/10.1016/j.imu.2021.100699
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821001830
N2  - Drug–drug interaction (DDI) is a major public health problem contributing to 30% of the unexpected clinical adverse drug events. Informatics-based studies for DDI signal detection have been evolving in the last decade. We aim at providing a boosted machine learning (ML) framework to predict novel DDI safety signals with high precision. We propose a similarity-based machine learning framework called “SMDIP” using DrugBank as one of the most reliable pharmaceutical knowledge bases. For this study, DrugBank provides the latest drug information in terms of DDIs, targets, enzymes, transporters, and carriers. We computed drug–drug similarities using a Russell–Rao measure for the available biological and structural information on DrugBank for representing the sparse feature space. Logistic regression is adopted to conduct DDI classification with a focus on searching for key similarity predictors. Six types of ML models are deployed on the selected DDI key features. Our study reveals that SMDIP has yielded favourable predictive performance compared to relevant studies with results as follows: AUC 76%, precision 82%, accuracy 79%, recall 62%, specificity 90%, and F-measure 78%. To further confirm the reliability and reproducibility of SMDIP, we investigate SMDIP on an unseen subset of direct-acting-antiviral (DAA) drugs for treating hepatitis C infections. Forty novel DAA DDIs are predicted that show consistency with the pharmacokinetic and pharmacodynamic profiles of these drugs. Furthermore, several reports from the pharmacovigilance literature corroborate our framework results. Those evaluations show that SMDIP is a promising framework for uncovering DDIs, which can be multifariously feasible in drug development, postmarketing surveillance, and public health fields.
ER  - 
TY  - JOUR
T1  - Multi-scale channel importance sorting and spatial attention mechanism for retinal vessels segmentation
A1  - Tang, Xianlun
A1  - Zhong, Bing
A1  - Peng, Jiangping
A1  - Hao, Bohui
A1  - Li, Jie
Y1  - 2020///
KW  -  Asymmetric cascade convolution
KW  -  Channel independence
KW  -  Retinal vessels
KW  -  Spatial attention mechanism
KW  - Multi-scale
JF  - Applied Soft Computing
VL  - 93
SP  - 106353
EP  - 106353
DO  - https://doi.org/10.1016/j.asoc.2020.106353
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620302933
N2  - Retinal Vessels segmentation is an important procedure for detecting and diagnosing a variety of pathological diseases. However, the inherent complex properties around the disc make it challenging to improve the segmenting accuracy of capillaries and the retinal blood vessels at the ends. In this paper, we proposed a multi-scale channel importance sorting and important spatial information positioning (MSCS) encoder–decoder for segmentation in Retinal Vessels. Firstly, the fully convolutional encoder–decoder is formed to implement a series of linear and non-linear transformation and achieve end-to-end segmentation tasks. Then, the channel importance sorting module is employed to suppress useless feature responses during the process of encoding and to identify effective channels, whose information is utilized to recognize capillaries and the retinal vessels at the ends. Finally, in the decoding stage, the spatial attention mechanism module is designed to extract the positioning information of multi-scale feature maps. The spatial information of retinal vessels is collected to better locate the position of the vessels. In addition, aiming at taking fully advantage of the network, the multi-scale asymmetric cascade convolution module is proposed to reduce the parameters of the model and increase the operation rate. Experimental results on DRIVE, STARE datasets indicate that the proposed method outperforms other state-of-the-art strategies. This system, as demonstrated, can greatly decrease false positive rate of the blood vessels at the ends and enhance the sharpness of retinal vessels.
ER  - 
TY  - JOUR
T1  - Additive manufacturing of pharmaceuticals for precision medicine applications: A review of the promises and perils in implementation
A1  - Trivedi, Megha
A1  - Jee, Joann
A1  - Silva, Suzanila
A1  - Blomgren, Carmel
A1  - Pontinha, Vasco M
A1  - Dixon, Dave L
A1  - Van Tassel, Benjamin
A1  - Bortner, Michael J
A1  - Williams, Christopher
A1  - Gilmer, Eric
A1  - Haring, Alexander P
A1  - Halper, Justin
A1  - Johnson, Blake N
A1  - Kong, Zhenyu
A1  - Halquist, Matthew S
A1  - Rocheleau, Paul F
A1  - Long, Timothy E
A1  - Roper, Thomas
A1  - Wijesinghe, Dayanjan S
Y1  - 2018///
JF  - Additive Manufacturing
VL  - 23
SP  - 319
EP  - 328
DO  - https://doi.org/10.1016/j.addma.2018.07.004
UR  - https://www.sciencedirect.com/science/article/pii/S221486041830335X
N2  - Precision medicine is an emerging field in healthcare that seeks to tailor preventive and therapeutic strategies to the unique physiology, biochemistry, lifestyles, and genetics of individual patients. There are several technologies that are key to the successful delivery of precision medicine including pharmacogenomics, pharmacometabolomics, improved point-of-care testing, and therapeutically-tailored medications. The inclusion of additive manufacturing (AM) technology, more commonly known as 3D printing, in the manufacture of oral dosage forms such as tablets, provides an avenue for the implementation of precision medicine in current healthcare practice via the prescription of specific dosage forms and drug combinations tailored to individual needs. Widespread commercialization of AM of pharmaceuticals has the potential to disrupt the supply chain used by the healthcare industry worldwide with the cost-saving potential of minimizing waste related to unused, expired medications. Despite the potential of this technology, many clinical and regulatory challenges will need to be addressed prior to large-scale implementation of AM fabricated therapeutics for precision medicine applications. This review investigates both the potential and the challenges of delivering AM fabricated medications for therapeutic use in precision medicine applications.
ER  - 
TY  - JOUR
T1  - Guided parallelized stochastic gradient descent for delay compensation
A1  - Sharma, Anuraganand
Y1  - 2021///
KW  -  Classification
KW  -  Deep learning
KW  -  Gradient Methods
KW  -  Stochastic gradient descent
KW  - Asynchronous/synchronous stochastic gradient descent
JF  - Applied Soft Computing
VL  - 102
SP  - 107084
EP  - 107084
DO  - https://doi.org/10.1016/j.asoc.2021.107084
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621000077
N2  - Stochastic gradient descent (SGD) algorithm and its variations have been effectively used to optimize neural network models. However, with the rapid growth of big data and deep learning, SGD is no longer the most suitable choice due to its natural behavior of sequential optimization of the error function. This has led to the development of parallel SGD algorithms, such as asynchronous SGD (ASGD) and synchronous SGD (SSGD) to train deep neural networks. However, it introduces a high variance due to the delay in parameter (weight) update. We address this delay in our proposed algorithm and try to minimize its impact. We employed guided SGD (gSGD) that encourages consistent examples to steer the convergence by compensating the unpredictable deviation caused by the delay. Its convergence rate is also similar to A/SSGD, however, some additional (parallel) processing is required to compensate for the delay. The experimental results demonstrate that our proposed approach has been able to mitigate the impact of delay for the quality of classification accuracy. The guided approach with SSGD clearly outperforms sequential SGD and even achieves an accuracy close to sequential SGD for some benchmark datasets.
ER  - 
TY  - JOUR
T1  - Characterization of changes in blood vessel width and tortuosity in retinopathy of prematurity using image analysis
A1  - Heneghan, Conor
A1  - Flynn, John
A1  - O’Keefe, Michael
A1  - Cahill, Mark
Y1  - 2002///
KW  -  Retinopathy of prematurity
KW  -  Screening
KW  -  Segmentation
KW  - Morphological processing
JF  - Medical Image Analysis
VL  - 6
IS  - 4
SP  - 407
EP  - 429
DO  - https://doi.org/10.1016/S1361-8415(02)00058-0
UR  - https://www.sciencedirect.com/science/article/pii/S1361841502000580
N2  - Many retinal diseases are characterised by changes to retinal vessels. For example, a common condition associated with retinopathy of prematurity (ROP) is so-called plus disease, characterised by increased vascular dilation and tortuosity. This paper presents a general technique for segmenting out vascular structures in retinal images, and characterising the segmented blood vessels. The segmentation technique consists of several steps. Morphological preprocessing is used to emphasise linear structures such as vessels. A second derivative operator is used to further emphasise thin vascular structures, and is followed by a final morphological filtering stage. Thresholding of this image is used to provide a segmented vascular mask. Skeletonisation of this mask allows identification of points in the image where vessels cross (bifurcations and crossing points) and allows the width and tortuosity of vessel segments to be calculated. The accuracy of the segmentation stage is quite dependent on the parameters used, particularly at the thresholding stage. However, reliable measurements of vessel width and tortuosity were shown using test images. Using these tools, a set of images drawn from 23 subjects being screened for the presence of threshold ROP disease is considered. Of these subjects, 11 subsequently required treatment for ROP, 9 had no evidence of ROP, and 3 had spontaneously regressed ROP. The average vessel width and tortuosity for the treated subjects was 96.8 μm and 1.125. The corresponding figures for the non-treated cohort were 86.4 μm and 1.097. These differences were statistically significant at the 99% and 95% significance level, respectively. Subjects who progressed to threshold disease during the course of screening showed an average increase in vessel width of 9.6 μm and in tortuosity of +0.008. Only the change in width was statistically significant. Applying a simple retrospective screening paradigm based solely on vessel width and tortuosity yields a screening test with a sensitivity and specificity of 82% and 75%. Factors confounding a more accurate test include poor image quality, inaccuracies in vessel segmentation, inaccuracies in measurement of vessel width and tortuosity, and limitations inherent in screening based solely on examination of the posterior pole.
ER  - 
TY  - JOUR
T1  - Blocking-free and self-contained immunoassay platform for one-step point-of-care testing
A1  - Liu, Qi
A1  - Zhou, Xiaohu
A1  - Wu, Han
A1  - Zheng, Bo
Y1  - 2020///
KW  -  Excipient
KW  -  Fluorescence quenching
KW  -  Point-of-care testing
KW  -  Polydopamine
KW  -  Quantitative immunoassay
KW  - Microarray
JF  - Biosensors and Bioelectronics
VL  - 165
SP  - 112394
EP  - 112394
DO  - https://doi.org/10.1016/j.bios.2020.112394
UR  - https://www.sciencedirect.com/science/article/pii/S0956566320303882
N2  - This paper reports a quantitative and sensitive one-step point-of-care testing (POCT) chip built on a perfluorinated substrate patterned with polydopamine microspots array. The capture antibody was covalently immobilized on the polydopamine microspots, while the fluorescently labelled detection antibody was physically adsorbed on the perfluorinated surface. The POCT chip allowed one-step sandwich immunoassay and was able to directly detect the analytes from the whole blood without sample preprocessing. By further taking advantages of the strong fluorescence quenching ability of the polydopamine, the blocking-free substrate was able to achieve similar performance in detecting and quantifying the protein biomarkers as the substrate with the blocking treatment. The blocking-free strategy not only made the fabrication of the chip simple and convenient, but also improved the chip's sensitivity for biomarker quantification. Finally, we demonstrated that the self-contained POCT platform maintained the performance for one-step immunoassay even after long-term storage. With the POCT platform, we are one step closer to a sample-in-answer-out diagnostic system.
ER  - 
TY  - JOUR
T1  - Linear programming minimum sphere set covering for extreme learning machines
A1  - Wei, Xun-Kai
A1  - Li, Ying-Hong
Y1  - 2008///
KW  -  Linear programming
KW  -  Minimum sphere set covering
KW  -  Pattern classification
KW  - Extreme learning machines
JF  - Neurocomputing
VL  - 71
IS  - 4
SP  - 570
EP  - 575
DO  - https://doi.org/10.1016/j.neucom.2007.07.021
UR  - https://www.sciencedirect.com/science/article/pii/S0925231207002913
N1  - Neural Networks: Algorithms and Applications 50 Years of Artificial Intelligence: a Neuronal Approach
N2  - A novel optimum extreme learning machines (ELM) construction method was proposed. We define an extended covering matrix with smooth function, relax the objective and constraints to formulate a more general linear programming method for the minimum sphere set covering problem. We call this method linear programming minimum sphere set covering (LPMSSC). We also present a corresponding kernelized LPMSSC and extended LPMSSC with non-Euclidean L1 and L-infinity metric. We then propose to apply the LPMSSC method to ELM and propose a data dependent ELM (DDELM) algorithm. We can obtain compact ELM for pattern classification via LPMSSC. We investigate the performances of the proposed method through UCI benchmark data sets.
ER  - 
TY  - JOUR
T1  - Enrichment of Association Rules through Exploitation of Ontology Properties – Healthcare Case Study
A1  - Bytyçi, Eliot
A1  - Ahmedi, Lule
A1  - Lisi, Francesca A
Y1  - 2017///
KW  -  property chains
KW  -  semantic web
KW  - association rules
JF  - Procedia Computer Science
VL  - 113
SP  - 360
EP  - 367
DO  - https://doi.org/10.1016/j.procs.2017.08.345
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917317556
N1  - The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops
N2  - Association rule mining as descriptive data mining category aims to find interesting patterns on data. The quality of the patterns is measured with two metrics: confidence and support. Especially in fields dealing with sensitive data, such as healthcare, the resulting patterns should be novel and interesting. To achieve that, not only the quality of the data itself should be superior, but also other additional attributes added, do support the results. That should be achieved by using Semantic Web technologies and thus enriching data used with semantic relations between properties. A hypothesis suggests that especially tackling property relations, chain property being part of the current version of the W3C Web Ontology Language (OWL), will yield better rules. To validate the hypothesis, experiments were performed on raw data, then on an older version of OWL, which does not support the chain properties and finally on the current version of language involving chain properties. Results obtained suggest that the latter produces novel rules with strong confidence and support, not encountered in former two experiments.
ER  - 
TY  - JOUR
T1  - Paroxysmal atrial fibrillation prediction based on morphological variant P-wave analysis with wideband ECG and deep learning
A1  - Tzou, Heng-An
A1  - Lin, Shien-Fong
A1  - Chen, Peng-Sheng
Y1  - 2021///
KW  -  -wave morphology
KW  -  Atrial fibrillation prediction
KW  -  Deep learning
KW  -  Explainable AI
KW  -  Skin sympathetic nerve activity
KW  - Wideband electrocardiography
JF  - Computer Methods and Programs in Biomedicine
VL  - 211
SP  - 106396
EP  - 106396
DO  - https://doi.org/10.1016/j.cmpb.2021.106396
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721004703
N2  - Background and objective
Atrial fibrillation (AF) is one of the most frequent asymptomatic arrhythmias associated with significant morbidity and mortality. Identifying the susceptibility to AF based on routine or continuous ECG recording is of considerable interest. Despite several P-wave characteristics and skin sympathetic nerve activity (SKNA) linked to AF onset, neither factor has offered accurate predictability. We propose a deep learning enabled method for AF risk prediction.
Methods
We develop a novel MVPNet to predict the upcoming onset of paroxysmal AF. MVPNet combines wavelet-based feature extraction and a deep learning classifier. MVPNet detect the approaching of AF onset by analyzing the template and frequency in P-wave segments. The morphological variant P-wave (MVP) analysis includes P-wave and SKNA features cross temporal-spectral domain. Subsequently, we designed an optimized lightweight convolutional neural network model to detect the MVP features of pre-AF episodes during sinus rhythm segments. Wideband ECG data obtained through the neuECG protocol from eight PAF patients with 177 times AF occurrence in this study. We compared the accuracy of AF prediction between ordinary ECG and neuECG.
Results
The MVPNet effectively predicted the onset of AF episodes. 89% of ECG recorded at 5 min before the AF onset can be identified using neuECG. The proposed deep learning model, MVPNet, obtained a better precision and inference speed with less computing resources than existing models. The gradient activation map showed that neuECG recording may be a superior AF risk predictor.
Conclusions
MVP analysis combined SKNA and P-wave parameters to improve predictive accuracy. The proposed MVPNet based on neuECG is superior to existing AF risk assessment with improved reliability and effectiveness. The method can be potentially applied in clinical scenarios for real-time, continuous AF prediction.
ER  - 
TY  - JOUR
T1  - Metaheuristic Algorithms for Healthcare: Open Issues and Challenges
A1  - Tsai, Chun-Wei
A1  - Chiang, Ming-Chao
A1  - Ksentini, Adlen
A1  - Chen, Min
Y1  - 2016///
KW  -  and smart home
KW  -  data
KW  -  healthcare
KW  - Metaheuristic algorithm
JF  - Computers & Electrical Engineering
VL  - 53
SP  - 421
EP  - 434
DO  - https://doi.org/10.1016/j.compeleceng.2016.03.005
UR  - https://www.sciencedirect.com/science/article/pii/S0045790616300532
N2  - Inspired by the observation that a healthcare system usually involves various intelligent technologies from different disciplines, especially metaheuristics and data mining, this paper provides a brief survey of metaheuristics for healthcare system and a roadmap for researchers working on metaheuristics and healthcare to develop a more efficient and effective healthcare system. This paper begins with a discussion of changes for healthcare, followed by a brief review of the features of “up-to-date technologies for healthcare.”. Then, a learnable big data analytics framework for healthcare system is presented which provides a high performance solution to the forthcoming challenges of big data. Finally, changes, potentials, open issues, and future trends of metaheuristics for healthcare are addressed.
ER  - 
TY  - JOUR
T1  - A three-dimensional quantification of calcified and non-calcified plaques in coronary arteries based on computed tomography coronary angiography images: Comparison with expert's annotations and virtual histology intravascular ultrasound
A1  - Kigka, Vassiliki I
A1  - Sakellarios, Antonis
A1  - Kyriakidis, Savvas
A1  - Rigas, George
A1  - Athanasiou, Lambros
A1  - Siogkas, Panagiotis
A1  - Tsompou, Panagiota
A1  - Loggitsi, Dimitra
A1  - Benz, Dominik C
A1  - Buechel, Ronny
A1  - Lemos, Pedro A
A1  - Pelosi, Gualtiero
A1  - Michalis, Lampros K
A1  - Fotiadis, Dimitrios I
Y1  - 2019///
KW  -  Active contour models
KW  -  Atherosclerotic plaque
KW  -  Calcified plaque
KW  -  Coronary artery disease
KW  -  Non-calcified plaque
KW  -  Segmentation
KW  - Computed tomography coronary angiography
JF  - Computers in Biology and Medicine
VL  - 113
SP  - 103409
EP  - 103409
DO  - https://doi.org/10.1016/j.compbiomed.2019.103409
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519302860
N2  - The detection, quantification and characterization of coronary atherosclerotic plaques has a major effect on the diagnosis and treatment of coronary artery disease (CAD). Different studies have reported and evaluated the noninvasive ability of Computed Tomography Coronary Angiography (CTCA) to identify coronary plaque features. The identification of calcified plaques (CP) and non-calcified plaques (NCP) using CTCA has been extensively studied in cardiovascular research. However, NCP detection remains a challenging problem in CTCA imaging, due to the similar intensity values of NCP compared to the perivascular tissue, which surrounds the vasculature. In this work, we present a novel methodology for the identification of the plaque burden of the coronary artery and the volumetric quantification of CP and NCP utilizing CTCA images and we compare the findings with virtual histology intravascular ultrasound (VH-IVUS) and manual expert's annotations. Bland–Altman analyses were employed to assess the agreement between the presented methodology and VH-IVUS. The assessment of the plaque volume, the lesion length and the plaque area in 18 coronary lesions indicated excellent correlation with VH-IVUS. More specifically, for the CP lesions the correlation of plaque volume, lesion length and plaque area was 0.93, 0.84 and 0.85, respectively, whereas the correlation of plaque volume, lesion length and plaque area for the NCP lesions was 0.92, 0.95 and 0.81, respectively. In addition to this, the segmentation of the lumen, CP and NCP in 1350 CTCA slices indicated that the mean value of DICE coefficient is 0.72, 0.7 and 0.62, whereas the mean HD value is 1.95, 1.74 and 1.95, for the lumen, CP and NCP, respectively.
ER  - 
TY  - JOUR
T1  - A self referencing platinum nanoparticle decorated enzyme-based microbiosensor for real time measurement of physiological glucose transport
A1  - McLamore, E S
A1  - Shi, J
A1  - Jaroch, D
A1  - Claussen, J C
A1  - Uchida, A
A1  - Jiang, Y
A1  - Zhang, W
A1  - Donkin, S S
A1  - Banks, M K
A1  - Buhman, K K
A1  - Teegarden, D
A1  - Rickus, J L
A1  - Porterfield, D M
Y1  - 2011///
KW  -  Biosensor
KW  -  Carbon nanotubes
KW  -  Physiology
KW  -  Self referencing
KW  - Glucose flux
JF  - Biosensors and Bioelectronics
VL  - 26
IS  - 5
SP  - 2237
EP  - 2245
DO  - https://doi.org/10.1016/j.bios.2010.09.041
UR  - https://www.sciencedirect.com/science/article/pii/S095656631000655X
N2  - Glucose is the central molecule in many biochemical pathways, and numerous approaches have been developed for fabricating micro biosensors designed to measure glucose concentration in/near cells and/or tissues. An inherent problem for microsensors used in physiological studies is a low signal-to-noise ratio, which is further complicated by concentration drift due to the metabolic activity of cells. A microsensor technique designed to filter extraneous electrical noise and provide direct quantification of active membrane transport is known as self-referencing. Self-referencing involves oscillation of a single microsensor via computer-controlled stepper motors within a stable gradient formed near cells/tissues (i.e., within the concentration boundary layer). The non-invasive technique provides direct measurement of trans-membrane (or trans-tissue) analyte flux. A glucose micro biosensor was fabricated using deposition of nanomaterials (platinum black, multiwalled carbon nanotubes, Nafion) and glucose oxidase on a platinum/iridium microelectrode. The highly sensitive/selective biosensor was used in the self-referencing modality for cell/tissue physiological transport studies. Detailed analysis of signal drift/noise filtering via phase sensitive detection (including a post-measurement analytical technique) are provided. Using this highly sensitive technique, physiological glucose uptake is demonstrated in a wide range of metabolic and pharmacological studies. Use of this technique is demonstrated for cancer cell physiology, bioenergetics, diabetes, and microbial biofilm physiology. This robust and versatile biosensor technique will provide much insight into biological transport in biomedical, environmental, and agricultural research applications.
ER  - 
TY  - JOUR
T1  - Examining the effect of prescription sequence on developing adverse drug reactions: The case of renal failure in diabetic patients
A1  - Davazdahemami, Behrooz
A1  - Delen, Dursun
Y1  - 2019///
KW  -  Adverse drug reactions
KW  -  Electronic health records
KW  -  Emergent pattern mining
KW  -  Prescriptions sequence
KW  - Adverse drug events
JF  - International Journal of Medical Informatics
VL  - 125
SP  - 62
EP  - 70
DO  - https://doi.org/10.1016/j.ijmedinf.2019.02.010
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618307986
N2  - Objectives
While the effect of medications in development of Adverse Drug Reactions (ADRs) have been widely studied in the past, the literature lacks sufficient coverage in investigating whether the sequence in which [ADR-prone] drugs are prescribed (and administered) can increase the chances of ADR development. The present study investigates this potential effect by applying emergent sequential pattern mining techniques to electronic health records.
Materials and methods
Using longitudinal medication and diagnosis records from more than 377,000 diabetic patients, in this study, we assessed the possible effect of prescription sequences in developing acute renal failure as a prevalent ADR among this group of patients. Relying on emergent sequential pattern mining, two statistical case-control approaches were designed and employed for this purpose.
Results
The results taken from the two employed approaches (i.e. 76.7% total agreement and 68.4% agreement on the existence of some significant effect) provide evidence for the potential effect of prescription sequence on ADRs development evidenced by the discovery that certain sequential patterns occurred more frequently in one group of patients than the other.
Conclusion
Given the significant effects shown by our data analyses, we believe that design and implementation of automated clinical decision support systems to constantly monitor patients’ medication transactions (and the sequence in which they are administered) and make appropriate alerts to prevent certain possible ADRs, may decrease ADR occurrences and save lives and money.
ER  - 
TY  - JOUR
T1  - Towards development of IoT-ML driven healthcare systems: A survey
A1  - Sworna, Nabila Sabrin
A1  - Islam, A K M Muzahidul
A1  - Shatabda, Swakkhar
A1  - Islam, Salekul
Y1  - 2021///
KW  -  Cloud computing
KW  -  Communication
KW  -  IoT
KW  -  Machine learning
KW  -  Taxonomy
KW  - Healthcare applications
JF  - Journal of Network and Computer Applications
VL  - 196
SP  - 103244
EP  - 103244
DO  - https://doi.org/10.1016/j.jnca.2021.103244
UR  - https://www.sciencedirect.com/science/article/pii/S1084804521002423
N2  - The impact of IoT-ML in the healthcare sector is very significant and it has helped us to change our view at the traditional treatment methods. In IoT-ML-based healthcare applications, the sensing layer is responsible for collecting information from humans and transferring it to the storage layer through communication technology. ML is implemented to make intelligent decisions for healthcare applications. This survey shows all the fields starting from the IoT sensor devices to the deployment of ML in the healthcare sector. We have conducted a comprehensive survey of the existing literature covering IoT and ML strategies from a healthcare perspective. We also provide insights into the different types of network storage and computing strategies used for other health-based applications. We believe that the presented work is innovative as no other survey is furnished in such manner. From this survey, researchers can get an overview of IoT-ML and cloud-based healthcare applications under the single system. We have proposed a unique taxonomy from an IoT-ML-based healthcare perspective where we have highlighted key steps in developing healthcare systems. We have culminated the most striking technologies in IoT, communications, network storage and computing, and ML for healthcare systems. Another contribution of our survey is that we have collected and discussed surveys and scientific literature based on the proposed taxonomy and their sub-taxonomy throughout this paper. Besides that we have reviewed several types of popularly used sensors, development boards in healthcare with various examples. We also show the mapping of communication technology with the protocols used by IoT sensors. In the ML section, we have shown an ML pipeline centering on healthcare application and discussed every step of it. Finally, we have identified a number of research challenges including exploration of Deep Learning based models, proper data acquisition and handling of data, privacy and ethics, security issues in WBAN, etc. These research challenges will provide the researchers the necessary future research directions while developing IoT-ML-based healthcare applications.
ER  - 
TY  - JOUR
T1  - Combining a gravitational search algorithm, particle swarm optimization, and fuzzy rules to improve the classification performance of a feed-forward neural network
A1  - Huang, Mei-Ling
A1  - Chou, Yueh-Ching
Y1  - 2019///
KW  -  Artificial neural network
KW  -  Gravitational search algorithm
KW  -  Mesothelioma disease
KW  -  Particle swarm optimization
KW  - Chronic kidney disease
JF  - Computer Methods and Programs in Biomedicine
VL  - 180
SP  - 105016
EP  - 105016
DO  - https://doi.org/10.1016/j.cmpb.2019.105016
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719304614
N2  - Background and objective
A feed-forward neural network (FNN) is a type of artificial neural network that has been widely used in medical diagnosis, data mining, stock market analysis, and other fields. Many studies have used FNN to develop medical decision-making systems to assist doctors in clinical diagnosis. The aim of the learning process in FNN is to find the best combination of connection weights and biases to achieve the minimum error. However, in many cases, FNNs converge to the local optimum but not the global optimum. Using open disease datasets, the purpose of this study was to optimize the connection weights and biases of the FNN to minimize the error and improve the accuracy of disease diagnosis.
Method
In this study, the chronic kidney disease (CKD) and mesothelioma (MES) disease datasets from the University of California Irvine (UCI) machine learning repository were used as research objects. This study applied the FNN to learn the features of each datum and used particle swarm optimization (PSO) and a gravitational search algorithm (GSA) to optimize the weights and biases of the FNN classifiers based on the algorithms inspired by the observation of natural phenomena. Moreover, fuzzy rules were used to optimize the parameters of the GSA to improve the performance of the algorithm in the classifier.
Results
When applied to the CKD dataset, the accuracies of PSO and GSA were 99%. By using fuzzy rules to optimize the GSA parameter, the accuracy of fuzzy–GSA was 99.25%. The accuracies of the combined algorithms PSO–GSA and fuzzy–PSO–GSA reached 100%. In the MES disease dataset, all methods exhibited good performance with 100% accuracy.
Conclusions
This study used PSO, GSA, fuzzy–GSA, PSO–GSA, and fuzzy–PSO–GSA on CKD and MES disease datasets to identify the disease, and the performance of different algorithms was explored. Compared with other methods in the literature, our proposed method achieved higher accuracy
ER  - 
TY  - JOUR
T1  - Swarm intelligence-based approach for educational data classification
A1  - Yahya, Anwar Ali
Y1  - 2019///
KW  -  Bloom's taxonomy
KW  -  Educational data mining
KW  -  Questions classification
KW  -  Rocchio Algorithm
KW  - Particle swarm classification
JF  - Journal of King Saud University - Computer and Information Sciences
VL  - 31
IS  - 1
SP  - 35
EP  - 51
DO  - https://doi.org/10.1016/j.jksuci.2017.08.002
UR  - https://www.sciencedirect.com/science/article/pii/S1319157817301799
N2  - This paper explores the effectiveness of Particle Swarm Classification (PSC) for a classification task in the field of educational data mining. More specifically, it proposes PSC to design a classification model capable of classifying questions into the six cognitive levels of Bloom's taxonomy. To this end, this paper proposes a novel specialized initialization mechanism based on Rocchio Algorithm (RA) to mitigate the adverse effects of the curse of dimensionality on the PSC performance. Furthermore, in the design of the RA-based PSC model of questions classification, several feature selection approaches are investigated. In doing so, a dataset of teachers' classroom questions was collected, annotated manually with Bloom's cognitive levels, and transformed into a vector space representation. Using this dataset, several experiments are conducted, and the results show a poor performance of the standard PSC due to the curse of dimensionality. However, when the proposed RA-based initialization mechanism is used, a significant improvement in the average performance, from 0.243 to 0.663, is obtained. In addition, the results indicate that the feature selection approaches play a role in the performance of the RA-based PSC (average performance ranges from 0.535 to 0.708). Finally, a comparison between the performance of RA-based PSC (average performance=0.663) and seven machine learning approaches (best average performance=0.646) confirms the effectiveness of the proposed RA-based PSC approach.
ER  - 
TY  - JOUR
T1  - Classification models for heart disease prediction using feature selection and PCA
A1  - Gárate-Escamila, Anna Karen
A1  - Hajjam El Hassani, Amir
A1  - Andrès, Emmanuel
Y1  - 2020///
KW  -  Apache spark
KW  -  Feature selection
KW  -  Heart disease
KW  -  PCA
KW  - Machine learning
JF  - Informatics in Medicine Unlocked
VL  - 19
SP  - 100330
EP  - 100330
DO  - https://doi.org/10.1016/j.imu.2020.100330
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820300125
N2  - The prediction of cardiac disease helps practitioners make more accurate decisions regarding patients' health. Therefore, the use of machine learning (ML) is a solution to reduce and understand the symptoms related to heart disease. The aim of this work is the proposal of a dimensionality reduction method and finding features of heart disease by applying a feature selection technique. The information used for this analysis was obtained from the UCI Machine Learning Repository called Heart Disease. The dataset contains 74 features and a label that we validated by six ML classifiers. Chi-square and principal component analysis (CHI-PCA) with random forests (RF) had the highest accuracy, with 98.7% for Cleveland, 99.0% for Hungarian, and 99.4% for Cleveland-Hungarian (CH) datasets. From the analysis, ChiSqSelector derived features of anatomical and physiological relevance, such as cholesterol, highest heart rate, chest pain, features related to ST depression, and heart vessels. The experimental results proved that the combination of chi-square with PCA obtains greater performance in most classifiers. The usage of PCA directly from the raw data computed lower results and would require greater dimensionality to improve the results.
ER  - 
TY  - JOUR
T1  - Distant supervision for treatment relation extraction by leveraging MeSH subheadings
A1  - Tran, Tung
A1  - Kavuluru, Ramakanth
Y1  - 2019///
KW  -  Distant supervision
KW  -  MeSH subheadings
KW  -  Medical treatment relation
KW  - Relation extraction
JF  - Artificial Intelligence in Medicine
VL  - 98
SP  - 18
EP  - 26
DO  - https://doi.org/10.1016/j.artmed.2019.06.002
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718304913
N2  - The growing body of knowledge in biomedicine is too vast for human consumption. Hence there is a need for automated systems able to navigate and distill the emerging wealth of information. One fundamental task to that end is relation extraction, whereby linguistic expressions of semantic relationships between biomedical entities are recognized and extracted. In this study, we propose a novel distant supervision approach for relation extraction of binary treatment relationships such that high quality positive/negative training examples are generated from PubMed abstracts by leveraging associated MeSH subheadings. The quality of generated examples is assessed based on the quality of supervised models they induce; that is, the mean performance of trained models (derived via bootstrapped ensembling) on a gold standard test set is used as a proxy for data quality. We show that our approach is preferable to traditional distant supervision for treatment relations and is closer to human crowd annotations in terms of annotation quality. For treatment relations, our generated training data performs at 81.38%, compared to traditional distant supervision at 64.33% and crowd-sourced annotations at 90.57% on the model-wide PR-AUC metric. We also demonstrate that examples generated using our method can be used to augment crowd-sourced datasets. Augmented models improve over non-augmented models by more than two absolute points on the more established F1 metric. We lastly demonstrate that performance can be further improved by implementing a classification loss that is resistant to label noise.
ER  - 
TY  - JOUR
T1  - A breathalyzer for the assessment of chronic kidney disease patients’ breathprint: Breath flow dynamic simulation on the measurement chamber and experimental investigation
A1  - Kalidoss, Ramji
A1  - Umapathy, Snekhalatha
A1  - Rani Thirunavukkarasu, Usha
Y1  - 2021///
KW  -  Breathalyzer
KW  -  Chronic kidney disease
KW  -  Computational fluid dynamics
KW  -  FFT feature extraction
KW  -  Measurement chamber optimization
KW  - Breathprint
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 103060
EP  - 103060
DO  - https://doi.org/10.1016/j.bspc.2021.103060
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421006571
N2  - Chemical sensor technologies with appropriate chamber design influence the kinetic adsorption of exhaled breath. Moreover, emphasize on feature extraction from breathprint may improve the accuracy of e-nose system. In this work, optimization of hardware and software have been elucidated for the improvement in the accuracy of classification between healthy and Chronic Kidney Disease (CKD) breathprint using a commercial sensor (MQ 135). Dimensioning the chamber to accommodate complete alveolar breath along with sensor positioning in the effective flow path of breath sample have been modeled using Computational Fluid Dynamics (CFD) which solved Navier-stokes equation for 3D chambers. The simulation results showed the cylindrical chamber with tangential sensor position to the flow path have effective adsorption of breath sample. The average of saturation voltage of breathprint acquired from 51 hemodialysis patients using the optimized hardware was 13.1% higher than the average saturation voltage of 47 healthy subjects. Fast Fourier Transform (FFT) coefficients extracted from breathprints exhibited high statistical significance between groups (p < 0.05). Susequently, the subset of features ranked by gain ratio algorithm showed highest accuracy of 85.7% for Support Vector Machine (SVM) classifier. These results indicated the importance of hardware and software optimization of e-nose systems for the potential applications in real time disease diagnosis.
ER  - 
TY  - JOUR
T1  - Automatic detection of referral patients due to retinal pathologies through data mining
A1  - Quellec, Gwenolé
A1  - Lamard, Mathieu
A1  - Erginay, Ali
A1  - Chabouis, Agnès
A1  - Massin, Pascale
A1  - Cochener, Béatrice
A1  - Cazuguel, Guy
Y1  - 2016///
KW  -  Anomaly detection
KW  -  Bag of Visual Words model
KW  -  Data mining
KW  - Retinal pathologies
JF  - Medical Image Analysis
VL  - 29
SP  - 47
EP  - 64
DO  - https://doi.org/10.1016/j.media.2015.12.006
UR  - https://www.sciencedirect.com/science/article/pii/S1361841515001875
N2  - With the increased prevalence of retinal pathologies, automating the detection of these pathologies is becoming more and more relevant. In the past few years, many algorithms have been developed for the automated detection of a specific pathology, typically diabetic retinopathy, using eye fundus photography. No matter how good these algorithms are, we believe many clinicians would not use automatic detection tools focusing on a single pathology and ignoring any other pathology present in the patient’s retinas. To solve this issue, an algorithm for characterizing the appearance of abnormal retinas, as well as the appearance of the normal ones, is presented. This algorithm does not focus on individual images: it considers examination records consisting of multiple photographs of each retina, together with contextual information about the patient. Specifically, it relies on data mining in order to learn diagnosis rules from characterizations of fundus examination records. The main novelty is that the content of examination records (images and context) is characterized at multiple levels of spatial and lexical granularity: 1) spatial flexibility is ensured by an adaptive decomposition of composite retinal images into a cascade of regions, 2) lexical granularity is ensured by an adaptive decomposition of the feature space into a cascade of visual words. This multigranular representation allows for great flexibility in automatically characterizing normality and abnormality: it is possible to generate diagnosis rules whose precision and generalization ability can be traded off depending on data availability. A variation on usual data mining algorithms, originally designed to mine static data, is proposed so that contextual and visual data at adaptive granularity levels can be mined. This framework was evaluated in e-ophtha, a dataset of 25,702 examination records from the OPHDIAT screening network, as well as in the publicly-available Messidor dataset. It was successfully applied to the detection of patients that should be referred to an ophthalmologist and also to the specific detection of several pathologies.
ER  - 
TY  - JOUR
T1  - A Study on Ontology Based Abstractive Summarization
A1  - Mohan, M Jishma
A1  - Sunitha, C
A1  - Ganesh, Amal
A1  - Jaya, A
Y1  - 2016///
KW  -  Ontology
KW  - Abstractive summarization
JF  - Procedia Computer Science
VL  - 87
SP  - 32
EP  - 37
DO  - https://doi.org/10.1016/j.procs.2016.05.122
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916304616
N1  - Fourth International Conference on Recent Trends in Computer Science &amp; Engineering (ICRTCSE 2016)
N2  - With widespread use of Internet and the emergence of information aggregation on a large scale, a quality text summarization is essential to effectively condense the information. Automatic summarization systems condense the documents by extracting the most relevant facts. Summarization is commonly classified into two types, extractive and abstractive. Summarization by abstraction needs understanding of the original text and then generating the summary which is semantically related. Abstractive summarization requires the understanding of complex natural language processing tasks. There are many methods adopted for abstractive summarization. Ontology is one among the approach used for getting abstractive summary for a specific domain. In this paper, we discuss about various works carried out using ontology for abstractive text summarization.
ER  - 
TY  - JOUR
T1  - An Efficient Hybrid Feature Selection model for Dimensionality Reduction
A1  - Jain, Divya
A1  - Singh, Vijendra
Y1  - 2018///
KW  -  Disease Diagnosis
KW  -  Hybrid Feature Selection
KW  -  PCA
KW  -  ReliefF
KW  - Chronic Disease Datasets
JF  - Procedia Computer Science
VL  - 132
SP  - 333
EP  - 341
DO  - https://doi.org/10.1016/j.procs.2018.05.188
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918309220
N1  - International Conference on Computational Intelligence and Data Science
N2  - This paper presents a novel approach based on hybrid feature selection that significantly reduces dimensionality of features. In this paper, an efficient method consisting of ReliefF and PCA is proposed which shows remarkable results with different chronic disease datasets. The presented work is suitable for both text and micro-array datasets which determines the optimal value of threshold for the selection of relevant and non-redundant features. To validate the performance of proposed work, ten popular benchmark datasets are used. With the results obtained, it is found that the presented approach reduces more than 50% irrelevant and redundant features from the dataset. Also with the proposed method, the computation time significantly decreases for all considered chronic disease datasets. Moreover, it is experimentally depicted that the threshold value significantly affects the selection of appropriate features.
ER  - 
TY  - JOUR
T1  - When Ensemble Learning Meets Deep Learning: a New Deep Support Vector Machine for Classification
A1  - Qi, Zhiquan
A1  - Wang, Bo
A1  - Tian, Yingjie
A1  - Zhang, Peng
Y1  - 2016///
KW  -  Deep architectures
KW  -  Support vector machine
KW  - Pattern recognition
JF  - Knowledge-Based Systems
VL  - 107
SP  - 54
EP  - 60
DO  - https://doi.org/10.1016/j.knosys.2016.05.055
UR  - https://www.sciencedirect.com/science/article/pii/S0950705116301605
N2  - Recently, Deep Learning (DL) method has received a significant breakthrough in the data representation, whose success mainly depends on its deep structure. In this paper, we focus on the DL research based on Support Vector Machine (SVM), and first present an Ex-Adaboost learning strategy, and then propose a new Deep Support Vector Machine (called DeepSVM). Unlike other DL algorithms based on SVM, in each layer, Ex-Adaboost is applied to not only select SVMs with the minimal error rate and the highest diversity, but also to produce the weight for each feature. In this way, new training data is obtained. By stacking these SVMs into multiple layers following the same way, we finally acquire a new set of deep features that can greatly boost the classification performance. In the end, the training data represented by these new features is regarded as the input for a standard SVM classifier. In the experimental part, we offer these answers to the following questions: 1) is the deep structure of DeepSVM really useful for classification problem? 2) Does Ex-Adaboost work, and is it helpful for further improving on DeepSVM’s performance with respect to the deep structure? 3) How much improvement in classification accuracy of DeepSVM, compared with other exist algorithms?
ER  - 
TY  - JOUR
T1  - Automated detection of age-related macular degeneration using empirical mode decomposition
A1  - Mookiah, Muthu Rama Krishnan
A1  - Acharya, U Rajendra
A1  - Fujita, Hamido
A1  - Koh, Joel E W
A1  - Tan, Jen Hong
A1  - Chua, Chua Kuang
A1  - Bhandary, Sulatha V
A1  - Noronha, Kevin
A1  - Laude, Augustinus
A1  - Tong, Louis
Y1  - 2015///
KW  -  Decision support system
KW  -  Empirical mode decomposition
KW  -  Fundus imaging
KW  -  Locality sensitive discriminant analysis
KW  - Age-related macular degeneration
JF  - Knowledge-Based Systems
VL  - 89
SP  - 654
EP  - 668
DO  - https://doi.org/10.1016/j.knosys.2015.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S0950705115003512
N2  - Age-related Macular Degeneration (AMD) is the posterior segment eye disease affecting elderly people and may lead to loss of vision. AMD is diagnosed using clinical features like drusen, Geographic Atrophy (GA) and Choroidal NeoVascularization (CNV) present in the fundus image. It is mainly classified into dry and wet type. Dry AMD is most common among elderly people. At present there is no treatment available for dry AMD. Early diagnosis and treatment to the affected eye may reduce the progression of disease. Manual screening of fundus images is time consuming and subjective. Hence in this study we are proposing an Empirical Mode Decomposition (EMD)-based nonlinear feature extraction to characterize and classify normal and AMD fundus images. EMD is performed on 1D Radon Transform (RT) projections to generate different Intrinsic Mode Functions (IMF). Various nonlinear features are extracted from the IMFs. The dimensionality of the extracted features are reduced using Locality Sensitive Discriminant Analysis (LSDA). Then the reduced LSDA features are ranked using minimum Redundancy Maximum Relevance (mRMR), Kullback–Leibler Divergence (KLD) and Chernoff Bound and Bhattacharyya Distance (CBBD) techniques. Ranked LSDA components are sequentially fed to Support Vector Machine (SVM) classifier to discriminate normal and AMD classes. The performance of the current study is experimented using private and two public datasets namely Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE). The 10-fold cross validation approach is used to evaluate the performance of the classifiers and obtained highest average classification accuracy of 100%, sensitivity of 100% and specificity of 100% for STARE dataset using only two ranked LSDA components. Our results reveal that the proposed system can be used as a decision support tool for clinicians for mass AMD screening.
ER  - 
TY  - JOUR
T1  - Quantitative evaluation of gray matter alterations in patients with mesial temporal lobe epilepsy (MTLE)
A1  - Li, Zhensheng
A1  - Gao, Quwen
A1  - Peng, Kairun
A1  - Lin, Jian
A1  - Wang, Wei
A1  - Wang, Weimin
A1  - Deng, Bingmei
Y1  - 2021///
KW  -  Mesial temporal lobe epilepsy
KW  -  Quantitative evaluation
KW  - Gray matter
JF  - Neuroscience Informatics
VL  - 1
IS  - 4
SP  - 100018
EP  - 100018
DO  - https://doi.org/10.1016/j.neuri.2021.100018
UR  - https://www.sciencedirect.com/science/article/pii/S2772528621000182
N2  - Objective: The purpose of this study was to evaluate the gray matter (GM) alterations in patients with mesial temporal lobe epilepsy (MTLE) based on statistical models. Methods: High-resolution MRI data were collected from 64 adult MTLE patients and 100 healthy participants. Voxel Based Morphometry (VBM) and Surfaced Based Morphometry (SBM) analysis were used to detect the alterations of cerebral GM of MTLE. We also built general linear models (GLM) for quantitatively evaluating regions of interest (ROIs) from the corrected VBM, SBM analysis. Results: The corrected VBM and SBM analyses identified discriminable GM alterations in the left and right MTLE groups located in the ipsilateral hippocampus, the perisylvian cortex in the left MTLE group and the bilateral orbitofrontal cortex in the right MTLE group. We extracted the ipsilateral hippocampal volume GLM of both unilateral MTLE groups. History of febrile convulsions and an increased number of AEDs were recognized as risk factors for ipsilateral hippocampus atrophy of unilateral MTLE. Moreover, early onset age was also considered a risk factor for left hippocampus atrophy in left MTLE. Significance: The current study established a quantitative assessment of unilateral MTLE based solely on GM alterations, without any influence of demographic risk factors. By using this GM-based protocol, we found that regions both inside and outside of the temporal lobes may be associated with the epileptic network of MTLE.
ER  - 
TY  - JOUR
T1  - Towards Identifying of Effective Personalized Antihypertensive Treatment Rules from Electronic Health Records Data Using Classification Methods: Initial Model
A1  - Semakova, Anna
A1  - Zvartau, Nadezhda
A1  - Bochenina, Klavdiya
A1  - Konradi, Aleksandra
Y1  - 2017///
KW  -  Classification problem
KW  -  Decision trees
KW  -  Model quality measures
KW  -  Personalized medicine
KW  - Electronic health records
JF  - Procedia Computer Science
VL  - 121
SP  - 852
EP  - 858
DO  - https://doi.org/10.1016/j.procs.2017.11.110
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917323128
N1  - CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017
N2  - Traditional clinical diagnosis and management are regulated by standards, patient management protocols with specific nosology and clinical guidelines that are limited, and their use in practice is confronted with the gap between “efficacy” and “effectiveness”. Data stored patients’ electronic health records (EHRs) provide previously unknown predictors that have affected the disease outcome, and allow to develop personalized treatment guidelines with the application of statistical methods and powerful machine learning techniques. This study aims to predict treatment effect of monotherapy with five main classes of antihypertensive drugs based on individual patients’ profiles for a single decision time point. We transform the estimation of effective personalized antihypertensive treatment rules into a classification problem, and propose the method to adapt the CART algorithm for building a decision tree for effective personalized approach to choose monotherapy in hypertension.
ER  - 
TY  - JOUR
T1  - TransforMED: End-to-Εnd Transformers for Evidence-Based Medicine and Argument Mining in medical literature
A1  - Stylianou, Nikolaos
A1  - Vlahavas, Ioannis
Y1  - 2021///
KW  -  Argument mining
KW  -  Evidence based medicine
KW  -  Natural language processing
KW  - Deep learning
JF  - Journal of Biomedical Informatics
VL  - 117
SP  - 103767
EP  - 103767
DO  - https://doi.org/10.1016/j.jbi.2021.103767
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000964
N2  - Argument Mining (AM) refers to the task of automatically identifying arguments in a text and finding their relations. In medical literature this is done by identifying Claims and Premises and classifying their relations as either Support or Attack. Evidence-Based Medicine (EBM) refers to the task of identifying all related evidence in medical literature to allow medical practitioners to make informed choices and form accurate treatment plans. This is achieved through the automatic identification of Population, Intervention, Comparator and Outcome entities (PICO) in the literature to limit the collection to only the most relevant documents. In this work, we combine EBM with AM in medical literature to increase the performance of the individual models and create high quality argument graphs, annotated with PICO entities. To that end, we introduce a state-of-the-art EBM model, used to predict the PICO entities and two novel Argument Identification and Argument Relation classification models that utilize the PICO entities to enhance their performance. Our final system works in a pipeline and is able to identify all PICO entities in a medical publication, the arguments presented in them and their relations.
ER  - 
TY  - JOUR
T1  - Individualized prediction of depressive disorder in the elderly: A multitask deep learning approach
A1  - Xu, Zhongzhi
A1  - Zhang, Qingpeng
A1  - Li, Wentian
A1  - Li, Mingyang
A1  - Yip, Paul Siu Fai
Y1  - 2019///
KW  -  Deep learning
KW  -  Depression
KW  -  Patient progression model
KW  - Depressive disorder prediction
JF  - International Journal of Medical Informatics
VL  - 132
SP  - 103973
EP  - 103973
DO  - https://doi.org/10.1016/j.ijmedinf.2019.103973
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619303314
N2  - Introduction
Depressive disorder is one of the major public health problems among the elderly. An effective depression risk prediction model can provide insights on the disease progression and potentially inform timely targeted interventions. Therefore, research on predicting the onset of depressive disorder for elderly adults considering the sequential progression patterns is critically needed.
Objective
This research aims to develop a state-of-the-art deep learning model for the individualized prediction of depressive disorder with a 22-year longitudinal survey data among elderly people in the United States.
Methods
We obtain the 22-year longitudinal survey data from the University of Michigan Health and Retirement Study, which consists of information on 20,000 elderly people in the United States from 1992 to 2014. To capture temporal and high-order interactions among risk factors, the proposed deep learning model utilizes a recurrent neural network framework with a multitask structure. The C-statistic and the mean absolute error are used to evaluate the prediction accuracy of the proposed model and a set of baseline models.
Results
The experiments with the 22-year longitudinal survey data indicate that (a) machine learning models can provide an accurate prediction of the onset of depressive disorder for elderly individuals; (b) the temporal patterns of risk factors are associated with the onset of depressive disorder; and (c) the proposed multitask deep learning model exhibits superior performance as compared with baseline models.
Conclusion
The results demonstrate the capability of deep learning-based prediction models in capturing temporal and high-order interactions among risk factors, which are usually ignored by traditional regression models. This research sheds light on the use of machine learning models to predict the onset of depressive disorder among elderly people. Practically, the proposed methods can be implemented as a decision support system to help clinicians make decisions and inform actionable intervention strategies for elderly people.
ER  - 
TY  - JOUR
T1  - Artificial plant optimization algorithm to detect heart rate & presence of heart disease using machine learning
A1  - Sharma, Prerna
A1  - Choudhary, Krishna
A1  - Gupta, Kshitij
A1  - Chawla, Rahul
A1  - Gupta, Deepak
A1  - Sharma, Arun
Y1  - 2020///
KW  -  Artificial neural network
KW  -  Extreme gradient boosting
KW  -  Machine learning
KW  -  Savitzky-Golay filter
KW  - Modified artificial plant optimization algorithm
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101752
EP  - 101752
DO  - https://doi.org/10.1016/j.artmed.2019.101752
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719304312
N2  - In today’s world, cardiovascular diseases are prevalent becoming the leading cause of death; more than half of the cardiovascular diseases are due to Coronary Heart Disease (CHD) which generates the demand of predicting them timely so that people can take precautions or treatment before it becomes fatal. For serving this purpose a Modified Artificial Plant Optimization (MAPO) algorithm has been proposed which can be used as an optimal feature selector along with other machine learning algorithms to predict the heart rate using the fingertip video dataset which further predicts the presence or absence of Coronary Heart Disease in an individual at the moment. Initially, the video dataset has been pre-processed, noise is filtered and then MAPO is applied to predict the heart rate with a Pearson correlation and Standard Error Estimate of 0.9541 and 2.418 respectively. The predicted heart rate is used as a feature in other two datasets and MAPO is again applied to optimize the features of both datasets. Different machine learning algorithms are then applied to the optimized dataset to predict values for presence of current heart disease. The result shows that MAPO reduces the dimensionality to the most significant information with comparable accuracies for different machine learning models with maximum dimensionality reduction of 81.25%. MAPO has been compared with other optimizers and outperforms them with better accuracy.
ER  - 
TY  - JOUR
T1  - Analysis of Demographic Characteristics Creating Coronary Artery Disease Susceptibility Using Random Forests Classifier
A1  - Akyol, Kemal
A1  - Çalik, Elif
A1  - Bayir, Şafak
A1  - Şen, Baha
A1  - Çavuşoğlu, Abdullah
Y1  - 2015///
KW  -  Coronary Artery Disease
KW  -  Random Forests ;
KW  - Classification
JF  - Procedia Computer Science
VL  - 62
SP  - 39
EP  - 46
DO  - https://doi.org/10.1016/j.procs.2015.08.407
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915025429
N1  - Proceedings of the 2015 International Conference on Soft Computing and Software Engineering (SCSE'15)
N2  - Cardiovascular system diseases are an important health problem. These diseases are very common also responsible for many deaths. With this study, it is aimed to analyze factors that cause Coronary Artery Disease using Random Forests Classifier. According to the analysis, we observed correct classification ratio and performance measure that creates susceptibility to Coronary Artery Disease for each factor. The performance measure results clearly show the impact of demographic characteristics on CAD. Additionally, this study shows that random forests algorithm can be used to the processing and classification of medical data such as CAD.
ER  - 
TY  - JOUR
T1  - LiftRight: Quantifying strength training performance using a wearable sensor
A1  - Milanko, Slobodan
A1  - Jain, Shubham
Y1  - 2020///
KW  -  Digital health
KW  -  Exercise kinematics
KW  -  Exercise segmentation
KW  -  Motion tracking
KW  -  Wearable sensor systems
KW  - Strength training
JF  - Smart Health
VL  - 16
SP  - 100115
EP  - 100115
DO  - https://doi.org/10.1016/j.smhl.2020.100115
UR  - https://www.sciencedirect.com/science/article/pii/S235264832030009X
N2  - Exercise is shown to affect the physical, psychological, and cognitive aspects of one's health. Despite the prevalence of fitness trackers for tracking heart-rate, steps, and other wellness metrics, very few tools exist to monitor strength training performance. This paper presents LiftRight which introduces a low-cost approach for quantifying an individual's performance during strength training sessions. Unique challenges include the lack of useful qualitative or quantitative metrics, and monitoring the complex motion of the exercises. We leverage an arm-mounted inertial sensor for tracking the user's arm and computing associated kinetics. We choose three upper-body strength training exercises, and segment the workout trace into constituent sets, repetitions, and phases. We also compute performance metrics such as velocity, range of motion, stability angle, and sticking points for eight candidates over a total period of 26 weeks. LiftRight achieves 96% accuracy in identifying sets and repetitions in a workout. In terms of lifting practices and its affects on performance, our quantitative and qualitative observations are found to be consistent with existing literature. We believe that the ability to accurately monitor weight training will lower the entrance barrier and help prevent injuries by helping users and trainers alike.
ER  - 
TY  - JOUR
T1  - Self-supervised iterative refinement learning for macular OCT volumetric data classification
A1  - Qiu, Jiaming
A1  - Sun, Yankui
Y1  - 2019///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Image classification
KW  -  Self-supervised learning
KW  - Optical coherence tomography
JF  - Computers in Biology and Medicine
VL  - 111
SP  - 103327
EP  - 103327
DO  - https://doi.org/10.1016/j.compbiomed.2019.103327
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519301969
N2  - We present self-supervised iterative refinement learning (SIRL) as a pipeline to improve a type of macular optical coherence tomography (OCT) volumetric image classification algorithms. In this type of algorithms, first, two-dimensional (2D) image classification algorithms are applied to each B-scan in an OCT volume, and then B-scan level classification results are combined to obtain the classification result of the volume. Specifically, SIRL consists of repetitive training–sieving–relabeling steps. In the initialization stage, the label of each 2D image is assigned as the label of the volume they belong to, yielding an initial label set. In the training stage, the network is trained using the current label set. In the sieving and relabeling stage, the label of each 2D image is renewed based on the classification result of the trained network, and a new label set is obtained. Experiments are conducted on a clinical dataset and public dataset, on which the performances of the models trained by a normal scheme and our proposed methods are compared under a five-fold cross validation. Our proposed method achieves sensitivity, specificity, and accuracy of 89.74%, 94.87%, and 93.18%, respectively, on the clinical dataset. On the public dataset, the results of the corresponding three metrics are 98.22%, 90.43% and 95.88%. The results demonstrate the effectiveness of our proposed method as an approach to improve the B-scan-classification-based macular OCT volumetric image classification algorithms.
ER  - 
TY  - JOUR
T1  - Examining health disparities by gender: A multimorbidity network analysis of electronic medical record
A1  - Kalgotra, Pankush
A1  - Sharda, Ramesh
A1  - Croff, Julie M
Y1  - 2017///
KW  -  Multimorbidity
KW  -  Network analysis
KW  - Gender disparity
JF  - International Journal of Medical Informatics
VL  - 108
SP  - 22
EP  - 28
DO  - https://doi.org/10.1016/j.ijmedinf.2017.09.014
UR  - https://www.sciencedirect.com/science/article/pii/S138650561730237X
N2  - Problem
Multimorbidity health disparities have not been well examined by gender. Co-occurring diseases may be mutually deleterious, co-occurring independently, or co-occurring from a common antecedent. Diseases linked by a common antecedent may be caused by biological, behavioral, social, or environmental factors. This paper aims to address the co-occurrences of diseases using network analysis.
Methods
In this study, we identify these multi-morbidities from a large electronic medical record (EMR) containing diagnoses, symptoms and treatment data on more than 22.1 million patients. We create multimorbidity networks from males and females medical records and compare their structural properties.
Results
Our macro analysis at the organ-level indicates that females have a stronger multimorbidity network than males. For example, the female multimorbidity network includes six linkages to mental health, wherein the male multimorbidity network includes only two linkages to mental health. The strength of some disease associations between lipid metabolism and chronic heart disorders is stronger in males than females.
Conclusion
Our multimorbidity network analysis by gender identifies specific differences in disease diagnosis by gender, and presents questions for biological, behavioral, clinical, and policy research.
ER  - 
TY  - JOUR
T1  - Mixed-integer optimization approach to learning association rules for unplanned ICU transfer
A1  - Chou, Chun-An
A1  - Cao, Qingtao
A1  - Weng, Shao-Jen
A1  - Tsai, Che-Hung
Y1  - 2020///
KW  -  Association rule
KW  -  Critical care
KW  -  Mixed-integer optimization
KW  -  Unplanned ICU transfer
KW  - Emergency department
JF  - Artificial Intelligence in Medicine
VL  - 103
SP  - 101806
EP  - 101806
DO  - https://doi.org/10.1016/j.artmed.2020.101806
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719302027
N2  - After admission to emergency department (ED), patients with critical illnesses are transferred to intensive care unit (ICU) due to unexpected clinical deterioration occurrence. Identifying such unplanned ICU transfers is urgently needed for medical physicians to achieve two-fold goals: improving critical care quality and preventing mortality. A priority task is to understand the crucial rationale behind diagnosis results of individual patients during stay in ED, which helps prepare for an early transfer to ICU. Most existing prediction studies were based on univariate analysis or multiple logistic regression to provide one-size-fit-all results. However, patient condition varying from case to case may not be accurately examined by such a simplistic judgment. In this study, we present a new decision tool using a mathematical optimization approach aiming to automatically discover rules associating diagnostic features with high-risk outcome (i.e., unplanned transfers) in different deterioration scenarios. We consider four mutually exclusive patient subgroups based on the principal reasons of ED visits: infections, cardiovascular/respiratory diseases, gastrointestinal diseases, and neurological/other diseases at a suburban teaching hospital. The analysis results demonstrate significant rules associated with unplanned transfer outcome for each subgroups and also show comparable prediction accuracy (>70%) compared to state-of-the-art machine learning methods while providing easy-to-interpret symptom-outcome information.
ER  - 
TY  - JOUR
T1  - Multi-server verifiable delegation of computations: Unconditional security and practical efficiency
A1  - Zhang, Liang Feng
Y1  - 2021///
KW  -  Function privacy
KW  -  Input privacy
KW  -  Verifiable computation
KW  - Delegation of computation
JF  - Information and Computation
VL  - 281
SP  - 104740
EP  - 104740
DO  - https://doi.org/10.1016/j.ic.2021.104740
UR  - https://www.sciencedirect.com/science/article/pii/S0890540121000559
N2  - Outsourcing computation has gained significant popularity in recent years due to the prevalence of cloud computing. There are two main security concerns in outsourcing computation: how to guarantee the cloud server performs the computation correctly and how to keep the client's data secret. The single-server verifiable computation (SSVC) of Gennaro, Gentry and Parno (Crypto'10) enables a client to delegate the computation of a function f on any input x with both concerns highly relieved, but only results in computationally secure schemes that lack practical efficiency. While the SSVC schemes use a single server, in this paper we develop a multi-server verifiable computation (MSVC) model where the client shares both f and x among multiple servers, each server performs a set of computations on its shares, and finally the client reconstructs f(x) from all servers' results. In this MSVC model we propose a generic construction for outsourcing computations of the form Fx, where F is a matrix and x is a vector. Our generic construction achieves information-theoretic security, input privacy and function privacy. By optimizing the parameters, we obtain both a 3-server scheme, which uses the least number of servers, and a 4-server scheme, which incurs the least workload. By decomposing many polynomial computations as a two-stage computation, where the first-stage has the form Fx and the second-stage is fast, and delegating the first-stage computation, we obtain MSVC schemes for these polynomials. We implement our MSVC schemes and show that they are among the most practical ones to date.
ER  - 
TY  - JOUR
T1  - Fuzzy spectral clustering for automated delineation of chronic wound region using digital images
A1  - Manohar Dhane, Dhiraj
A1  - Maity, Maitreya
A1  - Mungle, Tushar
A1  - Bar, Chittaranjan
A1  - Achar, Arun
A1  - Kolekar, Maheshkumar
A1  - Chakraborty, Chandan
Y1  - 2017///
KW  -  Color constancy
KW  -  Digital image processing
KW  -  Fuzzy similarity measure
KW  -  Segmentation
KW  -  Spectral clustering
KW  - Chronic wound
JF  - Computers in Biology and Medicine
VL  - 89
SP  - 551
EP  - 560
DO  - https://doi.org/10.1016/j.compbiomed.2017.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517300914
N2  - Chronic wound is an abnormal disease condition of localized injury to the skin and its underlying tissues having physiological impaired healing response. Assessment and management of such wound is a significant burden on the healthcare system. Currently, precise wound bed estimation depends on the clinical judgment and remains a difficult task. The paper introduces a novel method for ulcer boundary demarcation and estimation, using optical images captured by a hand-held digital camera. The proposed approach involves gray based fuzzy similarity measure using spatial knowledge of an image. The fuzzy measure is used to construct similarity matrix. The best color channel was chosen by calculating the mean contrast for 26 different color channels of 14 color spaces. It was found that Db color channel has highest mean contrast which provide best segmentation result in comparison with other color channels. The fuzzy spectral clustering (FSC) method was applied on Db color channel for effective delineation of wound region. The segmented wound regions were effectively post-processed using various morphological operations. The performance of proposed segmentation technique was validated by ground-truth images labeled by two experienced dermatologists and a surgeon. The FSC approach was tested on 70 images. FSC effectively segmented targeted ulcer boundary yielding 91.5% segmentation accuracy, 86.7%, Dice index and 79.0%. Jaccard score. The sensitivity and specificity was found to be 87.3% and 95.7% respectively. The performance evaluation shows the robustness of the proposed method of wound area segmentation and its potential to be used for designing patient comfort centric wound care system.
ER  - 
TY  - JOUR
T1  - Deep learning in ECG diagnosis: A review
A1  - Liu, Xinwen
A1  - Wang, Huan
A1  - Li, Zongjin
A1  - Qin, Lang
Y1  - 2021///
KW  -  Convolutional neural network
KW  -  Deep belief network
KW  -  Deep learning
KW  -  Recurrent neural network
KW  -  Stacked auto-encoders
KW  - Electrocardiogram
JF  - Knowledge-Based Systems
VL  - 227
SP  - 107187
EP  - 107187
DO  - https://doi.org/10.1016/j.knosys.2021.107187
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121004494
N2  - Cardiovascular disease (CVD) is a general term for a series of heart or blood vessels abnormality that serves as a global leading reason for death. The earlier the abnormal heart rhythm is discovered, the less severe the sequela and the faster the recovery. Electrocardiogram (ECG), as a main way to detect the electrical activity of heart, is a very important harmless means of predicting and diagnosing CVDs. However, ECG signal has characteristics of complex and high chaos, making it time-consuming and exhausting to interpret ECG signal even for experts. Hence, computer-aided methods are required to relief human burden and reduce errors caused by tiredness, inter- and intra-difference. Deep learning shows outstanding performance on ECG classification studies recent few years. Its hierarchical architecture enables higher-level features obtained and its strong ability to feature extraction contributes to classification project. Latest studies can achieve higher accuracy and efficiency than manual classification by experts. In this paper, we review the existing studies of deep learning applied in ECG diagnosis according to four typical algorithms: stacked auto-encoders, deep belief network, convolutional neural network and recurrent neural network. We first introduced the mechanism, development and application of the algorithms. Then we review their applications in ECG diagnosis systematically, discussing their highlights and limitations. Our view about future potential development of deep learning in ECG diagnosis is stated in the final part of this paper.
ER  - 
TY  - JOUR
T1  - Post-evolution of variable-length class prototypes to unlock decision making within support vector machines
A1  - Stoean, Catalin
A1  - Stoean, Ruxandra
Y1  - 2014///
KW  -  Cooperative coevolution
KW  -  Evolutionary computation
KW  -  Feature selection
KW  -  Opaque prediction
KW  -  Prototype discovery
KW  - Support vector machines
JF  - Applied Soft Computing
VL  - 25
SP  - 159
EP  - 173
DO  - https://doi.org/10.1016/j.asoc.2014.09.017
UR  - https://www.sciencedirect.com/science/article/pii/S1568494614004694
N2  - Although neural networks and support vector machines (SVMs) are the traditional predictors for the classification of complex problems, these opaque paradigms cannot explain the logic behind the discrimination process. Therefore, within the quite unexplored area of evolutionary algorithms opening the SVM decision black box, the paper appoints a cooperative coevolutionary (CC) technique to extract discriminative and compact class prototypes following a SVM model. Various interactions between the SVM and CC are considered, while many experiments test three decisive hypotheses: fidelity to the SVM prediction, superior accuracy to the CC classifier alone and a compact and comprehensive resulting output, achieved through a class-oriented form of feature selection. Results support the hybridization by statistically and visually demonstrating its advantages.
ER  - 
TY  - JOUR
T1  - Partition based clustering of large datasets using MapReduce framework: An analysis of recent themes and directions
A1  - Sardar, Tanvir Habib
A1  - Ansari, Zahid
Y1  - 2018///
KW  -  Hadoop
KW  -  MapReduce
KW  -  Partition-based clustering
KW  - Data mining
JF  - Future Computing and Informatics Journal
VL  - 3
IS  - 2
SP  - 247
EP  - 261
DO  - https://doi.org/10.1016/j.fcij.2018.06.002
UR  - https://www.sciencedirect.com/science/article/pii/S231472881730065X
N2  - Data clustering is one of the fundamental techniques in scientific analysis and data mining, which describes a dataset according to similarities among its objects. Partition based clustering algorithms are the most popular and widely used clustering technique. In this information era, due to the digitization of every field, the huge volume of data is available to data analysts. The quick growth of such datasets makes decade old computing platforms, programming paradigms, and clustering algorithms become inadequate to obtain knowledge from these datasets. To cluster such large datasets, Hadoop distributed platform, MapReduce programming paradigm and modified clustering algorithms are being used to shrink the computational time by distributing clustering job across multiple computing nodes. This paper provides a comprehensive review of Hadoop and MapReduce and their components. This paper aims to survey recent research works on partition based clustering algorithms which use MapReduce as their programming paradigm. In many recent works, the traditional partition based clustering algorithms like K-means, K-prototypes, K-medoids, K-modes and Fuzzy C-means are modified for MapReduce paradigm in order to obtain different clustering objectives on different datasets for reducing the computational time. The contribution of this paper is (1) to provide an overview of clustering challenges in real world large dataset clustering and the role of MapReduce programming paradigm and its supporting platforms in dealing the challenges for several tasks in different datasets and (2) to review recent works in partition based clustering using MapReduce paradigm for different clustering objectives for different datasets employing different strategies.
ER  - 
TY  - JOUR
T1  - Discovery of novel and potent P2Y14R antagonists via structure-based virtual screening for the treatment of acute gouty arthritis
A1  - Wang, Weiwei
A1  - Liu, Chunxiao
A1  - Li, Hanwen
A1  - Tian, Sheng
A1  - Liu, Yingxian
A1  - Wang, Nanxi
A1  - Yan, Duanyang
A1  - Li, Huanqiu
A1  - Hu, Qinghua
Y1  - 2020///
KW  -  Acute gouty arthritis
KW  -  Homology modeling
KW  -  Molecular docking
KW  -  Pyroptosis
KW  -  Virtual screening
KW  - P2YR
JF  - Journal of Advanced Research
VL  - 23
SP  - 133
EP  - 142
DO  - https://doi.org/10.1016/j.jare.2020.02.007
UR  - https://www.sciencedirect.com/science/article/pii/S209012322030031X
N2  - P2Y14 nucleotide receptor is a Gi protein-coupled receptor, which is widely involved in physiological and pathologic events. Although several P2Y14R antagonists have been developed thus far, few have successfully been developed into a therapeutic drug. In this study, on the basis of two P2Y14R homology models, Glide docking-based virtual screening (VS) strategy was employed for finding potent P2Y14R antagonists with novel chemical architectures. A total of 19 structurally diverse compounds identified by VS and drug-like properties testing were set to experimental testing. 10 of them showed good inhibitory effects against the P2Y14R (IC50 < 50 nM), including four compounds (compounds 8, 10, 18 and 19) with IC50 value below 10 nM. The best VS hit, compound 8 exhibited the best antagonistic activity, with IC50 value of 2.46 nM. More importantly, compound 8 restrained monosodium uric acid (MSU)-induced pyroptosis of THP-1 cells through blocking the activation of Nod-like receptor 3 (NLRP3) inflammasome, which was attributed to its inhibitory effects on P2Y14R-cAMP pathways. The key favorable residues uncovered using MM/GBSA binding free energy calculations/decompositions were detected and discussed. These findings suggest that the compound 8 can be used as a good lead compound for further optimization to obtain more promising P2Y14R antagonists for the treatment of acute gouty arthritis.
ER  - 
TY  - JOUR
T1  - Use of automated fetal heart rate analysis to identify risk factors for umbilical cord acidosis at birth
A1  - Houzé de l’Aulnoit, A
A1  - Génin, M
A1  - Boudet, S
A1  - Demailly, R
A1  - Ternynck, C
A1  - Babykina, G
A1  - Houzé de l’Aulnoit, D
A1  - Beuscart, R
Y1  - 2019///
KW  -  Cardiotocography
KW  -  Computer-assisted
KW  -  Fetal hypoxia
KW  -  Fetal monitoring
KW  -  Intrapartum risk factors
KW  -  Neonatal acidaemia
KW  -  Neonatal acidosis
KW  -  Neonatal outcome
KW  - Cord blood gas analysis
JF  - Computers in Biology and Medicine
VL  - 115
SP  - 103525
EP  - 103525
DO  - https://doi.org/10.1016/j.compbiomed.2019.103525
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519303841
N2  - Objective
To identify clinical parameters and intrapartum fetal heart rate parameters associated with a risk of umbilical cord acidosis at birth, using an automated analysis method based on empirical mode decomposition.
Methods
Our single-center study included 381 cases (arterial cord blood pH at birth pHa ≤7.15) and 1860 controls (pHa ≥7.25) extracted from a database comprising 8,383 full datasets for over-18 mothers after vaginal or caesarean non-twin, non-breech deliveries at term (>37 weeks of amenorrhea). The analysis of a 120-min period of the FHR recording (before maternal pushing or the decision to perform a caesarean section during labor) led to the extraction of morphological, frequency-related, and long- and short-term heart rate variability variables. After univariate analyses, sparse partial least square selection and logistic regression were applied.
Results
Several clinical factors were predictive of fetal acidosis in a multivariate analysis: nulliparity (odds ratio (OR) 95% confidence interval (CI)]: 1.769 [1.362–2.300]), a male fetus (1.408 [1.097–1.811]), and the term of the pregnancy (1.333 [1.189–1.497]). The risk of acidosis increased with the time interval between the end of the FHR recording and the delivery (OR [95%CI] for a 1-min increment: 1.022 [1.012–1.031]). The risk factors related to the FHR signal were mainly the difference between the mean baseline and the mean FHR (OR [95%CI]: 1.292 [1.174–1.424]), the baseline range (1.027 [1.014–1.040]), fetal bradycardia (1.038 [1.003–1.075]) and the late deceleration area (1.002 [1.000–1.005]). The area under the curve for the multivariate model was 0.79 [0.76; 0.81].
Conclusion
In addition to clinical predictors, the automated FHR analysis highlighted other significant predictors, such as the baseline range, the instability of the FHR signal and the late deceleration area. This study further extends the routine application of automated FHR analysis during labor and, ultimately, contributes to the development of predictive scores for fetal acidosis.
ER  - 
TY  - JOUR
T1  - Randomly selected decision tree for test-cost sensitive learning
A1  - Qiu, Chen
A1  - Jiang, Liangxiao
A1  - Li, Chaoqun
Y1  - 2017///
KW  -  Classification accuracy
KW  -  Random selection
KW  -  Test cost
KW  -  Test-cost sensitive learning
KW  - Decision tree learning
JF  - Applied Soft Computing
VL  - 53
SP  - 27
EP  - 33
DO  - https://doi.org/10.1016/j.asoc.2016.12.047
UR  - https://www.sciencedirect.com/science/article/pii/S1568494616306767
N2  - In many real-world applications, decision trees that take account of the cost of acquiring attributes for decision making have been the research focuses. The decision-making process must learn which sequence to perform, and how to build an inexpensive and reliable inductive learning model to accomplish its task. Many previous works in the area of test-cost sensitive decision tree learning have successfully reduced the total test cost, unfortunately also degraded the classification accuracy simultaneously. This paper works on a new idea, i.e., it does not has to reduce the total test cost at the cost of the loss of classification accuracy. For that, we propose a multi-target adaptive attribute selection measure and a simple but effective method for building and testing decision trees. Instead of using a greedy attribute selection measure like many other decision tree learning algorithms, our algorithm uses a random attribute selection measure to find an appropriate attribute to test at each node in the tree. Specifically, we conduct a random search through the whole space of attributes in tree building, and we call the resulting model randomly selected decision tree (RSDT). By this way, RSDT significantly reduces the total test cost, yet at the same time maintains the higher classification accuracy compared to its competitors. The experimental results on 36 UCI datasets validate the effectiveness of our proposed RSDT.
ER  - 
TY  - JOUR
T1  - Computing DIT from energy expenditure measures in a respiratory chamber: a direct modeling method
A1  - Marino, S
A1  - De Gaetano, A
A1  - Giancaterini, A
A1  - Giordano, D
A1  - Manco, M
A1  - Greco, A V
A1  - Mingrone, G
Y1  - 2002///
KW  - Diet Induced Thermogenesis
KW  - Energy Expenditure
KW  - Mathematical Model
KW  - Obesity
JF  - Computers in Biology and Medicine
VL  - 32
IS  - 4
SP  - 297
EP  - 309
DO  - https://doi.org/10.1016/S0010-4825(02)00007-0
UR  - https://www.sciencedirect.com/science/article/pii/S0010482502000070
N2  - The possibility of computing Diet Induced Thermogenesis (DIT) is an important feature of metabolic investigations. However, methodological problems have affected the determination of DIT in the indirect calorimetric chamber. DIT has been commonly estimated by regressing energy expenditure on a measure of physical activity. Although used for many years as the only feasible approach to calculate DIT in a respiratory chamber, this traditional method has been criticized because of an apparent underestimation of the DIT, but no alternative method has been suggested so far. The present work proposes to estimate DIT directly by means of a mathematical model. This approach also allows to simultaneously estimate other parameters, namely resting energy expenditure (REE), physical activity (PA) and physical exercise (PE).
ER  - 
TY  - JOUR
T1  - A framework for hierarchical division of retinal vascular networks
A1  - Yu, Linfang
A1  - Qin, Zhen
A1  - Zhuang, Tianming
A1  - Ding, Yi
A1  - Qin, Zhiguang
A1  - Raymond Choo, Kim-Kwang
Y1  - 2020///
KW  -  Bifurcations
KW  -  Classification
KW  -  Framework
KW  -  Hierarchical division
KW  - Retinal vessels
JF  - Neurocomputing
VL  - 392
SP  - 221
EP  - 232
DO  - https://doi.org/10.1016/j.neucom.2018.11.113
UR  - https://www.sciencedirect.com/science/article/pii/S092523121930476X
N2  - Human retinal vascular network plays an important role in ophthalmology diagnosis. For example, in the diagnosis of ophthalmology, the severity of the disease has a direct correlation with the lesion location, in the sense that the closer the lesion area is to the optic disk, the higher will be the severity of the disease. If a framework is able to provide the hierarchical structure of the retinal vascular network, then the severity of disease can be quantified by leveraging the hierarchical characteristics of vessels in the vicinity of the lesion location. Thus, in this paper, an executable framework is recommended for the hierarchical division of the retinal vascular networks. Specifically, a supervised method based on deep neural network is used for retinal blood vessel segmentation. A graph-based method is also applied to generate vascular trees from the segmented retinal vessels. As part of our proposed approach, we present two algorithms: the potential landmark detection algorithm (PLDA) is used to identify the bifurcations and crossings; and the adaptive hierarchical classification algorithm (AHCA) is used in the hierarchical characteristics classification of vascular bifurcations. By classifying the hierarchical characteristics of vascular bifurcation, the hierarchical characteristics of the vessel segments containing these bifurcations are identified. Thus, the hierarchical division of retinal vascular network is realized. When applied to two publicly available datasets, DRIVE and STARE, the proposed framework achieves an accurate rate of 98.99% and sensitivity rate of 92.17%.
ER  - 
TY  - JOUR
T1  - Continuous blood pressure estimation based on multiple parameters from eletrocardiogram and photoplethysmogram by Back-propagation neural network
A1  - Xu, Zhihong
A1  - Liu, Jiexin
A1  - Chen, Xianxiang
A1  - Wang, Yilong
A1  - Zhao, Zhan
Y1  - 2017///
KW  -  Back-Propagation neural network
KW  -  Multi-parameter fusion
KW  -  Pulse transit time (PTT)
KW  - Continuous blood pressure
JF  - Computers in Industry
VL  - 89
SP  - 50
EP  - 59
DO  - https://doi.org/10.1016/j.compind.2017.04.003
UR  - https://www.sciencedirect.com/science/article/pii/S0166361516303219
N2  - The cuff-less continuous blood pressure monitoring provides reliable and invaluable information about the individuals’ health condition. Conventional sphygmomanometer with a cuff measures only the value of the blood pressure intermittently and the measurement process is sometimes inconvenient. In this work, a systematic approach with multi-parameter fusion has been proposed to estimate the non-invasive beat-to-beat systolic and diastolic blood pressure with high accuracy. The methods involve real-time monitoring of the electrocardiogram (ECG) and photoplethysmogram (PPG), and extracting the R peak from the ECG and relevant feature parameters from the synchronous PPG. Also, it covers the creation of the topological model of back-propagation neural network that has fifteen neurons in the input layer, ten neurons in the single interlayer, and two neurons in the output layer, where all the neurons are fully connected. As for the results, the proposed method was validated on the volunteers. The reference blood pressure (BP) is from Finometer (MIDI, Finapres Medical System, Netherlands). The results showed that the mean±S.D. for the estimated systolic BP (SBP) and diastolic BP (DBP) with the proposed method against reference were −0.41±2.02mmHg and 0.46±2.21mmHg, respectively. Thus, the continuous blood pressure algorithm based on Back-Propagation neural network provides a continuous BP with a high accuracy.
ER  - 
TY  - JOUR
T1  - Learning prototypes and distances: A prototype reduction technique based on nearest neighbor error minimization
A1  - Paredes, Roberto
A1  - Vidal, Enrique
Y1  - 2006///
KW  - Condensing
KW  - Nearest neighbor
KW  - Weighted dissimilarity distances
JF  - Pattern Recognition
VL  - 39
IS  - 2
SP  - 180
EP  - 188
DO  - https://doi.org/10.1016/j.patcog.2005.06.001
UR  - https://www.sciencedirect.com/science/article/pii/S0031320305002232
N1  - Part Special Issue: Complexity Reduction
N2  - A prototype reduction algorithm is proposed, which simultaneously trains both a reduced set of prototypes and a suitable local metric for these prototypes. Starting with an initial selection of a small number of prototypes, it iteratively adjusts both the position (features) of these prototypes and the corresponding local-metric weights. The resulting prototypes/metric combination minimizes a suitable estimation of the classification error probability. Good performance of this algorithm is assessed through experiments with a number of benchmark data sets and with a real task consisting in the verification of images of human faces.
ER  - 
TY  - JOUR
T1  - Clinical questionnaire filling based on question answering framework
A1  - Ren, Jiangtao
A1  - Liu, Naiyin
A1  - Wu, Xiaojing
Y1  - 2020///
KW  - EHR
KW  - Information extraction
KW  - Medical text
KW  - Multi-label classification
KW  - Question answering
JF  - International Journal of Medical Informatics
VL  - 141
SP  - 104225
EP  - 104225
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104225
UR  - https://www.sciencedirect.com/science/article/pii/S1386505619310883
N2  - Background
Electronic Health Records (EHR) are the foundation of much medical research. However, analyzing the text data of EHRs directly is an challenging task. Therefore, physicians often use questionnaires to first convert text data to structured data. Filling in these questionnaires requires a considerable amount of time and medical knowledge. It is a significant task to develop an algorithm to make computers fill out these questionnaires automatically.
Objective
This research aims to build a deep learning model that can automatically complete questionnaires with given medical text.
Methods
This task is a part of Information Extraction (IE), but it differs from the existing tasks of medical IE. Because of the questions in questionnaires are closed-end type, which refers to making a selection among given options, we could treat this task as a classification problem. However, conventional classification algorithms are resource-consuming when filling out one entire questionnaire with one model. They also could not use the question information to guide the questionnaire filling task. To handle these issues, we propose a neural network model based on question answering (QA) framework in this paper. With this framework, our neural network can fill out one complete questionnaire using only one model.
Results
We perform experiments on three real-world Chinese medical datasets and related clinical questionnaires. Our model respectively achieves F1 scores of 0.9273, 0.8834, and 0.9846. The results outperform several baseline models.
Conclusion
The strong performance of our QA model will allow us to build a system which can help physicians to fill out questionnaires and convert text data to structured data. This system can reduce the workload of physicians.
ER  - 
TY  - JOUR
T1  - Physical activity classification in free-living conditions using smartphone accelerometer data and exploration of predicted results
A1  - Lee, Kangjae
A1  - Kwan, Mei-Po
Y1  - 2018///
KW  -  Accelerometer data
KW  -  GIS
KW  -  GPS trajectory
KW  -  Machine learning
KW  - Physical activity classification
JF  - Computers, Environment and Urban Systems
VL  - 67
SP  - 124
EP  - 131
DO  - https://doi.org/10.1016/j.compenvurbsys.2017.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S0198971517301977
N2  - In recent decades, decreasing physical activity has emerged as one of the major issues affecting human health since people increasingly engaged in sedentary behavior in their homes and workplaces. In physical activity research, using GPS trajectories and advanced GIS methods has a potential for greatly enhancing our understanding of the association between objectively measured moderate and vigorous physical activity and physical and social environments. Relying only on objectively measured physical activity intensity, however, ignores the role of different places and types of physical activity on people's health outcomes. The aim of this study is to propose an approach to classifying physical activity in free-living conditions for physical activity research using published smartphone accelerometer data. Random forest and gradient boosting are used to predict jogging, walking, sitting, and standing. Generated training models based on the two classifiers are tested on accelerometer data collected from the smartphones of two subjects in free-living conditions. GPS trajectories with predicted physical activity labels are visually explored on a map to offer new insight on the assessment of the predicted results of daily activities and the identification of any difference in the results between random forest and gradient boosting. The findings of this study indicate that random forest and gradient boosting enable accurate physical activity classification in free-living conditions. GPS trajectories linked with predicted labels on a map assist the visual exploration of the erroneous prediction in daily activities including in-vehicle activities.
ER  - 
TY  - JOUR
T1  - A multi-scale tensor voting approach for small retinal vessel segmentation in high resolution fundus images
A1  - Christodoulidis, Argyrios
A1  - Hurtut, Thomas
A1  - Tahar, Houssem Ben
A1  - Cheriet, Farida
Y1  - 2016///
KW  - Diabetic retinopathy
KW  - Fundus imaging
KW  - Multi-scale line detection
KW  - Perceptual organization
KW  - Retinal blood vessel segmentation
JF  - Computerized Medical Imaging and Graphics
VL  - 52
SP  - 28
EP  - 43
DO  - https://doi.org/10.1016/j.compmedimag.2016.06.001
UR  - https://www.sciencedirect.com/science/article/pii/S0895611116300490
N2  - Segmenting the retinal vessels from fundus images is a prerequisite for many CAD systems for the automatic detection of diabetic retinopathy lesions. So far, research efforts have concentrated mainly on the accurate localization of the large to medium diameter vessels. However, failure to detect the smallest vessels at the segmentation step can lead to false positive lesion detection counts in a subsequent lesion analysis stage. In this study, a new hybrid method for the segmentation of the smallest vessels is proposed. Line detection and perceptual organization techniques are combined in a multi-scale scheme. Small vessels are reconstructed from the perceptual-based approach via tracking and pixel painting. The segmentation was validated in a high resolution fundus image database including healthy and diabetic subjects using pixel-based as well as perceptual-based measures. The proposed method achieves 85.06% sensitivity rate, while the original multi-scale line detection method achieves 81.06% sensitivity rate for the corresponding images (p<0.05). The improvement in the sensitivity rate for the database is 6.47% when only the smallest vessels are considered (p<0.05). For the perceptual-based measure, the proposed method improves the detection of the vasculature by 7.8% against the original multi-scale line detection method (p<0.05).
ER  - 
TY  - JOUR
T1  - Construction of a smart management system for physical health based on IoT and cloud computing with big data
A1  - Zhang, Ning
A1  - Zhang, Chenfei
A1  - Wu, Dengpan
Y1  - 2021///
KW  -  Big data
KW  -  Cloud computing
KW  -  Physical health
KW  -  Smart management
KW  - Internet of Things
JF  - Computer Communications
VL  - 179
SP  - 183
EP  - 194
DO  - https://doi.org/10.1016/j.comcom.2021.08.018
UR  - https://www.sciencedirect.com/science/article/pii/S0140366421003121
N2  - In response to the needs of physical health data management in the context of the Internet of Everything, this article first uses cloud computing, big data, mobile Internet and other technologies to build a physical health smart management system. When the system is deployed, edge nodes are introduced in each data collection area, and the system is composed of data collection, transmission, and query and analysis modules. Secondly, it uses convolutional neural network to learn features from body measurement data unsupervised. Then, based on the Gaussian mixture distribution, a three-level physical fitness assessment model was established. Finally, input the learned features into the evaluation model to get the result of physical fitness evaluation. The results show that the system not only has a better response to the family, but also can reduce operating costs and improve work efficiency. Moreover, the algorithm in this paper is not affected by individual physical fitness assessment methods and results, and provides new ideas and methods for physical fitness assessment.
ER  - 
TY  - JOUR
T1  - Machine learning techniques to discover genes with potential prognosis role in Alzheimer’s disease using different biological sources
A1  - Martínez-Ballesteros, María
A1  - García-Heredia, José M
A1  - Nepomuceno-Chamorro, Isabel A
A1  - Riquelme-Santos, José C
Y1  - 2017///
KW  - Alzheimer’s disease
KW  - Association rules
KW  - Biological knowledge integration
KW  - Ensemble learning
KW  - Gene expression profiles
KW  - Statistical significant genes
JF  - Information Fusion
VL  - 36
SP  - 114
EP  - 129
DO  - https://doi.org/10.1016/j.inffus.2016.11.005
UR  - https://www.sciencedirect.com/science/article/pii/S1566253516301300
N2  - Alzheimer’s disease is a complex progressive neurodegenerative brain disorder, being its prevalence expected to rise over the next decades. Unconventional strategies for elucidating the genetic mechanisms are necessary due to its polygenic nature. In this work, the input information sources are five: a public DNA microarray that measures expression levels of control and patient samples, repositories of known genes associated to Alzheimer’s disease, additional data, Gene Ontology and finally, a literature review or expert knowledge to validate the results. As methodology to identify genes highly related to this disease, we present the integration of three machine learning techniques: particularly, we have used decision trees, quantitative association rules and hierarchical cluster to analyze Alzheimer’s disease gene expression profiles to identify genes highly linked to this neurodegenerative disease, through changes in their expression levels between control and patient samples. We propose an ensemble of decision trees and quantitative association rules to find the most suitable configurations of the multi-objective evolutionary algorithm GarNet, in order to overcome the complex parametrization intrinsic to this type of algorithms. To fulfill this goal, GarNet has been executed using multiple configuration settings and the well-known C4.5 has been used to find the minimum accuracy to be satisfied. Then, GarNet is rerun to identify dependencies between genes and their expression levels, so we are able to distinguish between healthy individuals and Alzheimer’s patients using the configurations that overcome the minimum threshold of accuracy defined by C4.5 algorithm. Finally, a hierarchical cluster analysis has been used to validate the obtained gene-Alzheimer’s Disease associations provided by GarNet. The results have shown that the obtained rules were able to successfully characterize the underlying information, grouping relevant genes for Alzheimer Disease. The genes reported by our approach provided two well defined groups that perfectly divided the samples between healthy and Alzheimer’s Disease patients. To prove the relevance of the obtained results, a statistical test and gene expression fold-change were used. Furthermore, this relevance has been summarized in a volcano plot, showing two clearly separated and significant groups of genes that are up or down-regulated in Alzheimer’s Disease patients. A biological knowledge integration phase was performed based on the information fusion of systematic literature review, enrichment Gene Ontology terms for the described genes found in the hippocampus of patients. Finally, a validation phase with additional data and a permutation test is carried out, being the results consistent with previous studies.
ER  - 
TY  - JOUR
T1  - Ambulatory measurement of cortisol: Where do we stand, and which way to follow?
A1  - Hogenelst, Koen
A1  - Soeter, Marieke
A1  - Kallen, Victor
Y1  - 2019///
JF  - Sensing and Bio-Sensing Research
VL  - 22
SP  - 100249
EP  - 100249
DO  - https://doi.org/10.1016/j.sbsr.2018.100249
UR  - https://www.sciencedirect.com/science/article/pii/S221418041830076X
N2  - Accumulating evidence supports the harmful effects of stress on health, including the development and progress of psychopathology (e.g. anxiety disorders), metabolic disorders (e.g. diabetes type II), inflammatory disturbances, and cardiovascular disease. These harmful effects are often expressed as disturbances in cortisol levels, patterns, or responses. Unfortunately, at present, cortisol assessment is only performed in the laboratory. This hinders rapid quantification, let alone being determined by individuals themselves, with self-testing devices or sensors. More accurate and timely detection of cortisol may have important implications for the prevention, diagnosis, and treatment of stress-related disorders as well as for those suffering from adrenal insufficiencies. The present review provides an overview of the most promising and challenging technologies for cortisol measurement. An important first conclusion might be that almost all reviewed technologies were at the proof-of-concept stage, meaning it was premature to interpret the findings in light of regulatory requirements for in vitro diagnostics. Nevertheless, several promising proto-types, including electrochemical sensors with wearable potential, were found and are consequently discussed. Overall the findings suggest that with significant additional investments and research efforts in the coming years, accurate, rapid, and repeated cortisol assessment in everyday life can become reality.
ER  - 
TY  - JOUR
T1  - Exudate-based diabetic macular edema detection in fundus images using publicly available datasets
A1  - Giancardo, Luca
A1  - Meriaudeau, Fabrice
A1  - Karnowski, Thomas P
A1  - Li, Yaqin
A1  - Garg, Seema
A1  - Tobin, Kenneth W
A1  - Chaum, Edward
Y1  - 2012///
KW  - Automatic diagnosis
KW  - Exudates segmentation
KW  - Feature extraction
KW  - Lesion probability
KW  - Wavelets
JF  - Medical Image Analysis
VL  - 16
IS  - 1
SP  - 216
EP  - 226
DO  - https://doi.org/10.1016/j.media.2011.07.004
UR  - https://www.sciencedirect.com/science/article/pii/S1361841511001010
N2  - Diabetic macular edema (DME) is a common vision threatening complication of diabetic retinopathy. In a large scale screening environment DME can be assessed by detecting exudates (a type of bright lesions) in fundus images. In this work, we introduce a new methodology for diagnosis of DME using a novel set of features based on colour, wavelet decomposition and automatic lesion segmentation. These features are employed to train a classifier able to automatically diagnose DME through the presence of exudation. We present a new publicly available dataset with ground-truth data containing 169 patients from various ethnic groups and levels of DME. This and other two publicly available datasets are employed to evaluate our algorithm. We are able to achieve diagnosis performance comparable to retina experts on the MESSIDOR (an independently labelled dataset with 1200 images) with cross-dataset testing (e.g., the classifier was trained on an independent dataset and tested on MESSIDOR). Our algorithm obtained an AUC between 0.88 and 0.94 depending on the dataset/features used. Additionally, it does not need ground truth at lesion level to reject false positives and is computationally efficient, as it generates a diagnosis on an average of 4.4s (9.3s, considering the optic nerve localisation) per image on an 2.6GHz platform with an unoptimised Matlab implementation.
ER  - 
TY  - JOUR
T1  - Multiscale sequential convolutional neural networks for simultaneous detection of fovea and optic disc
A1  - Al-Bander, Baidaa
A1  - Al-Nuaimy, Waleed
A1  - Williams, Bryan M
A1  - Zheng, Yalin
Y1  - 2018///
KW  -  Convlutional neural networks
KW  -  Fovea detection
KW  -  Optic disc detection
KW  - Diabetes
JF  - Biomedical Signal Processing and Control
VL  - 40
SP  - 91
EP  - 101
DO  - https://doi.org/10.1016/j.bspc.2017.09.008
UR  - https://www.sciencedirect.com/science/article/pii/S1746809417302173
N2  - Detecting the locations of the optic disc and fovea is a crucial task towards developing automatic diagnosis and screening tools for retinal disease. We propose to address this challenging problem by investigating the potential of applying deep learning techniques to this field. In the proposed method, simultaneous detection of the centers of the fovea and the optic disc (OD) from color fundus images is considered as a regression problem. A deep multiscale sequential convolutional neural network (CNN) is designed and trained. The publically available MESSIDOR and Kaggle datasets are used to train the network and evaluate its performance. The centers of the fovea and the OD in each image were marked by expert graders as the ground truth. The proposed method achieves an accuracy of 97%, 96.7% for the detection of the OD center and 96.6%, 95.6% for the detection of the foveal center of the MESSIDOR and Kaggle test sets respectively. Our promising results demonstrate the excellent performance of the proposed CNNs in simultaneously detecting the centers of both the fovea and OD without human intervention or handcrafted features. Moreover, we can localize the landmarks of an image in 0.007s. This approach could be used as a crucial part of automated diagnosis systems for better management of eye disease.
ER  - 
TY  - JOUR
T1  - Explainable decision support through the learning and visualization of preferences from a formal ontology of antibiotic treatments
A1  - Lamy, Jean-Baptiste
A1  - Sedki, Karima
A1  - Tsopra, Rosy
Y1  - 2020///
KW  -  Antibiotics
KW  -  Explainable artificial intelligence
KW  -  Ontologies
KW  -  Preference learning
KW  -  Preference visualization
KW  - Clinical decision support system
JF  - Journal of Biomedical Informatics
VL  - 104
SP  - 103407
EP  - 103407
DO  - https://doi.org/10.1016/j.jbi.2020.103407
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300356
N2  - The aim of eXplainable Artificial Intelligence (XAI) is to design intelligent systems that can explain their predictions or recommendations to humans. Such systems are particularly desirable for therapeutic decision support, because physicians need to understand rcommendations to have confidence in their application and to adapt them if required, e.g. in case of patient contraindication. We propose here an explainable and visual approach for decision support in antibiotic treatment, based on an ontology. There were three steps to our method. We first generated a tabular dataset from the ontology, containing features defined on various domains and n-ary features. A preference model was then learned from patient profiles, antibiotic features and expert recommendations found in clinical practice guidelines. This model made the implicit rationale of the expert explicit, including the way in which missing data was treated. We then visualized the preference model and its application to all antibiotics available on the market for a given clinical situation, using rainbow boxes, a recently developed technique for set visualization. The resulting preference model had an error rate of 3.5% on the learning data, and 5.2% on test data (10-fold validation). These findings suggest that our system can help physicians to prescribe antibiotics correctly, even for clinical situations not present in the guidelines (e.g. due to allergies or contraindications for the recommended treatment).
ER  - 
TY  - JOUR
T1  - A review of applications in federated learning
A1  - Li, Li
A1  - Fan, Yuxi
A1  - Tse, Mike
A1  - Lin, Kuo-Yi
Y1  - 2020///
KW  - Citation analysis
KW  - Federated learning
KW  - Literature review
KW  - Research front
JF  - Computers & Industrial Engineering
VL  - 149
SP  - 106854
EP  - 106854
DO  - https://doi.org/10.1016/j.cie.2020.106854
UR  - https://www.sciencedirect.com/science/article/pii/S0360835220305532
N2  - Federated Learning (FL) is a collaboratively decentralized privacy-preserving technology to overcome challenges of data silos and data sensibility. Exactly what research is carrying the research momentum forward is a question of interest to research communities as well as industrial engineering. This study reviews FL and explores the main evolution path for issues exist in FL development process to advance the understanding of FL. This study aims to review prevailing application in industrial engineering to guide for the future landing application. This study also identifies six research fronts to address FL literature and help advance our understanding of FL for future optimization. This study contributes to conclude application in industrial engineering and computer science and summarize a review of applications in FL.
ER  - 
TY  - JOUR
T1  - Secure large-scale genome data storage and query
A1  - Chen, Luyao
A1  - Aziz, Md Momin
A1  - Mohammed, Noman
A1  - Jiang, Xiaoqian
Y1  - 2018///
KW  - Genome data storage Neo4j
KW  - Graph database
KW  - Homomorphic encryption
KW  - Secure computation on genome data
KW  - Secure genome data storage
JF  - Computer Methods and Programs in Biomedicine
VL  - 165
SP  - 129
EP  - 137
DO  - https://doi.org/10.1016/j.cmpb.2018.08.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718303031
N2  - Background and Objective
Cloud computing plays a vital role in big data science with its scalable and cost-efficient architecture. Large-scale genome data storage and computations would benefit from using these latest cloud computing infrastructures, to save cost and speedup discoveries. However, due to the privacy and security concerns, data owners are often disinclined to put sensitive data in a public cloud environment without enforcing some protective measures. An ideal solution is to develop secure genome database that supports encrypted data deposition and query.
Methods
Nevertheless, it is a challenging task to make such a system fast and scalable enough to handle real-world demands providing data security as well. In this paper, we propose a novel, secure mechanism to support secure count queries on an open source graph database (Neo4j) and evaluated the performance on a real-world dataset of around 735,317 Single Nucleotide Polymorphisms (SNPs). In particular, we propose a new tree indexing method that offers constant time complexity (proportion to the tree depth), which was the bottleneck of existing approaches.
Results
The proposed method significantly improves the runtime of query execution compared to the existing techniques. It takes less than one minute to execute an arbitrary count query on a dataset of 212 GB, while the best-known algorithm takes around 7 min.
Conclusions
The outlined framework and experimental results show the applicability of utilizing graph database for securely storing large-scale genome data in untrusted environment. Furthermore, the crypto-system and security assumptions underlined are much suitable for such use cases which be generalized in future work.
ER  - 
TY  - JOUR
T1  - Applying natural language processing techniques to develop a task-specific EMR interface for timely stroke thrombolysis: A feasibility study
A1  - Sung, Sheng-Feng
A1  - Chen, Kuanchin
A1  - Wu, Darren Philbert
A1  - Hung, Ling-Chien
A1  - Su, Yu-Hsiang
A1  - Hu, Ya-Han
Y1  - 2018///
KW  -  Electronic medical record
KW  -  Intravenous thrombolysis
KW  -  Natural language processing
KW  - Acute ischemic stroke
JF  - International Journal of Medical Informatics
VL  - 112
SP  - 149
EP  - 157
DO  - https://doi.org/10.1016/j.ijmedinf.2018.02.005
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618300376
N2  - Objective
To reduce errors in determining eligibility for intravenous thrombolytic therapy (IVT) in stroke patients through use of an enhanced task-specific electronic medical record (EMR) interface powered by natural language processing (NLP) techniques.
Materials and methods
The information processing algorithm utilized MetaMap to extract medical concepts from IVT eligibility criteria and expanded the concepts using the Unified Medical Language System Metathesaurus. Concepts identified from clinical notes by MetaMap were compared to those from IVT eligibility criteria. The task-specific EMR interface displays IVT-relevant information by highlighting phrases that contain matched concepts. Clinical usability was assessed with clinicians staffing the acute stroke team by comparing user performance while using the task-specific and the current EMR interfaces.
Results
The algorithm identified IVT-relevant concepts with micro-averaged precisions, recalls, and F1 measures of 0.998, 0.812, and 0.895 at the phrase level and of 1, 0.972, and 0.986 at the document level. Users using the task-specific interface achieved a higher accuracy score than those using the current interface (91% versus 80%, p = 0.016) in assessing the IVT eligibility criteria. The completion time between the interfaces was statistically similar (2.46 min versus 1.70 min, p = 0.754).
Discussion
Although the information processing algorithm had room for improvement, the task-specific EMR interface significantly reduced errors in assessing IVT eligibility criteria.
Conclusion
The study findings provide evidence to support an NLP enhanced EMR system to facilitate IVT decision-making by presenting meaningful and timely information to clinicians, thereby offering a new avenue for improvements in acute stroke care.
ER  - 
TY  - JOUR
T1  - Semantic enrichment for medical ontologies
A1  - Lee, Yugyung
A1  - Geller, James
Y1  - 2006///
KW  -  Controlled Medical Vocabularies
KW  -  Ontology
KW  -  Semantic Web
KW  -  Semantic enrichment
KW  -  Semantics
KW  -  Two-level ontology
KW  -  Unified Medical Language System
KW  - Terminology
JF  - Journal of Biomedical Informatics
VL  - 39
IS  - 2
SP  - 209
EP  - 226
DO  - https://doi.org/10.1016/j.jbi.2005.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046405000742
N2  - The Unified Medical Language System (UMLS) contains two separate but interconnected knowledge structures, the Semantic Network (upper level) and the Metathesaurus (lower level). In this paper, we have attempted to work out better how the use of such a two-level structure in the medical field has led to notable advances in terminologies and ontologies. However, most ontologies and terminologies do not have such a two-level structure. Therefore, we present a method, called semantic enrichment, which generates a two-level ontology from a given one-level terminology and an auxiliary two-level ontology. During semantic enrichment, concepts of the one-level terminology are assigned to semantic types, which are the building blocks of the upper level of the auxiliary two-level ontology. The result of this process is the desired new two-level ontology. We discuss semantic enrichment of two example terminologies and how we approach the implementation of semantic enrichment in the medical domain. This implementation performs a major part of the semantic enrichment process with the medical terminologies, with difficult cases left to a human expert.
ER  - 
TY  - JOUR
T1  - Machine learning-based prognostic modeling using clinical data and quantitative radiomic features from chest CT images in COVID-19 patients
A1  - Shiri, Isaac
A1  - Sorouri, Majid
A1  - Geramifar, Parham
A1  - Nazari, Mostafa
A1  - Abdollahi, Mohammad
A1  - Salimi, Yazdan
A1  - Khosravi, Bardia
A1  - Askari, Dariush
A1  - Aghaghazvini, Leila
A1  - Hajianfar, Ghasem
A1  - Kasaeian, Amir
A1  - Abdollahi, Hamid
A1  - Arabi, Hossein
A1  - Rahmim, Arman
A1  - Radmard, Amir Reza
A1  - Zaidi, Habib
Y1  - 2021///
KW  -  Computed tomography (CT)
KW  -  Modeling
KW  -  Prognosis
KW  -  Radiomics
KW  - COVID-19
JF  - Computers in Biology and Medicine
VL  - 132
SP  - 104304
EP  - 104304
DO  - https://doi.org/10.1016/j.compbiomed.2021.104304
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521000986
N2  - Objective
To develop prognostic models for survival (alive or deceased status) prediction of COVID-19 patients using clinical data (demographics and history, laboratory tests, visual scoring by radiologists) and lung/lesion radiomic features extracted from chest CT images.
Methods
Overall, 152 patients were enrolled in this study protocol. These were divided into 106 training/validation and 46 test datasets (untouched during training), respectively. Radiomic features were extracted from the segmented lungs and infectious lesions separately from chest CT images. Clinical data, including patients’ history and demographics, laboratory tests and radiological scores were also collected. Univariate analysis was first performed (q-value reported after false discovery rate (FDR) correction) to determine the most predictive features among all imaging and clinical data. Prognostic modeling of survival was performed using radiomic features and clinical data, separately or in combination. Maximum relevance minimum redundancy (MRMR) and XGBoost were used for feature selection and classification. The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC), sensitivity, specificity, and accuracy were used to assess the prognostic performance of the models on the test datasets.
Results
For clinical data, cancer comorbidity (q-value < 0.01), consciousness level (q-value < 0.05) and radiological score involved zone (q-value < 0.02) were found to have high correlated features with outcome. Oxygen saturation (AUC = 0.73, q-value < 0.01) and Blood Urea Nitrogen (AUC = 0.72, q-value = 0.72) were identified as high clinical features. For lung radiomic features, SAHGLE (AUC = 0.70) and HGLZE (AUC = 0.67) from GLSZM were identified as most prognostic features. Amongst lesion radiomic features, RLNU from GLRLM (AUC = 0.73), HGLZE from GLSZM (AUC = 0.73) had the highest performance. In multivariate analysis, combining lung, lesion and clinical features was determined to provide the most accurate prognostic model (AUC = 0.95 ± 0.029 (95%CI: 0.95–0.96), accuracy = 0.88 ± 0.046 (95% CI: 0.88–0.89), sensitivity = 0.88 ± 0.066 (95% CI = 0.87–0.9) and specificity = 0.89 ± 0.07 (95% CI = 0.87–0.9)).
Conclusion
Combination of radiomic features and clinical data can effectively predict outcome in COVID-19 patients. The developed model has significant potential for improved management of COVID-19 patients.
ER  - 
TY  - JOUR
T1  - Dynamic mortality prediction using machine learning techniques for acute cardiovascular cases
A1  - Metsker, Oleg
A1  - Sikorsky, Sergey
A1  - Yakovlev, Aleksey
A1  - Kovalchuk, Sergey
Y1  - 2018///
KW  -  Acute Coronary Syndrome
KW  -  Cardiology
KW  -  Decision Support
KW  -  Treatment Process Analysis
KW  - Machine Learning
JF  - Procedia Computer Science
VL  - 136
SP  - 351
EP  - 358
DO  - https://doi.org/10.1016/j.procs.2018.08.279
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918315850
N1  - 7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece
N2  - This paper represents the research results of applying machine learning methods for early predicting of cardiovascular patients mortality. The classification task is solved by analyzing the dynamics data from electronic health records of the patients with the acute coronary syndrome, infarction, and stable angina. Moreover, the approach for identification of model components and their connection is developed. The model structure identification assimilates the patient condition, treatment phases in treatment dynamic. It provides the prediction of better model structure on following steps and request for necessary data to improve the forecast for more informed decision-making. The dynamic data extracted directly from medical information system were analyzed, that is very close to the real process. Using machine learning methods it is possible to make an early prediction of mortality risks. The prediction of laboratory results allows saving the resources. Jointly, both can be offered to clinicians as support for accurate, reasonable saving clinical decisions with minimization risks for patient’s health. The simple lab test results like hemoglobin (HGB), red blood cells (RBC), alanine transaminase (ALT), aspartate transaminase (AST), glucose, platelet (PLT), creatinine levels are used as a predictor. Such a simple approach to solving critical tasks can make the method widely used in clinical practice. The identification of the patient groups’ individuality into account the dynamics probably can contribute to E-science.
ER  - 
TY  - JOUR
T1  - Integrating associative rule-based classification with Naïve Bayes for text classification
A1  - Hadi, Wa'el
A1  - Al-Radaideh, Qasem A
A1  - Alhawari, Samer
Y1  - 2018///
KW  -  Associative classification
KW  -  Classification
KW  -  Naive bayes
KW  - Data mining
JF  - Applied Soft Computing
VL  - 69
SP  - 344
EP  - 356
DO  - https://doi.org/10.1016/j.asoc.2018.04.056
UR  - https://www.sciencedirect.com/science/article/pii/S1568494618302564
N2  - Associative classification (AC) integrates the task of mining association rules with the classification task to increase the efficiency of the classification process. AC algorithms produce accurate classification and generate easy to understand rules. However, AC algorithms suffer from two drawbacks: the large number of classification rules, and using different pruning methods that may remove vital information to achieve the right decision. In this paper, a new hybrid AC algorithm (HAC) is proposed. HAC applies the power of the Naïve Bayes (NB) algorithm to reduce the number of classification rules and to produce several rules that represent each attribute value. Two experiments are conducted on an Arabic textual dataset and the standard Reuters-21578 datasets using six different algorithms, namely J48, NB, classification based on associations (CBA), multi-class classification based on association rules (MCAR), expert multi-class classification based on association rules (EMCAR), and fast associative classification algorithm (FACA). The results of the experiments showed that the HAC approach produced higher classification accuracy than MCAR, CBA, EMCAR, FACA, J48 and NB with gains of 3.95%, 6.58%, 3.48%, 1.18%, 5.37% and 8.05% respectively. Furthermore, on Reuters-21578 datasets, the results indicated that the HAC algorithm has an excellent and stable performance in terms of classification accuracy and F measure.
ER  - 
TY  - JOUR
T1  - A generic optimising feature extraction method using multiobjective genetic programming
A1  - Zhang, Yang
A1  - Rockett, Peter I
Y1  - 2011///
KW  - Feature extraction
KW  - Genetic programming
KW  - Multiobjective optimisation
KW  - Pattern recognition
JF  - Applied Soft Computing
VL  - 11
IS  - 1
SP  - 1087
EP  - 1097
DO  - https://doi.org/10.1016/j.asoc.2010.02.008
UR  - https://www.sciencedirect.com/science/article/pii/S1568494610000372
N2  - In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem.
ER  - 
TY  - JOUR
T1  - Theoretical analysis on feature extraction capability of class-augmented PCA
A1  - Park, Myoung Soo
A1  - Choi, Jin Young
Y1  - 2009///
KW  -  CA-PCA (class-augmented principal component analy
KW  -  Class information
KW  -  Classification
KW  -  PCA (principal component analysis)
KW  - Feature extraction
JF  - Pattern Recognition
VL  - 42
IS  - 11
SP  - 2353
EP  - 2362
DO  - https://doi.org/10.1016/j.patcog.2009.04.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320309001526
N2  - In this paper, we present a theoretical analysis on a novel supervised feature extraction method called class-augmented principal component analysis (CA-PCA), which is composed of processes for encoding the class information, augmenting the encoded information to data, and extracting features from class-augmented data by applying PCA. Through a combination of these processes, CA-PCA can extract features appropriate for classification. Our theoretical analysis aims to clarify the role of these processes and to provide an explanation on how CA-PCA can extract good features. Experimental results for various datasets are provided in order to show the validity of the proposed method for real problems. The effect of parameters on the quality of extracted features is also investigated and the rules of thumb for determining the appropriate parameters are provided.
ER  - 
TY  - JOUR
T1  - Locally adaptive multiple kernel clustering
A1  - Zhang, Lujiang
A1  - Hu, Xiaohui
Y1  - 2014///
KW  -  Kernel -means clustering
KW  -  Localized multiple kernel clustering
KW  -  Multiple kernel learning
KW  - Kernel clustering methods
JF  - Neurocomputing
VL  - 137
SP  - 192
EP  - 197
DO  - https://doi.org/10.1016/j.neucom.2013.05.064
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214002628
N1  - Advanced Intelligent Computing Theories and Methodologies
N2  - Conventional multiple kernel learning aims to construct a global combination of multiple kernels in input space. For a data set which has varying local distributions in input space, using a uniform combination of multiple kernels may not always work well. In this paper, we proposed a localized multiple kernel learning method for clustering. Instead of using a uniform combinational kernel over the whole input space, our method associates to each cluster a localized kernel. We assign to each cluster a weight vector for feature selection and combine each weight vector with a Gaussian kernel to form a unique kernel for the corresponding cluster. By optimizing the weight vector and the width parameter of Gaussian kernel jointly for each cluster, each kernel can be localized to match the data distribution of its corresponding cluster. A locally adaptive strategy based on the kernel k-means clustering is used to optimize the kernel for each cluster. We experimentally compared our methods to the kernel k-means clustering, averaged multiple kernel clustering, self-tuning spectral clustering and Variable Bandwidth Mean Shift algorithm. Experimental results demonstrate the effectiveness of our method.
ER  - 
TY  - JOUR
T1  - Orthogonal moments for determining correspondence between vessel bifurcations for retinal image registration
A1  - Patankar, Sanika S
A1  - Kulkarni, Jayant V
Y1  - 2015///
KW  - Diabetic retinopathy
KW  - Orthogonal moment invariants features
KW  - Periodic screening
KW  - Registration
KW  - Similarity transformation
JF  - Computer Methods and Programs in Biomedicine
VL  - 119
IS  - 3
SP  - 121
EP  - 141
DO  - https://doi.org/10.1016/j.cmpb.2015.02.009
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715000449
N2  - Retinal image registration is a necessary step in diagnosis and monitoring of Diabetes Retinopathy (DR), which is one of the leading causes of blindness. Long term diabetes affects the retinal blood vessels and capillaries eventually causing blindness. This progressive damage to retina and subsequent blindness can be prevented by periodic retinal screening. The extent of damage caused by DR can be assessed by comparing retinal images captured during periodic retinal screenings. During image acquisition at the time of periodic screenings translation, rotation and scale (TRS) are introduced in the retinal images. Therefore retinal image registration is an essential step in automated system for screening, diagnosis, treatment and evaluation of DR. This paper presents an algorithm for registration of retinal images using orthogonal moment invariants as features for determining the correspondence between the dominant points (vessel bifurcations) in the reference and test retinal images. As orthogonal moments are invariant to TRS; moment invariants features around a vessel bifurcation are unaltered due to TRS and can be used to determine the correspondence between reference and test retinal images. The vessel bifurcation points are located in segmented, thinned (mono pixel vessel width) retinal images and labeled in corresponding grayscale retinal images. The correspondence between vessel bifurcations in reference and test retinal image is established based on moment invariants features. Further the TRS in test retinal image with respect to reference retinal image is estimated using similarity transformation. The test retinal image is aligned with reference retinal image using the estimated registration parameters. The accuracy of registration is evaluated in terms of mean error and standard deviation of the labeled vessel bifurcation points in the aligned images. The experimentation is carried out on DRIVE database, STARE database, VARIA database and database provided by local government hospital in Pune, India. The experimental results exhibit effectiveness of the proposed algorithm for registration of retinal images.
ER  - 
TY  - JOUR
T1  - Detection of Bundle Branch Block using Adaptive Bacterial Foraging Optimization and Neural Network
A1  - Kora, Padmavthi
A1  - Kalva, Sri Rama Krishna
Y1  - 2017///
KW  -  ABFO
KW  -  Bundle Branch Block
KW  -  LMNN
KW  -  MIT–BIH Arrhythmia database
KW  - ECG
JF  - Egyptian Informatics Journal
VL  - 18
IS  - 1
SP  - 67
EP  - 74
DO  - https://doi.org/10.1016/j.eij.2016.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S1110866516300147
N2  - The medical practitioners analyze the electrical activity of the human heart so as to predict various ailments by studying the data collected from the Electrocardiogram (ECG). A Bundle Branch Block (BBB) is a type of heart disease which occurs when there is an obstruction along the pathway of an electrical impulse. This abnormality makes the heart beat irregular as there is an obstruction in the branches of heart, this results in pulses to travel slower than the usual. Our current study involved is to diagnose this heart problem using Adaptive Bacterial Foraging Optimization (ABFO) Algorithm. The Data collected from MIT/BIH arrhythmia BBB database applied to an ABFO Algorithm for obtaining best(important) feature from each ECG beat. These features later fed to Levenberg Marquardt Neural Network (LMNN) based classifier. The results show the proposed classification using ABFO is better than some recent algorithms reported in the literature.
ER  - 
TY  - JOUR
T1  - HANA: A Healthy Artificial Nutrition Analysis model during COVID-19 pandemic
A1  - Shams, Mahmoud Y
A1  - Elzeki, Omar M
A1  - Abouelmagd, Lobna M
A1  - Hassanien, Aboul Ella
A1  - Elfattah, Mohamed Abd
A1  - Salem, Hanaa
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Healthy food
KW  -  Machine learning
KW  -  Nutrition analysis
KW  -  Regression
KW  - COVID-19
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104606
EP  - 104606
DO  - https://doi.org/10.1016/j.compbiomed.2021.104606
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521004005
N2  - Background and objective
The impact of diet on COVID-19 patients has been a global concern since the pandemic began. Choosing different types of food affects peoples’ mental and physical health and, with persistent consumption of certain types of food and frequent eating, there may be an increased likelihood of death. In this paper, a regression system is employed to evaluate the prediction of death status based on food categories.
Methods
A Healthy Artificial Nutrition Analysis (HANA) model is proposed. The proposed model is used to generate a food recommendation system and track individual habits during the COVID-19 pandemic to ensure healthy foods are recommended. To collect information about the different types of foods that most of the world's population eat, the COVID-19 Healthy Diet Dataset was used. This dataset includes different types of foods from 170 countries around the world as well as obesity, undernutrition, death, and COVID-19 data as percentages of the total population. The dataset was used to predict the status of death using different machine learning regression models, i.e., linear regression (ridge regression, simple linear regularization, and elastic net regression), and AdaBoost models.
Results
The death status was predicted with high accuracy, and the food categories related to death were identified with promising accuracy. The Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and R2 metrics and 20-fold cross-validation were used to evaluate the accuracy of the prediction models for the COVID-19 Healthy Diet Dataset. The evaluations demonstrated that elastic net regression was the most efficient prediction model. Based on an in-depth analysis of recent nutrition recommendations by WHO, we confirm the same advice already introduced in the WHO report1. Overall, the outcomes also indicate that the remedying effects of COVID-19 patients are most important to people which eat more vegetal products, oilcrops grains, beverages, and cereals - excluding beer. Moreover, people consuming more animal products, animal fats, meat, milk, sugar and sweetened foods, sugar crops, were associated with a higher number of deaths and fewer patient recoveries. The outcome of sugar consumption was important and the rates of death and recovery were influenced by obesity.
Conclusions
Based on evaluation metrics, the proposed HANA model may outperform other algorithms used to predict death status. The results of this study may direct patients to eat particular types of food to reduce the possibility of becoming infected with the COVID-19 virus.
ER  - 
TY  - JOUR
T1  - Data-Driven Identification of Hypertensive Patient Profiles for Patient Population Simulation
A1  - Semakova, Anna
A1  - Zvartau, Nadezhda
Y1  - 2018///
KW  -  clustering
KW  -  data-driven identification
KW  -  electronic health records
KW  -  probabilistic modelling
KW  - Arterial hypertension
JF  - Procedia Computer Science
VL  - 136
SP  - 433
EP  - 442
DO  - https://doi.org/10.1016/j.procs.2018.08.269
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918315746
N1  - 7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece
N2  - Arterial hypertension is a chronic disease with multifactorial origin, which has a variety of developments in different patients and can progress with multiple comorbidities. This study aims to develop approach for data-driven identifying hypertensive patient profiles. A digital twin of the hypertensive patient profile will allow to lead various virtual clinical trials, that are necessary to reduce uncertainty within the conditions of clinical decision making. This study represents two approaches for probabilistic modelling of the annual average blood pressure variability based on diverse features of the patient profiles.
ER  - 
TY  - JOUR
T1  - Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry
A1  - Tai, Andy M Y
A1  - Albuquerque, Alcides
A1  - Carmona, Nicole E
A1  - Subramanieapillai, Mehala
A1  - Cha, Danielle S
A1  - Sheko, Margarita
A1  - Lee, Yena
A1  - Mansur, Rodrigo
A1  - McIntyre, Roger S
Y1  - 2019///
KW  - ADHD
KW  - AI
KW  - Algorithms
KW  - Alzheimer
KW  - Big data
KW  - DSM-5. Schizophrenia
KW  - Data mining
KW  - Decision trees
KW  - Depression
KW  - IBM Watson
KW  - MRI
KW  - Machine learning
KW  - Mental disease
KW  - Mental health
KW  - Neuro networking
KW  - Precision medicine
KW  - Psychiatry
KW  - RDoC
KW  - Random forests
KW  - Research domain criteria
KW  - Support vector machines
KW  - fMRI
JF  - Artificial Intelligence in Medicine
VL  - 99
SP  - 101704
EP  - 101704
DO  - https://doi.org/10.1016/j.artmed.2019.101704
UR  - https://www.sciencedirect.com/science/article/pii/S0933365717301781
N2  - Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.
ER  - 
TY  - JOUR
T1  - Using clustering to learn distance functions for supervised similarity assessment
A1  - Eick, Christoph F
A1  - Rouhana, Alain
A1  - Bagherjeiran, A
A1  - Vilalta, R
Y1  - 2006///
KW  -  Nearest neighbor
KW  -  Supervised clustering
KW  - Distance function learning
JF  - Engineering Applications of Artificial Intelligence
VL  - 19
IS  - 4
SP  - 395
EP  - 401
DO  - https://doi.org/10.1016/j.engappai.2006.01.004
UR  - https://www.sciencedirect.com/science/article/pii/S0952197606000303
N1  - Recent Advances in Data Mining
N2  - Assessing the similarity between objects is a prerequisite for many data mining techniques. This paper introduces a novel approach to learn distance functions that maximizes the clustering of objects belonging to the same class. Objects belonging to a data set are clustered with respect to a given distance function and the local class density information of each cluster is then used by a weight adjustment heuristic to modify the distance function so that the class density is increased in the attribute space. This process of interleaving clustering with distance function modification is repeated until a “good” distance function has been found. We implemented our approach using the k-means clustering algorithm. We evaluated our approach using seven UCI data sets for a traditional 1-nearest-neighbor (1-NN) classifier and a compressed 1-NN classifier, called NCC, that uses the learnt distance function and cluster centroids instead of all the points of a training set. The experimental results show that attribute weighting leads to statistically significant improvements in prediction accuracy over a traditional 1-NN classifier for two of the seven data sets tested, whereas using NCC significantly improves the accuracy of the 1-NN classifier for four of the seven data sets.
ER  - 
TY  - JOUR
T1  - Self-attention based recurrent convolutional neural network for disease prediction using healthcare data
A1  - Usama, Mohd
A1  - Ahmad, Belal
A1  - Xiao, Wenjing
A1  - Hossain, M Shamim
A1  - Muhammad, Ghulam
Y1  - 2020///
KW  -  Biomedicine
KW  -  Disease prediction
KW  -  Healthcare data
KW  -  Self-attention
KW  - Deep learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 190
SP  - 105191
EP  - 105191
DO  - https://doi.org/10.1016/j.cmpb.2019.105191
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719311708
N2  - Background and Objective: Nowadays computer-aided disease diagnosis from medical data through deep learning methods has become a wide area of research. Existing works of analyzing clinical text data in the medical domain, which substantiate useful information related to patients with disease in large quantity, benefits early-stage disease diagnosis. However, benefits of analysis not achieved well when the traditional rule-based and classical machine learning methods used; which are unable to handle the unstructured clinical text and only a single method is not able to handle all challenges related to the analysis of the unstructured text, Moreover, the contribution of all words in clinical text is not the same in the prediction of disease. Therefore, there is a need to develop a neural model which solve the above clinical application problems, is an interesting topic which needs to be explored. Methods:Thus considering the above problems, first, this paper present self-attention based recurrent convolutional neural network (RCNN) model using real-life clinical text data collected from a hospital in Wuhan, China. This model automatically learns high-level semantic features from clinical text by using bi-direction recurrent connection within convolution. Second, to deal with other clinical text challenges, we combine the ability of RCNN with the self-attention mechanism. Thus, self-attention gets the focus of the model on essential convolve features which have effective meaning in the clinical text by calculating the probability of each convolve feature through softmax. Results:The proposed model is evaluated on real-life hospital dataset and used measurement metrics as Accuracy and recall. Experiment results exhibit that the proposed model reaches up to accuracy 95.71%, which is better than many existing methods for cerebral infarction disease. Conclusions:This article presented the self-attention based RCNN model by combining the RCNN with self-attention mechanism for prediction of cerebral infarction disease. The obtained results show that the presented model better predict the cerebral infarction disease risk compared to many existing methods. The same model can also be used for the prediction of other disease risks.
ER  - 
TY  - JOUR
T1  - Secondary use of electronic health records for building cohort studies through top-down information extraction
A1  - Kreuzthaler, Markus
A1  - Schulz, Stefan
A1  - Berghold, Andrea
Y1  - 2015///
KW  - Clinical narrative
KW  - Information extraction
KW  - Secondary use
JF  - Journal of Biomedical Informatics
VL  - 53
SP  - 188
EP  - 195
DO  - https://doi.org/10.1016/j.jbi.2014.10.010
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414002329
N2  - Controlled clinical trials are usually supported with an in-front data aggregation system, which supports the storage of relevant information according to the trial context within a highly structured environment. In contrast to the documentation of clinical trials, daily routine documentation has many characteristics that influence data quality. One such characteristic is the use of non-standardized text, which is an indispensable part of information representation in clinical information systems. Based on a cohort study we highlight challenges for mining electronic health records targeting free text entry fields within semi-structured data sources. Our prototypical information extraction system achieved an F-measure of 0.91 (precision=0.90, recall=0.93) for the training set and an F-measure of 0.90 (precision=0.89, recall=0.92) for the test set. We analyze the obtained results in detail and highlight challenges and future directions for the secondary use of routine data in general.
ER  - 
TY  - JOUR
T1  - Automated lesion detectors in retinal fundus images
A1  - Figueiredo, I N
A1  - Kumar, S
A1  - Oliveira, C M
A1  - Ramos, J D
A1  - Engquist, B
Y1  - 2015///
KW  -  Bright lesion detector
KW  -  Cartoon+texture decomposition
KW  -  Computer-aided diagnosis
KW  -  Hemorrhage detector
KW  -  Multiscale analysis
KW  -  Retinal fundus image
KW  -  Variational segmentation
KW  -  Wavelets
KW  - Microaneurysm detector
JF  - Computers in Biology and Medicine
VL  - 66
SP  - 47
EP  - 65
DO  - https://doi.org/10.1016/j.compbiomed.2015.08.008
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515002851
N2  - Diabetic retinopathy (DR) is a sight-threatening condition occurring in persons with diabetes, which causes progressive damage to the retina. The early detection and diagnosis of DR is vital for saving the vision of diabetic persons. The early signs of DR which appear on the surface of the retina are the dark lesions such as microaneurysms (MAs) and hemorrhages (HEMs), and bright lesions (BLs) such as exudates. In this paper, we propose a novel automated system for the detection and diagnosis of these retinal lesions by processing retinal fundus images. We devise appropriate binary classifiers for these three different types of lesions. Some novel contextual/numerical features are derived, for each lesion type, depending on its inherent properties. This is performed by analysing several wavelet bands (resulting from the isotropic undecimated wavelet transform decomposition of the retinal image green channel) and by using an appropriate combination of Hessian multiscale analysis, variational segmentation and cartoon+texture decomposition. The proposed methodology has been validated on several medical datasets, with a total of 45,770 images, using standard performance measures such as sensitivity and specificity. The individual performance, per frame, of the MA detector is 93% sensitivity and 89% specificity, of the HEM detector is 86% sensitivity and 90% specificity, and of the BL detector is 90% sensitivity and 97% specificity. Regarding the collective performance of these binary detectors, as an automated screening system for DR (meaning that a patient is considered to have DR if it is a positive patient for at least one of the detectors) it achieves an average 95–100% of sensitivity and 70% of specificity at a per patient basis. Furthermore, evaluation conducted on publicly available datasets, for comparison with other existing techniques, shows the promising potential of the proposed detectors.
ER  - 
TY  - JOUR
T1  - Semi-supervised learning of the electronic health record for phenotype stratification
A1  - Beaulieu-Jones, Brett K
A1  - Greene, Casey S
Y1  - 2016///
KW  -  Denoising autoencoder
KW  -  Disease subtyping
KW  -  Electronic phenotyping
KW  -  Patient stratification
KW  -  Unsupervised
KW  - Electronic health record
JF  - Journal of Biomedical Informatics
VL  - 64
SP  - 168
EP  - 178
DO  - https://doi.org/10.1016/j.jbi.2016.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S153204641630140X
N2  - Patient interactions with health care providers result in entries to electronic health records (EHRs). EHRs were built for clinical and billing purposes but contain many data points about an individual. Mining these records provides opportunities to extract electronic phenotypes, which can be paired with genetic data to identify genes underlying common human diseases. This task remains challenging: high quality phenotyping is costly and requires physician review; many fields in the records are sparsely filled; and our definitions of diseases are continuing to improve over time. Here we develop and evaluate a semi-supervised learning method for EHR phenotype extraction using denoising autoencoders for phenotype stratification. By combining denoising autoencoders with random forests we find classification improvements across multiple simulation models and improved survival prediction in ALS clinical trial data. This is particularly evident in cases where only a small number of patients have high quality phenotypes, a common scenario in EHR-based research. Denoising autoencoders perform dimensionality reduction enabling visualization and clustering for the discovery of new subtypes of disease. This method represents a promising approach to clarify disease subtypes and improve genotype-phenotype association studies that leverage EHRs.
ER  - 
TY  - JOUR
T1  - Surrogate-assisted multi-objective model selection for support vector machines
A1  - Rosales-Pérez, Alejandro
A1  - Gonzalez, Jesus A
A1  - Coello Coello, Carlos A
A1  - Escalante, Hugo Jair
A1  - Reyes-Garcia, Carlos A
Y1  - 2015///
KW  -  Multi-objective optimization
KW  -  Support vector machines
KW  -  Surrogate-assisted optimization
KW  - Model selection
JF  - Neurocomputing
VL  - 150
SP  - 163
EP  - 172
DO  - https://doi.org/10.1016/j.neucom.2014.08.075
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214012612
N1  - Bioinspired and knowledge based techniques and applications The Vitality of Pattern Recognition and Image Analysis Data Stream Classification and Big Data Analytics
N2  - Classification is one of the most well-known tasks in supervised learning. A vast number of algorithms for pattern classification have been proposed so far. Among these, support vector machines (SVMs) are one of the most popular approaches, due to the high performance reached by these methods in a wide number of pattern recognition applications. Nevertheless, the effectiveness of SVMs highly depends on their hyper-parameters. Besides the fine-tuning of their hyper-parameters, the way in which the features are scaled as well as the presence of non-relevant features could affect their generalization performance. This paper introduces an approach for addressing model selection for support vector machines used in classification tasks. In our formulation, a model can be composed of feature selection and pre-processing methods besides the SVM classifier. We formulate the model selection problem as a multi-objective one, aiming to minimize simultaneously two components that are closely related to the error of a model: bias and variance components, which are estimated in an experimental fashion. A surrogate-assisted evolutionary multi-objective optimization approach is adopted to explore the hyper-parameters space. We adopted this approach due to the fact that estimating the bias and variance could be computationally expensive. Therefore, by using surrogate-assisted optimization, we expect to reduce the number of solutions evaluated by the fitness functions so that the computational cost would also be reduced. Experimental results conducted on benchmark datasets widely used in the literature, indicate that highly competitive models with a fewer number of fitness function evaluations are obtained by our proposal when it is compared to state of the art model selection methods.
ER  - 
TY  - JOUR
T1  - Extracting similar terms from multiple EMR-based semantic embeddings to support chart reviews
A1  - Ye, Cheng
A1  - Fabbri, Daniel
Y1  - 2018///
KW  -  Clinical similar terms
KW  -  Highlighting
KW  -  Query expansion
KW  -  Search engines
KW  -  Semantic embeddings
KW  - Electronic medical records (EMR)
JF  - Journal of Biomedical Informatics
VL  - 83
SP  - 63
EP  - 72
DO  - https://doi.org/10.1016/j.jbi.2018.05.014
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300972
N2  - Objective
Word embeddings project semantically similar terms into nearby points in a vector space. When trained on clinical text, these embeddings can be leveraged to improve keyword search and text highlighting. In this paper, we present methods to refine the selection process of similar terms from multiple EMR-based word embeddings, and evaluate their performance quantitatively and qualitatively across multiple chart review tasks.
Materials and methods
Word embeddings were trained on each clinical note type in an EMR. These embeddings were then combined, weighted, and truncated to select a refined set of similar terms to be used in keyword search and text highlighting. To evaluate their quality, we measured the similar terms’ information retrieval (IR) performance using precision-at-K (P@5, P@10). Additionally a user study evaluated users’ search term preferences, while a timing study measured the time to answer a question from a clinical chart.
Results
The refined terms outperformed the baseline method’s information retrieval performance (e.g., increasing the average P@5 from 0.48 to 0.60). Additionally, the refined terms were preferred by most users, and reduced the average time to answer a question.
Conclusions
Clinical information can be more quickly retrieved and synthesized when using semantically similar term from multiple embeddings.
ER  - 
TY  - JOUR
T1  - SiMCAL 1 algorithm for analysis of gene expression data related to the phosphatidylserine receptor
A1  - Dvorkin, Daniel
A1  - Fadok, Valerie
A1  - Cios, Krzysztof
Y1  - 2005///
KW  - Clustering
KW  - Inflammation
KW  - Jarris-Patrick
KW  - Microarray
KW  - Nearest neighbor
KW  - Nonparametric
KW  - PSR
KW  - Phosphatidylserine
KW  - SiMCAL
KW  - TGF-β
JF  - Artificial Intelligence in Medicine
VL  - 35
IS  - 1
SP  - 49
EP  - 60
DO  - https://doi.org/10.1016/j.artmed.2005.01.010
UR  - https://www.sciencedirect.com/science/article/pii/S0933365705000606
N1  - Computational Intelligence Techniques in Bioinformatics
N2  - Summary
Objective:
SiMCAL 1 (simple multilevel clustering and linking, version 1) is a novel clustering algorithm for time-series microarray data, presented here with an application to a specific data set. The purpose of the algorithm is to present a complete feature set not found in either Jarvis-Patrick clustering, from which it is derived, or in other popular clustering methods such as hierarchical and k-means. The data concern the activity of the phosphatidylserine receptor (PSR) which is believed to be a crucial molecular switch in the mediation of inflammatory response in apoptosis and lysis. By analyzing the behavior of PSR-related genes in mouse macrophages, we hope to elucidate the mechanisms involved in this important biological process.
Methods and materials:
SiMCAL 1 is implemented in the Python programming language using the Numerical Python extensions, and the data are stored using the MySQL database management system. The data are derived from exposures of multiple Affymetrix mouse gene microarray chips to elevated levels of PSR antibody and control conditions. Code and data are available at http://www.dvorkin.com/daniel/Simcal1.zip (accessed: 17 January 2005).
Results:
The algorithm meets its objectives: it is simple, in that it is computationally inexpensive; it is multilevel, in that it provides a small number of clearly defined hierarchical levels of clusters; and it offers linking between clusters at the same level in each hierarchy. Clustering and linking results indicate previously unknown co-regulation for genes expressing PGH synthase (COX2) and PGE2, appear to confirm increased production of proteins for clearance of apoptotic cells in the presence of PSR antibody, and correspond to other findings regarding the temporal relationship between PGE2 production and B cell proliferation and differentiation. These results are promising but should be taken as highly preliminary.
Conclusion:
Both the algorithm and its application to this problem show great potential for future development. We plan to improve and extend the SiMCAL family of algorithms, and to obtain new data so that the algorithm(s) may be further applied to this and other problems of interest.
ER  - 
TY  - JOUR
T1  - Optic disc detection in the presence of strong technical artifacts
A1  - Dietter, Johannes
A1  - Haq, Wadood
A1  - Ivanov, Iliya V
A1  - Norrenberg, Lars A
A1  - Völker, Michael
A1  - Dynowski, Marek
A1  - Röck, Daniel
A1  - Ziemssen, Focke
A1  - Leitritz, Martin A
A1  - Ueffing, Marius
Y1  - 2019///
KW  -  Optic disc segmentation
KW  -  Technical artifacts
KW  - Optic disc detection
JF  - Biomedical Signal Processing and Control
VL  - 53
SP  - 101535
EP  - 101535
DO  - https://doi.org/10.1016/j.bspc.2019.04.012
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419301090
N2  - The inspection of retinal fundus pictures taken by a fundus camera is one of the key procedures for diagnosing a wide range of diseases like diabetic retinopathy, hypertension or hypercranial pressure. The detection of the optic disc (OD) is of particular relevance since changes of the OD itself may indicate specific diseases. Moreover, the OD serves as a landmark for retinal image analysis. Here, we present a method to detect and segment the OD that can cope with strong technical artifacts. Conceptually building on two published methods, we developed a two-stage approach to localize and then segment the border of the OD. First, we use vessel orientation and brightness to determine the center of the OD. Second, we modify a score function from literature whose maximum indicates the pixel that possesses the best OD-border like structure around it. Using six publicly available and three in-house databases, we analyzed a total of 5052 retinal images, resulting in an average detection rate of 95.9%. Taking a selection of 61 pictures of one of our in-house datasets, we achieved an average overlap ratio of 88.8% between the OD marked by an expert and the OD determined through our algorithm.
ER  - 
TY  - JOUR
T1  - A two-phase decision support framework for the automatic screening of digital fundus images
A1  - Antal, Bálint
A1  - Hajdu, András
A1  - Maros-Szabó, Zsuzsanna
A1  - Török, Zsolt
A1  - Csutak, Adrienne
A1  - Pető, Tünde
Y1  - 2012///
KW  - Biomedical image processing
KW  - Medical decision-making
KW  - Medical expert systems
JF  - Journal of Computational Science
VL  - 3
IS  - 5
SP  - 262
EP  - 268
DO  - https://doi.org/10.1016/j.jocs.2012.01.001
UR  - https://www.sciencedirect.com/science/article/pii/S1877750312000038
N1  - Advanced Computing Solutions for Health Care and Medicine
N2  - In this paper we give a brief review on the present status of automated detection systems describe for the screening of diabetic retinopathy. We further detail an enhanced detection procedure that consists of two steps. First, a pre-screening algorithm is considered to classify the input digital fundus images based on the severity of abnormalities. If an image is found to be seriously abnormal, it will not be analysed further with robust lesion detector algorithms. As a further improvement, we introduce a novel feature extraction approach based on clinical observations. The second step of the proposed method detects regions of interest with possible lesions on the images that previously passed the pre-screening step. These regions will serve as input to the specific lesion detectors for detailed analysis. This procedure can increase the computational performance of a screening system. Experimental results show that both two steps of the proposed approach are capable to efficiently exclude a large amount of data from further processing, thus, to decrease the computational burden of the automatic screening system.
ER  - 
TY  - JOUR
T1  - Dimension reduction of multimodal data by auto-weighted local discriminant analysis
A1  - Lu, Rongxiu
A1  - Cai, Yingjie
A1  - Zhu, Jianyong
A1  - Nie, Feiping
A1  - Yang, Hui
Y1  - 2021///
KW  -  -norm minimization
KW  -  Auto-weighted
KW  -  Locality preserved
KW  -  Multimodal data
KW  - Linear discriminant analysis (LDA)
JF  - Neurocomputing
VL  - 461
SP  - 27
EP  - 40
DO  - https://doi.org/10.1016/j.neucom.2021.06.035
UR  - https://www.sciencedirect.com/science/article/pii/S0925231221009395
N2  - Dimension reduction technology is playing an increasingly important role in machine learning. One of the most important reduction technologies is linear discriminant analysis (LDA), but the main disadvantage of LDA is that it is unable to find the local manifold structure, which means that it may fail to dispose of multimodal data. However, high-dimensional multimodal data are ubiquitous in many rations. In this paper, we propose a new dimensionality reduction method called auto-weighted local discriminant analysis (ALDA). Our method learns the similarity matrix and updates in the subspace simultaneously so that the neighborships can be evaluated in the optimal subspaces instead of in the original space. Furthermore, the new model is built based on the ℓ2,1-norm and automatically assigns a small weight to the pairwise points with large distance, and vice versa; thus, the local structure information can be captured by the ALDA. Additionally, an iterative re-weighted optimization algorithm is provided to efficiently solve the proposed model. Finally, extensive experiments conducted on several benchmark datasets and some synthetic datasets demonstrate the effectiveness of ALDA when comparing with some state-of-the-art dimensionality reduction methods.
ER  - 
TY  - JOUR
T1  - Supervised sampling for networked data
A1  - Fang, Meng
A1  - Yin, Jie
A1  - Zhu, Xingquan
Y1  - 2016///
KW  - Information network
KW  - Random walks
KW  - Supervised sampling
JF  - Signal Processing
VL  - 124
SP  - 93
EP  - 102
DO  - https://doi.org/10.1016/j.sigpro.2015.09.040
UR  - https://www.sciencedirect.com/science/article/pii/S0165168415003461
N1  - Big Data Meets Multimedia Analytics
N2  - Traditional graph sampling methods reduce the size of a large network via uniform sampling of nodes from the original network. The sampled network can be used to estimate the topological properties of the original network. However, in some application domains (e.g., disease surveillance), the goal of sampling is also to help identify a specified category of nodes (e.g., affected individuals) in a large network. This work therefore aims to, given a large information network, sample a subgraph under a specific goal of acquiring as many nodes with a particular category as possible. We refer to this problem as supervised sampling, where we sample a large network for a specific category of nodes. To this end, we model a network as a Markov chain and derive supervised random walks to learn stationary distributions of the sampled network. The learned stationary distribution can help identify the best node to be sampled in the next iteration. The iterative sampling process ensures that with new sampled nodes being acquired, supervised sampling can be strengthened in turn. Experiments on synthetic as well as real-world networks show that our supervised sampling algorithm outperforms existing methods in obtaining target nodes in the sampled networks.
ER  - 
TY  - JOUR
T1  - A novel LS-SVMs hyper-parameter selection based on particle swarm optimization
A1  - Guo, X C
A1  - Yang, J H
A1  - Wu, C G
A1  - Wang, C Y
A1  - Liang, Y C
Y1  - 2008///
KW  -  Classification
KW  -  Parameter selection
KW  -  Particle swarm optimization
KW  - Least-squares support vector machines
JF  - Neurocomputing
VL  - 71
IS  - 16
SP  - 3211
EP  - 3215
DO  - https://doi.org/10.1016/j.neucom.2008.04.027
UR  - https://www.sciencedirect.com/science/article/pii/S0925231208002932
N1  - Advances in Neural Information Processing (ICONIP 2006) / Brazilian Symposium on Neural Networks (SBRN 2006)
N2  - The selection of hyper-parameters plays an important role to the performance of least-squares support vector machines (LS-SVMs). In this paper, a novel hyper-parameter selection method for LS-SVMs is presented based on the particle swarm optimization (PSO). The proposed method does not need any priori knowledge on the analytic property of the generalization performance measure and can be used to determine multiple hyper-parameters at the same time. The feasibility of this method is examined on benchmark data sets. Different kinds of kernel families are investigated by using the proposed method. Experimental results show that the best or quasi-best test performance could be obtained by using the scaling radial basis kernel function (SRBF) and RBF kernel functions, respectively.
ER  - 
TY  - JOUR
T1  - Passive detection of accelerometer-recorded fetal movements using a time–frequency signal processing approach
A1  - Boashash, B
A1  - Khlif, M S
A1  - Ben-Jabeur, T
A1  - East, C E
A1  - Colditz, P B
Y1  - 2014///
KW  -  Fetal movement
KW  -  Matched filter
KW  -  Matching pursuit
KW  -  Newborn health outcomes
KW  -  Time–frequency analysis
KW  - Accelerometer
JF  - Digital Signal Processing
VL  - 25
SP  - 134
EP  - 155
DO  - https://doi.org/10.1016/j.dsp.2013.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S1051200413002212
N2  - This paper describes a multi-sensor fetal movement (FetMov) detection system based on a time–frequency (TF) signal processing approach. Fetal motor activity is clinically useful as a core aspect of fetal screening for well-being to reduce the current high incidence of fetal deaths in the world. FetMov are present in early gestation but become more complex and sustained as the fetus progresses through gestation. A decrease in FetMov is an important element to consider for the detection of fetal compromise. Current methods of FetMov detection include maternal perception, which is known to be inaccurate, and ultrasound imaging which is intrusive and costly. An alternative passive method for the detection of FetMov uses solid-state accelerometers, which are safe and inexpensive. This paper describes a digital signal processing (DSP) based experimental approach to the detection of FetMov from recorded accelerometer signals. The paper provides an overview of the significant measurement and signal processing challenges, followed by an approach that uses quadratic time–frequency distributions (TFDs) to appropriately deal with the non-stationary nature of the signals. The paper then describes a proof-of-concept with a solution consisting of a detection method that includes (1) a new experimental set-up, (2) an improved data acquisition procedure, and (3) a TF approach for the detection of FetMov including TF matching pursuit (TFMP) decomposition and TF matched filter (TFMF) based on high-resolution quadratic TFDs. Detailed suggestions for further refinement are provided with preliminary results to establish feasibility, and considerations for application to clinical practice are reviewed.
ER  - 
TY  - JOUR
T1  - In silico identification of drug candidates against COVID-19
A1  - Wu, Yifei
A1  - Chang, Kuan Y
A1  - Lou, Lei
A1  - Edwards, Lorette G
A1  - Doma, Bly K
A1  - Xie, Zhong-Ru
Y1  - 2020///
KW  - COVID-19
KW  - Drug repurposing
KW  - Ligand-protein docking
KW  - Main protease
KW  - RNA-dependent RNA polymerase
KW  - Remdesivir
KW  - Virtual screening
JF  - Informatics in Medicine Unlocked
VL  - 21
SP  - 100461
EP  - 100461
DO  - https://doi.org/10.1016/j.imu.2020.100461
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820306110
N2  - The COVID-19 pandemic has caused unprecedented health and economic crisis throughout the world. However, there is no effective medication or therapeutic strategy for treatment of this disease currently. Here, to elucidate the inhibitory effects, we first tested binding affinities of 11 HIV-1 protease inhibitors or their pharmacoenhancers docked onto SARS-CoV-2 main protease (Mpro), and 12 nucleotide-analog inhibitors docked onto RNA dependent RNA polymerase (RdRp). To further obtain the effective drug candidates, we screened 728 approved drugs via virtual screening on SARS-CoV-2 Mpro. Our results demonstrate that remdesivir shows the best binding energy on RdRp and saquinvir is the best inhibitor of Mpro. Based on the binding energies, we also list 10 top-ranked approved drugs which can be potential inhibitors for Mpro. Overall, our results do not only propose drug candidates for further experiments and clinical trials but also pave the way for future lead optimization and drug design.
ER  - 
TY  - JOUR
T1  - TAQIH, a tool for tabular data quality assessment and improvement in the context of health data
A1  - Álvarez Sánchez, Roberto
A1  - Beristain Iraola, Andoni
A1  - Epelde Unanue, Gorka
A1  - Carlin, Paul
Y1  - 2019///
KW  -  Data pre-processing
KW  -  Exploratory data analysis
KW  - Data quality
JF  - Computer Methods and Programs in Biomedicine
VL  - 181
SP  - 104824
EP  - 104824
DO  - https://doi.org/10.1016/j.cmpb.2018.12.029
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718304188
N1  - SI: Data Quality Assessment
N2  - Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.
ER  - 
TY  - JOUR
T1  - Statistical single cell multi-omics integration
A1  - Colomé-Tatché, M
A1  - Theis, F J
Y1  - 2018///
JF  - Current Opinion in Systems Biology
VL  - 7
SP  - 54
EP  - 59
DO  - https://doi.org/10.1016/j.coisb.2018.01.003
UR  - https://www.sciencedirect.com/science/article/pii/S2452310018300039
N1  - • Future of systems biology• Genomics and epigenomics
N2  - Single cell high throughput genomic measurements are revolutionizing the fields of biology and medicine, providing a means to tackle biological problems that have thus far been inaccessible, such as the systematic discovery of new cell types, the identification of cellular heterogeneity in health and disease, or the cell-fate decisions taking place during differentiation and reprogramming. Recently implemented multi–omics measurements of genomes, transcriptomes, epigenomes, proteomes and chromatin organization are opening up new avenues to begin to disentangle the causal relationship between -omics layers and how these co-determine higher-order cellular phenotypes. This technological revolution is not restricted to basic science but promises major breakthroughs in medical diagnostics and treatments. In this paper we review existing computational methods for the analysis and integration of different -omics layers and discuss what new approaches are needed to leverage the full potential of single cell multi-omics data.
ER  - 
TY  - JOUR
T1  - Search and visualization of gene-drug-disease interactions for pharmacogenomics and precision medicine research using GeneDive
A1  - Wong, Mike
A1  - Previde, Paul
A1  - Cole, Jack
A1  - Thomas, Brook
A1  - Laxmeshwar, Nayana
A1  - Mallory, Emily
A1  - Lever, Jake
A1  - Petkovic, Dragutin
A1  - Altman, Russ B
A1  - Kulkarni, Anagha
Y1  - 2021///
KW  - Biomedical information retrieval
KW  - Gene interactions
KW  - Gene sets
KW  - Gene-disease and gene-drug relationships
KW  - Retrieval and visualization
JF  - Journal of Biomedical Informatics
VL  - 117
SP  - 103732
EP  - 103732
DO  - https://doi.org/10.1016/j.jbi.2021.103732
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000617
N2  - Background
Understanding the relationships between genes, drugs, and disease states is at the core of pharmacogenomics. Two leading approaches for identifying these relationships in medical literature are: human expert led manual curation efforts, and modern data mining based automated approaches. The former generates small amounts of high-quality data, and the latter offers large volumes of mixed quality data. The algorithmically extracted relationships are often accompanied by supporting evidence, such as, confidence scores, source articles, and surrounding contexts (excerpts) from the articles, that can be used as data quality indicators. Tools that can leverage these quality indicators to help the user gain access to larger and high-quality data are needed.
Approach
We introduce GeneDive, a web application for pharmacogenomics researchers and precision medicine practitioners that makes gene, disease, and drug interactions data easily accessible and usable. GeneDive is designed to meet three key objectives: (1) provide functionality to manage information-overload problem and facilitate easy assimilation of supporting evidence, (2) support longitudinal and exploratory research investigations, and (3) offer integration of user-provided interactions data without requiring data sharing.
Results
GeneDive offers multiple search modalities, visualizations, and other features that guide the user efficiently to the information of their interest. To facilitate exploratory research, GeneDive makes the supporting evidence and context for each interaction readily available and allows the data quality threshold to be controlled by the user as per their risk tolerance level. The interactive search-visualization loop enables relationship discoveries between diseases, genes, and drugs that might not be explicitly described in literature but are emergent from the source medical corpus and deductive reasoning. The ability to utilize user’s data either in combination with the GeneDive native datasets or in isolation promotes richer data-driven exploration and discovery. These functionalities along with GeneDive’s applicability for precision medicine, bringing the knowledge contained in biomedical literature to bear on particular clinical situations and improving patient care, are illustrated through detailed use cases.
Conclusion
GeneDive is a comprehensive, broad-use biological interactions browser. The GeneDive application and information about its underlying system architecture are available at http://www.genedive.net. GeneDive Docker image is also available for download at this URL, allowing users to (1) import their own interaction data securely and privately; and (2) generate and test hypotheses across their own and other datasets.
ER  - 
TY  - JOUR
T1  - Examining drug and side effect relation using author–entity pair bipartite networks
A1  - Jeong, Yoo Kyung
A1  - Xie, Qing
A1  - Yan, Erjia
A1  - Song, Min
Y1  - 2020///
KW  -  Biological entity relation
KW  -  Knowledge discovery
KW  -  Knowledge structure
KW  -  Ranking algorithm
KW  - Bipartite network
JF  - Journal of Informetrics
VL  - 14
IS  - 1
SP  - 100999
EP  - 100999
DO  - https://doi.org/10.1016/j.joi.2019.100999
UR  - https://www.sciencedirect.com/science/article/pii/S1751157719302354
N2  - The current study has two objectives. First, we explore the characteristics of biological entities, such as drugs, and their side effects using an author–entity pair bipartite network. Second, we use the constructed network to examine whether there are outstanding features of relations between drugs and side effects. We extracted drug and side effect names from 169,766 PubMed abstracts published between 2010 to 2014 and constructed author–entity pair bipartite networks after ambiguous author names were processed. We propose a new ranking algorithm that takes into consideration the characteristics of bipartite networks to identify top-ranked biological drug and side effect pairs. To investigate the relationship between a particular drug and a side effect, we compared the drug and side effect pairs obtained from the network containing both drug and side effect with those observed in SIDER, a human expert-curated database. The results of this study indicate that our approach was able to identify a wide range of patterns of drug–side effect relations from the perspective of authors’ research interests. Further, our approach also identified the unique characteristics of the relation of biomedical entities obtained using an author–entity pair bipartite network.
ER  - 
TY  - JOUR
T1  - Design of a portable urine glucose monitoring system for health care
A1  - Park, Ho Dong
A1  - Lee, Kyoung Joung
A1  - Yoon, Hyung Ro
A1  - Nam, Hak Hyun
Y1  - 2005///
KW  -  Coefficient of variation (CV)
KW  -  Regression analysis
KW  -  Standard deviation (SD)
KW  - Urine glucose
JF  - Computers in Biology and Medicine
VL  - 35
IS  - 4
SP  - 275
EP  - 286
DO  - https://doi.org/10.1016/j.compbiomed.2004.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0010482504000289
N2  - This paper describes the design of a monitoring system that can be used to measure urine glucose during daily life. It consists of a bio-chemical sensor, hardware with PIC microcontroller and control circuits, and signal analyzing part. To evaluate the performance, we compared the analyzed glucose levels of the developed system to a standard instrument, YSI glucose analyzer, based on regression analysis using standard glucose solutions mixed with urine. Also, standard deviation and coefficient of variation were computed. In conclusion, the developed system showed it could be used for the measurement of urine glucose.
ER  - 
TY  - JOUR
T1  - Localization of optic disc and fovea in retinal images using intensity based line scanning analysis
A1  - Kamble, Ravi
A1  - Kokare, Manesh
A1  - Deshmukh, Girish
A1  - Hussin, Fawnizu Azmadi
A1  - Mériaudeau, Fabrice
Y1  - 2017///
KW  -  Fovea
KW  -  Macular edema (ME)
KW  -  Optic disc (OD)
KW  -  Peak-valley detection
KW  -  Wavelet transform.
KW  - Diabetic retinopathy (DR)
JF  - Computers in Biology and Medicine
VL  - 87
SP  - 382
EP  - 396
DO  - https://doi.org/10.1016/j.compbiomed.2017.04.016
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517301130
N2  - Accurate detection of diabetic retinopathy (DR) mainly depends on identification of retinal landmarks such as optic disc and fovea. Present methods suffer from challenges like less accuracy and high computational complexity. To address this issue, this paper presents a novel approach for fast and accurate localization of optic disc (OD) and fovea using one-dimensional scanned intensity profile analysis. The proposed method utilizes both time and frequency domain information effectively for localization of OD. The final OD center is located using signal peak-valley detection in time domain and discontinuity detection in frequency domain analysis. However, with the help of detected OD location, the fovea center is located using signal valley analysis. Experiments were conducted on MESSIDOR dataset, where OD was successfully located in 1197 out of 1200 images (99.75%) and fovea in 1196 out of 1200 images (99.66%) with an average computation time of 0.52s. The large scale evaluation has been carried out extensively on nine publicly available databases. The proposed method is highly efficient in terms of quickly and accurately localizing OD and fovea structure together compared with the other state-of-the-art methods.
ER  - 
TY  - JOUR
T1  - Retinal vessel extraction using Lattice Neural Networks with dendritic processing
A1  - Vega, Roberto
A1  - Sanchez-Ante, Gildardo
A1  - Falcon-Morales, Luis E
A1  - Sossa, Humberto
A1  - Guevara, Elizabeth
Y1  - 2015///
KW  -  Blood vessel segmentation
KW  -  Dendritic processing
KW  -  Diabetic retinopathy
KW  -  Machine vision
KW  -  Neural networks
KW  - Pattern recognition
JF  - Computers in Biology and Medicine
VL  - 58
SP  - 20
EP  - 30
DO  - https://doi.org/10.1016/j.compbiomed.2014.12.016
UR  - https://www.sciencedirect.com/science/article/pii/S001048251400362X
N2  - Retinal images can be used to detect and follow up several important chronic diseases. The classification of retinal images requires an experienced ophthalmologist. This has been a bottleneck to implement routine screenings performed by general physicians. It has been proposed to create automated systems that can perform such task with little intervention from humans, with partial success. In this work, we report advances in such endeavor, by using a Lattice Neural Network with Dendritic Processing (LNNDP). We report results using several metrics, and compare against well known methods such as Support Vector Machines (SVM) and Multilayer Perceptrons (MLP). Our proposal shows better performance than other approaches reported in the literature. An additional advantage is that unlike those other tools, LNNDP requires no parameters, and it automatically constructs its structure to solve a particular problem. The proposed methodology requires four steps: (1) Pre-processing, (2) Feature computation, (3) Classification and (4) Post-processing. The Hotelling T2 control chart was used to reduce the dimensionality of the feature vector, from 7 that were used before to 5 in this work. The experiments were run on images of DRIVE and STARE databases. The results show that on average, F1-Score is better in LNNDP, compared with SVM and MLP implementations. Same improvement is observed for MCC and the accuracy.
ER  - 
TY  - JOUR
T1  - A probabilistic method for keyword retrieval in handwritten document images
A1  - Cao, Huaigu
A1  - Bhardwaj, Anurag
A1  - Govindaraju, Venu
Y1  - 2009///
KW  -  Handwriting recognition
KW  -  Information retrieval
KW  - Word spotting
JF  - Pattern Recognition
VL  - 42
IS  - 12
SP  - 3374
EP  - 3382
DO  - https://doi.org/10.1016/j.patcog.2009.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0031320309000715
N1  - New Frontiers in Handwriting Recognition
N2  - Keyword retrieval in handwritten document images is a challenging task because handwriting recognition does not perform adequately to produce the transcriptions, specially when using large lexicons. Existing methods build indices using OCR distances or image features for the purpose of retrieval. These alternative methods are complimentary to the traditional approaches that build indices on OCR’ed text. In this paper, we describe an improvement to the existing keyword retrieval (word spotting) methods by modeling imperfect word segmentation as probabilities and integrating these probabilities into the word spotting algorithm. The scores returned by the word recognizer are also converted into probabilities and integrated into the probabilistic word spotting model.
ER  - 
TY  - JOUR
T1  - Progressive interactive training: A sequential neural network ensemble learning method
A1  - Akhand, M A H
A1  - Islam, Md. Monirul
A1  - Murase, K
Y1  - 2009///
KW  - Bagging and boosting
KW  - Indirect communication
KW  - Negative correlation learning
KW  - Neural network ensemble
JF  - Neurocomputing
VL  - 73
IS  - 1
SP  - 260
EP  - 273
DO  - https://doi.org/10.1016/j.neucom.2009.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209003208
N1  - Timely Developments in Applied Neural Computing (EANN 2007) / Some Novel Analysis and Learning Methods for Neural Networks (ISNN 2008) / Pattern Recognition in Graphical Domains
N2  - This paper introduces a progressive interactive training scheme (PITS) for neural network (NN) ensembles. The scheme trains NNs in an ensemble one by one in a sequential fashion where the outputs of all previously trained NNs are stored and updated in a common location, called information center (IC). The communication among NNs is maintained indirectly through IC, reducing interaction among NNs. In this study, PITS is formulated as a derivative of simultaneous interactive training, negative correlation learning. The effectiveness of PITS is evaluated on a suite of 20 benchmark classification problems. The experimental results show that the proposed training scheme can improve the performance of ensembles. Furthermore, the PITS is incorporated with two very popular ensemble training methods, bagging and boosting. It is found that the performance of bagging and boosting algorithms can be improved by incorporating PITS with their training processes.
ER  - 
TY  - JOUR
T1  - Deep residual transfer learning for automatic diagnosis and grading of diabetic retinopathy
A1  - Martinez-Murcia, Francisco J
A1  - Ortiz, Andrés
A1  - Ramírez, Javier
A1  - Górriz, Juan M
A1  - Cruz, Ricardo
Y1  - 2021///
KW  -  Convolutional neural network
KW  -  Diabetic retinopathy
KW  -  Residual learning
KW  -  Retinography
KW  -  Transfer learning
KW  - Deep learning
JF  - Neurocomputing
VL  - 452
SP  - 424
EP  - 434
DO  - https://doi.org/10.1016/j.neucom.2020.04.148
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220316520
N2  - Evaluation and diagnosis of retina pathology is usually made via the analysis of different image modalities that allow to explore its structure. The most popular retina image method is retinography, a technique that displays the fundus of the eye, including the retina and other structures. Retinography is the most common imaging method to diagnose retina diseases such as Diabetic Retinopathy (DB) or Macular Edema (ME). However, retinography evaluation to score the image according to the disease grade presents difficulties due to differences in contrast, brightness and the presence of artifacts. Therefore, it is mainly done via manual analysis; a time consuming task that requires a trained clinician to examine and evaluate the images. In this paper, we present a computer aided diagnosis tool that takes advantage of the performance provided by deep learning architectures for image analysis. Our proposal is based on a deep residual convolutional neural network for extracting discriminatory features with no prior complex image transformations to enhance the image quality or to highlight specific structures. Moreover, we used the transfer learning paradigm to reuse layers from deep neural networks previously trained on the ImageNet dataset, under the hypothesis that first layers capture abstract features than can be reused for different problems. Experiments using different convolutional architectures have been carried out and their performance has been evaluated on the MESSIDOR database using cross-validation. Best results were found using a ResNet50-based architecture, showing an AUC of 0.93 for grades 0 + 1, AUC of 0.81 for grade 2 and AUC of 0.92 for grade 3 labelling, as well as AUCs higher than 0.97 when considering a binary classification problem (grades 0 vs 3).
ER  - 
TY  - JOUR
T1  - Wireless non-invasive motion tracking of functional behavior
A1  - Zhang, Heng
A1  - Smeddinck, Jan
A1  - Malaka, Rainer
A1  - Shu, Yao
A1  - Chen, Chong
A1  - He, Bo
A1  - Fu, Zengjun
A1  - Lawo, Michael
Y1  - 2019///
KW  -  Digital health
KW  -  Sedentary lifestyle
KW  -  Signal processing
KW  -  Situated research
KW  -  Wi-Fi
KW  -  eHealth
KW  - Activity recognition
JF  - Pervasive and Mobile Computing
VL  - 54
SP  - 29
EP  - 44
DO  - https://doi.org/10.1016/j.pmcj.2019.01.006
UR  - https://www.sciencedirect.com/science/article/pii/S1574119217305904
N2  - The prevalence of a sedentary lifestyle is a major contributor to many chronic afflictions in modern society. Objective study and monitoring to gain an accurate understanding of situated sedentary behavior, for example when at home, present considerable challenges, e.g. regarding ecological validity. Non-intrusive monitoring based on Wi-Fi signals provides a new way to gain insights into populations that are at risk of the negative effects of a sedentary lifestyle, or who are already in functional rehabilitation. In this paper we describe a tracking technology for everyday activities that consists of two parts: (1) recognizing general physical activity, as well as the activities of common classes; and (2) measuring the statistical duration of these recognized categories. Employing common commercial Wi-Fi equipment, we performed validation studies in a typical noisy family home environment, achieving the following key results: (1) a recognition rate of the general presence of physical activity of 99.05%, an average recognition rate of 92% when detecting four common classes of activities; and (2) Kappa coefficient analysis to evaluate the consistency of the statistical duration of the automatic activity detection based on Wi-Fi signals and manually coded activity detection based on camera recordings. The coefficient for the presence of general physical activity of .93 and the average consistency coefficient of the classified activity categories of .72 suggest a high reliability of the automatic detection outcomes. This work aims to support both research and interventions for the prevention, treatment, and rehabilitation of the consequences of a sedentary lifestyle, by establishing new technologies and methods for observing everyday functional activities that are crucial for individual independent living and well-being.
ER  - 
TY  - JOUR
T1  - A novel purity-based k nearest neighbors imputation method and its application in financial distress prediction
A1  - Cheng, Ching-Hsue
A1  - Chan, Chia-Pang
A1  - Sheu, Yu-Jheng
Y1  - 2019///
KW  - Financial distress
KW  - Imputation
KW  - Missing value
KW  - Purity-based k nearest neighbor
JF  - Engineering Applications of Artificial Intelligence
VL  - 81
SP  - 283
EP  - 299
DO  - https://doi.org/10.1016/j.engappai.2019.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S0952197619300478
N2  - Financial distress research often has missing values problems, and the different missing values handling techniques have an impact on the classification results. Furthermore, missing values handling in the data sciences is an important issue, and the different missing values handling approaches restrict on the application and performance of the classification. In missing values research, previous studies usually focused on the accuracy of classification, however, they address less the overall performance of the different missing degrees. To obtain better accuracy and maintain the integrity of data on the classification, this study proposes a purity-based k nearest neighbor algorithm to improve the performance of the missing value imputation. To verify, this study implemented different missing degree and different noise rate experiments for demonstrating the better performance because the proposed method is less affected by the noise. Furthermore, this paper also implemented MAR, MCAR, and MNAR type experiments, and compared the proposed method with the listed imputation techniques. Furthermore, this study practically collected Taiwan Economic Journal (TEJ) datasets as MNAR type missing values, and then employed the proposed purity-based k nearest neighbor algorithm to build a financial distress prediction model. Finally, this study compared the proposed imputation algorithm with common imputation methods and different classifiers, the results show that the proposed imputation algorithm obtains better accuracy and more stable in different missing degrees and noise.
ER  - 
TY  - JOUR
T1  - A Hybrid Approach for Drug Abuse Events Extraction from Twitter
A1  - Jenhani, Ferdaous
A1  - Gouider, Mohamed Salah
A1  - Said, Lamjed Ben
Y1  - 2016///
KW  - Drug Abuse
KW  - Events Extraction
KW  - Hybrid Approach
KW  - ODIN
KW  - Twitter
JF  - Procedia Computer Science
VL  - 96
SP  - 1032
EP  - 1040
DO  - https://doi.org/10.1016/j.procs.2016.08.121
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916319226
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 20th International Conference KES-2016
N2  - Since their emergence, social media have become a reliable source of social events which attracted the interest of research community to extract them for many business requirements. However, unlike formal sources like news articles, social data exploitation for events extraction is much harder regarding the complex character of social text. Many approaches, ranging from linguistic techniques to learning algorithms, were proposed to succeed this task. Nevertheless, achieved results are weak regarding the complexity and completeness of the task. In this paper, we focus on private events extraction from Twitter by tracking digital drug abusers. We propose a hybrid approach in which we combine strengths of linguistic rules and learning techniques looking for better performance. In fact, we use linguistic rules to build an automatically annotated training set and extract a set of features as well, to be used in a learning process in order to improve obtained results. The proposed approach outperforms the baseline by 24,8% thanks to combination of techniques.
ER  - 
TY  - JOUR
T1  - Automatic wavelet-based retinal blood vessels segmentation and vessel diameter estimation
A1  - Fathi, Abdolhossein
A1  - Naghsh-Nilchi, Ahmad Reza
Y1  - 2013///
KW  - Blood vessel detection
KW  - Multi-scale analysis
KW  - Retinal image segmentation
KW  - Vessel diameter estimation
JF  - Biomedical Signal Processing and Control
VL  - 8
IS  - 1
SP  - 71
EP  - 80
DO  - https://doi.org/10.1016/j.bspc.2012.05.005
UR  - https://www.sciencedirect.com/science/article/pii/S1746809412000663
N2  - Automatic detection of retinal blood vessels and measurement of vessel diameter are important steps in the computer aided diagnosis in ophthalmology. Here, we present a new multi-scale vessel enhancement method based on complex continuous wavelet transform (CCWT). The parameters of CCWT are optimized to represent line structures in all directions and separate them from simple edges. The final vessel network is obtained by applying an adaptive histogram-based thresholding process along with a proper length filtering method. An efficient circular structure operator is employed on the centerline of vessels to estimate their diameters. The performance of the proposed method is measured on the publicly available DRIVE and STARE databases and compared with several state-of-the-art methods as well as second observer. The proposed method shows much higher accuracy (95%) and sensitivity (79%) in the same range of specificity (97%). The predictive value of it is higher than 72.9%. The vessel diameter estimation process also shows lower root mean square error compared to the existing methods and second observer.
ER  - 
TY  - JOUR
T1  - Action video game experience is associated with increased resting state functional connectivity in the caudate nucleus and decreased functional connectivity in the hippocampus
A1  - Benady-Chorney, Jessica
A1  - Aumont, Étienne
A1  - Yau, Yvonne
A1  - Zeighami, Yashar
A1  - Bohbot, Veronique D
A1  - West, Greg L
Y1  - 2020///
KW  -  Action video games
KW  -  Caudate nucleus
KW  -  Hippocampus
KW  - Resting state functional connectivity
JF  - Computers in Human Behavior
VL  - 106
SP  - 106200
EP  - 106200
DO  - https://doi.org/10.1016/j.chb.2019.106200
UR  - https://www.sciencedirect.com/science/article/pii/S0747563219304200
N2  - Habitual action video game experience is associated with both increased grey matter and activity in the striatum and decreased grey matter in the hippocampus. To further investigate this relationship, we tested differences in resting state functional connectivity (rsFC) between action video games players (actionVGPs) compared to non-video game players (NVGPs) using the hippocampus, the caudate nucleus and the nucleus accumbens as regions of interest. Seventeen actionVGPs and 16 NVGPs were scanned using fMRI to measure rsFC. Results show that when compared to NVGPs, actionVGPs have increased rsFC between the nucleus accumbens and the subgenual anterior cingulate cortex and between the caudate nucleus and the precentral gyrus. ActionVGPs also displayed decreased rsFC between the hippocampus and the superior temporal gyrus and between the nucleus accumbens and the ventral tegmental area. Together, these results follow previous research examining changes in grey matter and suggest that frequent action video game playing is associated with higher functional activity in the reward circuit and lower functional activity within the hippocampus.
ER  - 
TY  - JOUR
T1  - Optimum green plane masking for the contrast enhancement of retinal images using enhanced genetic algorithm
A1  - Daniel, Ebenezer
A1  - Anitha, J
Y1  - 2015///
KW  -  Enhanced genetic algorithm
KW  -  Retinal imaging
KW  -  Unsharp masking
KW  - Contrast enhancement
JF  - Optik
VL  - 126
IS  - 18
SP  - 1726
EP  - 1730
DO  - https://doi.org/10.1016/j.ijleo.2015.05.027
UR  - https://www.sciencedirect.com/science/article/pii/S0030402615003277
N2  - Masking based techniques are well known and effective for contrast enhancement applications. The conventional unsharp masking in which fixed scale value is using irrespective of the types of test images. In this paper we propose an optimum green plane masking (OGPM) using enhanced genetic algorithm (EGA) for the contrast enhancement of retinal images. The green plane has more details of retinal images than the other two planes. The EGA can adaptively perform the selection, crossover and mutation of chromosomes. First, the proposed approach is evaluated using the standard test images and real time images for different contrast enhancement techniques and optimization techniques. Finally the proposed approach is used for the enhancement of retinal images. Results are analyzed using various performance measures and our OGPM shows better enhancement than other reported literature.
ER  - 
TY  - JOUR
T1  - Computer-aided prediction of inhibitors against STAT3 for managing COVID-19 associated cytokine storm
A1  - Dhall, Anjali
A1  - Patiyal, Sumeet
A1  - Sharma, Neelam
A1  - Devi, Naorem Leimarembi
A1  - Raghava, Gajendra.P.S.
Y1  - 2021///
KW  -  COVID-19
KW  -  Cytokine
KW  -  FDA-Approved
KW  -  Inhibitors
KW  - STAT3
JF  - Computers in Biology and Medicine
VL  - 137
SP  - 104780
EP  - 104780
DO  - https://doi.org/10.1016/j.compbiomed.2021.104780
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521005746
N2  - Background
Proinflammatory cytokines are correlated with the severity of disease in patients with COVID-19. IL6-mediated activation of STAT3 proliferates proinflammatory responses that lead to cytokine storm promotion. Thus, STAT3 inhibitors may play a crucial role in managing the COVID-19 pathogenesis. The present study discusses a method for predicting inhibitors against the STAT3 signaling pathway.
Method
The main dataset comprises 1565 STAT3 inhibitors and 1671 non-inhibitors used for training, testing, and evaluation of models. A number of machine learning classifiers have been implemented to develop the models.
Results
The outcomes of the data analysis show that rings and aromatic groups are significantly abundant in STAT3 inhibitors compared to non-inhibitors. First, we developed models using 2-D and 3-D chemical descriptors and achieved a maximum AUC of 0.84 and 0.73, respectively. Second, fingerprints are used to build predictive models and achieved 0.86 AUC with an accuracy of 78.70% on the validation dataset. Finally, models were developed using hybrid descriptors, which achieved a maximum of 0.87 AUC with 78.55% accuracy on the validation dataset.
Conclusion
We used the best model to identify STAT3 inhibitors in FDA-approved drugs and found few drugs (e.g., Tamoxifen and Perindopril) to manage the cytokine storm in COVID-19 patients. A webserver “STAT3In” (https://webs.iiitd.edu.in/raghava/stat3in/) has been developed to predict and design STAT3 inhibitors.
ER  - 
TY  - JOUR
T1  - Simpler is better: Lifting interpretability-performance trade-off via automated feature engineering
A1  - Gosiewska, Alicja
A1  - Kozak, Anna
A1  - Biecek, Przemysław
Y1  - 2021///
KW  -  Decision-making
KW  -  Feature engineering
KW  -  Machine learning
KW  - Interpretability
JF  - Decision Support Systems
VL  - 150
SP  - 113556
EP  - 113556
DO  - https://doi.org/10.1016/j.dss.2021.113556
UR  - https://www.sciencedirect.com/science/article/pii/S016792362100066X
N1  - Interpretable Data Science For Decision Making
N2  - Machine learning has proved to generate useful predictive models that can and should support decision makers in many areas. The availability of tools for AutoML makes it possible to quickly create an effective but complex predictive model. However, the complexity of such models is often a major obstacle in applications, especially in terms of high-stake decisions. We are experiencing a growing number of examples where the use of black boxes leads to decisions that are harmful, unfair or simply wrong. In this paper, we show that very often we can simplify complex models without compromising their performance; however, with the benefit of much needed transparency. We propose a framework that uses elastic black boxes as supervisor models to create simpler, less opaque, yet still accurate and interpretable glass box models. The new models were created using newly engineered features extracted with the help of a supervisor model. We supply the analysis using a large-scale benchmark on several tabular data sets from the OpenML database. There are tree main results of this paper: 1) we show that extracting information from complex models may improve the performance of simpler models, 2) we question a common myth that complex predictive models outperform simpler predictive models, 3) we present a real-life application of the proposed method.
ER  - 
TY  - JOUR
T1  - in vivo laser speckle imaging by adaptive contrast computation for microvasculature assessment
A1  - Basak, Kausik
A1  - Dey, Goutam
A1  - Mahadevappa, Manjunatha
A1  - Mandal, Mahitosh
A1  - Dutta, Pranab Kumar
Y1  - 2014///
KW  - Blood flow
KW  - Laser speckle
KW  - Retinal vasculature
KW  - Time integrated speckle
KW  - Tissue perfusion
JF  - Optics and Lasers in Engineering
VL  - 62
SP  - 87
EP  - 94
DO  - https://doi.org/10.1016/j.optlaseng.2014.05.009
UR  - https://www.sciencedirect.com/science/article/pii/S0143816614001328
N2  - Interference of light backscattered from a diffused surface leads to speckle formation in laser speckle imaging. These time integrated speckle patterns can be statistically analyzed to study the flow profile of moving scatterers. Simple speckle contrast analysis techniques have limited ability to distinguish thin structures due to presence of corrupting speckles. This paper presents a high resolution imaging technique by adaptive computation of contrast for laser speckle contrast analysis (adLASCA). Speckle images of retinal microvasculature in mice model are acquired during normal and reduced blood flow conditions. Initially, the speckle images are registered to compensate for movements, associated with heart beating and respiration. Adaptive computation is performed using local image statistics, estimated within a spatially moving window over successive time frames. Experimental evidence suggests that adLASCA outperforms other contrast analysis methods, substantiating significant improvement in contrast resolution. Fine vessels can be distinguished more efficiently with reduced fluctuations in contrast level. Quantitative performance of adLASCA is evaluated by computing standard deviation, corresponding to speckle fluctuations due to unwanted speckles. There is a significant reduction in standard deviation compared to other methods. Therefore, adLASCA can be used for enhancing microvasculature in high resolution perfusion imaging with reduced effect of corrupting speckles for effective assessment.
ER  - 
TY  - JOUR
T1  - A new ECG beat clustering method based on kernelized fuzzy c-means and hybrid ant colony optimization for continuous domains
A1  - Doğan, Berat
A1  - Korürek, Mehmet
Y1  - 2012///
KW  - Ant colony optimization
KW  - Arrhythmia classification
KW  - ECG classification
KW  - ECG clustering
KW  - Fuzzy c-means
KW  - Kernelized fuzzy c-means
KW  - Swarm
JF  - Applied Soft Computing
VL  - 12
IS  - 11
SP  - 3442
EP  - 3451
DO  - https://doi.org/10.1016/j.asoc.2012.07.007
UR  - https://www.sciencedirect.com/science/article/pii/S1568494612003079
N2  - The kernelized fuzzy c-means algorithm uses kernel methods to improve the clustering performance of the well known fuzzy c-means algorithm by mapping a given dataset into a higher dimensional space non-linearly. Thus, the newly obtained dataset is more likely to be linearly seprable. However, to further improve the clustering performance, an optimization method is required to overcome the drawbacks of the traditional algorithms such as, sensitivity to initialization, trapping into local minima and lack of prior knowledge for optimum paramaters of the kernel functions. In this paper, to overcome these drawbacks, a new clustering method based on kernelized fuzzy c-means algorithm and a recently proposed ant based optimization algorithm, hybrid ant colony optimization for continuous domains, is proposed. The proposed method is applied to a dataset which is obtained from MIT–BIH arrhythmia database. The dataset consists of six types of ECG beats including, Normal Beat (N), Premature Ventricular Contraction (PVC), Fusion of Ventricular and Normal Beat (F), Artrial Premature Beat (A), Right Bundle Branch Block Beat (R) and Fusion of Paced and Normal Beat (f). Four time domain features are extracted for each beat type and training and test sets are formed. After several experiments it is observed that the proposed method outperforms the traditional fuzzy c-means and kernelized fuzzy c-means algorithms.
ER  - 
TY  - JOUR
T1  - Opportunities and challenges of deep learning methods for electrocardiogram data: A systematic review
A1  - Hong, Shenda
A1  - Zhou, Yuxi
A1  - Shang, Junyuan
A1  - Xiao, Cao
A1  - Sun, Jimeng
Y1  - 2020///
KW  -  Deep neural network(s)
KW  -  Electrocardiogram (ECG/EKG)
KW  -  Systematic review
KW  - Deep learning
JF  - Computers in Biology and Medicine
VL  - 122
SP  - 103801
EP  - 103801
DO  - https://doi.org/10.1016/j.compbiomed.2020.103801
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520301694
N2  - Background
The electrocardiogram (ECG) is one of the most commonly used diagnostic tools in medicine and healthcare. Deep learning methods have achieved promising results on predictive healthcare tasks using ECG signals.
Objective
This paper presents a systematic review of deep learning methods for ECG data from both modeling and application perspectives.
Methods
We extracted papers that applied deep learning (deep neural network) models to ECG data that were published between January 1st of 2010 and February 29th of 2020 from Google Scholar, PubMed, and the Digital Bibliography & Library Project. We then analyzed each article according to three factors: tasks, models, and data. Finally, we discuss open challenges and unsolved problems in this area.
Results
The total number of papers extracted was 191. Among these papers, 108 were published after 2019. Different deep learning architectures have been used in various ECG analytics tasks, such as disease detection/classification, annotation/localization, sleep staging, biometric human identification, and denoising.
Conclusion
The number of works on deep learning for ECG data has grown explosively in recent years. Such works have achieved accuracy comparable to that of traditional feature-based approaches and ensembles of multiple approaches can achieve even better results. Specifically, we found that a hybrid architecture of a convolutional neural network and recurrent neural network ensemble using expert features yields the best results. However, there are some new challenges and problems related to interpretability, scalability, and efficiency that must be addressed. Furthermore, it is also worth investigating new applications from the perspectives of datasets and methods.
Significance
This paper summarizes existing deep learning research using ECG data from multiple perspectives and highlights existing challenges and problems to identify potential future research directions.
ER  - 
TY  - JOUR
T1  - Using machine-learning approaches to predict non-participation in a nationwide general health check-up scheme
A1  - Shimoda, Akihiro
A1  - Ichikawa, Daisuke
A1  - Oyama, Hiroshi
Y1  - 2018///
KW  -  Machine-learning
KW  -  Prediction
KW  -  Segmentation
KW  - Health check-up
JF  - Computer Methods and Programs in Biomedicine
VL  - 163
SP  - 39
EP  - 46
DO  - https://doi.org/10.1016/j.cmpb.2018.05.032
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718301962
N2  - Background
In the time since the launch of a nationwide general health check-up and instruction program in Japan in 2008, interest in the formulation of an effective and efficient strategy to improve the participation rate has been growing. The aim of this study was to develop and evaluate models identifying those who are unlikely to undergo general health check-ups. We used machine-learning methods to select interventional targets more efficiently.
Methods
We used information from a local government database of Japan. The study population included 7290 individuals aged 40–74 years who underwent at least one general health check-up between 2012 and 2015. We developed four predictive models based on the extreme gradient boosting (XGBoost), random forest (RF), support vector machines (SVMs), and logistic regression (LR) algorithms, using machine-learning techniques, and compared the areas under the curves (AUCs) of the models with those of the heuristic method (which presumes that the individuals who underwent a general health check-up in the previous year will do so again in the following year).
Results
The AUCs for the XGBoost, RF, SVMs, LR, and heuristic models/method were 0.829 (95% confidence interval [CI]: 0.806–0.853), 0.821 (95% CI: 0.797–0.845), 0.812 (95% CI: 0.787–0.837), 0.816 (95% CI: 0.791–0.841), and 0.683 (95% CI: 0.657–0.708), respectively. XGBoost model exhibited the best AUC, and the performance was significantly better than that of SVMs (p = 0.034), LR (p = 0.017), and heuristic method (p < 0.001). However, the performance of XGBoost did not differ significantly from that of RF (p = 0.229).
Conclusion
Predictive models using machine-learning techniques outperformed the existing heuristic method when used to predict participation in a general health check-up system by eligible participants.
ER  - 
TY  - JOUR
T1  - Noisy multi-label semi-supervised dimensionality reduction
A1  - Mikalsen, Karl Øyvind
A1  - Soguero-Ruiz, Cristina
A1  - Bianchi, Filippo Maria
A1  - Jenssen, Robert
Y1  - 2019///
KW  -  Dimensionality reduction
KW  -  Multi-label learning
KW  -  Semi-supervised learning
KW  - Label noise
JF  - Pattern Recognition
VL  - 90
SP  - 257
EP  - 270
DO  - https://doi.org/10.1016/j.patcog.2019.01.033
UR  - https://www.sciencedirect.com/science/article/pii/S0031320319300615
N2  - Noisy labeled data represent a rich source of information that often are easily accessible and cheap to obtain, but label noise might also have many negative consequences if not accounted for. How to fully utilize noisy labels has been studied extensively within the framework of standard supervised machine learning over a period of several decades. However, very little research has been conducted on solving the challenge posed by noisy labels in non-standard settings. This includes situations where only a fraction of the samples are labeled (semi-supervised) and each high-dimensional sample is associated with multiple labels. In this work, we present a novel semi-supervised and multi-label dimensionality reduction method that effectively utilizes information from both noisy multi-labels and unlabeled data. With the proposed Noisy multi-label semi-supervised dimensionality reduction (NMLSDR) method, the noisy multi-labels are denoised and unlabeled data are labeled simultaneously via a specially designed label propagation algorithm. NMLSDR then learns a projection matrix for reducing the dimensionality by maximizing the dependence between the enlarged and denoised multi-label space and the features in the projected space. Extensive experiments on synthetic data, benchmark datasets, as well as a real-world case study, demonstrate the effectiveness of the proposed algorithm and show that it outperforms state-of-the-art multi-label feature extraction algorithms.
ER  - 
TY  - JOUR
T1  - Federated learning of predictive models from federated Electronic Health Records
A1  - Brisimi, Theodora S
A1  - Chen, Ruidi
A1  - Mela, Theofanie
A1  - Olshevsky, Alex
A1  - Paschalidis, Ioannis Ch.
A1  - Shi, Wei
Y1  - 2018///
KW  - Distributed learning
KW  - Electronic Health Records (EHRs)
KW  - Federated databases
KW  - Heart diseases
KW  - Hospitalization
KW  - Predictive models
JF  - International Journal of Medical Informatics
VL  - 112
SP  - 59
EP  - 67
DO  - https://doi.org/10.1016/j.ijmedinf.2018.01.007
UR  - https://www.sciencedirect.com/science/article/pii/S138650561830008X
N2  - Background
In an era of “big data,” computationally efficient and privacy-aware solutions for large-scale machine learning problems become crucial, especially in the healthcare domain, where large amounts of data are stored in different locations and owned by different entities. Past research has been focused on centralized algorithms, which assume the existence of a central data repository (database) which stores and can process the data from all participants. Such an architecture, however, can be impractical when data are not centrally located, it does not scale well to very large datasets, and introduces single-point of failure risks which could compromise the integrity and privacy of the data. Given scores of data widely spread across hospitals/individuals, a decentralized computationally scalable methodology is very much in need.
Objective
We aim at solving a binary supervised classification problem to predict hospitalizations for cardiac events using a distributed algorithm. We seek to develop a general decentralized optimization framework enabling multiple data holders to collaborate and converge to a common predictive model, without explicitly exchanging raw data.
Methods
We focus on the soft-margin l1-regularized sparse Support Vector Machine (sSVM) classifier. We develop an iterative cluster Primal Dual Splitting (cPDS) algorithm for solving the large-scale sSVM problem in a decentralized fashion. Such a distributed learning scheme is relevant for multi-institutional collaborations or peer-to-peer applications, allowing the data holders to collaborate, while keeping every participant's data private.
Results
We test cPDS on the problem of predicting hospitalizations due to heart diseases within a calendar year based on information in the patients Electronic Health Records prior to that year. cPDS converges faster than centralized methods at the cost of some communication between agents. It also converges faster and with less communication overhead compared to an alternative distributed algorithm. In both cases, it achieves similar prediction accuracy measured by the Area Under the Receiver Operating Characteristic Curve (AUC) of the classifier. We extract important features discovered by the algorithm that are predictive of future hospitalizations, thus providing a way to interpret the classification results and inform prevention efforts.
ER  - 
TY  - JOUR
T1  - Comparison of coronary artery dynamics pre- and post-stenting
A1  - Zhu, Hui
A1  - Warner, John J
A1  - Gehrig, Thomas R
A1  - Friedman, Morton H
Y1  - 2003///
JF  - Journal of Biomechanics
VL  - 36
IS  - 5
SP  - 689
EP  - 697
DO  - https://doi.org/10.1016/S0021-9290(02)00447-5
UR  - https://www.sciencedirect.com/science/article/pii/S0021929002004475
N1  - Cardiovascular Biomechanics
N2  - Stents have dramatically improved the treatment of coronary artery disease. Since the implantation of stents changes the geometry and dynamics of the coronary artery, it is reasonable to hypothesize that some of these changes may have an important effect on the development of atherosclerosis by modulating the mechanical environment. In this paper, we presented a method to compare the geometric dynamics of the coronary artery before and after stenting using biplane angiography. Two cases are reviewed and a number of parameters are proposed to describe the longitudinal change of the vessel before and after stenting. This analysis technique has the potential to identify some aspects of stent design and procedure that might improve the success rate with this therapeutic approach.
ER  - 
TY  - JOUR
T1  - Retinal artery/vein classification using genetic-search feature selection
A1  - Huang, Fan
A1  - Dashtbozorg, Behdad
A1  - Tan, Tao
A1  - ter Haar Romeny, Bart M
Y1  - 2018///
KW  -  Artery/vein classification
KW  -  Genetic search feature selection
KW  - Fundus image
JF  - Computer Methods and Programs in Biomedicine
VL  - 161
SP  - 197
EP  - 207
DO  - https://doi.org/10.1016/j.cmpb.2018.04.016
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717312415
N2  - Background and objectives: The automatic classification of retinal blood vessels into artery and vein (A/V) is still a challenging task in retinal image analysis. Recent works on A/V classification mainly focus on the graph analysis of the retinal vasculature, which exploits the connectivity of vessels to improve the classification performance. While they have overlooked the importance of pixel-wise classification to the final classification results. This paper shows that a complicated feature set is efficient for vessel centerline pixels classification. Methods: We extract enormous amount of features for vessel centerline pixels, and apply a genetic-search based feature selection technique to obtain the optimal feature subset for A/V classification. Results: The proposed method achieves an accuracy of 90.2%, the sensitivity of 89.6%, the specificity of 91.3% on the INSPIRE dataset. It shows that our method, using only the information of centerline pixels, gives a comparable performance as the techniques which use complicated graph analysis. In addition, the results on the images acquired by different fundus cameras show that our framework is capable for discriminating vessels independent of the imaging device characteristics, image resolution and image quality. Conclusion: The complicated feature set is essential for A/V classification, especially on the individual vessels where graph-based methods receive limitations. And it could provide a higher entry to the graph-analysis to achieve a better A/V labeling.
ER  - 
TY  - JOUR
T1  - Predicting the occurrence of wrist tremor based on electromyography using a hidden Markov model and entropy based learning algorithm
A1  - Samaee, Saeedeh
A1  - Kobravi, Hamid Reza
Y1  - 2020///
KW  -  Hidden Markov model
KW  -  Maximum entropy model
KW  -  Viterbi algorithm
KW  - Pathological tremor
JF  - Biomedical Signal Processing and Control
VL  - 57
SP  - 101739
EP  - 101739
DO  - https://doi.org/10.1016/j.bspc.2019.101739
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419303209
N2  - Pathological tremor is a well-known movement disorder ensues from some diseases such as Parkinson and essential tremor. Develop technologies for tremor suppression is an attractive and open research problem. Incorporating the processing methodologies applicable to the prediction of the occurrence of the tremor burst can corroborate the efficacy of such technologies. Therefore, in this study, a predictive model has been proposed to predict the incidence time of tremor bursts. In the proposed approach, the Markov nonlinear hidden model was employed. The mentioned model was trained once by using the algorithm of Baum Welch and again by combining this algorithm with the maximum entropy algorithm. The Hidden Markov models (HMM) were once trained with raw EMG (Electromyogram) data and by using the extracted features from the EMG signal. The output of the model predicts the occurrence or absence of tremors. The EMG signals were recorded from 11 patients with different pathologic abnormalities. The features such as integrated EMG, mean frequency, and peak frequency were extracted from EMG data and ranked using the RELIEF algorithm. The results showed that the HMM trained with the entropy-based learning method, in the conditions where the EMG signal was its inputs, has the highest performance.
ER  - 
TY  - JOUR
T1  - Induction of ordinal classification rules from decision tables with unknown monotonicity
A1  - Wang, Hailiang
A1  - Zhou, Mingtian
A1  - She, Kun
Y1  - 2015///
KW  -  Dominance relations
KW  -  Inconsistency rates
KW  -  Multiple criteria decision aiding
KW  - Rough sets
JF  - European Journal of Operational Research
VL  - 242
IS  - 1
SP  - 172
EP  - 181
DO  - https://doi.org/10.1016/j.ejor.2014.09.034
UR  - https://www.sciencedirect.com/science/article/pii/S0377221714007735
N2  - We are considering induction of ordinal classification rules, which assign objects to preference-ordered decision classes, within the dominance-based rough set approach. In order to extract such rules, it is necessary to define dominance inconsistencies with respect to a set of condition attributes containing at least one ordinal condition attribute. Furthermore, it is also assumed that we know if there exist increasing or decreasing monotonicity relationships between the values of ordinal condition and decision attributes. Very often, however, this information is unknown a priori. One solution to this issue is to transform the ordinal condition attributes with unknown directions of preference to pairs of attributes with supposed inverse monotonic relationships. Both local and global monotonicity relationships can be represented by decision rules induced from transformed decision tables. However, in some cases, transforming a decision table in this way is overcomplex. In this paper, we propose the inconsistency rates based on dominance and fuzzy preference relations that have the capacity of discovering monotonic relationships directly from data rather than induced decision rules. Moreover, we propose a refined transformation method by introducing an additional monotonicity checking using these inconsistency rates to determine whether an ordinal condition attribute should be cloned or not. Experiments are also provided to evaluate the usefulness of the refined transformation method.
ER  - 
TY  - JOUR
T1  - Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain
A1  - Alobaidi, Mazen
A1  - Malik, Khalid Mahmood
A1  - Hussain, Maqbool
Y1  - 2018///
KW  - Linked biomedical ontologies
KW  - Ontology generation
KW  - Semantic web
JF  - Computer Methods and Programs in Biomedicine
VL  - 165
SP  - 117
EP  - 128
DO  - https://doi.org/10.1016/j.cmpb.2018.08.010
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717315791
N2  - Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation.
ER  - 
TY  - JOUR
T1  - A novel version of k nearest neighbor: Dependent nearest neighbor
A1  - Ertuğrul, Ömer Faruk
A1  - Tağluk, Mehmet Emin
Y1  - 2017///
KW  -  Dependent nearest neighbor
KW  -  Similarity
KW  -  k nearest neighbor
KW  - Dependency
JF  - Applied Soft Computing
VL  - 55
SP  - 480
EP  - 490
DO  - https://doi.org/10.1016/j.asoc.2017.02.020
UR  - https://www.sciencedirect.com/science/article/pii/S1568494617300984
N2  - k nearest neighbor (kNN) is one of the basic processes behind various machine learning methods In kNN, the relation of a query to a neighboring sample is basically measured by a similarity metric, such as Euclidean distance. This process starts with mapping the training dataset onto a one-dimensional distance space based on the calculated similarities, and then labeling the query in accordance with the most dominant or mean of the labels of the k nearest neighbors, in classification or regression issues, respectively. The number of nearest neighbors (k) is chosen according to the desired limit of success. Nonetheless, two distinct samples may have equal distances to query but, with different angles in the feature space. The similarity of the query to these two samples needs to be weighted in accordance with the angle going between the query and each of the samples to differentiate between the two distances in reference to angular information. This opinion can be analyzed in the context of dependency and can be utilized to increase the precision of classifier. With this point of view, instead of kNN, the query is labeled according to its nearest dependent neighbors that are determined by a joint function, which is built on the similarity and the dependency. This method, therefore, may be called dependent NN (d-NN). To demonstrate d-NN, it is applied to synthetic datasets, which have different statistical distributions, and 4 benchmark datasets, which are Pima Indian, Hepatitis, approximate Sinc and CASP datasets. Results showed the superiority of d-NN in terms of accuracy and computation cost as compared to other employed popular machine learning methods.
ER  - 
TY  - JOUR
T1  - Fault recognition using an ensemble classifier based on Dempster–Shafer Theory
A1  - Wang, Zhen
A1  - Wang, Rongxi
A1  - Gao, Jianmin
A1  - Gao, Zhiyong
A1  - Liang, Yanjie
Y1  - 2020///
KW  - Correlation entropy
KW  - Dempster–Shafer Theory
KW  - Ensemble classifier
KW  - Evidence weight
KW  - Fault recognition
JF  - Pattern Recognition
VL  - 99
SP  - 107079
EP  - 107079
DO  - https://doi.org/10.1016/j.patcog.2019.107079
UR  - https://www.sciencedirect.com/science/article/pii/S0031320319303802
N2  - Aiming at the poor performance of individual classifier in the field of fault recognition, in this paper, a new ensemble classifier is constructed to improve the classification accuracy by combining multiple classifiers based on Dempster–Shafer Theory (DST). However, in some specific cases, especially when dealing with the combination of conflicting evidences, the DST may produce counter-intuitive results and loss its advantages in combining classifiers. To solve this problem, a new improved combination method is proposed to alleviate the conflicts between evidences and a new ensemble technique is developed for the combination of individual classifiers, which can be well used in the design of accurate classifier ensembles. The main advantage of the proposed combination method is that of making the combination process more efficient and accurate by defining the objective weights and subjective weights of member classifiers’ outputs. To verify the effectiveness of the proposed combination method, four individual classifiers are selected for constructing ensemble classifier and tested on Tennessee-Eastman Process (TEP) datasets and UCI machine learning datasets. The experimental results show that the ensemble classifier can significantly improve the classification accuracy and outperforms all the selected individual classifiers. By comparison with other combination methods based on DST and some state-of-the-art ensemble methods, the proposed combination method shows better abilities in dealing with the combination of individual classifiers and outperforms the others in multiple performance measurements. Finally, the proposed ensemble classifier is applied to the fault recognition in real chemical plant.
ER  - 
TY  - JOUR
T1  - An improved ensemble learning approach for the prediction of heart disease risk
A1  - Mienye, Ibomoiye Domor
A1  - Sun, Yanxia
A1  - Wang, Zenghui
Y1  - 2020///
KW  -  Data partitioning
KW  -  Ensemble learning
KW  -  Heart disease
KW  -  Machine learning
KW  - CART
JF  - Informatics in Medicine Unlocked
VL  - 20
SP  - 100402
EP  - 100402
DO  - https://doi.org/10.1016/j.imu.2020.100402
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820304184
N2  - Heart disease is the leading cause of death globally, and early detection is crucial in preventing the progression of the disease. In this paper, an improved machine learning method is proposed for the prediction of heart disease risk. The technique involves randomly partitioning the dataset into smaller subsets using a mean based splitting approach. The various partitions are then modeled using classification and regression tree (CART). A homogeneous ensemble is then created from the different CART models using an accuracy based weighted aging classifier ensemble, which is a modification of the weighted aging classifier ensemble (WAE). The approach ensures optimal performance is achieved. The experimental results on the Cleveland and Framingham datasets achieved classification accuracies of 93% and 91%, respectively, which outperformed other machine learning algorithms and similar scholarly works. The receiver operating characteristic curves further validates the improved performance of the proposed ensemble learning approach. The results show that heart disease risk can be predicted effectively by the proposed ensemble.
ER  - 
TY  - JOUR
T1  - A visual attention guided unsupervised feature learning for robust vessel delineation in retinal images
A1  - Srinidhi, Chetan L
A1  - Aparna, P
A1  - Rajan, Jeny
Y1  - 2018///
KW  - Retinal image
KW  - Unsupervised feature learning
KW  - Vessel segmentation
KW  - Visual attention
JF  - Biomedical Signal Processing and Control
VL  - 44
SP  - 110
EP  - 126
DO  - https://doi.org/10.1016/j.bspc.2018.04.016
UR  - https://www.sciencedirect.com/science/article/pii/S1746809418301010
N2  - Background and objective
Accurate segmentation of retinal vessels from color fundus images play a significant role in early diagnosis of various ocular, systemic and neuro-degenerative diseases. Segmenting retinal vessels is challenging due to varying nature of vessel caliber, the proximal presence of pathological lesions, strong central vessel reflex and relatively low contrast images. Most existing methods mainly rely on carefully designed hand-crafted features to model the local geometrical appearance of vasculature structures, which often lacks the discriminative capability in segmenting vessels from a noisy and cluttered background.
Methods
We propose a novel visual attention guided unsupervised feature learning (VA-UFL) approach to automatically learn the most discriminative features for segmenting vessels in retinal images. Our VA-UFL approach captures both the knowledge of visual attention mechanism and multi-scale contextual information to selectively visualize the most relevant part of the structure in a given local patch. This allows us to encode a rich hierarchical information into unsupervised filtering learning to generate a set of most discriminative features that aid in the accurate segmentation of vessels, even in the presence of cluttered background.
Results
Our proposed method is validated on the five publicly available retinal datasets: DRIVE, STARE, CHASE_DB1, IOSTAR and RC-SLO. The experimental results show that the proposed approach significantly outperformed the state-of-the-art methods in terms of sensitivity, accuracy and area under the receiver operating characteristic curve across all five datasets. Specifically, the method achieved an average sensitivity greater than 0.82, which is 7% higher compared to all existing approaches validated on DRIVE, CHASE_DB1, IOSTAR and RC-SLO datasets, and outperformed even second-human observer. The method is shown to be robust to segmentation of thin vessels, strong central vessel reflex, complex crossover structures and fares well on abnormal cases.
Conclusions
The discriminative features learned via visual attention mechanism is superior to hand-crafted features, and it is easily adaptable to various kind of datasets where generous training images are often scarce. Hence, our approach can be easily integrated into large-scale retinal screening programs where the expensive labelled annotation is often unavailable.
ER  - 
TY  - JOUR
T1  - Discrimination between different degrees of coronary artery disease using time-domain features of the finger photoplethysmogram in response to reactive hyperemia
A1  - Hosseini, Zahra Sadat
A1  - Zahedi, Edmond
A1  - Movahedian Attar, Hamid
A1  - Fakhrzadeh, Hossein
A1  - Parsafar, Mohammad Habib
Y1  - 2015///
KW  -  Coronary stenosis
KW  -  ECG
KW  -  Flow-mediated dilation
KW  -  Triage
KW  - Photoplethysmogram
JF  - Biomedical Signal Processing and Control
VL  - 18
SP  - 282
EP  - 292
DO  - https://doi.org/10.1016/j.bspc.2014.12.011
UR  - https://www.sciencedirect.com/science/article/pii/S1746809414002067
N2  - Atherosclerosis is a major cause of coronary artery disease leading to morbidity and mortality worldwide. Currently, coronary angiography is considered to be the most accurate technique to diagnose coronary artery disease (CAD). However, this technique is an invasive and expensive procedure with risks of serious complications. Since the symptoms of CAD are not noticed until advanced stages of the disease, early and effective diagnosis of CAD is considered a pertinent measure. In this paper, a non-invasive optical signal, the finger photoplethysmogram (PPG) obtained before and after reactive hyperemia is investigated to discriminate between subjects with different CAD conditions. To this end, the PPG from both index fingers and standard 3-lead ECG of 48 patients (16 females, age 54.3±9.6 years and 32 males, age 59.9±10.6 years) scheduled for diagnostic angiography were recorded. The coronary condition of each subject was determined by three expert cardiologists (ground truth) based on these coronary angiograms. Of the 48 patients, 18 were diagnosed as having no disease (normal coronary – NC), 3 were diagnosed as having mild stenosis (MLD), 11 had single-vessel disease (SVD), 5 had two-vessel disease (2VD) and the remaining 11 were reported to have three-vessel disease (3VD). A vessel disease was determined when a significant (more than 50%) stenosis of the lumen cross-sectional area was observed. The 48 subjects were first grouped into two classes, namely high-risk: Class 1={2VD, 3VD} (N=16) and low-risk: Class 2={NC, Mild, SVD} (N=32). Using this approach, classification using a k-Nearest Neighbor classifier leads to an accuracy of 81.5%, a sensitivity of 82.0% and a specificity of 80.9%. Then all 48 subjects were regrouped slightly differently by moving the SVD subjects from the second (low-risk) to the first (high-risk) class. Therefore for the second approach high-risk: Class 1={SVD, 2VD, 3VD} (N=27), whereas low-risk: Class 2={NC, Mild} (N=21). This second approach resulted in an accuracy of 78.8%, a sensitivity of 79.3% and a specificity of 78.3%. We submit that this technique can be employed to implement an efficient triage system for scheduling coronary angiography, as it is able to identify non-invasively patients at greater risk of coronary stenosis.
ER  - 
TY  - JOUR
T1  - Temporal electronic phenotyping by mining careflows of breast cancer patients
A1  - Dagliati, A
A1  - Sacchi, L
A1  - Zambelli, A
A1  - Tibollo, V
A1  - Pavesi, L
A1  - Holmes, J H
A1  - Bellazzi, R
Y1  - 2017///
KW  -  Careflow mining
KW  -  Heterogeneous data sets
KW  -  Temporal data mining
KW  - Electronic phenotyping
JF  - Journal of Biomedical Informatics
VL  - 66
SP  - 136
EP  - 147
DO  - https://doi.org/10.1016/j.jbi.2016.12.012
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416301861
N2  - In this work we present a careflow mining approach designed to analyze heterogeneous longitudinal data and to identify phenotypes in a patient cohort. The main idea underlying our approach is to combine methods derived from sequential pattern mining and temporal data mining to derive frequent healthcare histories (careflows) in a population of patients. This approach was applied to an integrated data repository containing clinical and administrative data of more than 4000 breast cancer patients. We used the mined histories to identify sub-cohorts of patients grouped according to healthcare activities pathways, then we characterized these sub-cohorts with clinical data. In this way, we were able to perform temporal electronic phenotyping of electronic health records (EHR) data.
ER  - 
TY  - JOUR
T1  - gsaINknn: A GSA optimized, lattice computing knn classifier
A1  - Jamshidi, Yazdan
A1  - Kaburlasos, Vassilis G
Y1  - 2014///
KW  -  Classification
KW  -  Gravitational search algorithm (GSA)
KW  -  Intervals׳ number (IN)
KW  -  Lattice computing (LC)
KW  -  k-Nearest neighbor (knn)
KW  - Computing with words (CWW)
JF  - Engineering Applications of Artificial Intelligence
VL  - 35
SP  - 277
EP  - 285
DO  - https://doi.org/10.1016/j.engappai.2014.06.018
UR  - https://www.sciencedirect.com/science/article/pii/S0952197614001407
N2  - This work proposes an effective synergy of the Intervals׳ Number k-nearest neighbor (INknn) classifier, that is a granular extension of the conventional knn classifier in the metric lattice of Intervals׳ Numbers (INs), with the gravitational search algorithm (GSA) for stochastic search and optimization. Hence, the gsaINknn classifier emerges whose effectiveness is demonstrated here on 12 benchmark classification datasets. The experimental results show that the gsaINknn classifier compares favorably with alternative classifiers from the literature. The far-reaching potential of the gsaINknn classifier in computing with words is also delineated.
ER  - 
TY  - JOUR
T1  - Effect of the internal carotid artery degree of stenosis on wall and plaque distensibility
A1  - Loizou, Christos P
A1  - Pantzaris, Marios
A1  - Kyriacou, Efthyvoulos
A1  - Nicolaides, Andrew N
A1  - Pattichis, Constantinos S
Y1  - 2021///
KW  - B-mode ultrasound
KW  - Carotid artery
KW  - Degree of stenosis
KW  - M-mode
KW  - State-based identification
KW  - Strain measurements
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102572
EP  - 102572
DO  - https://doi.org/10.1016/j.bspc.2021.102572
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421001695
N2  - Carotid distensibility is an indicator of carotid elasticity, which can be used as a cardiovascular disease risk factor. The objective of this study was to investigate how the degree of stenosis influences the strain characteristics of wall and plaque in the internal carotid artery (ICA). Ultrasound videos of the ICA from 83 subjects with atherosclerotic plaques at the origin of the ICA producing variable degrees of stenosis from 10 % to 85 % were investigated. We estimate for each video a carotid diameter during contraction (CDC), a carotid diameter during distension (CDD) in mm and a % of carotid plaque distension (%CPD). We have also estimated the radial strain at wall and plaque (%RSw, %RSp), the longitudinal strain at wall (%LSw) and the shear strain at wall (%SSw) and plaque (%SSp) respectively. The main findings were the following: 1) The %CPD is linearly related to the percentage of ICA stenosis decreasing with an increase in stenosis. 2) There is a statistical significant difference between the % of ICA plaque distensibility of the AS group vs the SY group. 3) There is a statistical significant difference for the RSw, and LSw and %SSw of ICA wall of the AS group vs the SY group. 4) There is a statistical significant difference for the %SSp and %RSp between the AS group vs the SY group. The degree of the ICA stenosis is associated with strain metrics of the carotid wall and plaque. Future work will investigate the proposed methodology on more subjects.
ER  - 
TY  - JOUR
T1  - Detection of congestive heart failure from short-term heart rate variability segments using hybrid feature selection approach
A1  - Jovic, Alan
A1  - Brkic, Karla
A1  - Krstacic, Goran
Y1  - 2019///
KW  -  Classification
KW  -  Congestive heart failure
KW  -  Electrocardiogram analysis
KW  -  Feature extraction
KW  -  Heart rate variability
KW  - Feature selection
JF  - Biomedical Signal Processing and Control
VL  - 53
SP  - 101583
EP  - 101583
DO  - https://doi.org/10.1016/j.bspc.2019.101583
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419301636
N2  - Objectives
The aim of this work is to investigate the accuracy limits of automated detection of congestive heart failure (CHF) from short-term heart rate variability (HRV) series. Short-term HRV analysis uses 5-minute segments from HRV recordings to diagnose a disorder. This work proposes a hybrid feature selection procedure aimed at finding highly accurate models containing only a few highly informative features, which enables physiological interpretation of the features relevant for the model.
Materials and methods
Short-term HRV segments are analyzed for CHF diagnosis. Subjects' records from four public PhysioNet databases are considered (66 healthy subjects and 42 CHF subjects). The problem is approached from a machine learning perspective, by extracting 111 linear time domain, frequency domain, time-frequency, nonlinear and symbolic dynamics HRV features. A multistage hybrid feature selection method is proposed that eventually eliminates most features. The method uses a symmetrical uncertainty filter, Naive Bayes wrapper with best first search, and final greedy iterative feature elimination. For classification purposes, we use rotation forest (RTF), radial based support vector machines (SVM), random forest (RF), multilayer perceptron artificial neural network, and k-nearest neighbors’ classifiers in order to evaluate the feature sets at each step of the process and to obtain as accurate model as possible. Leave-one-subject-out cross-validation evaluation method was used, with two variants: subject-level (coarse-grained) and feature vector-level (fine-grained).
Results
The results show that the feature selection method is capable of either improving or retaining the classification accuracy of the full feature set (RTF: subject-level ACC = 88.9%, feature vector-level ACC = 85.6%; SVM: subject-level ACC = 89.8%, feature vector-level ACC = 83.5%; RF: subject-level ACC = 87.0%, feature vector-level ACC = 85.5%), while greatly reducing the number of included features, to only four HRV features for RTF and RF, and only two HRV features for SVM. The resulting best models for subject-level classification achieved are: RTF: ACC = 90.7%, SENS = 78.6%, SPEC = 98.6%, obtained with features: LF/HF ratio, maximum alphabet entropy, alphabet entropy variance, and HaarWaveletSD (scale = 8); SVM: ACC = 88.0%, SENS = 78.6%, SPEC = 93.9%, obtained with features: LF/HF ratio and Rate_U; RF: ACC = 90.7%, SENS = 78.6%, SPEC = 98.6%, obtained with features: LF/HF ratio, maximum alphabet entropy, Rate_U, and Rate_B. Other classifiers provided similar, but somewhat lower results. A comparison of the procedure with the results of individual filter, wrapper, and simple hybrid approaches is provided, which demonstrates the efficiency of the proposed procedure.
Conclusions
The results suggest that the method can achieve accurate generalizable models for automated diagnosis of CHF from short-term HRV segments in subjects with very few informative features. The choice of the best features and the classification results are similar between the three best classifiers, so the use of any of them with the proposed method is recommended. Nonlinear and symbolic dynamics features are shown to have an important role in the resulting models. The presented methodology may be useful for first-hand screening for CHF as well as for similar diagnostic or automated detection problems in biomedicine.
ER  - 
TY  - JOUR
T1  - Adaptive diagnostic machine learning technique for classification of cell decisions for AKT protein
A1  - Salau, Ayodeji Olalekan
A1  - Jain, Shruti
Y1  - 2021///
KW  - AKT Protein
KW  - Distribution ID plots
KW  - MLP
KW  - Neural networks
KW  - RBF
JF  - Informatics in Medicine Unlocked
VL  - 23
SP  - 100511
EP  - 100511
DO  - https://doi.org/10.1016/j.imu.2021.100511
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821000010
N2  - Artificial intelligence techniques can unravel clinically consistent information in clinical data which in turn assist in decision-making. With the help of state-of-the-art techniques, diagnostic systems are used to identify various diseases by using different machine learning (ML) techniques. In this paper, a combination of ML techniques such as Radial Basis Function (RBF) and Multiple Layer Perceptron (MLP) was used to predict cell decisions (cell survival/death) of AKT protein. AKT signalling networks have various downstream consequences on cellular metabolism either directly through the regulation of nutrient transporters, metabolic enzymes or indirectly through the control of transcription factors that regulate the expression of metabolic pathways which determine cell survival, cell growth, and cell death. Experimental analysis was performed in this work to examine the signalling networks that determine cell survival/death decisions by using an amalgamation of three proteins for ten different combinations in 13 different slices for a period of 0–24 h. P-P plot, Q-Q plot, and histogram tests were used for data visualization to determine which distribution the data fits. In addition, goodness of fit test was also employed using distribution functions such as Weibull, Exponential, and Normal distribution to determine whether the data fits a distribution of a certain population. The results were validated by calculating the MTTF values. The results of the analysis performed show that the Weibull distribution yields remarkable results. Also the results obtained with the Multiple Layer Perceptron, MLP 10-8-1 was found to perform better than other techniques giving an accuracy of 99.33% when the exponential activation function was used. The results of the experimental study indicate that it is possible to create self-consistent cell-signalling compendia based on AKT protein data that have been computationally simulated to provide valuable insights for cell survival/death regulation.
ER  - 
TY  - JOUR
T1  - A disease-specific language representation model for cerebrovascular disease research
A1  - Lin, Ching-Heng
A1  - Hsu, Kai-Cheng
A1  - Liang, Chih-Kuang
A1  - Lee, Tsong-Hai
A1  - Liou, Chia-Wei
A1  - Lee, Jiann-Der
A1  - Peng, Tsung-I
A1  - Shih, Ching-Sen
A1  - Fann, Yang C
Y1  - 2021///
KW  -  Cerebrovascular disease
KW  -  Specific language representation model
KW  - Natural language processing
JF  - Computer Methods and Programs in Biomedicine
VL  - 211
SP  - 106446
EP  - 106446
DO  - https://doi.org/10.1016/j.cmpb.2021.106446
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721005204
N2  - Background
Effectively utilizing disease-relevant text information from unstructured clinical notes for medical research presents many challenges. BERT (Bidirectional Encoder Representation from Transformers) related models such as BioBERT and ClinicalBERT, pre-trained on biomedical corpora and general clinical information, have shown promising performance in various biomedical language processing tasks.
Objectives
This study aims to explore whether a BERT-based model pre-trained on disease-related clinical information can be more effective for cerebrovascular disease-relevant research.
Methods
This study proposed the StrokeBERT which was initialized from BioBERT and pre-trained on large-scale cerebrovascular disease related clinical text information. The pre-trained corpora contained 113,590 discharge notes, 105,743 radiology reports, and 38,199 neurological reports. Two real-world empirical clinical tasks were conducted to validate StrokeBERT's performance. The first task identified extracranial and intracranial artery stenosis from two independent sets of radiology angiography reports. The second task predicted the risk of recurrent ischemic stroke based on patients’ first discharge information.
Results
In stenosis detection, StrokeBERT showed improved performance on targeted carotid arteries, with an average AUC compared to that of ClinicalBERT of 0.968 ± 0.021 and 0.956 ± 0.018, respectively. In recurrent ischemic stroke prediction, after 10-fold cross-validation on 1,700 discharge information, StrokeBERT presented better prediction ability (AUC±SD = 0.838 ± 0.017) than ClinicalBERT (AUC±SD = 0.808 ± 0.045). The attention scores of StrokeBERT showed better ability to detect and associate cerebrovascular disease related terms than current BERT based models.
Conclusions
This study shows that a disease-specific BERT model improved the performance and accuracy of various disease-specific language processing tasks and can readily be fine-tuned to advance cerebrovascular disease research and further developed for clinical applications.
ER  - 
TY  - JOUR
T1  - Identification of key features using topological data analysis for accurate prediction of manufacturing system outputs
A1  - Guo, Wei
A1  - Banerjee, Ashis G
Y1  - 2017///
KW  -  Fault detection
KW  -  Feature selection
KW  -  Yield prediction
KW  - Topological data analysis
JF  - Journal of Manufacturing Systems
VL  - 43
SP  - 225
EP  - 234
DO  - https://doi.org/10.1016/j.jmsy.2017.02.015
UR  - https://www.sciencedirect.com/science/article/pii/S0278612517300286
N1  - High Performance Computing and Data Analytics for Cyber Manufacturing
N2  - Topological data analysis (TDA) has emerged as one of the most promising approaches to extract insights from high-dimensional data of varying types such as images, point clouds, and meshes, in an unsupervised manner. To the best of our knowledge, here, we provide the first successful application of TDA in the manufacturing systems domain. We apply a widely used TDA method, known as the Mapper algorithm, on two benchmark data sets for chemical process yield prediction and semiconductor wafer fault detection, respectively. The algorithm yields topological networks that capture the intrinsic clusters and connections among the clusters present in the data sets, which are difficult to detect using traditional methods. We select key process variables or features that impact the system outcomes by analyzing the network shapes. We then use predictive models to evaluate the impact of the selected features. Results show that the models achieve at least the same level of high prediction accuracy as with all the process variables, thereby, providing a way to carry out process monitoring and control in a more cost-effective manner.
ER  - 
TY  - JOUR
T1  - An automated workflow for the biomechanical simulation of a tibia with implant using computed tomography and the finite element method
A1  - Dahmen, T
A1  - Roland, M
A1  - Tjardes, T
A1  - Bouillon, B
A1  - Slusallek, P
A1  - Diebels, S
Y1  - 2015///
KW  -  Finite element method
KW  -  Fractured tibia with implant
KW  -  Hanging nodes
KW  -  Patient-specific simulations
KW  - Adaptive volume-mesh generation
JF  - Computers & Mathematics with Applications
VL  - 70
IS  - 5
SP  - 903
EP  - 916
DO  - https://doi.org/10.1016/j.camwa.2015.06.009
UR  - https://www.sciencedirect.com/science/article/pii/S0898122115002953
N2  - In this study, a fully automated workflow is presented for the biomechanical simulation of bone–implant systems using the example of a fractured tibia. The workflow is based on routinely acquired tomographic data and consists of an automatic segmentation and material assignment, followed by a mesh generation step and, finally, a mechanical simulation using the finite element method (FEM). Because of the high computational costs of the FEM simulations, an adaptive mesh refinement scheme was developed that limits the highest resolution to materials that can take large amounts of mechanical stress. The scheme was analyzed and it was shown that it has no relevant impact on the simulation precision. Thus, a fully automatic, reliable and computationally feasible method to simulate mechanical properties of bone–implant systems was presented, which can be used for numerous applications, ranging from the design of patient-specific implants to surgery preparation and post-surgery implant verification.
ER  - 
TY  - JOUR
T1  - Prediction of COVID Criticality Score with Laboratory, Clinical and CT Images using Hybrid Regression Models
A1  - Perumal, Varalakshmi
A1  - Narayanan, Vasumathi
A1  - Rajasekar, Sakthi Jaya Sundar
Y1  - 2021///
KW  - COVID Criticality score
KW  - Chest CT images
KW  - Clinical features
KW  - Convolutional neural network (CNN)
KW  - Laboratory features
KW  - Regression models
JF  - Computer Methods and Programs in Biomedicine
VL  - 209
SP  - 106336
EP  - 106336
DO  - https://doi.org/10.1016/j.cmpb.2021.106336
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721004107
N2  - Background and Objective: Rapid and precise diagnosis of COVID-19 is very critical in hotspot regions. The main aim of this proposed work is to investigate the baseline, laboratory and CT features of COVID-19 affected patients of two groups (Early and Critical stages). The detection model for COVID-19 is built depending upon the manifestations that define the severity of the disease. Methods: The CT scan images are fed into the various deep learning, machine learning and hybrid learning models to mine the necessary features and predict CT Score. The predicted CT score along with other clinical, laboratory and CT scan image features are then passed to train the various Regression models for predicting the COVID Criticality (CC) Score. These baseline, laboratory and CT features of COVID-19 are reduced using Statistical analysis and Univariate logistic regression analysis. Results: When analysing the prediction of CT scores using images alone, AlexNet+Lasso yields better outcome with regression score of 0.9643 and RMSE of 0.0023 when compared with Decision tree (RMSE of 0.0034; Regression score of 0.9578) and GRU (RMSE of 0.1253; regression score of 0.9323). When analysing the prediction of CC scores using CT scores and other baseline, laboratory and CT features, VGG-16+Linear Regression yields better results with regression score of 0.9911 and RMSE of 0.0002 when compared with Linear SVR (RMSE of 0.0006; Regression score of 0.9911) and LSTM (RMSE of 0.0005; Regression score of 0.9877). The correlation analysis is performed to identify the significance of utilizing other features in prediction of CC Score. The correlation coefficient of CT scores with actual value is 0.93 and 0.92 for Early stage group and Critical stage group respectively. The correlation coefficient of CC scores with actual value is 0.96 for Early stage group and 0.95 for Critical stage group.The classification of COVID-19 patients are carried out with the help of predicted CC Scores. Conclusions: This proposed work is carried out in the motive of helping radiologists in faster categorization of COVID patients as Early or Severe staged using CC Scores. The automated prediction of COVID Criticality Score using our diagnostic model can help radiologists and physicians save time for carrying out further treatment and procedures.
ER  - 
TY  - JOUR
T1  - Automatic identification of anterior segment eye abnormality
A1  - Acharya U, R
A1  - Wong, L Y
A1  - Ng, E Y K
A1  - Suri, J S
Y1  - 2007///
KW  - Arc cornéen
KW  - Brume cornéenne
KW  - Cataract
KW  - Cataracte
KW  - Corneal arcus
KW  - Corneal haze
KW  - Eye
KW  - Iridocyclite
KW  - Iridocyclitis
KW  - Sensibilité
KW  - Sensitivity
KW  - Specificity
KW  - Spécificité
KW  - Œil
JF  - IRBM
VL  - 28
IS  - 1
SP  - 35
EP  - 41
DO  - https://doi.org/10.1016/j.rbmret.2007.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S1297956207000083
N2  - The eyes are complex sensory organs and are designed to optimize vision under conditions of varying light. There are a number of eye disorders that can influence vision. Eye disorders among the elderly are a major health problem. With advancing age, the normal function of eye tissues decreases and there is an increased incidence of ocular pathology. The most common symptoms elicited from ocular diseases are few in number and non-specific in nature: blurred vision, pain, and redness. Cataracts occur most frequently in older people and have significant impact on an individual's quality of life. There are effective therapies and visual aids for these potential vision-limiting conditions. Corneal haze a complication of refractive surgery is characterized by the cloudiness of the normally clear cornea. Iridocyclitis is the inflammation of the Iris and ciliary body. In corneal arcus are white circles in the cornea of the eye caused by fatty deposits. So, there is a need to diagnose to the normal eye from the abnormal one. This paper presents an identification of normal eye image and abnormal (consists of five kinds of eye images) classes using radial basis function (RBF) classifier. The features are extracted from the raw images using the image processing techniques and fuzzy K-means algorithm. Our system uses 150 subjects, consisting of five different kinds of eye disease conditions. We demonstrated a sensitivity of 90%, for the classifier with the specificity of 100%. Our systems are ready clinically to run on large amount of data sets.
Résumé
Les yeux sont des organes sensoriels complexes conçus pour optimiser la vision en ambiance lumineuse variable. De nombreuses pathologies de l'œil peuvent détériorer la vision et les détecter chez les personnes âgées est un problème de santé majeur. Avec l'allongement de l'âge, les fonctions normales des tissus oculaires déclinent avec une fréquence accrue des pathologies. Les symptômes les plus couramment découverts sont peu nombreux et de nature non spécifique: vision floue, douleur et rougeur. Les cataractes surviennent fréquemment chez les sujets âgés et ont un impact considérable sur la qualité de vie. Des thérapies efficaces et des aides visuelles existent mais peuvent présenter des complications. La brume cornéenne est une complication chirurgicale caractérisée par l'aspect nuageux de la cornée normalement claire. L'iridocyclite est une inflammation de l'iris et du corps ciliaire. Dans l'arc cornéen, des cercles blancs correspondent à des dépôts gras sur la cornée. Il existe donc un besoin important de diagnostic pour différencier l'œil pathologique de l'œil normal. Ce papier présente une méthode de classification automatique d'image d'œil anormal en classes utilisant un classifieur à fonction de base radiale (RBF). Les traits caractéristiques sont extraits des données brutes en utilisant des techniques de traitement d'images et un algorithme des K-means flou. Notre système a été testé sur 150 sujets présentant cinq types différents de pathologie oculaire. Nous avons observé une sensibilité du classifieur de 90% et une spécificité de 100%. Notre système est maintenant prêt à être utilisé en routine clinique sur un grand nombre de sujets.
ER  - 
TY  - JOUR
T1  - An LDA based sensor selection approach used in breath analysis system
A1  - Guo, Dongmin
A1  - Zhang, David
A1  - Zhang, Lei
Y1  - 2011///
KW  -  Breath analysis
KW  -  Linear discriminant analysis
KW  -  Sensor selection
KW  - Disease recognition
JF  - Sensors and Actuators B: Chemical
VL  - 157
IS  - 1
SP  - 265
EP  - 274
DO  - https://doi.org/10.1016/j.snb.2011.03.061
UR  - https://www.sciencedirect.com/science/article/pii/S0925400511002577
N2  - Abstract
In the application of breath analysis, the redundant specificities of a sensor array often exceed the needs of the discrimination application. When recognizing types of diseases, not all sensors are required. However, it is difficult to decide which sensor is more useful for an unknown sample because some sensors are cross-sensitive to the biomarkers of the diseases. In this paper, we propose a linear discriminant analysis (LDA) based sensor selection technique (LDASS) which chooses an optimal configuration of sensors for a particular application from a whole set of available sensors. The proposed method finds the direction w via the LDA such that when data are projected onto this direction, the samples from two classes are as separate as possible. It is found that after projection, the difference of means of the two distinct sample classes can be expressed as the linear combination of the responses of all the sensors in the system, and w can be regarded as the weight vectors for these sensors which indicate the contribution weight of each sensor. Accordingly, it is possible to determine which sensor has a greater contribution in classifying the two classes. A series of experiments on different databases show that the proposed method outperforms other sensor selection techniques, such as the sequential forward selection (SFS) and genetic algorithm (GA) in recognition accuracy and processing time. This technique is not only applicable for breath analysis, but also useful in the general applications of e-noses.
ER  - 
TY  - JOUR
T1  - Histogram equalization techniques for enhancement of low radiance retinal images for early detection of diabetic retinopathy
A1  - Singh, Navdeep
A1  - Kaur, Lakhwinder
A1  - Singh, Kuldeep
Y1  - 2019///
KW  -  Entropy
KW  -  Radiance
KW  -  Retinal image enhancement
KW  - Histogram equalization
JF  - Engineering Science and Technology, an International Journal
VL  - 22
IS  - 3
SP  - 736
EP  - 745
DO  - https://doi.org/10.1016/j.jestch.2019.01.014
UR  - https://www.sciencedirect.com/science/article/pii/S221509861831797X
N2  - Images are sometimes affected by improper illumination and are dark. This happens usually in medical images or the images acquired in low light conditions. This paper focuses on retinal imaging and proposes two techniques, RIHE-RVE (Radiance indicator based histogram equalization for retinal vessel enhancement) and RIHE-RRVE (Radiance indicator based histogram equalization for recursive retinal vessel enhancement) to address the problem of low light radiance. The techniques separate the histogram into sub-histograms at the split value determined by the tuneable parameter, ψ. RIHE-RVE recursively performs histogram integration after each split followed by equalization whereas in RIHE-RRVE histogram split can be done to any level (which is decided by the parameter,r) followed by equalization and integration. It has been observed from a comprehensive literature survey that very few algorithms exist that enhance the quality of retinal images. The proposed methods efficiently address the low light radiance problem. Performance evaluation of the techniques is done in terms of Information content (Entropy), PSNR (Peak signal to noise ratio), SSIM (Structure similarity index measurement), Euclidean distance and visual quality inspection. To demonstrate the robustness of the proposed methods, the techniques are not only applied specifically to publicly available retinal databases DRIVE, STARE and CHASE_DB1 but also to some of the MRI images taken from publicly available OASIS database. Results show that the proposed techniques outperform the state of the art techniques especially in low radiance images.
ER  - 
TY  - JOUR
T1  - A cognitive IoT-based framework for effective diagnosis of COVID-19 using multimodal data
A1  - Jayachitra, V P
A1  - Nivetha, S
A1  - Nivetha, R
A1  - Harini, R
Y1  - 2021///
KW  -  Deep learning
KW  -  Image
KW  -  Multimodal fusion
KW  -  Reduced network complexity
KW  -  Self-diagnosis
KW  - Audio
JF  - Biomedical Signal Processing and Control
VL  - 70
SP  - 102960
EP  - 102960
DO  - https://doi.org/10.1016/j.bspc.2021.102960
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421005577
N2  - The COVID-19 emerged at the end of 2019 and has become a global pandemic. There are many methods for COVID-19 prediction using a single modality. However, none of them predicts with 100% accuracy, as each individual exhibits varied symptoms for the disease. To decrease the rate of misdiagnosis, multiple modalities can be used for prediction. Besides, there is also a need for a self-diagnosis system to narrow down the risk of virus spread in testing centres. Therefore, we propose a robust IoT and deep learning-based multi-modal data classification method for the accurate prediction of COVID-19. Generally, highly accurate models require deep architectures. In this work, we introduce two lightweight models, namely CovParaNet for audio (cough, speech, breathing) classification and CovTinyNet for image (X-rays, CT scans) classification. These two models were identified as the best unimodal models after comparative analysis with the existing benchmark models. Finally, the obtained results of the five independently trained unimodal models are integrated by a novel dynamic multimodal Random Forest classifier. The lightweight CovParaNet and CovTinyNet models attain a maximum accuracy of 97.45% and 99.19% respectively even with a small dataset. The proposed dynamic multimodal fusion model predicts the final result with 100% accuracy, precision, and recall, and the online retraining mechanism enables it to extend its support even in a noisy environment. Furthermore, the computational complexity of all the unimodal models is minimized tremendously and the system functions effectively with 100% reliability even in the absence of any one of the input modalities during testing.
ER  - 
TY  - JOUR
T1  - Word embeddings and external resources for answer processing in biomedical factoid question answering
A1  - Dimitriadis, Dimitris
A1  - Tsoumakas, Grigorios
Y1  - 2019///
KW  -  Answer processing
KW  -  Supervised method
KW  -  Word embeddings
KW  - Biomedical question answering
JF  - Journal of Biomedical Informatics
VL  - 92
SP  - 103118
EP  - 103118
DO  - https://doi.org/10.1016/j.jbi.2019.103118
UR  - https://www.sciencedirect.com/science/article/pii/S153204641930036X
N2  - Biomedical question answering (QA) is a challenging task that has not been yet successfully solved, according to results on international benchmarks, such as BioASQ. Recent progress on deep neural networks has led to promising results in domain independent QA, but the lack of large datasets with biomedical question-answer pairs hinders their successful application to the domain of biomedicine. We propose a novel machine-learning based answer processing approach that exploits neural networks in an unsupervised way through word embeddings. Our approach first combines biomedical and general purpose tools to identify the candidate answers from a set of passages. Candidates are then represented using a combination of features based on both biomedical external resources and input textual sources, including features based on word embeddings. Candidates are then ranked based on the score given at the output of a binary classification model, trained from candidates extracted from a small number of questions, related passages and correct answer triplets from the BioASQ challenge. Our experimental results show that the use of word embeddings, combined with other features, improves the performance of answer processing in biomedical question answering. In addition, our results show that the use of several annotators improves the identification of answers in passages. Finally, our approach has participated in the last two versions (2017, 2018) of the BioASQ challenge achieving competitive results.
ER  - 
TY  - JOUR
T1  - Random forest swarm optimization-based for heart diseases diagnosis
A1  - Asadi, Shahrokh
A1  - Roshan, SeyedEhsan
A1  - Kattan, Michael W
Y1  - 2021///
KW  - Data mining
KW  - Diversity
KW  - Ensemble learning
KW  - Heart disease
KW  - Random forest
JF  - Journal of Biomedical Informatics
VL  - 115
SP  - 103690
EP  - 103690
DO  - https://doi.org/10.1016/j.jbi.2021.103690
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000198
N2  - Heart disease has been one of the leading causes of death worldwide in recent years. Among diagnostic methods for heart disease, angiography is one of the most common methods, but it is costly and has side effects. Given the difficulty of heart disease prediction, data mining can play an important role in predicting heart disease accurately. In this paper, by combining the multi-objective particle swarm optimization (MOPSO) and Random Forest, a new approach is proposed to predict heart disease. The main goal is to produce diverse and accurate decision trees and determine the (near) optimal number of them simultaneously. In this method, an evolutionary multi-objective approach is used instead of employing a commonly used approach, i.e., bootstrap, feature selection in the Random Forest, and random number selection of training sets. By doing so, different training sets with different samples and features for training each tree are generated. Also, the obtained solutions in Pareto-optimal fronts determine the required number of training sets to build the random forest. By doing so, the random forest's performance can be enhanced, and consequently, the prediction accuracy will be improved. The proposed method's effectiveness is investigated by comparing its performance over six heart datasets with individual and ensemble classifiers. The results suggest that the proposed method with the (near) optimal number of classifiers outperforms the random forest algorithm with different classifiers.
ER  - 
TY  - JOUR
T1  - Gradient based adaptive thresholding
A1  - Yazid, Haniza
A1  - Arof, Hamzah
Y1  - 2013///
KW  - Adaptive thresholding
KW  - Binarization
KW  - Diabetic retinopathy
KW  - Exudate detection
KW  - Gradient based thresholding
KW  - Handwritten document images
KW  - Image segmentation
KW  - Medical image analysis
JF  - Journal of Visual Communication and Image Representation
VL  - 24
IS  - 7
SP  - 926
EP  - 936
DO  - https://doi.org/10.1016/j.jvcir.2013.06.001
UR  - https://www.sciencedirect.com/science/article/pii/S1047320313001144
N2  - For images with poor and non-uniform illumination, adaptive thresholding is required to separate the objects of interest from the background. In this paper a new approach to create an adaptive threshold surface is proposed to segment an image. The technique is inspired by the Yanowitz’s method and is improved upon by the introduction of a simpler and more accurate threshold surface. The method is tested on several images of different patterns with varying illumination and the results are compared to the ones produced by a number of adaptive thresholding algorithms. In order to demonstrate the effectiveness, the proposed method had been implemented in medical and document images. The proposed method compares favorably against those using watershed and morphology in medical image and favorably against variable threshold and adaptive Otsu’s N-thresholding for document image.
ER  - 
TY  - JOUR
T1  - Deep image mining for diabetic retinopathy screening
A1  - Quellec, Gwenolé
A1  - Charrière, Katia
A1  - Boudi, Yassine
A1  - Cochener, Béatrice
A1  - Lamard, Mathieu
Y1  - 2017///
KW  -  Diabetic retinopathy screening
KW  -  Image mining
KW  -  Lesion detection
KW  - Deep learning
JF  - Medical Image Analysis
VL  - 39
SP  - 178
EP  - 193
DO  - https://doi.org/10.1016/j.media.2017.04.012
UR  - https://www.sciencedirect.com/science/article/pii/S136184151730066X
N2  - Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: Az=0.954 in Kaggle’s dataset and Az=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor® system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.
ER  - 
TY  - JOUR
T1  - Automatically finding relevant citations for clinical guideline development
A1  - Bui, Duy Duc An
A1  - Jonnalagadda, Siddhartha
A1  - Del Fiol, Guilherme
Y1  - 2015///
KW  -  Medical subject headings
KW  -  Natural language processing
KW  -  Practice guideline
KW  -  PubMed
KW  - Information retrieval
JF  - Journal of Biomedical Informatics
VL  - 57
SP  - 436
EP  - 445
DO  - https://doi.org/10.1016/j.jbi.2015.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001951
N2  - Objective
Literature database search is a crucial step in the development of clinical practice guidelines and systematic reviews. In the age of information technology, the process of literature search is still conducted manually, therefore it is costly, slow and subject to human errors. In this research, we sought to improve the traditional search approach using innovative query expansion and citation ranking approaches.
Methods
We developed a citation retrieval system composed of query expansion and citation ranking methods. The methods are unsupervised and easily integrated over the PubMed search engine. To validate the system, we developed a gold standard consisting of citations that were systematically searched and screened to support the development of cardiovascular clinical practice guidelines. The expansion and ranking methods were evaluated separately and compared with baseline approaches.
Results
Compared with the baseline PubMed expansion, the query expansion algorithm improved recall (80.2% vs. 51.5%) with small loss on precision (0.4% vs. 0.6%). The algorithm could find all citations used to support a larger number of guideline recommendations than the baseline approach (64.5% vs. 37.2%, p<0.001). In addition, the citation ranking approach performed better than PubMed’s “most recent” ranking (average precision +6.5%, recall@k +21.1%, p<0.001), PubMed’s rank by “relevance” (average precision +6.1%, recall@k +14.8%, p<0.001), and the machine learning classifier that identifies scientifically sound studies from MEDLINE citations (average precision +4.9%, recall@k +4.2%, p<0.001).
Conclusions
Our unsupervised query expansion and ranking techniques are more flexible and effective than PubMed’s default search engine behavior and the machine learning classifier. Automated citation finding is promising to augment the traditional literature search.
ER  - 
TY  - JOUR
T1  - An ensemble-based system for automatic screening of diabetic retinopathy
A1  - Antal, Bálint
A1  - Hajdu, András
Y1  - 2014///
KW  - Automatic screening
KW  - Decision making
KW  - Diabetic retinopathy
KW  - Ensemble learning
KW  - Machine learning
JF  - Knowledge-Based Systems
VL  - 60
SP  - 20
EP  - 27
DO  - https://doi.org/10.1016/j.knosys.2013.12.023
UR  - https://www.sciencedirect.com/science/article/pii/S0950705114000021
N2  - In this paper, an ensemble-based method for the screening of diabetic retinopathy (DR) is proposed. This approach is based on features extracted from the output of several retinal image processing algorithms, such as image-level (quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms, exudates) and anatomical (macula, optic disk) components. The actual decision about the presence of the disease is then made by an ensemble of machine learning classifiers. We have tested our approach on the publicly available Messidor database, where 90% sensitivity, 91% specificity and 90% accuracy and 0.989 AUC are achieved in a disease/no-disease setting. These results are highly competitive in this field and suggest that retinal image processing is a valid approach for automatic DR screening.
ER  - 
TY  - JOUR
T1  - Incorporating repeating temporal association rules in Naïve Bayes classifiers for coronary heart disease diagnosis
A1  - Orphanou, Kalia
A1  - Dagliati, Arianna
A1  - Sacchi, Lucia
A1  - Stassopoulou, Athena
A1  - Keravnou, Elpida
A1  - Bellazzi, Riccardo
Y1  - 2018///
KW  -  Temporal abstraction
KW  -  Temporal association rules
KW  -  Temporal reasoning
KW  -  Time series classification
KW  - Bayesian models
JF  - Journal of Biomedical Informatics
VL  - 81
SP  - 74
EP  - 82
DO  - https://doi.org/10.1016/j.jbi.2018.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418300406
N2  - In this paper, we develop a Naïve Bayes classification model integrated with temporal association rules (TARs). A temporal pattern mining algorithm is used to detect TARs by identifying the most frequent temporal relationships among the derived basic temporal abstractions (TA). We develop and compare three classifiers that use as features the most frequent TARs as follows: (i) representing the most frequent TARs detected within the target class (‘Disease = Present’), (ii) representing the most frequent TARs from both classes (‘Disease = Present’, ‘Disease = Absent’), (iii) representing the most frequent TARs, after removing the ones that are low-risk predictors for the disease. These classifiers incorporate the horizontal support of TARs, which defines the number of times that a particular temporal pattern is found in some patient’s record, as their features. All of the developed classifiers are applied for diagnosis of coronary heart disease (CHD) using a longitudinal dataset. We compare two ways of feature representation, using horizontal support or the mean duration of each TAR, on a single patient. The results obtained from this comparison show that the horizontal support representation outperforms the mean duration. The main effort of our research is to demonstrate that where long time periods are of significance in some medical domain, such as the CHD domain, the detection of the repeated occurrences of the most frequent TARs can yield better performances. We compared the classifier that uses the horizontal support representation and has the best performance with a Baseline Classifier which uses the binary representation of the most frequent TARs. The results obtained illustrate the comparatively high performance of the classifier representing the horizontal support, over the Baseline Classifier.
ER  - 
TY  - JOUR
T1  - Rule-based information extraction from patients’ clinical data
A1  - Mykowiecka, Agnieszka
A1  - Marciniak, Małgorzata
A1  - Kupść, Anna
Y1  - 2009///
KW  -  Linguistic analysis
KW  -  Polish clinical data
KW  - Rule-based information extraction
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 5
SP  - 923
EP  - 936
DO  - https://doi.org/10.1016/j.jbi.2009.07.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046409001002
N1  - Biomedical Natural Language Processing
N2  - The paper describes a rule-based information extraction (IE) system developed for Polish medical texts. We present two applications designed to select data from medical documentation in Polish: mammography reports and hospital records of diabetic patients. First, we have designed a special ontology that subsequently had its concepts translated into two separate models, represented as typed feature structure (TFS) hierarchies, complying with the format required by the IE platform we adopted. Then, we used dedicated IE grammars to process documents and fill in templates provided by the models. In particular, in the grammars, we addressed such linguistic issues as: ambiguous keywords, negation, coordination or anaphoric expressions. Resolving some of these problems has been deferred to a post-processing phase where the extracted information is further grouped and structured into more complex templates. To this end, we defined special heuristic algorithms on the basis of sample data. The evaluation of the implemented procedures shows their usability for clinical data extraction tasks. For most of the evaluated templates, precision and recall well above 80% were obtained.
ER  - 
TY  - JOUR
T1  - A new sampling method for classifying imbalanced data based on support vector machine ensemble
A1  - Jian, Chuanxia
A1  - Gao, Jian
A1  - Ao, Yinhui
Y1  - 2016///
KW  -  Sampling
KW  -  Support vector machine
KW  - Imbalanced data
JF  - Neurocomputing
VL  - 193
SP  - 115
EP  - 122
DO  - https://doi.org/10.1016/j.neucom.2016.02.006
UR  - https://www.sciencedirect.com/science/article/pii/S0925231216001843
N2  - The insufficient information from the minority examples cannot exactly represent the inherent structure of the dataset, which leads to a low prediction accuracy of the minority through the existing classification methods. The over- and under-sampling methods help to increase the prediction accuracy of the minority. However, the two methods either lose important information or add trivial information for classification, so as to affect the prediction accuracy of the minority. Therefore, a new different contribution sampling method (DCS) based on the contributions of the support vectors (SVs) and the nonsupport vectors (NSVs) to classification is proposed in this paper. The proposed DCS method applies different sampling methods for the SVs and the NSVs and uses the biased support vector machine (B-SVM) method to identify the SVs and the NSVs of an imbalanced data. Moreover, the synthetic minority over-sampling technique (SMOTE) and the random under-sampling technique (RUS) are used in the proposed method to re-sample the SVs in the minority and the NSVs in the majority, respectively. Examples are labeled by the ensemble of support vector machine (SVMen). Experiments are carried out on the imbalanced dataset which is selected from UCI, AVU06a, Statlog, DP01a, JP98a and CWH03a repositories. Experimental results show that for the imbalanced datasets, the proposed DCS method achieves a better performance in the aspects of Receiver Operating Characteristic (ROC) curve than other methods. The proposed DCS method improves 20.80%, 5.97%, 8.66% and 9.35% in terms of the geometric mean prediction accuracy Gmean as compared with that achieved by using the NS, the US, the SMOTE and the ROS, respectively.
ER  - 
TY  - JOUR
T1  - An annotated association mining approach for extracting and visualizing interesting clinical events
A1  - Shrestha, Aashara
A1  - Zikos, Dimitrios
A1  - Fegaras, Leonidas
Y1  - 2021///
KW  -  Clinical event recognition
KW  -  Sequence mining
KW  - Association rule mining
JF  - International Journal of Medical Informatics
VL  - 148
SP  - 104366
EP  - 104366
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104366
UR  - https://www.sciencedirect.com/science/article/pii/S138650562031902X
N2  - Objective
This work aims at deriving interesting clinical events using association rule mining based on a user-annotated order of clinical features.
Materials and methods
A user specifies a partial temporal order of features by indexing features of interest, with repeated and bundled indexes allowed as needed. An association mining algorithm plugin was designed to generate rules that adhere to the user-specified temporal order. The plugin uses temporal and sequence constraints to reduce rule permutations early in the rule generation process. The method was evaluated with a large medical claims dataset to generate clinical events.
Results
Using the plug-in algorithm, the database is scanned to calculate the support of item sequences whose sequential order conforms with the user annotated feature order. In our experiments with 20,000 medical claim data records, our method generated rules in a significantly less time than the standalone Apriori algorithm. Our approach generates dendrograms to organize the rules into meaningful hierarchies and provides a graphical interface to navigate the rules and unfold interesting clinical events.
Discussion
Since many associations in healthcare are of sequential nature, some of the derived rules may describe interesting clinical flows or events, while others may be contextually irrelevant. Our method exploits user-specified sequence constraints to eliminate irrelevant rules and reduce rule permutations, speeding up rule mining.
Conclusion
This work can be the foundation for future association rule mining studies to extract sequential events based on interestingness. The work can support clinical education where the instructor defines feature sequence constraints, and students unfold and examine extracted sequential rules.
ER  - 
TY  - JOUR
T1  - Retinal artery and venular caliber grading: A semi-automated evaluation tool
A1  - Bhuiyan, Alauddin
A1  - Karmakar, Chandan
A1  - Kawasaki, Ryo
A1  - Lamoureux, Ecosse
A1  - Ramamohanarao, Kotagiri
A1  - Kanagasingam, Yogesan
A1  - Wong, Tien Y
Y1  - 2014///
KW  -  Affine transformation
KW  -  Artery and venular caliber
KW  -  Gradient image
KW  -  Vessel central reflex
KW  - Retinal image
JF  - Computers in Biology and Medicine
VL  - 44
SP  - 1
EP  - 9
DO  - https://doi.org/10.1016/j.compbiomed.2013.07.018
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513001923
N2  - Retinal imaging can facilitate the measurement and quantification of subtle variations and abnormalities in retinal vasculature. Retinal vascular imaging may thus offer potential as a noninvasive research tool to probe the role and pathophysiology of the microvasculature, and as a cardiovascular risk prediction tool. In order to perform this, an accurate method must be provided that is statistically sound and repeatable. This paper presents the methodology of such a system that assists physicians in measuring vessel caliber (i.e., diameters or width) from digitized fundus photographs. The system involves texture and edge information to measure and quantify vessel caliber. The graphical user interfaces are developed to allow retinal image graders to select individual vessel area that automatically returns the vessel calibers for noisy images. The accuracy of the method is validated using the measured caliber from graders and an existing method. The system provides very high accuracy vessel caliber measurement which is also reproducible with high consistency.
ER  - 
TY  - JOUR
T1  - Novel Approach to Predict Hospital Readmissions Using Feature Selection from Unstructured Data with Class Imbalance
A1  - Sundararaman, Arun
A1  - Valady Ramanathan, Srinivasan
A1  - Thati, Ramprasad
Y1  - 2018///
KW  -  Class imbalance
KW  -  Discharge summary
KW  -  Domain related stop words
KW  -  Feature selection
KW  -  Unstructured data
KW  - Predictive analytics
JF  - Big Data Research
VL  - 13
SP  - 65
EP  - 75
DO  - https://doi.org/10.1016/j.bdr.2018.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S2214579617303131
N1  - Big Medical/Healthcare Data Analytics
N2  - Feature selection for predictive analytics continues to be a major challenge in the healthcare industry, particularly as it relates to readmission prediction. Several research works in mining healthcare data have focused on structured data for readmission prediction. Even within those works that are based on unstructured data, significant gaps exist in addressing class imbalance, context specific noise removal which thus necessitates new approaches readmission prediction using unstructured data. In this work, a novel approach is proposed for feature selection and domain related stop words removal from unstructured with class imbalance in discharge summary notes. The proposed predictive model uses these features along with other relevant structured data. Five iterations of predictions were performed to tune and improve the models, results of which are presented and analyzed in this paper. The authors suggest future directions in implementing the proposed approach in hospitals or clinics aimed at leveraging structured and unstructured discharge summary notes.
ER  - 
TY  - JOUR
T1  - Molecular descriptor analysis of approved drugs using unsupervised learning for drug repurposing
A1  - Madugula, Sita Sirisha
A1  - John, Lijo
A1  - Nagamani, Selvaraman
A1  - Gaur, Anamika Singh
A1  - Poroikov, Vladimir V
A1  - Sastry, G Narahari
Y1  - 2021///
KW  -  Approved drugs
KW  -  Clustering
KW  -  Dimensionality reduction
KW  -  Drug repurposing
KW  -  PASS
KW  - Unsupervised learning
JF  - Computers in Biology and Medicine
VL  - 138
SP  - 104856
EP  - 104856
DO  - https://doi.org/10.1016/j.compbiomed.2021.104856
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521006508
N2  - Machine learning and data-driven approaches are currently being widely used in drug discovery and development due to their potential advantages in decision-making based on the data leveraged from existing sources. Applying these approaches to drug repurposing (DR) studies can identify new relationships between drug molecules, therapeutic targets and diseases that will eventually help in generating new insights for developing novel therapeutics. In the current study, a dataset of 1671 approved drugs is analyzed using a combined approach involving unsupervised Machine Learning (ML) techniques (Principal Component Analysis (PCA) followed by k-means clustering) and Structure-Activity Relationships (SAR) predictions for DR. PCA is applied on all the two dimensional (2D) molecular descriptors of the dataset and the first five Principal Components (PC) were subsequently used to cluster the drugs into nine well separated clusters using k-means algorithm. We further predicted the biological activities for the drug-dataset using the PASS (Predicted Activities Spectra of Substances) tool. These predicted activity values are analyzed systematically to identify repurposable drugs for various diseases. Clustering patterns obtained from k-means showed that every cluster contains subgroups of structurally similar drugs that may or may not have similar therapeutic indications. We hypothesized that such structurally similar but therapeutically different drugs can be repurposed for the native indications of other drugs of the same cluster based on their high predicted biological activities obtained from PASS analysis. In line with this, we identified 66 drugs from the nine clusters which are structurally similar but have different therapeutic uses and can therefore be repurposed for one or more native indications of other drugs of the same cluster. Some of these drugs not only share a common substructure but also bind to the same target and may have a similar mechanism of action, further supporting our hypothesis. Furthermore, based on the analysis of predicted biological activities, we identified 1423 drugs that can be repurposed for 366 new indications against several diseases. In this study, an integrated approach of unsupervised ML and SAR analysis have been used to identify new indications for approved drugs and the study provides novel insights into clustering patterns generated through descriptor level analysis of approved drugs.
ER  - 
TY  - JOUR
T1  - An efficient local binary pattern based plantar pressure optical sensor image classification using convolutional neural networks
A1  - Wang, Cunlei
A1  - Li, Donghui
A1  - Li, Zairan
A1  - Wang, Dan
A1  - Dey, Nilanjan
A1  - Biswas, Anjan
A1  - Moraru, Luminita
A1  - Sherratt, R S
A1  - Shi, Fuqian
Y1  - 2019///
KW  - Convolutional neural network
KW  - Image classification
KW  - Image texture
KW  - Local binary pattern
KW  - Optical sensor imaging
KW  - Plantar pressure
JF  - Optik
VL  - 185
SP  - 543
EP  - 557
DO  - https://doi.org/10.1016/j.ijleo.2019.02.109
UR  - https://www.sciencedirect.com/science/article/pii/S0030402619302268
N2  - The objective of this study was to design and produce highly comfortable shoe products guided by a plantar pressure imaging data-set. In this research a plantar pressure optical imaging data-set based classification technology has been developed. In this paper, an improved local binary pattern (LBP) algorithm is used to retrieve texture-based features and recognize patterns from the data-set. A calculation model of plantar pressure imaging feature area is established subsequently. The data-set is classified by a neural network to guide the generation of various shoe-last surfaces. Firstly, the local binary mode is improved to adapt to the pressure imaging data-set, and the texture-based feature calculation is fully used to accurately generate the feature point set; hereafter, the plantar pressure imaging feature point set is then used to guide the design of last free surface forming. In the presented experiments of plantar imaging, multi-dimensional texture-based features and improved LBP features have been found by a convolution neural network (CNN), and compared with a 21-input-3-output two-layer perceptual neural network. Three feet types are investigated in the experiment, being flatfoot (F) referring to the lack of a normal arch, or arch collapse, Talipes Equinovarus (TE), being the front part of the foot is adduction, calcaneus varus, plantar flexion, or Achilles tendon contracture and Normal (N). This research has achieved an 82% accuracy rate with 10 hidden-layers CNN of rotation invariance LBP (RI-LBP) algorithm using 21 texture-based features by comparing other deep learning methods presented in the literature.
ER  - 
TY  - JOUR
T1  - Rule extraction from support vector machines: A review
A1  - Barakat, Nahla
A1  - Bradley, Andrew P
Y1  - 2010///
KW  -  Data mining
KW  -  Information extraction
KW  -  Knowledge discovery
KW  -  Pattern recognition applications
KW  -  SVMs
KW  - Machine learning
JF  - Neurocomputing
VL  - 74
IS  - 1
SP  - 178
EP  - 190
DO  - https://doi.org/10.1016/j.neucom.2010.02.016
UR  - https://www.sciencedirect.com/science/article/pii/S0925231210001591
N1  - Artificial Brains
N2  - Over the last decade, support vector machine classifiers (SVMs) have demonstrated superior generalization performance to many other classification techniques in a variety of application areas. However, SVMs have an inability to provide an explanation, or comprehensible justification, for the solutions they reach. It has been shown that the ‘black-box’ nature of techniques like artificial neural networks (ANNs) is one of the main obstacles impeding their practical application. Therefore, techniques for rule extraction from ANNs, and recently from SVMs, were introduced to ameliorate this problem and aid in the explanation of their classification decisions. In this paper, we conduct a formal review of the area of rule extraction from SVMs. The review provides a historical perspective for this area of research and conceptually groups and analyzes the various techniques. In particular, we propose two alternative groupings; the first is based on the SVM (model) components utilized for rule extraction, while the second is based on the rule extraction approach. The aim is to provide a better understanding of the topic in addition to summarizing the main features of individual algorithms. The analysis is then followed by a comparative evaluation of the algorithms’ salient features and relative performance as measured by a number of metrics. It is concluded that there is no one algorithm that can be favored in general. However, methods that are kernel independent, produce the most comprehensible rule set and have the highest fidelity to the SVM should be preferred. In addition, a specific method can be preferred if the context of the requirements of a specific application, so that appropriate tradeoffs may be made. The paper concludes by highlighting potential research directions such as the need for rule extraction methods in the case of SVM incremental and active learning and other application domains, where special types of SVMs are utilized.
ER  - 
TY  - JOUR
T1  - Text mining for traditional Chinese medical knowledge discovery: A survey
A1  - Zhou, Xuezhong
A1  - Peng, Yonghong
A1  - Liu, Baoyan
Y1  - 2010///
KW  - Review
KW  - Text mining
KW  - Traditional Chinese medicine
JF  - Journal of Biomedical Informatics
VL  - 43
IS  - 4
SP  - 650
EP  - 660
DO  - https://doi.org/10.1016/j.jbi.2010.01.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046410000031
N2  - Extracting meaningful information and knowledge from free text is the subject of considerable research interest in the machine learning and data mining fields. Text data mining (or text mining) has become one of the most active research sub-fields in data mining. Significant developments in the area of biomedical text mining during the past years have demonstrated its great promise for supporting scientists in developing novel hypotheses and new knowledge from the biomedical literature. Traditional Chinese medicine (TCM) provides a distinct methodology with which to view human life. It is one of the most complete and distinguished traditional medicines with a history of several thousand years of studying and practicing the diagnosis and treatment of human disease. It has been shown that the TCM knowledge obtained from clinical practice has become a significant complementary source of information for modern biomedical sciences. TCM literature obtained from the historical period and from modern clinical studies has recently been transformed into digital data in the form of relational databases or text documents, which provide an effective platform for information sharing and retrieval. This motivates and facilitates research and development into knowledge discovery approaches and to modernize TCM. In order to contribute to this still growing field, this paper presents (1) a comparative introduction to TCM and modern biomedicine, (2) a survey of the related information sources of TCM, (3) a review and discussion of the state of the art and the development of text mining techniques with applications to TCM, (4) a discussion of the research issues around TCM text mining and its future directions.
ER  - 
TY  - JOUR
T1  - Retinal vessel delineation using a brain-inspired wavelet transform and random forest
A1  - Zhang, Jiong
A1  - Chen, Yuan
A1  - Bekkers, Erik
A1  - Wang, Meili
A1  - Dashtbozorg, Behdad
A1  - ter Haar Romeny, Bart M
Y1  - 2017///
KW  -  Orientation score (OS)
KW  -  Retinal image
KW  -  Vessel segmentation
KW  -  Wavelet transform
KW  - Random forest
JF  - Pattern Recognition
VL  - 69
SP  - 107
EP  - 123
DO  - https://doi.org/10.1016/j.patcog.2017.04.008
UR  - https://www.sciencedirect.com/science/article/pii/S0031320317301498
N2  - This paper presents a supervised retinal vessel segmentation by incorporating vessel filtering and wavelet transform features from orientation scores (OSs), and green intensity. Through an anisotropic wavelet-type transform, a 2D image is lifted to a 3D orientation score in the Lie-group domain of positions and orientations R2⋊S1. Elongated structures are disentangled into their corresponding orientation planes and enhanced via multi-orientation vessel filtering. In addition, scale-selective OSs (in the domain of positions, orientations and scales R2⋊S1×R+) are obtained by adding a scale adaptation to the wavelet transform. Features are optimally extracted by taking maximum orientation responses at multiple scales, to represent vessels of changing calibers. Finally, we train a Random Forest classifier for vessel segmentation. Extensive validations show that our method achieves a competitive segmentation, and better vessel preservation with less false detections compared with the state-of-the-art methods.
ER  - 
TY  - JOUR
T1  - Evaluation of the Cardiovascular Risk in Middle-aged Workers: An Artificial Neural Networks-based Approach
A1  - Sboev, Alexander
A1  - Gorokhova, Svetlana
A1  - Pfaf, Viktor
A1  - Moloshnikov, Ivan
A1  - Gudovskikh, Dmitry
A1  - Rybka, Roman
A1  - Selivanov, Anton
A1  - Serenko, Aleksey
Y1  - 2016///
KW  -  cardiovascular risk
KW  -  data analysis
KW  -  neural network
KW  - data science
JF  - Procedia Computer Science
VL  - 80
SP  - 2418
EP  - 2422
DO  - https://doi.org/10.1016/j.procs.2016.05.540
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916310316
N1  - International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA
N2  - A method of the evaluation of the risk of cardiovascular events in the group of middle-aged male workers was developed on the basis of artificial neural networks (ANN). The list of analyzed variables included parameters of allostatic load and signs of myocardial involvement. The results were compared with traditional scales and risk charts (SCORE, PROCAM, and Framingham). A better prognostic value of the proposed model was observed, which makes it reasonable to use both additional markers and ANN.
ER  - 
TY  - JOUR
T1  - Local Learning for Multi-layer, Multi-component Predictive System
A1  - Al-Jubouri, Bassma
A1  - Gabrys, Bogdan
Y1  - 2016///
KW  -  feature selection
KW  -  multi-layer multi-component predictive system
KW  - local learning
JF  - Procedia Computer Science
VL  - 96
SP  - 723
EP  - 732
DO  - https://doi.org/10.1016/j.procs.2016.08.256
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916320671
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 20th International Conference KES-2016
N2  - This study introduces a new multi-layer multi-component ensemble. The components of this ensemble are trained locally on subsets of features for disjoint sets of data. The data instances are assigned to local regions using the similarity of their features pairwise squared correlation. Many ensemble methods encourage diversity among their base predictors by training them on different subsets of data or different subsets of features. In the proposed architecture the local regions contain disjoint sets of data and for this data only the most similar features are selected. The pairwise squared correlations of the features are used to weight the predictions of the ensemble's models. The proposed architecture has been tested on a number of data sets and its performance was compared to five benchmark algorithms. The results showed that the testing accuracy of the developed architecture is comparable to the rotation forest and is better than the other benchmark algorithms.
ER  - 
TY  - JOUR
T1  - A review of privacy-preserving techniques for deep learning
A1  - Boulemtafes, Amine
A1  - Derhab, Abdelouahid
A1  - Challal, Yacine
Y1  - 2020///
KW  -  Deep neural network
KW  -  Privacy
KW  -  Privacy preserving
KW  -  Sensitive data
KW  -  Taxonomy
KW  - Deep learning
JF  - Neurocomputing
VL  - 384
SP  - 21
EP  - 45
DO  - https://doi.org/10.1016/j.neucom.2019.11.041
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219316431
N2  - Deep learning is one of the advanced approaches of machine learning, and has attracted a growing attention in the recent years. It is used nowadays in different domains and applications such as pattern recognition, medical prediction, and speech recognition. Differently from traditional learning algorithms, deep learning can overcome the dependency on hand-designed features. Deep learning experience is particularly improved by leveraging powerful infrastructures such as clouds and adopting collaborative learning for model training. However, this comes at the expense of privacy, especially when sensitive data are processed during the training and the prediction phases, as well as when training model is shared. In this paper, we provide a review of the existing privacy-preserving deep learning techniques, and propose a novel multi-level taxonomy, which categorizes the current state-of-the-art privacy-preserving deep learning techniques on the basis of privacy-preserving tasks at the top level, and key technological concepts at the base level. This survey further summarizes evaluation results of the reviewed solutions with respect to defined performance metrics. In addition, it derives a set of learned lessons from each privacy-preserving task. Finally, it highlights open research challenges and provides some recommendations as future research directions.
ER  - 
TY  - JOUR
T1  - Machine Learning for personalised stress detection: Inter-individual variability of EEG-ECG markers for acute-stress response
A1  - Gonzalez-Carabarin, L
A1  - Castellanos-Alvarado, E A
A1  - Castro-Garcia, P
A1  - Garcia-Ramirez, M A
Y1  - 2021///
KW  -  ECG
KW  -  EEG
KW  -  Stress
KW  - Machine Learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 209
SP  - 106314
EP  - 106314
DO  - https://doi.org/10.1016/j.cmpb.2021.106314
UR  - https://www.sciencedirect.com/science/article/pii/S0169260721003886
N2  - Stress appears as a response for a broad variety of physiological stimuli. It does vary among individuals in amplitude, phase and frequency. Thus, the necessity for personalised diagnosis is key to prevent stress-related diseases. In order to evaluate stress levels, a multi-sensing system is proposed based on non-invasive EEG and ECG signals. A target population of 24 individuals which age range between 18–23 years old are intentionally exposed to control-induced stress tests while EEG and ECG are simultaneously recorded. The acquired signals are processed by using semisupevised Machine Learning techniques as those provide a patient-specific approach due to key characteristics such as adaptiveness and robustness. In here, a stress metric is proposed that jointly with each individual medical history provide mechanisms to prevent and avoid possible chronic-health issues for individuals whom are more sensitive to stressors. Finally, supervised learning techniques are used to classify the obtained featured clusters to evaluate specific and general subject models in order to pave the way for real time stress monitoring.
ER  - 
TY  - JOUR
T1  - Local feature selection based on artificial immune system for classification
A1  - Wang, Yi
A1  - Li, Tao
Y1  - 2020///
KW  -  Artificial immune system
KW  -  Classification
KW  -  Clonal selection algorithm
KW  -  Dimensionality reduction
KW  - Local feature selection
JF  - Applied Soft Computing
VL  - 87
SP  - 105989
EP  - 105989
DO  - https://doi.org/10.1016/j.asoc.2019.105989
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619307707
N2  - Conventional feature selection algorithms select a global feature subset for the entire sample space. In contrast, in this paper we propose an efficient filter local feature selection algorithm based on artificial immune system, which assigns a locally relevant feature subset for each neighboring region of the sample space. This algorithm introduces a clonal selection algorithm to explore the search space for the optimal feature subsets, and adopts local clustering idea as an evaluation criterion that maximizes the inter-class distance and minimizes the intra-class distance in the small region of each sample. Experimental results on a wide variety of synthetic and UCI datasets demonstrates that our proposed method achieves better performance than both state-of-the-art global feature selection algorithms and local feature selection algorithms. In addition, a main parameter analysis of the proposed method is carried out.
ER  - 
TY  - JOUR
T1  - Health Twitter Big Bata Management with Hadoop Framework
A1  - Cunha, João
A1  - Silva, Catarina
A1  - Antunes, Mário
Y1  - 2015///
KW  -  Apache Hadoop
KW  -  Healthcare
KW  -  Mahout
KW  -  Twitter.
KW  - Big Data
JF  - Procedia Computer Science
VL  - 64
SP  - 425
EP  - 431
DO  - https://doi.org/10.1016/j.procs.2015.08.536
UR  - https://www.sciencedirect.com/science/article/pii/S187705091502671X
N1  - Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015
N2  - Social media advancements and the rapid increase in volume and complexity of data generated by Internet services are becoming challenging not only technologically, but also in terms of application areas. Performance and availability of data processing are critical factors that need to be evaluated since conventional data processing mechanisms may not provide adequate support. Apache Hadoop with Mahout is a framework to storage and process data at large-scale, including different tools to distribute processing. It has been considered an effective tool currently used by both small and large businesses and corporations, like Google and Facebook, but also public and private healthcare institutions. Given its recent emergence and the increasing complexity of the associated technological issues, a variety of holistic framework solutions have been put forward for each specific application. In this work, we propose a generic functional architecture with Apache Hadoop framework and Mahout for handling, storing and analyzing big data that can be used in different scenarios. To demonstrate its value, we will show its features, advantages and applications on health Twitter data. We show that big health social data can generate important information, valuable both for common users and practitioners. Preliminary results of data analysis on Twitter health data using Apache Hadoop demonstrate the potential of the combination of these technologies.
ER  - 
TY  - JOUR
T1  - CSM-SD: Methodology for contrast set mining through subgroup discovery
A1  - Kralj Novak, Petra
A1  - Lavrač, Nada
A1  - Gamberger, Dragan
A1  - Krstačić, Antonija
Y1  - 2009///
KW  - Brain ischemia
KW  - Contrast set mining
KW  - Descriptive rules
KW  - Subgroup discovery
KW  - Supporting factors
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 1
SP  - 113
EP  - 122
DO  - https://doi.org/10.1016/j.jbi.2008.08.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046408001032
N2  - This paper addresses a data analysis task, known as contrast set mining, whose goal is to find differences between contrasting groups. As a methodological novelty, it is shown that this task can be effectively solved by transforming it to a more common and well-understood subgroup discovery task. The transformation is studied in two learning settings, a one-versus-all and a pairwise contrast set mining setting, uncovering the conditions for each of the two choices. Moreover, the paper shows that the explanatory potential of discovered contrast sets can be improved by offering additional contrast set descriptors, called the supporting factors. The proposed methodology has been applied to uncover distinguishing characteristics of two groups of brain stroke patients, both with rapidly developing loss of brain function due to ischemia:those with ischemia caused by thrombosis and by embolism, respectively.
ER  - 
TY  - JOUR
T1  - A novel medical diagnosis support system for predicting patients with atherosclerosis diseases
A1  - Terrada, Oumaima
A1  - Cherradi, Bouchaib
A1  - Raihani, Abdelhadi
A1  - Bouattane, Omar
Y1  - 2020///
KW  -  Cardiovascular disease (CVD)
KW  -  Classification
KW  -  Machine learning techniques
KW  -  Prediction
KW  - Atherosclerosis
JF  - Informatics in Medicine Unlocked
VL  - 21
SP  - 100483
EP  - 100483
DO  - https://doi.org/10.1016/j.imu.2020.100483
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820306341
N2  - Atherosclerosis diagnosis is an indistinct and complex cognitive process. Artificial intelligence methods, such as machine learning algorithms, have proven their efficiency in Medical Diagnosis Support Systems (MDSS). In this paper, we developed a novel machine learning MDSS to boost the diagnosis of cardiovascular diseases. Our study performed using 835 patient medical records that suffer from atherosclerosis, usually caused by coronary artery diseases (CAD), collected from three databases. The system input layer includes several input variables based on three databases, the Cleveland heart disease, Hungarian, and Z-Alizadeh Sani databases. Seven independent classification methods are applied to assess the system: Artificial Neural Network (ANN), K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Decision Tree (DT), Naïve Bayes (NB), Classification Ensemble (CE), and Discriminant Analysis (DA) algorithms. The robustness of the proposed methods was evaluated through several performance measures. The results showed that the proposed MDSS reached an accuracy of (98%), which is a higher accuracy than the existing approaches. These results are a promising step toward facilitating large-scale clinical diagnostics for atherosclerosis diseases.
ER  - 
TY  - JOUR
T1  - An optimized nearest prototype classifier for power plant fault diagnosis using hybrid particle swarm optimization algorithm
A1  - Wang, Xiaoxia
A1  - Ma, Liangyu
A1  - Wang, Tao
Y1  - 2014///
KW  -  Constructive approach
KW  -  Fault diagnosis
KW  -  Particle swarm optimization
KW  -  Power plant
KW  - Nearest prototype classifier
JF  - International Journal of Electrical Power & Energy Systems
VL  - 58
SP  - 257
EP  - 265
DO  - https://doi.org/10.1016/j.ijepes.2014.01.016
UR  - https://www.sciencedirect.com/science/article/pii/S0142061514000295
N2  - Correct and rapid fault diagnosis is of great importance for the safe and reliable operation of a large-scale power plant. It is a difficult task, however, due to the structural complexity of a power plant, which needs to deal with hundreds of variables simultaneously in case of fault occurrence. A novel nearest prototype classifier is proposed in this paper to diagnose faults in a power plant. A constructive approach is employed to automatically determine the most appropriate number of prototypes per class, while a hybrid particle swarm optimization (HGLPSO) algorithm is used to optimize the position of the prototypes. The aim is to generate an automatic process for obtaining the number and position of prototypes in the nearest prototype classifier with high classification accuracy and low size. The effectiveness of the HGLPSO classifier is evaluated on eight real world classification problems. Finally, the classifier is applied to diagnose faults of a high-pressure feedwater heater system of a 600-MW coal-fired power unit. The obtained results demonstrate the validity of the proposed approach.
ER  - 
TY  - JOUR
T1  - An ensemble deep learning based approach for red lesion detection in fundus images
A1  - Orlando, José Ignacio
A1  - Prokofyeva, Elena
A1  - del Fresno, Mariana
A1  - Blaschko, Matthew B
Y1  - 2018///
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Red lesion detection
KW  - Fundus images
JF  - Computer Methods and Programs in Biomedicine
VL  - 153
SP  - 115
EP  - 127
DO  - https://doi.org/10.1016/j.cmpb.2017.10.017
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717307897
N2  - Background and objectives: Diabetic retinopathy (DR) is one of the leading causes of preventable blindness in the world. Its earliest sign are red lesions, a general term that groups both microaneurysms (MAs) and hemorrhages (HEs). In daily clinical practice, these lesions are manually detected by physicians using fundus photographs. However, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast. Computer-assisted diagnosis of DR based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy. Moreover, it provides comprehensive feedback that is easy to assess by the physicians. Several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections. Deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually. Methods: In this paper we propose a novel method for red lesion detection based on combining both deep learned and domain knowledge. Features learned by a convolutional neural network (CNN) are augmented by incorporating hand crafted features. Such ensemble vector of descriptors is used afterwards to identify true lesion candidates using a Random Forest classifier. Results: We empirically observed that combining both sources of information significantly improve results with respect to using each approach separately. Furthermore, our method reported the highest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for screening and need for referral on MESSIDOR compared to a second human expert. Conclusions: Results highlight the fact that integrating manually engineered approaches with deep learned features is relevant to improve results when the networks are trained from lesion-level annotated data. An open source implementation of our system is publicly available at https://github.com/ignaciorlando/red-lesion-detection.
ER  - 
TY  - JOUR
T1  - An ensemble method using credal decision trees
A1  - Abellán, Joaquín
A1  - Masegosa, Andrés R
Y1  - 2010///
KW  -  Credal sets
KW  -  Decision trees
KW  -  Imprecise Dirichlet model
KW  -  Supervised classification
KW  -  Uncertainty measures
KW  - Imprecise probabilities
JF  - European Journal of Operational Research
VL  - 205
IS  - 1
SP  - 218
EP  - 226
DO  - https://doi.org/10.1016/j.ejor.2009.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S0377221709009114
N2  - Supervised classification learning can be considered as an important tool for decision support. In this paper, we present a method for supervised classification learning, which ensembles decision trees obtained via convex sets of probability distributions (also called credal sets) and uncertainty measures. Our method forces the use of different decision trees and it has mainly the following characteristics: it obtains a good percentage of correct classifications and an improvement in time of processing compared with known classification methods; it not needs to fix the number of decision trees to be used; and it can be parallelized to apply it on very large data sets.
ER  - 
TY  - JOUR
T1  - Sentimental Analysis of Twitter Data with respect to General Elections in India
A1  - Sharma, Ankita
A1  - Ghose, Udayan
Y1  - 2020///
KW  -  Sentiment Analysis
KW  -  Social media
KW  -  Text Mining
KW  - Twitter mining
JF  - Procedia Computer Science
VL  - 173
SP  - 325
EP  - 334
DO  - https://doi.org/10.1016/j.procs.2020.06.038
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920315428
N1  - International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020
N2  - It is known that social media is one of the largest sources of unstructured data. Analyzing that data and harvesting meaning out of that is a tedious job. Recently opinion mining has become an emerging topic due to the vast amount of opinioned data available on the various social networking sites. Microblogging has appeared relatively recently, and twitter is the most popular microblogging sites used by the people. It is one of the biggest free, open data-source. In the world today, twitter often witnesses a lot of opinions. Opinion mining and sentiment analysis help researchers to gain insight into public emotions. In this paper, Twitter is used as a source of opinioned data. Twitter APIs are used for the collection of tweets. In this paper, R is used for the acquisition, pre- processing, analyzing the tweets, then sentiment analysis is performed based on the different approaches. In this paper, Tweets were collected from the period of Jan 2019 to March 2019. Using that tweets, sentiment analysis was performed to gain the opinion polarity of the folks concerning general elections held in India. Two candidates were considered for this study: Candidate-1 and Candidate-2. It was concluded that Candidate-1 is more liked and is famous as compared to Candidate-2. The result obtained in our paper was in full compliance with the actual election results obtained in May 2019.
ER  - 
TY  - JOUR
T1  - Analysis of eligibility criteria representation in industry-standard clinical trial protocols
A1  - Bhattacharya, Sanmitra
A1  - Cantor, Michael N
Y1  - 2013///
KW  -  Controlled vocabulary
KW  -  Eligibility determination
KW  -  Information retrieval
KW  -  Natural language processing
KW  - Clinical trials
JF  - Journal of Biomedical Informatics
VL  - 46
IS  - 5
SP  - 805
EP  - 813
DO  - https://doi.org/10.1016/j.jbi.2013.06.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046413000762
N2  - Previous research on standardization of eligibility criteria and its feasibility has traditionally been conducted on clinical trial protocols from ClinicalTrials.gov (CT). The portability and use of such standardization for full-text industry-standard protocols has not been studied in-depth. Towards this end, in this study we first compare the representation characteristics and textual complexity of a set of Pfizer’s internal full-text protocols to their corresponding entries in CT. Next, we identify clusters of similar criteria sentences from both full-text and CT protocols and outline methods for standardized representation of eligibility criteria. We also study the distribution of eligibility criteria in full-text and CT protocols with respect to pre-defined semantic classes used for eligibility criteria classification. We find that in comparison to full-text protocols, CT protocols are not only more condensed but also convey less information. We also find no correlation between the variations in word-counts of the ClinicalTrials.gov and full-text protocols. While we identify 65 and 103 clusters of inclusion and exclusion criteria from full text protocols, our methods found only 36 and 63 corresponding clusters from CT protocols. For both the full-text and CT protocols we are able to identify ‘templates’ for standardized representations with full-text standardization being more challenging of the two. In our exploration of the semantic class distributions we find that the majority of the inclusion criteria from both full-text and CT protocols belong to the semantic class “Diagnostic and Lab Results” while “Disease, Sign or Symptom” forms the majority for exclusion criteria. Overall, we show that developing a template set of eligibility criteria for clinical trials, specifically in their full-text form, is feasible and could lead to more efficient clinical trial protocol design.
ER  - 
TY  - JOUR
T1  - Context-based ensemble method for human energy expenditure estimation
A1  - Gjoreski, Hristijan
A1  - Kaluža, Boštjan
A1  - Gams, Matjaž
A1  - Milić, Radoje
A1  - Luštrek, Mitja
Y1  - 2015///
KW  -  Context
KW  -  Ensembles
KW  -  Machine learning
KW  -  Regression
KW  -  Wearable sensors
KW  - Human energy expenditure estimation
JF  - Applied Soft Computing
VL  - 37
SP  - 960
EP  - 970
DO  - https://doi.org/10.1016/j.asoc.2015.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S1568494615003014
N2  - Monitoring human energy expenditure (EE) is important in many health and sports applications, since the energy expenditure directly reflects the intensity of physical activity. The actual energy expenditure is unpractical to measure; therefore, it is often estimated from the physical activity measured with accelerometers and other sensors. Previous studies have demonstrated that using a person's activity as the context in which the EE is estimated, and using multiple sensors, improves the estimation. In this study, we go a step further by proposing a context-based reasoning method that uses multiple contexts provided by multiple sensors. The proposed Multiple Contexts Ensemble (MCE) approach first extracts multiple features from the sensor data. Each feature is used as a context for which multiple regression models are built using the remaining features as training data: for each value of the context feature, a regression model is trained on a subset of the dataset with that value. When evaluating a data sample, the models corresponding to the context (feature) values in the evaluated sample are assembled into an ensemble of regression models that estimates the EE of the user. Experiments showed that the MCE method outperforms (in terms of lower root means squared error and lower mean absolute error): (i) five single-regression approaches (linear and non-linear); (ii) two ensemble approaches: Bagging and Random subspace; (iii) an approach that uses artificial neural networks trained on accelerometer-data only; and (iv) BodyMedia (a state-of-the-art commercial EE-estimation device).
ER  - 
TY  - JOUR
T1  - LATTE: A knowledge-based method to normalize various expressions of laboratory test results in free text of Chinese electronic health records
A1  - Jiang, Kun
A1  - Yang, Tao
A1  - Wu, Chunyan
A1  - Chen, Luming
A1  - Mao, Longfei
A1  - Wu, Yongyou
A1  - Deng, Lizong
A1  - Jiang, Taijiao
Y1  - 2020///
KW  -  Data normalization
KW  -  Knowledge-based system
KW  - EHR-based phenotyping
JF  - Journal of Biomedical Informatics
VL  - 102
SP  - 103372
EP  - 103372
DO  - https://doi.org/10.1016/j.jbi.2019.103372
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302928
N2  - Background
A wealth of clinical information is buried in free text of electronic health records (EHR), and converting clinical information to machine-understandable form is crucial for the secondary use of EHRs. Laboratory test results, as one of the most important types of clinical information, are written in various styles in free text of EHRs. This has brought great difficulties for data integration and utilization of EHRs. Therefore, developing technology to normalize different expressions of laboratory test results in free text is indispensable for the secondary use of EHRs.
Methods
In this study, we developed a knowledge-based method named LATTE (transforming lab test results), which could transform various expressions of laboratory test results into a normalized and machine-understandable format. We first identified the analyte of a laboratory test result with a dictionary-based method and then designed a series of rules to detect information associated with the analyte, including its specimen, measured value, unit of measure, conclusive phrase and sampling factor. We determined whether a test result is normal or abnormal by understanding the meaning of conclusive phrases or by comparing its measured value with an appropriate normal range. Finally, we converted various expressions of laboratory test results, either in numeric or textual form, into a normalized form as “specimen-analyte-abnormality”. With this method, a laboratory test with the same type of abnormality would have the same representation, regardless of the way that it is mentioned in free text.
Results
LATTE was developed and optimized on a training set including 8894 laboratory test results from 756 EHRs, and evaluated on a test set including 3740 laboratory test results from 210 EHRs. Compared to experts’ annotations, LATTE achieved a precision of 0.936, a recall of 0.897 and an F1 score of 0.916 on the training set, and a precision of 0.892, a recall of 0.843 and an F1 score of 0.867 on the test set. For 223 laboratory tests with at least two different expression forms in the test set, LATTE transformed 85.7% (2870/3350) of laboratory test results into a normalized form. Besides, LATTE achieved F1 scores above 0.8 for EHRs from 18 of 21 different hospital departments, indicating its generalization capabilities in normalizing laboratory test results.
Conclusion
In conclusion, LATTE is an effective method for normalizing various expressions of laboratory test results in free text of EHRs. LATTE will facilitate EHR-based applications such as cohort querying, patient clustering and machine learning.
Availability
LATTE is freely available for download on GitHub (https://github.com/denglizong/LATTE).
ER  - 
TY  - JOUR
T1  - A comparison of word embeddings for the biomedical natural language processing
A1  - Wang, Yanshan
A1  - Liu, Sijia
A1  - Afzal, Naveed
A1  - Rastegar-Mojarad, Majid
A1  - Wang, Liwei
A1  - Shen, Feichen
A1  - Kingsbury, Paul
A1  - Liu, Hongfang
Y1  - 2018///
KW  -  Information extraction
KW  -  Information retrieval
KW  -  Machine learning
KW  -  Natural language processing
KW  - Word embeddings
JF  - Journal of Biomedical Informatics
VL  - 87
SP  - 12
EP  - 20
DO  - https://doi.org/10.1016/j.jbi.2018.09.008
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418301825
N2  - Background
Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources.
Methods
In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings’ ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen’s dataset, Hliaoutakis’s dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks.
Results
The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts’ judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task.
Conclusion
Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts’ judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.
ER  - 
TY  - JOUR
T1  - Artificial Neural Networks in the Determination of the Fluid Intake Needs of Endurance Athletes
A1  - Singh, Navin R
A1  - Peters, Edith M
Y1  - 2014///
KW  -  body mass
KW  -  classification and prediction
KW  -  environmental stress index
KW  -  exercise intensity
KW  -  gender
KW  - hydration status
JF  - AASRI Procedia
VL  - 8
SP  - 9
EP  - 14
DO  - https://doi.org/10.1016/j.aasri.2014.08.003
UR  - https://www.sciencedirect.com/science/article/pii/S2212671614000705
N1  - 2014 AASRI Conference on Sports Engineering and Computer Science (SECS 2014)
N2  - The aim of this study was to assess the efficacy of using artificial neural networks (ANNs) to classify hydration status and predict the fluid requirements of endurance athletes. Hydration classification models were built using a total of 237 data sets obtained from 148 participants (106 males,42 females) in field-and laboratory studies involving running or cycling. 116 data sets obtained from athletes who completed endurance events euhydrated (plasma osmolality: 275-295 mmol.kg-1) following ad libitum replenishment of fluid intake was used to design prediction models. A filtering algorithm was used to determine the optimal inputs to the models from a selection of 13 anthropometric, exercise performance, fluid intake and environmental factors. The combination of gender, body mass, exercise intensity and environmental stress index in the prediction model generated a root mean square error of 0.24 L.h-1 and a correlation of 0.90 between predicted and actual drinking rates of the euhydrated participants. Additional inclusion of actual fluid intake resulted in the design of a model that was 89% accurate in classifying the post-exercise hydration status of athletes. These findings suggest that the ANN modelling technique has merit in the prediction of fluid requirements and as a supplement to ad libitum fluid intake practices.
ER  - 
TY  - JOUR
T1  - Pharmaceutical drugs chatter on Online Social Networks
A1  - Wiley, Matthew T
A1  - Jin, Canghong
A1  - Hristidis, Vagelis
A1  - Esterling, Kevin M
Y1  - 2014///
KW  - Frequent itemsets
KW  - Health social media
KW  - Pharmaceutical drugs
KW  - Sentiment analysis
KW  - Social media
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 49
SP  - 245
EP  - 254
DO  - https://doi.org/10.1016/j.jbi.2014.03.006
UR  - https://www.sciencedirect.com/science/article/pii/S153204641400063X
N2  - The ubiquity of Online Social Networks (OSNs) is creating new sources for healthcare information, particularly in the context of pharmaceutical drugs. We aimed to examine the impact of a given OSN’s characteristics on the content of pharmaceutical drug discussions from that OSN. We compared the effect of four distinguishing characteristics from ten different OSNs on the content of their pharmaceutical drug discussions: (1) General versus Health OSN; (2) OSN moderation; (3) OSN registration requirements; and (4) OSNs with a question and answer format. The effects of these characteristics were measured both quantitatively and qualitatively. Our results show that an OSN’s characteristics indeed affect the content of its discussions. Based on their information needs, healthcare providers may use our findings to pick the right OSNs or to advise patients regarding their needs. Our results may also guide the creation of new and more effective domain-specific health OSNs. Further, future researchers of online healthcare content in OSNs may find our results informative while choosing OSNs as data sources. We reported several findings about the impact of OSN characteristics on the content of pharmaceutical drug discussion, and synthesized these findings into actionable items for both healthcare providers and future researchers of healthcare discussions on OSNs. Future research on the impact of OSN characteristics could include user demographics, quality and safety of information, and efficacy of OSN usage.
ER  - 
TY  - JOUR
T1  - Computational analysis of blood flow in the retinal arteries and veins using fundus image
A1  - Malek, Jihene
A1  - Azar, Ahmad Taher
A1  - Nasralli, Boutheina
A1  - Tekari, Mehdi
A1  - Kamoun, Heykel
A1  - Tourki, Rached
Y1  - 2015///
KW  -  Fundus images
KW  -  Retinal circulation
KW  -  Structured fractal tree
KW  - Blood flow modeling
JF  - Computers & Mathematics with Applications
VL  - 69
IS  - 2
SP  - 101
EP  - 116
DO  - https://doi.org/10.1016/j.camwa.2014.11.017
UR  - https://www.sciencedirect.com/science/article/pii/S0898122114005859
N2  - The retina is the only tissue in which blood vessels can be visualized non-invasively in vivo. Thus, the study of the retinal hemodynamic has special interest for both physiological and pathological conditions. The aim of this study has been to develop a detailed computational model for a quantitative analysis of the blood flow in physiologically realistic retinal arterial and venous networks. The geometrical outlines of both retinal artery and vein have been extracted from the retinal image acquired from a healthy young adult by a retinal camera Topcon TRC-50EX. The microvascular diameter effect (i.e., Fåhraeus–Lindqvist effect) and the hematocrit have been considered in determining the viscosity of the blood in the retinal vessel segments. The blood moves at a velocity that is 2 times less in the veins (maximum 5.4 cm/s) than the velocity at which it moves in the arteries (maximum 11 cm/s) which are in good agreement with in vivo measurements reported in the literature. The pressure drop has been in the range of 11–14 mmHg between the inlet and outlets for the arterial network, and 13–14 mmHg for the vein network. The developed method can be used as a tool for continuous monitoring of the retinal circulation for clinical assessments as well as experimental studies.
ER  - 
TY  - JOUR
T1  - A multistrategy approach to classification learning in databases
A1  - Lee, Chang-Hwan
A1  - Shin, Dong-Guk
Y1  - 1999///
KW  -  Data mining
KW  -  Database
KW  -  Hellinger divergence
KW  -  Lazy learning
KW  - Rule induction
JF  - Data & Knowledge Engineering
VL  - 31
IS  - 1
SP  - 67
EP  - 93
DO  - https://doi.org/10.1016/S0169-023X(99)00018-X
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X9900018X
N2  - This paper proposes a hybrid classification learning system for databases that integrates rule induction and lazy learning. For rule induction learning, we use an entropy function based on Hellinger divergence to measure the amount of information each inductive rule contains. For lazy learning, we also use the Hellinger measure to automatically generate attribute weights and to compute similarities between data values of non-numeric data types. Our system has been implemented and tested extensively on a number of well-known machine learning data sets. The performance of our system was favorable compared to those of other well-known classification learning techniques based on monostrategic learning methods.
ER  - 
TY  - JOUR
T1  - Domain-invariant interpretable fundus image quality assessment
A1  - Shen, Yaxin
A1  - Sheng, Bin
A1  - Fang, Ruogu
A1  - Li, Huating
A1  - Dai, Ling
A1  - Stolte, Skylar
A1  - Qin, Jing
A1  - Jia, Weiping
A1  - Shen, Dinggang
Y1  - 2020///
KW  -  Domain adaptation
KW  -  Interpretability
KW  -  Multi-task learning
KW  - Fundus image quality assessment
JF  - Medical Image Analysis
VL  - 61
SP  - 101654
EP  - 101654
DO  - https://doi.org/10.1016/j.media.2020.101654
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520300219
N2  - Objective and quantitative assessment of fundus image quality is essential for the diagnosis of retinal diseases. The major factors in fundus image quality assessment are image artifact, clarity, and field definition. Unfortunately, most of existing quality assessment methods focus on the quality of overall image, without interpretable quality feedback for real-time adjustment. Furthermore, these models are often sensitive to the specific imaging devices, and cannot generalize well under different imaging conditions. This paper presents a new multi-task domain adaptation framework to automatically assess fundus image quality. The proposed framework provides interpretable quality assessment with both quantitative scores and quality visualization for potential real-time image recapture with proper adjustment. In particular, the present approach can detect optic disc and fovea structures as landmarks, to assist the assessment through coarse-to-fine feature encoding. The framework also exploit semi-tied adversarial discriminative domain adaptation to make the model generalizable across different data sources. Experimental results demonstrated that the proposed algorithm outperforms different state-of-the-art approaches and achieves an area under the ROC curve of 0.9455 for the overall quality classification.
ER  - 
TY  - JOUR
T1  - An efficient detection of congestive heart failure using frequency localized filter banks for the diagnosis with ECG signals
A1  - Bhurane, Ankit A
A1  - Sharma, Manish
A1  - San-Tan, Ru
A1  - Acharya, U Rajendra
Y1  - 2019///
KW  -  Congestive heart failure
KW  -  Wavelets
KW  - Electrocardiogram signals
JF  - Cognitive Systems Research
VL  - 55
SP  - 82
EP  - 94
DO  - https://doi.org/10.1016/j.cogsys.2018.12.017
UR  - https://www.sciencedirect.com/science/article/pii/S1389041718308568
N2  - Congestive heart failure (CHF) refers to the condition in which the heart is unable to maintain the required blood flow under normal heart pressure. CHF is one of the major causes of death worldwide and is commonly caused by coronary artery disease, diabetes and high blood pressure. It typically affects the elderly population. The diagnosis of CHF is mostly based on clinical assessment of symptoms, signs, imaging findings, and invasive intracardiac pressure measurement. The electrocardiogram (ECG) is neither sensitive nor specific for diagnosis of CHF, and the analysis of the ECG signal for the possible presence of CHF is manually intensive and requires adequate skills and expertise for discerning subtle abnormalities in the electrical activity of the heart that may be associated with CHF. We hypothesized that this task of recognizing the multiparametric patterns of ECG signal aberrations that might occur in CHF could be expedited and optimized using machine learning. In this paper, we present an automated approach for the diagnosis of CHF using ECG signals. The proposed approach was tested on four different sets of normal and CHF ECG signals obtained from established public databases. The experiments were performed using short (2 second (s)) ECG segments. Five different features (fuzzy entropy, Renyi entropy, Higuchi’s fractal dimension, Kraskov entropy and energy) were extracted from the wavelet decomposition of ECG segments using frequency localized filter banks. For training and classification, we employed quadratic support vector machine (QSVM). A 10-fold cross-validation technique was used for evaluation. Accuracy ⩾99.66%, sensitivity ⩾99.82%, and specificity ⩾99.28%, were obtained across all four data sets used. The system can be deployed in hospitals to facilitate the diagnosis of CHF. The proposed system can reduce the time requirement and error rate associated with manual reading of large ECG signals.
ER  - 
TY  - JOUR
T1  - A two-stage ensemble method for the detection of class-label noise
A1  - Sabzevari, Maryam
A1  - Martínez-Muñoz, Gonzalo
A1  - Suárez, Alberto
Y1  - 2018///
KW  -  Ensemble learning
KW  -  Random forest
KW  -  Robust classification
KW  -  Subsampling
KW  - Noise detection
JF  - Neurocomputing
VL  - 275
SP  - 2374
EP  - 2383
DO  - https://doi.org/10.1016/j.neucom.2017.11.012
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217317265
N2  - The properties of bootstrap ensembles, such as bagging or random forest, are utilized to detect and handle label noise in classification problems. The first observation is that subsampling is a regularization mechanism that can be used to render bootstrap ensembles more robust to this type of noise. Furthermore, appropriate values of the sampling rate can be estimated using out-of-bag data. A second observation is that the ensemble classifiers tend to make more errors in incorrectly labeled instances. Thus, instances for which a sufficiently large fraction of ensemble predictors err are marked as noisy. Suitable values of this threshold, which are problem dependent, are determined by cross-validation using a wrapper method. Instances identified as noisy can then be either filtered (i.e. discarded for training), or cleaned by correcting their class labels. Finally, an ensemble is built afresh on these cleansed training data. Extensive experiments in classification problems from different areas of application show that this procedure is effective to build accurate ensembles, even in the presence of high levels of class-label noise.
ER  - 
TY  - JOUR
T1  - A fully automated cell segmentation and morphometric parameter system for quantifying corneal endothelial cell morphology
A1  - Al-Fahdawi, Shumoos
A1  - Qahwaji, Rami
A1  - Al-Waisy, Alaa S
A1  - Ipson, Stanley
A1  - Ferdousi, Maryam
A1  - Malik, Rayaz A
A1  - Brahma, Arun
Y1  - 2018///
KW  -  Automatic cell segmentation
KW  -  Corneal Confocal Microscopy
KW  -  Fast Fourier Transform
KW  -  Voronoi Tessellation
KW  -  Watershed transformation
KW  - Corneal endothelial cells
JF  - Computer Methods and Programs in Biomedicine
VL  - 160
SP  - 11
EP  - 23
DO  - https://doi.org/10.1016/j.cmpb.2018.03.015
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717310398
N2  - Background and Objective
Corneal endothelial cell abnormalities may be associated with a number of corneal and systemic diseases. Damage to the endothelial cells can significantly affect corneal transparency by altering hydration of the corneal stroma, which can lead to irreversible endothelial cell pathology requiring corneal transplantation. To date, quantitative analysis of endothelial cell abnormalities has been manually performed by ophthalmologists using time consuming and highly subjective semi-automatic tools, which require an operator interaction. We developed and applied a fully-automated and real-time system, termed the Corneal Endothelium Analysis System (CEAS) for the segmentation and computation of endothelial cells in images of the human cornea obtained by in vivo corneal confocal microscopy.
Methods
First, a Fast Fourier Transform (FFT) Band-pass filter is applied to reduce noise and enhance the image quality to make the cells more visible. Secondly, endothelial cell boundaries are detected using watershed transformations and Voronoi tessellations to accurately quantify the morphological parameters of the human corneal endothelial cells. The performance of the automated segmentation system was tested against manually traced ground-truth images based on a database consisting of 40 corneal confocal endothelial cell images in terms of segmentation accuracy and obtained clinical features. In addition, the robustness and efficiency of the proposed CEAS system were compared with manually obtained cell densities using a separate database of 40 images from controls (n = 11), obese subjects (n = 16) and patients with diabetes (n = 13).
Results
The Pearson correlation coefficient between automated and manual endothelial cell densities is 0.9 (p < 0.0001) and a Bland–Altman plot shows that 95% of the data are between the 2SD agreement lines.
Conclusions
We demonstrate the effectiveness and robustness of the CEAS system, and the possibility of utilizing it in a real world clinical setting to enable rapid diagnosis and for patient follow-up, with an execution time of only 6 seconds per image.
ER  - 
TY  - JOUR
T1  - Finger movements classification based on fractional Fourier transform coefficients extracted from surface EMG signals
A1  - Taghizadeh, Zahra
A1  - Rashidi, Saeid
A1  - Shalbaf, Ahmad
Y1  - 2021///
KW  -  Classification
KW  -  Feature selection
KW  -  Finger movements
KW  -  Fractional Fourier Transform
KW  - EMG
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102573
EP  - 102573
DO  - https://doi.org/10.1016/j.bspc.2021.102573
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421001701
N2  - EMG signals have played a pivotal role as a fundamental component of myriad modern prostheses to control prostheses’ movements as well as identifying individual and combined hand or finger gestures. Despite a great deal of interest in these signals, the non-stationary nature of biological EMG signals has led to complications in EMG applications. Stationary signals have been analyzed plainly by time domain approaches like Fourier Transform, while non-stationary signals analysis is not satisfactory to be carried out with such method as it is not capable to illustrate the incidence time of various frequency components, besides, extracting both time and frequency information is essential. The Fractional Fourier Transform (FrFT), which is a generalization of classical Fourier Transform, is able to demonstrate the variable frequency of non-stationary signals. In this paper, FrFT technique with different fractional orders is employed as a novel and sophisticated feature extraction method for EMG signals of 8 subjects including 6 men and 2 women recorded in 10 different finger movements which are 5 individual and 5 combined postures. Windowing method and the t-test approach are utilized to select the best FrFT extracted coefficients as features. Employing KNN method to classify 10 different classes and the average classification accuracy of 98.12% of proposed method is a significant indicator of its sufficient performance.
ER  - 
TY  - JOUR
T1  - An information-theoretic filter approach for value weighted classification learning in naive Bayes
A1  - Lee, Chang-Hwan
Y1  - 2018///
KW  -  Feature selection
KW  -  Kullback-Leibler
KW  -  Naive Bayes
KW  - Feature weighting
JF  - Data & Knowledge Engineering
VL  - 113
SP  - 116
EP  - 128
DO  - https://doi.org/10.1016/j.datak.2017.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X16301276
N2  - Assigning weights in features has been an important topic in some classification learning algorithms. In this paper, we propose a new paradigm of assigning weights in classification learning, called value weighting method. While the current weighting methods assign a weight to each feature, we assign a different weight to the values of each feature. The performance of naive Bayes learning with value weighting method is compared with that of some other traditional methods for a number of datasets. The experimental results show that the value weighting method could improve the performance of naive Bayes significantly.
ER  - 
TY  - JOUR
T1  - Random subspace evidence classifier
A1  - Li, Haisheng
A1  - Wen, Guihua
A1  - Yu, Zhiwen
A1  - Zhou, Tiangang
Y1  - 2013///
KW  -  Local hyperplane
KW  -  Nearest neighbors
KW  -  Random subspace
KW  - Evidence theory
JF  - Neurocomputing
VL  - 110
SP  - 62
EP  - 69
DO  - https://doi.org/10.1016/j.neucom.2012.11.019
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213000040
N2  - Although there exist a lot of k-nearest neighbor approaches and their variants, few of them consider how to make use of the information in both the whole feature space and subspaces. In order to address this limitation, we propose a new classifier named as the random subspace evidence classifier (RSEC). Specifically, RSEC first calculates the local hyperplane distance for each class as the evidences not only in the whole feature space, but also in randomly generated feature subspaces. Then, the basic belief assignment is computed according to these distances for the evidences of each class. In the following, all the evidences represented by basic belief assignments are pooled together by the Dempster's rule. Finally, RSEC assigns the class label to each test sample based on the combined belief assignment. The experiments in the datasets from UCI machine learning repository, artificial data and face image database illustrate that the proposed approach yields lower classification error in average comparing to 7 existing k-nearest neighbor approaches and variants when performing the classification task. In addition, RSEC has good performance in average on the high dimensional data and the minority class of the imbalanced data.
ER  - 
TY  - JOUR
T1  - Learning a Mahalanobis distance metric for data clustering and classification
A1  - Xiang, Shiming
A1  - Nie, Feiping
A1  - Zhang, Changshui
Y1  - 2008///
KW  -  Data clustering
KW  -  Face pose estimation
KW  -  Global optimization
KW  -  Interactive image segmentation
KW  -  Mahalanobis distance
KW  - Distance metric learning
JF  - Pattern Recognition
VL  - 41
IS  - 12
SP  - 3600
EP  - 3612
DO  - https://doi.org/10.1016/j.patcog.2008.05.018
UR  - https://www.sciencedirect.com/science/article/pii/S0031320308002057
N2  - Distance metric is a key issue in many machine learning algorithms. This paper considers a general problem of learning from pairwise constraints in the form of must-links and cannot-links. As one kind of side information, a must-link indicates the pair of the two data points must be in a same class, while a cannot-link indicates that the two data points must be in two different classes. Given must-link and cannot-link information, our goal is to learn a Mahalanobis distance metric. Under this metric, we hope the distances of point pairs in must-links are as small as possible and those of point pairs in cannot-links are as large as possible. This task is formulated as a constrained optimization problem, in which the global optimum can be obtained effectively and efficiently. Finally, some applications in data clustering, interactive natural image segmentation and face pose estimation are given in this paper. Experimental results illustrate the effectiveness of our algorithm.
ER  - 
TY  - JOUR
T1  - AI-driven quantification, staging and outcome prediction of COVID-19 pneumonia
A1  - Chassagnon, Guillaume
A1  - Vakalopoulou, Maria
A1  - Battistella, Enzo
A1  - Christodoulidis, Stergios
A1  - Hoang-Thi, Trieu-Nghi
A1  - Dangeard, Severine
A1  - Deutsch, Eric
A1  - Andre, Fabrice
A1  - Guillo, Enora
A1  - Halm, Nara
A1  - El Hajj, Stefany
A1  - Bompard, Florian
A1  - Neveu, Sophie
A1  - Hani, Chahinez
A1  - Saab, Ines
A1  - Campredon, Aliénor
A1  - Koulakian, Hasmik
A1  - Bennani, Souhail
A1  - Freche, Gael
A1  - Barat, Maxime
A1  - Lombard, Aurelien
A1  - Fournier, Laure
A1  - Monnier, Hippolyte
A1  - Grand, Téodor
A1  - Gregory, Jules
A1  - Nguyen, Yann
A1  - Khalil, Antoine
A1  - Mahdjoub, Elyas
A1  - Brillet, Pierre-Yves
A1  - Tran Ba, Stéphane
A1  - Bousson, Valérie
A1  - Mekki, Ahmed
A1  - Carlier, Robert-Yves
A1  - Revel, Marie-Pierre
A1  - Paragios, Nikos
Y1  - 2021///
KW  -  Artifial intelligence
KW  -  Biomarker discovery
KW  -  Deep learning
KW  -  Ensemble methods
KW  -  Prognosis
KW  -  Staging
KW  - COVID 19 pneumonia
JF  - Medical Image Analysis
VL  - 67
SP  - 101860
EP  - 101860
DO  - https://doi.org/10.1016/j.media.2020.101860
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520302243
N2  - Coronavirus disease 2019 (COVID-19) emerged in 2019 and disseminated around the world rapidly. Computed tomography (CT) imaging has been proven to be an important tool for screening, disease quantification and staging. The latter is of extreme importance for organizational anticipation (availability of intensive care unit beds, patient management planning) as well as to accelerate drug development through rapid, reproducible and quantified assessment of treatment response. Even if currently there are no specific guidelines for the staging of the patients, CT together with some clinical and biological biomarkers are used. In this study, we collected a multi-center cohort and we investigated the use of medical imaging and artificial intelligence for disease quantification, staging and outcome prediction. Our approach relies on automatic deep learning-based disease quantification using an ensemble of architectures, and a data-driven consensus for the staging and outcome prediction of the patients fusing imaging biomarkers with clinical and biological attributes. Highly promising results on multiple external/independent evaluation cohorts as well as comparisons with expert human readers demonstrate the potentials of our approach.
ER  - 
TY  - JOUR
T1  - Dynamic L-RNN recovery of missing data in IoMT applications
A1  - Turabieh, Hamza
A1  - Abu Salem, Amer
A1  - Abu-El-Rub, Noor
Y1  - 2018///
KW  -  Deep learning
KW  -  IoMT
KW  - Missing data
JF  - Future Generation Computer Systems
VL  - 89
SP  - 575
EP  - 583
DO  - https://doi.org/10.1016/j.future.2018.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18307490
N2  - One of the most important factors of success of the Internet of Medical Things (IoMT) applications is reliable data delivery. The high quality of data delivery is a vital issue for IoMT applications to provide a high-quality of services to the end users. However, IoMT applications may suffer from low quality of data delivery due to several reasons, such as sensing errors, bad connections or outside attacks. As a result, the collected data is incomplete. IoMT applications require a complete data to provide a high-quality of services to the end users; otherwise, the performance will decrease and not meet the main requirements of IoMT applications. In reality, missing data should be intelligently recovered to save time and cost. In this paper, we propose a Dynamic Layered-Recurrent Neural Network (Dynamic L-RNN) approach to recover missing data from IoMT applications. The main idea is to perform a dynamic L-RNN to predict any missing value in a simple fast manner to save time and cost. The collected data is divided into two categories, complete and incomplete data. A dynamic L-RNN is trained based on complete data, which is used to predict the missing data from incomplete data. This proposed method is able to recover the missing data for IoMT applications with high AUC value when applied to two different datasets. The obtained results show great enhancement in the AUC values after recovering the missing data.
ER  - 
TY  - JOUR
T1  - Semi-supervised hierarchical clustering ensemble and its application
A1  - Xiao, Wenchao
A1  - Yang, Yan
A1  - Wang, Hongjun
A1  - Li, Tianrui
A1  - Xing, Huanlai
Y1  - 2016///
KW  -  CHAMELEON
KW  -  Fault diagnosis
KW  -  Semi-supervised
KW  - Clustering ensemble
JF  - Neurocomputing
VL  - 173
SP  - 1362
EP  - 1376
DO  - https://doi.org/10.1016/j.neucom.2015.09.009
UR  - https://www.sciencedirect.com/science/article/pii/S092523121501317X
N2  - Clustering ensemble is an important part of ensemble learning. It aims to study and integrate multiple clustering results from different clustering algorithms or same algorithm with different initial parameters for the same dataset. CHAMELEON is a hierarchical clustering algorithm which can discover natural clusters of different shapes and sizes as the result of its merging decision dynamically adapts to the different clustering model characterized. Inspired by the idea of CHAMELEON, the paper proposes a novel clustering ensemble models including semi-supervised method and discusses its application in fault diagnosis of high speed train (HST) running gear. The contributions of this paper include: constructing a sparse graph via the similarity matrix which aggregates multiple clustering results; partitioning the sparse graph (vertex=object, edge weight=similarity) into a large number of relatively small sub-clusters; obtaining the final clustering partition by merging these sub-clusters repeatedly. The experimental results demonstrate that our method outperforms some of state-of-the-art ensemble algorithms regarding the accuracy and stability and recognizes fault patterns of HST running gear effectively.
ER  - 
TY  - JOUR
T1  - The C-loss function for pattern classification
A1  - Singh, Abhishek
A1  - Pokharel, Rosha
A1  - Principe, Jose
Y1  - 2014///
KW  -  Backprojection
KW  -  Correntropy
KW  -  Loss function
KW  -  Neural network
KW  - Classification
JF  - Pattern Recognition
VL  - 47
IS  - 1
SP  - 441
EP  - 453
DO  - https://doi.org/10.1016/j.patcog.2013.07.017
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313003154
N2  - This paper presents a new loss function for neural network classification, inspired by the recently proposed similarity measure called Correntropy. We show that this function essentially behaves like the conventional square loss for samples that are well within the decision boundary and have small errors, and L0 or counting norm for samples that are outliers or are difficult to classify. Depending on the value of the kernel size parameter, the proposed loss function moves smoothly from convex to non-convex and becomes a close approximation to the misclassification loss (ideal 0–1 loss). We show that the discriminant function obtained by optimizing the proposed loss function in the neighborhood of the ideal 0–1 loss function to train a neural network is immune to overfitting, more robust to outliers, and has consistent and better generalization performance as compared to other commonly used loss functions, even after prolonged training. The results also show that it is a close competitor to the SVM. Since the proposed method is compatible with simple gradient based online learning, it is a practical way of improving the performance of neural network classifiers.
ER  - 
TY  - JOUR
T1  - A novel data-driven robust framework based on machine learning and knowledge graph for disease classification
A1  - Lei, Zhenfeng
A1  - Sun, Yuan
A1  - Nanehkaran, Y A
A1  - Yang, Shuangyuan
A1  - Islam, Md. Saiful
A1  - Lei, Huiqing
A1  - Zhang, Defu
Y1  - 2020///
KW  -  Data fusion
KW  -  Knowledge graph
KW  -  Machine learning
KW  -  NCDs
KW  - Disease classification
JF  - Future Generation Computer Systems
VL  - 102
SP  - 534
EP  - 548
DO  - https://doi.org/10.1016/j.future.2019.08.030
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19306235
N2  - As Noncommunicable Diseases (NCDs) are affected or controlled by diverse factors such as age, regionalism, timeliness or seasonality, they are always challenging to be treated accurately, which has impacted on daily life and work of patients. Unfortunately, although a number of researchers have already made some achievements (including clinical or even computer-based) on certain diseases, current situation is eager to be improved via computer technologies such as data mining and Deep Learning. In addition, the progress of NCD research has been hampered by privacy of health and medical data. In this paper, a hierarchical idea has been proposed to study the effects of various factors on diseases, and a data-driven framework named d-DC with good extensibility is presented. d-DC is able to classify the disease according to the occupation on the premise where the disease is occurring in a certain region. During collecting data, we used a combination of personal or family medical records and traditional methods to build a data acquisition model. Not only can it realize automatic collection and replenishment of data, but it can also effectively tackle the cold start problem of the model with relatively few data effectively. The diversity of information gathering includes structured data and unstructured data (such as plain texts, images or videos), which contributes to improve the classification accuracy and new knowledge acquisition. Apart from adopting machine learning methods, d-DC has employed knowledge graph (KG) to classify diseases for the first time. The vectorization of medical texts by using knowledge embedding is a novel consideration in the classification of diseases. When results are singular, the medical expert system was proposed to address inconsistencies through knowledge bases or online experts. The results of d-DC are displayed by using a combination of KG and traditional methods, which intuitively provides a reasonable interpretation to the results (highly descriptive). Experiments show that d-DC achieved the improved accuracy than the other previous methods. Especially, a fusion method called RKRE based on both ResNet and the expert system attained an average correct proportion of 86.95%, which is a good feasibility study in the field of disease classification.
ER  - 
TY  - JOUR
T1  - A new approach to disentangle genetic and epigenetic components on disease comorbidities: studying correlation between genotypic and phenotypic disease networks
A1  - Tradigo, G
A1  - Vacca, R
A1  - Manini, T
A1  - Bird, V
A1  - Gerke, T
A1  - Veltri, P
A1  - Prosperi, M
Y1  - 2017///
KW  -  comorbidity network
KW  -  genetic analysis
KW  - GWAS
JF  - Procedia Computer Science
VL  - 110
SP  - 453
EP  - 458
DO  - https://doi.org/10.1016/j.procs.2017.06.119
UR  - https://www.sciencedirect.com/science/article/pii/S187705091731298X
N1  - 14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops
N2  - Disease comorbidity is a result of complex epigenetic interplay. A disease is rarely a consequence of an abnormality in a single gene; complex pathways to disease patterns emerge from gene-gene interactions and gene-environment interactions. Understanding these mechanisms of disease and comorbidity development, breaking down them into clusters and disentangling the epigenetic — actionable—components, is of utter importance from a public health perspective. With the increase in the average life expectancy, healthy aging becomes a primary objective, from both an individual (i.e. quality of life) and a societal (i.e. healthcare costs) standpoint. Many studies have analyzed disease networks based on common altered genes, on protein-protein interactions, or on shared disease comorbidites, i.e. phenotypic disease networks. In this work we aim at studying the relations between genotypic and phenotypic disease networks, using a large statewide cohort of individuals (100, 000+ from California, USA) with linked clinical and genotypic information, the Genetic Epidemiology Research on Adult Health and Aging (GERA). By comparing their phenotypic and genotypic networks, we try to disentangle the epigenetic component of disease comorbidity.
ER  - 
TY  - JOUR
T1  - Modern parameterization and explanation techniques in diagnostic decision support system: A case study in diagnostics of coronary artery disease
A1  - Kukar, Matjaž
A1  - Kononenko, Igor
A1  - Grošelj, Ciril
Y1  - 2011///
KW  -  Association rules
KW  -  Coronary artery disease diagnostics
KW  -  Multi-resolution image parameterization
KW  -  Principal component analysis
KW  - Machine learning
JF  - Artificial Intelligence in Medicine
VL  - 52
IS  - 2
SP  - 77
EP  - 90
DO  - https://doi.org/10.1016/j.artmed.2011.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S0933365711000509
N1  - Artificial Intelligence in Medicine AIME 2009
N2  - Objective
Coronary artery disease has been described as one of the curses of the western world, as it is one of its most important causes of mortality. Therefore, clinicians seek to improve diagnostic procedures, especially those that allow them to reach reliable early diagnoses. In the clinical setting, coronary artery disease diagnostics are typically performed in a sequential manner. The four diagnostic levels consist of evaluation of (1) signs and symptoms of the disease and electrocardiogram at rest, (2) sequential electrocardiogram testing during the controlled exercise, (3) myocardial perfusion scintigraphy, and (4) finally coronary angiography, that is considered as the “gold standard” reference method. Our study focuses on improving diagnostic performance of the third, virtually non-invasive, diagnostic level.
Methods and materials
Myocardial scintigraphy results in a series of medical images that are obtained by relatively inexpensive means. In clinical practice, these images are manually described (parameterized) by expert physicians. In the paper we present an innovative alternative to manual image evaluation—an automatic image parameterization on multiple resolutions, based on texture description with specialized association rules. Extracted image parameters are combined into more informative composite parameters by means of principal component analysis, and finally used to build automatic classifiers with machine learning methods.
Results
Our experiments with synthetic datasets show that association-rule-based multi-resolution image parameterization works very well for scintigraphic images of the heart. In coronary artery disease diagnostics we confirm these results as our approach significantly improves on clinical results in terms of diagnostic performance. We improve diagnostic accuracy by 17%, specificity by 12% and sensitivity by 22%. We also significantly improve the number of reliably diagnosed patients by 19% for positive diagnoses, and 16% for negative diagnoses, so that no costly further tests are necessary for them.
Conclusions
Multi-resolution image parameterization equals or even betters that of the physicians in terms of the diagnostic quality of image parameters. By using these parameters for building machine learning classifiers, we can significantly improve diagnostic performance with respect to the results of clinical practice, affect process rationalization, as well as possibly provide novel insights into the diagnostic problems, features and/or processes.
ER  - 
TY  - JOUR
T1  - A new machine learning approach for predicting the response to anemia treatment in a large cohort of End Stage Renal Disease patients undergoing dialysis
A1  - Barbieri, Carlo
A1  - Mari, Flavio
A1  - Stopper, Andrea
A1  - Gatti, Emanuele
A1  - Escandell-Montero, Pablo
A1  - Martínez-Martínez, José M
A1  - Martín-Guerrero, José D
Y1  - 2015///
KW  - Anemia
KW  - Chronic Kidney Disease
KW  - Hemoglobin
KW  - Machine learning
KW  - Prediction
JF  - Computers in Biology and Medicine
VL  - 61
SP  - 56
EP  - 61
DO  - https://doi.org/10.1016/j.compbiomed.2015.03.019
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515000979
N2  - Chronic Kidney Disease (CKD) anemia is one of the main common comorbidities in patients undergoing End Stage Renal Disease (ESRD). Iron supplement and especially Erythropoiesis Stimulating Agents (ESA) have become the treatment of choice for that anemia. However, it is very complicated to find an adequate treatment for every patient in each particular situation since dosage guidelines are based on average behaviors, and thus, they do not take into account the particular response to those drugs by different patients, although that response may vary enormously from one patient to another and even for the same patient in different stages of the anemia. This work proposes an advance with respect to previous works that have faced this problem using different methodologies (Machine Learning (ML), among others), since the diversity of the CKD population has been explicitly taken into account in order to produce a general and reliable model for the prediction of ESA/Iron therapy response. Furthermore, the ML model makes use of both human physiology and drug pharmacology to produce a model that outperforms previous approaches, yielding Mean Absolute Errors (MAE) of the Hemoglobin (Hb) prediction around or lower than 0.6g/dl in the three countries analyzed in the study, namely, Spain, Italy and Portugal.
ER  - 
TY  - JOUR
T1  - Mining Text for Disease Diagnosis
A1  - Tsumoto, Shusaku
A1  - Kimura, Tomohiro
A1  - Iwata, Haruko
A1  - Hirano, Shoji
Y1  - 2017///
KW  -  SVM
KW  -  classification
KW  -  correspondence analysis
KW  -  decision tree
KW  -  deep learning
KW  -  random forest
KW  -  text mining
KW  - Discharge summary
JF  - Procedia Computer Science
VL  - 122
SP  - 1133
EP  - 1140
DO  - https://doi.org/10.1016/j.procs.2017.11.483
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917327382
N1  - 5th International Conference on Information Technology and Quantitative Management, ITQM 2017
N2  - Electronic patient records (EPR) are rich in texts, where almost all the decision making processes of medical staff are written. Thus, mining in EPR is important for acquision of decision making process and diagnosis. In this paper, as a first step, we focus on text mining for discharge summaries, which include the compact explanation for the patient’s admission. A record of her complaints, physical findings, laboratory results and radiographic studies while hospitalized; a list of changes in her medications at discharge; and recommendations for follow up care. Text mining process consists of the following four processes: first, morphological analysis is applied to a set of summaries and a term matrix is generated. Second, correspond analysis is applied to the classification labels and the term matrix and generates two dimensional coordinates. By measuring the distances between categories and the assigned points, ranking of key words will be generated. Then, keywords are selected as attributes according to the rank, and training examples for classifiers will be generated. Finally, learning methods are applied to the training examples. Experimental validation shows that random forest achieved the best performance and the second best was the deep learner with a small difference, but decision tree methods with many keywords performed only a little worse than neural network or deep learning methods.
ER  - 
TY  - JOUR
T1  - Feature selection based on loss-margin of nearest neighbor classification
A1  - Li, Yun
A1  - Lu, Bao-Liang
Y1  - 2009///
KW  -  Energy-based model
KW  -  Loss function
KW  -  Margin
KW  - Feature selection
JF  - Pattern Recognition
VL  - 42
IS  - 9
SP  - 1914
EP  - 1921
DO  - https://doi.org/10.1016/j.patcog.2008.10.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320308004196
N2  - The problem of selecting a subset of relevant features is classic and found in many branches of science including—examples in pattern recognition. In this paper, we propose a new feature selection criterion based on low-loss nearest neighbor classification and a novel feature selection algorithm that optimizes the margin of nearest neighbor classification through minimizing its loss function. At the same time, theoretical analysis based on energy-based model is presented, and some experiments are also conducted on several benchmark real-world data sets and facial data sets for gender classification to show that the proposed feature selection method outperforms other classic ones.
ER  - 
TY  - JOUR
T1  - Combining integrated sampling with SVM ensembles for learning from imbalanced datasets
A1  - Liu, Yang
A1  - Yu, Xiaohui
A1  - Huang, Jimmy Xiangji
A1  - An, Aijun
Y1  - 2011///
KW  -  Classification
KW  -  Imbalanced data mining
KW  - Data sampling
JF  - Information Processing & Management
VL  - 47
IS  - 4
SP  - 617
EP  - 631
DO  - https://doi.org/10.1016/j.ipm.2010.11.007
UR  - https://www.sciencedirect.com/science/article/pii/S030645731000097X
N2  - Learning from imbalanced datasets is difficult. The insufficient information that is associated with the minority class impedes making a clear understanding of the inherent structure of the dataset. Most existing classification methods tend not to perform well on minority class examples when the dataset is extremely imbalanced, because they aim to optimize the overall accuracy without considering the relative distribution of each class. In this paper, we study the performance of SVMs, which have gained great success in many real applications, in the imbalanced data context. Through empirical analysis, we show that SVMs may suffer from biased decision boundaries, and that their prediction performance drops dramatically when the data is highly skewed. We propose to combine an integrated sampling technique, which incorporates both over-sampling and under-sampling, with an ensemble of SVMs to improve the prediction performance. Extensive experiments show that our method outperforms individual SVMs as well as several other state-of-the-art classifiers.
ER  - 
TY  - JOUR
T1  - Big Data and Machine Learning Based Secure Healthcare Framework
A1  - Kaur, Prableen
A1  - Sharma, Manik
A1  - Mittal, Mamta
Y1  - 2018///
KW  -  Big data analytics
KW  -  Healthcare
KW  -  disease diagnosis
KW  -  predictive analysis
KW  -  privacy
KW  -  security
KW  - Big Data
JF  - Procedia Computer Science
VL  - 132
SP  - 1049
EP  - 1059
DO  - https://doi.org/10.1016/j.procs.2018.05.020
UR  - https://www.sciencedirect.com/science/article/pii/S187705091830752X
N1  - International Conference on Computational Intelligence and Data Science
N2  - The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.
ER  - 
TY  - JOUR
T1  - Alzheimer’s disease progression detection model based on an early fusion of cost-effective multimodal data
A1  - El-Sappagh, Shaker
A1  - Saleh, Hager
A1  - Sahal, Radhya
A1  - Abuhmed, Tamer
A1  - Islam, S M Riazul
A1  - Ali, Farman
A1  - Amer, Eslam
Y1  - 2021///
KW  -  Disease progression detection
KW  -  Machine learning
KW  -  Multimodal data analysis
KW  - Alzheimer disease
JF  - Future Generation Computer Systems
VL  - 115
SP  - 680
EP  - 699
DO  - https://doi.org/10.1016/j.future.2020.10.005
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X20329824
N2  - Alzheimer’s disease (AD) is a severe neurodegenerative disease. The identification of patients at high risk of conversion from mild cognitive impairment to AD via earlier close monitoring, targeted investigations, and appropriate management is crucial. Recently, several machine learning (ML) algorithms have been used for AD progression detection. Most of these studies only utilized neuroimaging data from baseline visits. However, AD is a complex chronic disease, and usually, a medical expert will analyze the patient’s whole history when making a progression diagnosis. Furthermore, neuroimaging data are always either limited or not available, especially in developing countries, due to their cost. In this paper, we compare the performance of five widely used ML algorithms, namely, the support vector machine, random forest, k-nearest neighbor, logistic regression, and decision tree to predict AD progression with a prediction horizon of 2.5 years. We use 1029 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) database. In contrast to previous literature, our models are optimized using a collection of cost-effective time-series features including patient’s comorbidities, cognitive scores, medication history, and demographics. Medication and comorbidity text data are semantically prepared. Drug terms are collected and cleaned before encoding using the therapeutic chemical classification (ATC) ontology, and then semantically aggregated to the appropriate level of granularity using ATC to ensure a less sparse dataset. Our experiments assert that the early fusion of comorbidity and medication features with other features reveals significant predictive power with all models. The random forest model achieves the most accurate performance compared to other models. This study is the first of its kind to investigate the role of such multimodal time-series data on AD prediction.
ER  - 
TY  - JOUR
T1  - Scaling and contextualizing personalized healthcare: A case study of disease prediction algorithm integration
A1  - Feldman, Keith
A1  - Davis, Darcy
A1  - Chawla, Nitesh V
Y1  - 2015///
KW  -  Big Data
KW  -  Clinical informatics
KW  -  Data mining
KW  - Personalized healthcare
JF  - Journal of Biomedical Informatics
VL  - 57
SP  - 377
EP  - 385
DO  - https://doi.org/10.1016/j.jbi.2015.07.017
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001574
N2  - Today, advances in medical informatics brought on by the increasing availability of electronic medical records (EMR) have allowed for the proliferation of data-centric tools, especially in the context of personalized healthcare. While these tools have the potential to greatly improve the quality of patient care, the effective utilization of their techniques within clinical practice may encounter two significant challenges. First, the increasing amount of electronic data generated by clinical processes can impose scalability challenges for current computational tools, requiring parallel or distributed implementations of such tools to scale. Secondly, as technology becomes increasingly intertwined in clinical workflows these tools must not only operate efficiently, but also in an interpretable manner. Failure to identity areas of uncertainty or provide appropriate context creates a potentially complex situation for both physicians and patients. This paper will present a case study investigating the issues associated with first scaling a disease prediction algorithm to accommodate dataset sizes expected in large medical practices. It will then provide an analysis on the diagnoses predictions, attempting to provide contextual information to convey the certainty of the results to a physician. Finally it will investigate latent demographic features of the patient’s themselves, which may have an impact on the accuracy of the diagnosis predictions.
ER  - 
TY  - JOUR
T1  - LOINC® Codes for Hospital Information Systems Documents: A Case Study
A1  - Dugas, Martin
A1  - Thun, Sylvia
A1  - Frankewitsch, Thomas
A1  - Heitmann, Kai U
Y1  - 2009///
JF  - Journal of the American Medical Informatics Association
VL  - 16
IS  - 3
SP  - 400
EP  - 403
DO  - https://doi.org/10.1197/jamia.M2882
UR  - https://www.sciencedirect.com/science/article/pii/S106750270900022X
N2  - Hospital Information Systems (HIS) handle a large number of different types of documents. Exchange and analysis of data from different HIS is facilitated by the use of standardized codes to identify document types. HL7's Clinical Document Architecture (CDA) uses LOINC (logical observation identifiers names and Codes) codes for clinical documents. The authors assessed the coverage of LOINC codes for document types in a German HIS. The authors analyzed document types that occurred more than 10 times in approximately 1.3 million documents in a commercial HIS at a major German University Hospital. Document types were mapped manually to LOINC using the Regenstrief LOINC Mapping Assistant (RELMA). Each document type was coded by two physicians. In case of discrepancies a third expert was consulted to reach consensus. For 76 of 86 document categories a LOINC code was identified, but for 38 of these categories, the LOINC code was not specific as deemed necessary. More than 93% of our local HIS documents had local document types that could be assigned a LOINC code.
ER  - 
TY  - JOUR
T1  - Hyper-connectivity of functional networks for brain disease diagnosis
A1  - Jie, Biao
A1  - Wee, Chong-Yaw
A1  - Shen, Dinggang
A1  - Zhang, Daoqiang
Y1  - 2016///
KW  -  Alzheimer's disease
KW  -  Classification
KW  -  Hyper-network
KW  - Functional MR imaging
JF  - Medical Image Analysis
VL  - 32
SP  - 84
EP  - 100
DO  - https://doi.org/10.1016/j.media.2016.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S1361841516300044
N2  - Exploring structural and functional interactions among various brain regions enables better understanding of pathological underpinnings of neurological disorders. Brain connectivity network, as a simplified representation of those structural and functional interactions, has been widely used for diagnosis and classification of neurodegenerative diseases, especially for Alzheimer's disease (AD) and its early stage - mild cognitive impairment (MCI). However, the conventional functional connectivity network is usually constructed based on the pairwise correlation among different brain regions and thus ignores their higher-order relationships. Such loss of high-order information could be important for disease diagnosis, since neurologically a brain region predominantly interacts with more than one other brain regions. Accordingly, in this paper, we propose a novel framework for estimating the hyper-connectivity network of brain functions and then use this hyper-network for brain disease diagnosis. Here, the functional connectivity hyper-network denotes a network where each of its edges representing the interactions among multiple brain regions (i.e., an edge can connect with more than two brain regions), which can be naturally represented by a hyper-graph. Specifically, we first construct connectivity hyper-networks from the resting-state fMRI (R-fMRI) time series by using sparse representation. Then, we extract three sets of brain-region specific features from the connectivity hyper-networks, and further exploit a manifold regularized multi-task feature selection method to jointly select the most discriminative features. Finally, we use multi-kernel support vector machine (SVM) for classification. The experimental results on both MCI dataset and attention deficit hyperactivity disorder (ADHD) dataset demonstrate that, compared with the conventional connectivity network-based methods, the proposed method can not only improve the classification performance, but also help discover disease-related biomarkers important for disease diagnosis.
ER  - 
TY  - JOUR
T1  - Developing a FHIR-based EHR phenotyping framework: A case study for identification of patients with obesity and multiple comorbidities from discharge summaries
A1  - Hong, Na
A1  - Wen, Andrew
A1  - Stone, Daniel J
A1  - Tsuji, Shintaro
A1  - Kingsbury, Paul R
A1  - Rasmussen, Luke V
A1  - Pacheco, Jennifer A
A1  - Adekkanattu, Prakash
A1  - Wang, Fei
A1  - Luo, Yuan
A1  - Pathak, Jyotishman
A1  - Liu, Hongfang
A1  - Jiang, Guoqian
Y1  - 2019///
KW  -  Algorithm portability
KW  -  Electronic Health Records (EHRs)
KW  -  HL7 Fast Healthcare Interoperability Resources (FHIR)
KW  -  Natural language processing
KW  - Clinical phenotyping
JF  - Journal of Biomedical Informatics
VL  - 99
SP  - 103310
EP  - 103310
DO  - https://doi.org/10.1016/j.jbi.2019.103310
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302291
N2  - Background
Standards-based clinical data normalization has become a key component of effective data integration and accurate phenotyping for secondary use of electronic healthcare records (EHR) data. HL7 Fast Healthcare Interoperability Resources (FHIR) is an emerging clinical data standard for exchanging electronic healthcare data and has been used in modeling and integrating both structured and unstructured EHR data for a variety of clinical research applications. The overall objective of this study is to develop and evaluate a FHIR-based EHR phenotyping framework for identification of patients with obesity and its multiple comorbidities from semi-structured discharge summaries leveraging a FHIR-based clinical data normalization pipeline (known as NLP2FHIR).
Methods
We implemented a multi-class and multi-label classification system based on the i2b2 Obesity Challenge task to evaluate the FHIR-based EHR phenotyping framework. Two core parts of the framework are: (a) the conversion of discharge summaries into corresponding FHIR resources – Composition, Condition, MedicationStatement, Procedure and FamilyMemberHistory using the NLP2FHIR pipeline, and (b) the implementation of four machine learning algorithms (logistic regression, support vector machine, decision tree, and random forest) to train classifiers to predict disease state of obesity and 15 comorbidities using features extracted from standard FHIR resources and terminology expansions. We used the macro- and micro-averaged precision (P), recall (R), and F1 score (F1) measures to evaluate the classifier performance. We validated the framework using a second obesity dataset extracted from the MIMIC-III database.
Results
Using the NLP2FHIR pipeline, 1237 clinical discharge summaries from the 2008 i2b2 obesity challenge dataset were represented as the instances of the FHIR Composition resource consisting of 5677 records with 16 unique section types. After the NLP processing and FHIR modeling, a set of 244,438 FHIR clinical resource instances were generated. As the results of the four machine learning classifiers, the random forest algorithm performed the best with F1-micro(0.9466)/F1-macro(0.7887) and F1-micro(0.9536)/F1-macro(0.6524) for intuitive classification (reflecting medical professionals’ judgments) and textual classification (reflecting the judgments based on explicitly reported information of diseases), respectively. The MIMIC-III obesity dataset was successfully integrated for prediction with minimal configuration of the NLP2FHIR pipeline and machine learning models.
Conclusions
The study demonstrated that the FHIR-based EHR phenotyping approach could effectively identify the state of obesity and multiple comorbidities using semi-structured discharge summaries. Our FHIR-based phenotyping approach is a first concrete step towards improving the data aspect of phenotyping portability across EHR systems and enhancing interpretability of the machine learning-based phenotyping algorithms.
ER  - 
TY  - JOUR
T1  - Single-layered complex-valued neural network for real-valued classification problems
A1  - Faijul Amin, Md.
A1  - Murase, Kazuyuki
Y1  - 2009///
KW  -  Classification
KW  -  Complex-valued neural networks
KW  -  Generalization
KW  -  Phase-encoding
KW  - Activation function
JF  - Neurocomputing
VL  - 72
IS  - 4
SP  - 945
EP  - 955
DO  - https://doi.org/10.1016/j.neucom.2008.04.006
UR  - https://www.sciencedirect.com/science/article/pii/S0925231208002439
N1  - Brain Inspired Cognitive Systems (BICS 2006) / Interplay Between Natural and Artificial Computation (IWINAC 2007)
N2  - This paper presents a model of complex-valued neuron (CVN) for real-valued classification problems, introducing two new activation functions. In this CVN model, each real-valued input is encoded into a phase between 0 and π of a complex number of unity magnitude, and multiplied by a complex-valued weight. The weighted sum of inputs is then fed to an activation function. Both the proposed activation functions map complex values into real values, and their role is to divide the net-input (weighted sum) space into multiple regions representing the classes of input patterns. Gradient-based learning rules are derived for each of the activation functions. The ability of such CVN is discussed and tested with two-class problems, such as two- and three-input Boolean problems, and the symmetry detection in binary sequences. We show here that the CVN with both activation functions can form proper boundaries for these linear and nonlinear problems. For solving n-class problems, a complex-valued neural network (CVNN) consisting of n CVNs is also studied. We defined the one exhibiting the largest output among all the neurons as representing the output class. We tested such single-layered CVNNs on several real world benchmark problems. The results show that the classification ability of single-layered CVNN on unseen data is comparable to the conventional real-valued neural network (RVNN) having one hidden layer. Moreover, convergence of the CVNN is much faster than that of the RVNN in most cases.
ER  - 
TY  - JOUR
T1  - A relevance and quality-based ranking algorithm applied to evidence-based medicine
A1  - Serrano-Guerrero, Jesus
A1  - Romero, Francisco P
A1  - Olivas, Jose A
Y1  - 2020///
KW  -  Clustering
KW  -  Quality ranking
KW  -  Relevance ranking
KW  - Evidence-based medicine
JF  - Computer Methods and Programs in Biomedicine
VL  - 191
SP  - 105415
EP  - 105415
DO  - https://doi.org/10.1016/j.cmpb.2020.105415
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719303785
N2  - Background
The amount of information available about millions of different subjects is growing every day. This has led to the birth of new search tools specialized in different domains, because classical information retrieval models have trouble dealing with the special characteristics of some of these domains. Evidence-based Medicine is a case of a complex domain where classical information retrieval models can help search engines retrieve documents by considering the presence or absence of terms, but these must be complemented with other specific strategies which allow retrieving and ranking documents including the best current evidence and methodological quality.
Objective
The goal is to present a ranking algorithm able to select the best documents for clinicians considering aspects related to the relevance and the quality of said documents.
Methods
In order to assess the effectiveness of this proposal, an experimental methodology has been followed by using Medline as a data set and the Cochrane Library as a gold standard.
Results
Applying the evaluation methodology proposed, and after submitting 40 queries on the platform developed, the MAP (Mean Average Precision) obtained was 20.26%.
Conclusions
Successful results have been achieved with the experiments, improving on other studies, but under different and even more complex circumstances.
ER  - 
TY  - JOUR
T1  - Clinical information extraction for preterm birth risk prediction
A1  - Sterckx, Lucas
A1  - Vandewiele, Gilles
A1  - Dehaene, Isabelle
A1  - Janssens, Olivier
A1  - Ongenae, Femke
A1  - De Backere, Femke
A1  - De Turck, Filip
A1  - Roelens, Kristien
A1  - Decruyenaere, Johan
A1  - Van Hoecke, Sofie
A1  - Demeester, Thomas
Y1  - 2020///
KW  -  Clinical decision support models
KW  -  Preterm birth
KW  -  Text mining
KW  - Clinical information extraction
JF  - Journal of Biomedical Informatics
VL  - 110
SP  - 103544
EP  - 103544
DO  - https://doi.org/10.1016/j.jbi.2020.103544
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420301726
N2  - This paper contributes to the pursuit of leveraging unstructured medical notes to structured clinical decision making. In particular, we present a pipeline for clinical information extraction from medical notes related to preterm birth, and discuss the main challenges as well as its potential for clinical practice. A large collection of medical notes, created by staff during hospitalizations of patients who were at risk of delivering preterm, was gathered and analyzed. Based on an annotated collection of notes, we trained and evaluated information extraction components to discover clinical entities such as symptoms, events, anatomical sites and procedures, as well as attributes linked to these clinical entities. In a retrospective study, we show that these are highly informative for clinical decision support models that are trained to predict whether delivery is likely to occur within specific time windows, in combination with structured information from electronic health records.
ER  - 
TY  - JOUR
T1  - A one-class classification decision tree based on kernel density estimation
A1  - Itani, Sarah
A1  - Lecron, Fabian
A1  - Fortemps, Philippe
Y1  - 2020///
KW  -  Decision trees
KW  -  Explainable artificial intelligence
KW  -  Kernel density estimation
KW  - One-class classification
JF  - Applied Soft Computing
VL  - 91
SP  - 106250
EP  - 106250
DO  - https://doi.org/10.1016/j.asoc.2020.106250
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620301903
N2  - One-class Classification (OCC) is an important field of machine learning which aims at predicting a single class on the basis of its lonely representatives and potentially some additional counter-examples. OCC is thus opposed to traditional classification problems involving two or more classes, and addresses the issue of class unbalance. There is a wide range of one-class models which give satisfaction in terms of performance. But at the time of explainable artificial intelligence, there is an increasing need for interpretable models. The present work advocates a novel one-class model which tackles this challenge. Within a greedy and recursive approach, our proposal for an explainable One-Class decision Tree (OC-Tree) rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated effectiveness, through the ability to tackle interpretable medical diagnosis aid based on unbalanced datasets.
ER  - 
TY  - JOUR
T1  - Robust classifier learning with fuzzy class labels for large-margin support vector machines
A1  - Yang, Chan-Yun
A1  - Chou, Jui-Jen
A1  - Lian, Feng-Li
Y1  - 2013///
KW  -  Classification
KW  -  Lagrange constraint
KW  -  Loss function
KW  -  Machine learning
KW  -  Membership function
KW  -  Pattern recognition
KW  -  Support vector machines
KW  - Fuzzy class label
JF  - Neurocomputing
VL  - 99
SP  - 1
EP  - 14
DO  - https://doi.org/10.1016/j.neucom.2012.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S0925231212003438
N2  - Using class label fuzzification, this study develops the idea of refreshing the attitude of the difficult training examples and gaining a more robust classifier for large-margin support vector machines (SVMs). Fuzzification relaxes the specific hard-limited Lagrangian constraints of the difficult examples, extends the infeasible space of the canonical constraints for optimization, and reconfigures the consequent decision function with a wider margin. With the margin, a classifier capable of achieving a high generalization performance can be more robust. This paper traces the rationale for such a robust performance back to the changes of governing loss function. From the aspect of loss function, the reasons are causally explained. In the study, we also demonstrate a two-stage system for experiments to show the changes corresponding to the label fuzzification. The system first captures the difficult examples in the first-stage preprocessor, and assigns them various fuzzified class labels. Three types of membership functions, including a constant, a linear, and a sigmoidal membership function, are designated in the preprocessor to manipulate the within-class correlations of the difficult examples for reference of the fuzzification. The consequent performance benchmarks confirm the robust and generalized ability due to the label fuzzification. Since the change of yi′ is fundamental, the idea may be transplanted to different prototypes of SVM.
ER  - 
TY  - JOUR
T1  - Integrating SNOMED CT into the UMLS: An Exploration of Different Views of Synonymy and Quality of Editing
A1  - Fung, Kin Wah
A1  - Hole, William T
A1  - Nelson, Stuart J
A1  - Srinivasan, Suresh
A1  - Powell, Tammy
A1  - Roth, Laura
Y1  - 2005///
JF  - Journal of the American Medical Informatics Association
VL  - 12
IS  - 4
SP  - 486
EP  - 494
DO  - https://doi.org/10.1197/jamia.M1767
UR  - https://www.sciencedirect.com/science/article/pii/S1067502705000587
N2  - Objective
The integration of SNOMED CT into the Unified Medical Language System (UMLS) involved the alignment of two views of synonymy that were different because the two vocabulary systems have different intended purposes and editing principles. The UMLS is organized according to one view of synonymy, but its structure also represents all the individual views of synonymy present in its source vocabularies. Despite progress in knowledge-based automation of development and maintenance of vocabularies, manual curation is still the main method of determining synonymy. The aim of this study was to investigate the quality of human judgment of synonymy.
Design
Sixty pairs of potentially controversial SNOMED CT synonyms were reviewed by 11 domain vocabulary experts (six UMLS editors and five noneditors), and scores were assigned according to the degree of synonymy.
Measurements
The synonymy scores of each subject were compared to the gold standard (the overall mean synonymy score of all subjects) to assess accuracy. Agreement between UMLS editors and noneditors was measured by comparing the mean synonymy scores of editors to noneditors.
Results
Average accuracy was 71% for UMLS editors and 75% for noneditors (difference not statistically significant). Mean scores of editors and noneditors showed significant positive correlation (Spearman's rank correlation coefficient 0.654, two-tailed p < 0.01) with a concurrence rate of 75% and an interrater agreement kappa of 0.43.
Conclusion
The accuracy in the judgment of synonymy was comparable for UMLS editors and nonediting domain experts. There was reasonable agreement between the two groups.
ER  - 
TY  - JOUR
T1  - A study of the effectiveness of machine learning methods for classification of clinical interview fragments into a large number of categories
A1  - Hasan, Mehedi
A1  - Kotov, Alexander
A1  - Idalski Carcone, April
A1  - Dong, Ming
A1  - Naar, Sylvie
A1  - Brogan Hartlieb, Kathryn
Y1  - 2016///
KW  -  Annotation of clinical text
KW  -  Deep learning
KW  -  Motivational interviewing
KW  -  Text classification
KW  - Machine learning
JF  - Journal of Biomedical Informatics
VL  - 62
SP  - 21
EP  - 31
DO  - https://doi.org/10.1016/j.jbi.2016.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S153204641630034X
N2  - This study examines the effectiveness of state-of-the-art supervised machine learning methods in conjunction with different feature types for the task of automatic annotation of fragments of clinical text based on codebooks with a large number of categories. We used a collection of motivational interview transcripts consisting of 11,353 utterances, which were manually annotated by two human coders as the gold standard, and experimented with state-of-art classifiers, including Naïve Bayes, J48 Decision Tree, Support Vector Machine (SVM), Random Forest (RF), AdaBoost, DiscLDA, Conditional Random Fields (CRF) and Convolutional Neural Network (CNN) in conjunction with lexical, contextual (label of the previous utterance) and semantic (distribution of words in the utterance across the Linguistic Inquiry and Word Count dictionaries) features. We found out that, when the number of classes is large, the performance of CNN and CRF is inferior to SVM. When only lexical features were used, interview transcripts were automatically annotated by SVM with the highest classification accuracy among all classifiers of 70.8%, 61% and 53.7% based on the codebooks consisting of 17, 20 and 41 codes, respectively. Using contextual and semantic features, as well as their combination, in addition to lexical ones, improved the accuracy of SVM for annotation of utterances in motivational interview transcripts with a codebook consisting of 17 classes to 71.5%, 74.2%, and 75.1%, respectively. Our results demonstrate the potential of using machine learning methods in conjunction with lexical, semantic and contextual features for automatic annotation of clinical interview transcripts with near-human accuracy.
ER  - 
TY  - JOUR
T1  - An inverse finite-element model of heel-pad indentation
A1  - Erdemir, Ahmet
A1  - Viveiros, Meredith L
A1  - Ulbrecht, Jan S
A1  - Cavanagh, Peter R
Y1  - 2006///
KW  -  Diabetic foot
KW  -  Finite-element analysis
KW  -  Plantar
KW  -  Soft-tissue injuries
KW  -  Ulcers
KW  -  Ultrasonography
KW  - Calcaneus
JF  - Journal of Biomechanics
VL  - 39
IS  - 7
SP  - 1279
EP  - 1286
DO  - https://doi.org/10.1016/j.jbiomech.2005.03.007
UR  - https://www.sciencedirect.com/science/article/pii/S0021929005001454
N2  - A numerical–experimental approach has been developed to characterize heel-pad deformation at the material level. Left and right heels of 20 diabetic subjects and 20 nondiabetic subjects matched for age, gender and body mass index were indented using force-controlled ultrasound. Initial tissue thickness and deformation were measured using M-mode ultrasound; indentation forces were recorded simultaneously. An inverse finite-element analysis of the indentation protocol using axisymmetric models adjusted to reflect individual heel thickness was used to extract nonlinear material properties describing the hyperelastic behavior of each heel. Student's t-tests revealed that heel pads of diabetic subjects were not significantly different in initial thickness nor were they stiffer than those from nondiabetic subjects. Another heel-pad model with anatomically realistic surface representations of the calcaneus and soft tissue was developed to estimate peak pressure prediction errors when average rather than individualized material properties were used. Root-mean-square errors of up to 7% were calculated, indicating the importance of subject-specific modeling of the nonlinear elastic behavior of the heel pad. Indentation systems combined with the presented numerical approach can provide this information for further analysis of patient-specific foot pathologies and therapeutic footwear designs.
ER  - 
TY  - JOUR
T1  - A viscoelastic ellipsoidal model of the mechanics of plantar tissues
A1  - DeBerardinis, Jessica
A1  - Dufek, Janet S
A1  - Trabia, Mohamed B
Y1  - 2019///
KW  -  Ground reaction force
KW  -  Pressure-measuring insoles
KW  -  Walking
KW  - Varying contact area models
JF  - Journal of Biomechanics
VL  - 92
SP  - 137
EP  - 145
DO  - https://doi.org/10.1016/j.jbiomech.2019.05.041
UR  - https://www.sciencedirect.com/science/article/pii/S0021929019303859
N2  - Several assessments of the mechanics of plantar tissues, using various material models in conjunction with representing plantar regions using simple geometry, have been proposed. In this study, the plantar tissues were divided into eight regions to account for the various tissue characteristics. The plantar tissue model described each region as an ellipsoid, with a viscoelastic material model. The model combined varying elliptical contact areas with nonlinear tissue stiffness and damping. The main instruments used in this research were pressure-measuring insoles, which were used to determine the ground reaction force, as well as contact areas. The measured contact areas were fitted as elliptical areas to describe the compression of the corresponding ellipsoids. The approach was tested using walking data collected from 26 individuals: four men, 22 women, 24.4 ± 6.9 years old, 66.9 ± 21.4 kg of mass, 1.66 ± 0.12 m tall. The geometric and material variables of the proposed ellipsoidal model were optimized for each participant to match the ground reaction forces. Results suggest that the ellipsoid model is able to reproduce ground reaction force with reasonable accuracy. The largest errors were seen in heel and toe regions and were due to high-rate forces and small comparative areas, respectively. The model also showed that there are regional differences in the mechanical characteristics of plantar tissue, which confirms earlier research.
ER  - 
TY  - JOUR
T1  - Hybrid intelligent approach for diagnosis of the lung nodule from CT images using spatial kernelized fuzzy c-means and ensemble learning
A1  - Farahani, Farzad Vasheghani
A1  - Ahmadi, Abbas
A1  - Zarandi, Mohammad Hossein Fazel
Y1  - 2018///
KW  -  Computed tomography
KW  -  Ensemble learning
KW  -  Fuzzy Image processing
KW  -  Fuzzy c-means (FCM)
KW  - Lung cancer diagnosis
JF  - Mathematics and Computers in Simulation
VL  - 149
SP  - 48
EP  - 68
DO  - https://doi.org/10.1016/j.matcom.2018.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S0378475418300272
N2  - Lung cancer is one of the most common forms of cancer leading to over a million deaths per year throughout the world. The aim of this paper is to identify the pulmonary nodules in computed tomography (CT) images of the lung using a hybrid intelligent approach. At first, the proposed approach utilizes a type-II fuzzy algorithm to improve the quality of raw CT images. Then, a novel segmentation algorithm based on fuzzy c-means clustering, called modified spatial kernelized fuzzy c-means (MSFCM) clustering, is offered in order to achieve another representation of lung regions through an optimization methodology. Next, nodule candidates are detected among all available objects in the lung regions by a morphological procedure. This is followed by extracting significant statistical and morphological features from such nodule candidates and finally, an ensemble of three classifiers comprising Multilayer Perceptron (MLP), K-Nearest Neighbor (KNN), and Support Vector Machine (SVM) is employed for the actual diagnosis and determining whether the nodule candidate is nodule (cancerous) or non-nodule (healthy). The effectiveness of the hybrid intelligent approach is evaluated using a public dataset for lung CT images, viz.: Lung Image Database Consortium (LIDC). The experimental results positively demonstrate that the modified spatial kernelized FCM segmentation is superior to the other techniques existing in the literature. More importantly, a number of useful performance measurements in medical applications including accuracy, sensitivity, specificity, confusion matrix, as well as the area under the Receiver Operating Characteristic (ROC) curve are computed. The obtained results confirm the promising performance of the proposed hybrid approach in undertaking pulmonary nodules diagnosis.
ER  - 
TY  - JOUR
T1  - Application of deep learning for retinal image analysis: A review
A1  - Badar, Maryam
A1  - Haris, Muhammad
A1  - Fatima, Anam
Y1  - 2020///
KW  - Auto-encoder
KW  - Convolutional neural network
KW  - De-noised sparse auto-encoder
KW  - Deep learning
KW  - Deep neural network
KW  - Hidden layers
KW  - Random forest
KW  - Rectified linear unit
KW  - Softmax
KW  - Sparse stacked auto-encoder
JF  - Computer Science Review
VL  - 35
SP  - 100203
EP  - 100203
DO  - https://doi.org/10.1016/j.cosrev.2019.100203
UR  - https://www.sciencedirect.com/science/article/pii/S1574013719301327
N2  - Retinal image analysis holds an imperative position for the identification and classification of retinal diseases such as Diabetic Retinopathy (DR), Age Related Macular Degeneration (AMD), Macular Bunker, Retinoblastoma, Retinal Detachment, and Retinitis Pigmentosa. Automated identification of retinal diseases is a big step towards early diagnosis and prevention of exacerbation of the disease. A number of state-of-the-art methods have been developed in the past that helped in the automatic segmentation and identification of retinal landmarks and pathologies. However, the current unprecedented advancements in deep learning and modern imaging modalities in ophthalmology have opened a whole new arena for researchers. This paper is a review of deep learning techniques applied to 2-D fundus and 3-D Optical Coherence Tomography (OCT) retinal images for automated classification of retinal landmarks, pathology, and disease classification. The methodologies are analyzed in terms of sensitivity, specificity, Area under ROC curve, accuracy, and F score on publicly available datasets which includes DRIVE, STARE, CHASE_DB1, DRiDB, NIH AREDS, ARIA, MESSIDOR-2, E-OPTHA, EyePACS-1 DIARETDB and OCT image datasets.
ER  - 
TY  - JOUR
T1  - Two-stage artificial intelligence model for jointly measurement of atherosclerotic wall thickness and plaque burden in carotid ultrasound: A screening tool for cardiovascular/stroke risk assessment
A1  - Biswas, Mainak
A1  - Saba, Luca
A1  - Chakrabartty, Shubhro
A1  - Khanna, Narender N
A1  - Song, Hanjung
A1  - Suri, Harman S
A1  - Sfikakis, Petros P
A1  - Mavrogeni, Sophie
A1  - Viskovic, Klaudija
A1  - Laird, John R
A1  - Cuadrado-Godia, Elisa
A1  - Nicolaides, Andrew
A1  - Sharma, Aditya
A1  - Viswanathan, Vijay
A1  - Protogerou, Athanasios
A1  - Kitas, George
A1  - Pareek, Gyan
A1  - Miner, Martin
A1  - Suri, Jasjit S
Y1  - 2020///
KW  - AI
KW  - Carotid plaque
KW  - Common carotid artery
KW  - Deep learning
KW  - Noninvasive cardiology
KW  - Plaque area
KW  - Wall thickness
KW  - cIMT
JF  - Computers in Biology and Medicine
VL  - 123
SP  - 103847
EP  - 103847
DO  - https://doi.org/10.1016/j.compbiomed.2020.103847
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520302067
N2  - Motivation
The early screening of cardiovascular diseases (CVD) can lead to effective treatment. Thus, accurate and reliable atherosclerotic carotid wall detection and plaque measurements are crucial. Current measurement methods are time-consuming and do not utilize the power of knowledge-based paradigms such as artificial intelligence (AI). We present an AI-based methodology for the joint automated detection and measurement of wall thickness and carotid plaque (CP) in the form of carotid intima-media thickness (cIMT) and total plaque area (TPA), a class of AtheroEdge™ system (AtheroPoint™, CA, USA).
Method
The novel system consists of two stages, and each stage comprises an independent deep learning (DL) model. In Stage I, the first DL model segregates the common carotid artery (CCA) patches from ultrasound (US) images into the rectangular wall and non-wall patches. The characterized wall patches are integrated to form the region of interest (ROI), which is then fed into Stage II. In Stage II, the second DL model segments the far wall region. Lumen-intima (LI) and media-adventitial (MA) boundaries are then extracted from the wall region, which is then used for cIMT and PA measurement.
Results
Using the database of 250 carotid scans, the cIMT error using the AI model is 0.0935±0.0637 mm, which is lower than those of all previous methods. The PA error is found to be 2.7939±2.3702 mm2. The system's correlation coefficient (CC) between AI and ground truth (GT) values for cIMT is 0.99 (p < 0.0001), which is higher compared with the CC of 0.96 (p < 0.0001) shown by the earlier DL method. The CC for PA between AI and GT values is 0.89 (p < 0.0001).
Conclusion
A novel AI-based strategy was applied to carotid US images for the joint detection of carotid wall thickness (cWT) and plaque area (PA), followed by cIMT and PA measurement. This AI-based strategy shows improved performance using the patch technique compared with previous methods using full carotid scans.
ER  - 
TY  - JOUR
T1  - Prediction models to identify individuals at risk of metabolic syndrome who are unlikely to participate in a health intervention program
A1  - Shimoda, Akihiro
A1  - Ichikawa, Daisuke
A1  - Oyama, Hiroshi
Y1  - 2018///
KW  -  Instruction program
KW  -  Machine learning
KW  -  Prediction
KW  - Health check-up
JF  - International Journal of Medical Informatics
VL  - 111
SP  - 90
EP  - 99
DO  - https://doi.org/10.1016/j.ijmedinf.2017.12.009
UR  - https://www.sciencedirect.com/science/article/pii/S1386505617304550
N2  - Objectives
Since the launch of a nationwide general health check-up and instruction program in Japan in 2008, interest in strategies to improve implementation of the program based on predictive analytics has grown. We investigated the performance of prediction models developed to identify individuals classified as “requiring instruction” (high-risk) who were unlikely to participate in a health intervention program.
Methods
Data were obtained from one large health insurance union in Japan. The study population included individuals who underwent at least one general health check-up between 2008 and 2013 and were identified as “requiring instruction” in 2013. We developed three prediction models based on the gradient boosted trees (GBT), random forest (RF), and logistic regression (LR) algorithms using machine-learning techniques and compared the areas under the curve (AUC) of the developed models with those of two conventional methods The aim of the models was to identify at-risk individuals who were unlikely to participate in the instruction program in 2013 after being classified as requiring instruction at their general health check-up that year.
Results
At first we performed the analysis using data without multiple imputation. The AUC values for the GBT, RF, and LR prediction models and conventional methods: 1, and 2 were 0.893 (95%CI: 0.882–0.905), 0.889 (95%CI: 0.877–0.901), 0.885 (95%CI: 0.872–0.897), 0.784 (95%CI: 0.767–0.800), and 0.757 (95%CI: 0.741–0.773), respectively. Subsequently, we performed the analysis using data after multiple imputation. The AUC values for the GBT, RF, and LR prediction models and conventional methods: 1, and 2 were 0.894 (95%CI: 0.882–0.906), 0.889 (95%CI: 0.887–0.901), 0.885 (95%CI: 0.872–0.898), 0.784 (95%CI: 0.767–0.800), and 0.757 (95%CI: 0.741–0.773), respectively. In both analyses, the GBT model showed the highest AUC among that of other models, and statistically significant difference were found in comparison with the LR model, conventional method 1, and conventional method 2.
Conclusion
The prediction models using machine-learning techniques outperformed existing conventional methods: for predicting participation in the instruction program among participants identified as “requiring instruction” (high-risk).
ER  - 
TY  - JOUR
T1  - A hybrid classifier combining Borderline-SMOTE with AIRS algorithm for estimating brain metastasis from lung cancer: A case study in Taiwan
A1  - Wang, Kung-Jeng
A1  - Adrian, Angelia Melani
A1  - Chen, Kun-Huang
A1  - Wang, Kung-Min
Y1  - 2015///
KW  -  Borderline-synthetic minority over sampling technique
KW  -  Brain metastasis
KW  -  Imbalance dataset
KW  -  Lung cancer
KW  - Artificial immune recognition system
JF  - Computer Methods and Programs in Biomedicine
VL  - 119
IS  - 2
SP  - 63
EP  - 76
DO  - https://doi.org/10.1016/j.cmpb.2015.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715000577
N2  - Classifying imbalanced data in medical informatics is challenging. Motivated by this issue, this study develops a classifier approach denoted as BSMAIRS. This approach combines borderline synthetic minority oversampling technique (BSM) and artificial immune recognition system (AIRS) as global optimization searcher with the nearest neighbor algorithm used as a local classifier. Eight electronic medical datasets collected from University of California, Irvine (UCI) machine learning repository were used to evaluate the effectiveness and to justify the performance of the proposed BSMAIRS. Comparisons with several well-known classifiers were conducted based on accuracy, sensitivity, specificity, and G-mean. Statistical results concluded that BSMAIRS can be used as an efficient method to handle imbalanced class problems. To further confirm its performance, BSMAIRS was applied to real imbalanced medical data of lung cancer metastasis to the brain that were collected from National Health Insurance Research Database, Taiwan. This application can function as a supplementary tool for doctors in the early diagnosis of brain metastasis from lung cancer.
ER  - 
TY  - JOUR
T1  - Using smart offices to predict occupational stress
A1  - Alberdi, Ane
A1  - Aztiria, Asier
A1  - Basarab, Adrian
A1  - Cook, Diane J
Y1  - 2018///
KW  -  Automatic assessment
KW  -  Behavior
KW  -  Physiology
KW  -  Smart office
KW  - Stress
JF  - International Journal of Industrial Ergonomics
VL  - 67
SP  - 13
EP  - 26
DO  - https://doi.org/10.1016/j.ergon.2018.04.005
UR  - https://www.sciencedirect.com/science/article/pii/S016981411730361X
N2  - Occupational stress is increasingly present in our society. Usually, it is detected too late, resulting in physical and mental health problems for the worker, as well as economic losses for the companies due to the consequent absenteeism, presenteeism, reduced motivation or staff turnover. Therefore, the development of early stress detection systems that allow individuals to take timely action and prevent irreversible damage is required. To address this need, we investigate a method to analyze changes in physiological and behavioral patterns using unobtrusively and ubiquitously gathered smart office data. The goal of this paper is to build models that predict self-assessed stress and mental workload scores, as well as models that predict workload conditions based on physiological and behavior data. Regression models were built for the prediction of the self-reported stress and mental workload scores from data based on real office work settings. Similarly, classification models were employed to detect workload conditions and change in these conditions. Specific algorithms to deal with class-imbalance (SMOTEBoost and RUSBoost) were also tested. Results confirm the predictability of behavioral changes for stress and mental workload levels, as well as for change in workload conditions. Results also suggest that computer-use patterns together with body posture and movements are the best predictors for this purpose. Moreover, the importance of self-reported scores' standardization and the suitability of the NASA Task Load Index test for workload assessment is noticed. This work contributes significantly towards the development of an unobtrusive and ubiquitous early stress detection system in smart office environments, whose implementation in the industrial environment would make a great beneficial impact on workers’ health status and on the economy of companies.
ER  - 
TY  - JOUR
T1  - Monitoring cardiovascular changes due to local anesthesia during and after tooth extraction using pulse wave analysis
A1  - Veerabhadrappa, S T
A1  - Vyas, Anoop Lal
A1  - Anand, Sneh
A1  - Das, Nagarajappa
Y1  - 2015///
KW  -  ECG
KW  -  Local anesthesia and pulse transit time
KW  - Pulse
JF  - Biomedical Signal Processing and Control
VL  - 16
SP  - 17
EP  - 21
DO  - https://doi.org/10.1016/j.bspc.2014.09.014
UR  - https://www.sciencedirect.com/science/article/pii/S1746809414001499
N2  - A study was conducted to determine the ongoing short-term changes in pulse waveform and pulse transit time in patients during tooth extraction. The sympathetic stimulation produced during tooth extraction under local anesthesia (LA) utilizing 2% lignocaine with adrenaline is evaluated. The increase in pulse rate due to vasoconstriction in the peripheral blood vessels has been quantifying the sympathetic stimulation to the local anesthesia. The ECG and pulse signals were recorded from 16 non-medically compromised dental patients in resting condition (before local anesthesia), 2–3min after injection of local anesthesia and 5–10min after tooth extraction. The amplitudes and timings of pulse, width, slope, pulse rate interval and PTT were computed for every pulse. More than 25% increase in amplitude and slope of the forward wave and about 15% decrease in rise time and the width of the forward wave were observed after injection of local anesthesia (PostLA) as compared to pre-local anesthesia (PreLA) stages. Indices utilizing the variations on pulse amplitude can provide a window to detect volume of anesthetic and vascular stiffness. It is seen that the PTT decreases more than 5% after LA and increased about 5% after surgery as compared to resting conditions (PreLA). The result suggests that the contour analysis of pulse waveform and PTT can be used to quantify the sympathetic stimulation due to local anesthesia in healthy dental patients. The variations in amplitude shall be given more efforts and attention in order to ride the challenge in developing some means of pulse contour, which can be used to assist in the monitoring of the local anesthetic volume and duration.
ER  - 
TY  - JOUR
T1  - A probabilistic data-driven framework for scoring the preoperative recipient-donor heart transplant survival
A1  - Dag, Ali
A1  - Topuz, Kazim
A1  - Oztekin, Asil
A1  - Bulur, Serkan
A1  - Megahed, Fadel M
Y1  - 2016///
KW  -  Bayesian Belief Networks
KW  -  Data mining
KW  -  Genetic algorithms
KW  -  Medical decision making
KW  -  United Network for Organ Sharing (UNOS)
KW  - Healthcare analytics
JF  - Decision Support Systems
VL  - 86
SP  - 1
EP  - 12
DO  - https://doi.org/10.1016/j.dss.2016.02.007
UR  - https://www.sciencedirect.com/science/article/pii/S0167923616300185
N2  - Recent research has shown that data mining models can accurately predict the outcome of a heart transplant based on predictors that include patient and donor's health/demographics. These models have not been adopted in practice, however, since they did not: a) consider the interactions between the explanatory variables; b) provide a patient's specific risk of survival (reported results have been primarily deterministic); and c) offer an automated decision tool that can provide some data-driven insights to practitioners. In this study, we attempt to overcome these three limitations through the use of Bayesian Belief Networks (BBN). The proposed BBN framework is comprised of four phases. In the first two phases, the data is preprocessed, and a candidate set of predictors is generated based on employing several variable selection methods. The third phase involves the addition of medically relevant variables to the list. In phase four, the BBN model is applied. The results show that the proposed BBN method provides similar predictive performance to the best approaches in the literature. More importantly, our method provides novel information on the interactions among the predictors and the conditional probability of survival for a given set of relevant donor–recipient characteristics. We offer U.S. practitioners a decision support tool that presents an individualized survival score based on our BBN model (and the UNOS dataset).
ER  - 
TY  - JOUR
T1  - Selection of important features and predicting wine quality using machine learning techniques
A1  - Gupta, Yogesh
Y1  - 2018///
KW  -  neural network
KW  -  support vector machine
KW  -  wine quality
KW  - Linear regression
JF  - Procedia Computer Science
VL  - 125
SP  - 305
EP  - 312
DO  - https://doi.org/10.1016/j.procs.2017.12.041
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917328053
N1  - The 6th International Conference on Smart Computing and Communications
N2  - Nowadays, industries are using product quality certifications to promote their products. This is a time taking process and requires the assessment given by human experts which makes this process very expensive. This paper explores the usage of machine learning techniques such as linear regression, neural network and support vector machine for product quality in two ways. Firstly, determine the dependency of target variable on independent variables and secondly, predicting the value of target variable. In this paper, linear regression is used to determine the dependency of target variable on independent variables. On the basis of computed dependency, important variables are selected those make significant impact on dependent variable. Further, neural network and support vector machine are used to predict the values of dependent variable. All the experiments are performed on Red Wine and White Wine datasets. This paper proves that the better prediction can be made if selected features (variables) are being considered rather than considering all the features.
ER  - 
TY  - JOUR
T1  - Optimization of neural network using kidney-inspired algorithm with control of filtration rate and chaotic map for real-world rainfall forecasting
A1  - Jaddi, Najmeh Sadat
A1  - Abdullah, Salwani
Y1  - 2018///
KW  -  Artificial neural network
KW  -  Chaotic map
KW  -  Classification
KW  -  Filtration rate control
KW  -  Real-world rainfall forecasting
KW  -  Time series prediction
KW  - Kidney-inspired algorithm
JF  - Engineering Applications of Artificial Intelligence
VL  - 67
SP  - 246
EP  - 259
DO  - https://doi.org/10.1016/j.engappai.2017.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S0952197617302269
N2  - A broad variety of real-world problems have been solved using multilayer perceptron (MLP) artificial neural networks (ANNs). Optimization techniques aid ANNs to select suitable weights and achieve correct results. Recently, the kidney-inspired algorithm (KA) has been proposed for optimization problems. This algorithm is based on the filtration, reabsorption, secretion, and excretion processes that take place in the kidneys of the human body. In the KA, the value of α in the filtration rate formula is a constant value in the range of [0, 1] that is set in the initialization stage of the algorithm. In this paper, an improved KA for optimization of the ANN model is presented in which the filtration rate is controlled by changing the value of α from minimum to maximum during the search process, which helps in achieving a better balance between exploration and exploitation in the algorithm. In this algorithm if more solutes are filtered and moved to filtered blood it means that the algorithm has more exploration. In contrast, if more solutes move to waste it means that more exploitation is performed by the algorithm. In addition, the separate use of three chaotic maps instead of a random number in the movement formula of the modified KA is investigated in order to assess the ability of each map to help to achieve superior results. The proposed method is tested on benchmark classification and time series prediction problems. The method is also applied to a real-world rainfall forecasting problem. The results of a statistical analysis prove the ability of the method.
ER  - 
TY  - JOUR
T1  - Monitoring and fuzzy warning system for risk prevention of Guyon’s canal syndrome
A1  - Alpar, Orcan
Y1  - 2021///
KW  -  Ergonomics
KW  -  Fuzzy logic
KW  -  Guyon’s canal syndrome
KW  -  Intelligent monitoring
KW  -  Preventive medicine
KW  -  Wrist health
KW  - Ulnar nerve entrapment
JF  - Biomedical Signal Processing and Control
VL  - 64
SP  - 102228
EP  - 102228
DO  - https://doi.org/10.1016/j.bspc.2020.102228
UR  - https://www.sciencedirect.com/science/article/pii/S174680942030358X
N2  - Guyon’s canal is a very small tunnel in the wrist where the ulnar nerve and ulnar artery are passing through. As it goes down from the canal, the ulnar nerve runs through the last two fingers and provides sensory innervation of the fifth and half of the fourth fingers. When it is trapped in Guyon’s canal, due to continual compression or pressure, some symptoms could emerge, like feeling of pins and needles, numbness and decreased sensation. One of the characteristic reasons of Guyon’s canal syndrome (GCS) is habitual misuse of computer keyboards that may cause constant pressure to the ulnar nerve. Therefore, in this paper, we present a novel methodology of monitoring and identification supported by a fuzzy warning system that warns the users to prevent from wrong wrist posture and to reduce the risks of GCS. From the images taken by a camera mounted on a laptop monitor, hands of the user are segmented for location of the exact wrist region. Afterwards, the angle of the wrist is estimated and subsequently a fuzzy-based warning system is triggered by the angle and the duration data. The results are greatly encouraging that the hands on the keyboards and wrist angles are perfectly identified; and depending on the duration and the angles of wrists, the warning system is generated correct signals to warn the users for changing their wrist posture. As an outstanding instance of artificial intelligence-based systems in ergonomic research, the monitoring system would help the users with the bad posture habit to reposition their wrist for wrist health.
ER  - 
TY  - JOUR
T1  - Independent component analysis for synthetic aperture magnetometry in magnetocardiography
A1  - Kim, Kiwoong
A1  - Lee, Yong-Ho
A1  - Kwon, Hyukchan
A1  - Kim, Jin-Mok
A1  - Bae, Jang-Ho
Y1  - 2006///
KW  -  Independent component analysis (ICA)
KW  -  Inverse solution
KW  -  Synthetic aperture magnetometry (SAM)
KW  -  WPW syndrome
KW  - Magnetocardiogram (MCG)
JF  - Computers in Biology and Medicine
VL  - 36
IS  - 3
SP  - 253
EP  - 261
DO  - https://doi.org/10.1016/j.compbiomed.2004.11.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482505000053
N2  - We propose independent component analysis (ICA) as a pre-process for synthetic aperture magnetometry (SAM) in magnetocardiogram. SAM is a very useful method for source current imaging. However, SAM cannot separate one source from the others when there are time-correlated multi-sources, especially for successively active sources. The proposed method compensates the intrinsic drawback of SAM with ICA, which is feasible for 3-D imaging of the myocardial current distribution of specific temporal features. By using our method, we successfully localized an accessory pathway of a patient suffering from the WPW syndrome.
ER  - 
TY  - JOUR
T1  - Reducing the number of neurons in radial basis function networks with dynamic decay adjustment
A1  - Paetz, Jürgen
Y1  - 2004///
KW  -  Benchmark
KW  -  Classification
KW  -  Medical data
KW  -  Model complexity
KW  -  RBF network
KW  - Neural network
JF  - Neurocomputing
VL  - 62
SP  - 79
EP  - 91
DO  - https://doi.org/10.1016/j.neucom.2003.12.004
UR  - https://www.sciencedirect.com/science/article/pii/S0925231203005642
N2  - Classification is a common task for supervised neural networks. A specific radial basis function network for classification is the so-called RBF network with dynamic decay adjustment (RBFN-DDA). Fast training and good classification performance are properties of this network. RBFN-DDA is a dynamically growing network, i.e. neurons are inserted during training. A drawback of RBFN-DDA is its greedy insertion behavior. Too many superfluous neurons are inserted for noisy data, overlapping data or for outliers. We propose an online technique to reduce the number of neurons during training. We achieve our goal by deleting neurons after each training of one epoch. By using the improved algorithm on benchmark data and current medical data, the number of neurons is reduced clearly (up to 93.9% less neurons). Thus, we achieve a network with less complexity compared to the original RBFN-DDA.
ER  - 
TY  - JOUR
T1  - Portable automatic text classification for adverse drug reaction detection via multi-corpus training
A1  - Sarker, Abeed
A1  - Gonzalez, Graciela
Y1  - 2015///
KW  -  Adverse drug reaction
KW  -  Natural language processing
KW  -  Social media monitoring
KW  -  Text classification
KW  - Pharmacovigilance
JF  - Journal of Biomedical Informatics
VL  - 53
SP  - 196
EP  - 207
DO  - https://doi.org/10.1016/j.jbi.2014.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414002317
N2  - Objective
Automatic detection of adverse drug reaction (ADR) mentions from text has recently received significant interest in pharmacovigilance research. Current research focuses on various sources of text-based information, including social media—where enormous amounts of user posted data is available, which have the potential for use in pharmacovigilance if collected and filtered accurately. The aims of this study are: (i) to explore natural language processing (NLP) approaches for generating useful features from text, and utilizing them in optimized machine learning algorithms for automatic classification of ADR assertive text segments; (ii) to present two data sets that we prepared for the task of ADR detection from user posted internet data; and (iii) to investigate if combining training data from distinct corpora can improve automatic classification accuracies.
Methods
One of our three data sets contains annotated sentences from clinical reports, and the two other data sets, built in-house, consist of annotated posts from social media. Our text classification approach relies on generating a large set of features, representing semantic properties (e.g., sentiment, polarity, and topic), from short text nuggets. Importantly, using our expanded feature sets, we combine training data from different corpora in attempts to boost classification accuracies.
Results
Our feature-rich classification approach performs significantly better than previously published approaches with ADR class F-scores of 0.812 (previously reported best: 0.770), 0.538 and 0.678 for the three data sets. Combining training data from multiple compatible corpora further improves the ADR F-scores for the in-house data sets to 0.597 (improvement of 5.9 units) and 0.704 (improvement of 2.6 units) respectively.
Conclusions
Our research results indicate that using advanced NLP techniques for generating information rich features from text can significantly improve classification accuracies over existing benchmarks. Our experiments illustrate the benefits of incorporating various semantic features such as topics, concepts, sentiments, and polarities. Finally, we show that integration of information from compatible corpora can significantly improve classification performance. This form of multi-corpus training may be particularly useful in cases where data sets are heavily imbalanced (e.g., social media data), and may reduce the time and costs associated with the annotation of data in the future.
ER  - 
TY  - JOUR
T1  - Patients’ Disease Risk Predictive Modeling using MIMIC Data
A1  - Singh, Dhanjeet
A1  - Kumar, Vishal
A1  - Qiu, Robin G
Y1  - 2020///
KW  -  Deep Learning
KW  -  Disease Risk Assessment
KW  -  Predictive Modelling
KW  - Machine Learning
JF  - Procedia Computer Science
VL  - 168
SP  - 112
EP  - 117
DO  - https://doi.org/10.1016/j.procs.2020.02.271
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920304105
N1  - “Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019
N2  - The latest MIMC III (Medical Information Mart for Intensive Care III) database has rich information on over 58k patient’s medical histories for over 11 years. Based on MIMC III database, this paper presents a study of those patient’s primary disease risk prediction modeling, which focuses on assessing future disease risks for an individual who is ready for discharge. We explore a framework that combines regression modeling and deep learning techniques to substantially improve the performance of developed models. Firstly, a regression model will be applied to predicting the length of stay for a patient’s future ICU visit. Secondly, deep learning approach will be adopted to assess individual’s future visit in terms of time and the primary disease. If the modeling gets adopted in a hospital, the predicted results can be promisingly utilized as a reference for medical professionals and experts to oﬀer eﬀective health care guidance for patients. The proposed framework can also be utilized for developing an innovative tool that helps individuals create and maintain a better healthcare plan over time.
ER  - 
TY  - JOUR
T1  - A new density-based subspace selection method using mutual information for high dimensional outlier detection
A1  - Riahi-Madvar, Mahboobeh
A1  - Akbari Azirani, Ahmad
A1  - Nasersharif, Babak
A1  - Raahemi, Bijan
Y1  - 2021///
KW  -  Density-based representation
KW  -  Maximum-Relevance-to-Density
KW  -  Mutual information
KW  -  Relevant subspace selection
KW  -  minimum-Redundancy-Maximum-Relevance-to-Density
KW  - High dimensional data
JF  - Knowledge-Based Systems
VL  - 216
SP  - 106733
EP  - 106733
DO  - https://doi.org/10.1016/j.knosys.2020.106733
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120308625
N2  - Outlier detection in high dimensional data faces the challenge of curse of dimensionality, where irrelevant features may prevent detection of outliers. In this research, we propose a novel efficient unsupervised density-based subspace selection for outlier detection in the projected subspace. First, the Maximum-Relevance-to-Density algorithm(MRD) is proposed to select the relevant subspace based on the mutual information. Then, applying the concept of redundancy among features, we present an efficient relevant subspace selection method called minimum-Redundancy-Maximum-Relevance-to-Density (mRMRD). Finally, the degree of outlierness of data points in the corresponding relevant subspace is computed based on Local Outlier Factor(LOF). Experimental results on both real and synthetic data demonstrate that the proposed algorithms – based on MRD and mRMRD criteria – increase the accuracy of outlier detection while reducing computational complexity and execution time. Moreover, as the dimensionality increases, the accuracy of outlier detection on mRMRD-based relevant subspace is higher than MRD-based relevant subspace. This verifies that the proposed mRMRD-based subspace selection algorithm can efficiently select the subspace by considering the relevance between features.
ER  - 
TY  - JOUR
T1  - Adverse drug event detection and extraction from open data: A deep learning approach
A1  - Fan, Brandon
A1  - Fan, Weiguo
A1  - Smith, Carly
A1  - Garner, Harold “Skip”
Y1  - 2020///
KW  - BERT
KW  - Deep learning
KW  - Drug side effects
KW  - Information extraction
KW  - Natural language processing
KW  - Open data
KW  - Pharmacovigilance
JF  - Information Processing & Management
VL  - 57
IS  - 1
SP  - 102131
EP  - 102131
DO  - https://doi.org/10.1016/j.ipm.2019.102131
UR  - https://www.sciencedirect.com/science/article/pii/S0306457319308623
N2  - Drug prescription is a task that doctors face daily with each patient. However, when prescribing drugs, doctors must be conscious of all potential drug side effects. In fact, according to the U.S. Department of Health and Human Services, adverse drug events (ADEs), or harmful side effects, account for 1/3 of total hospital admissions each year. The goal of this research is to utilize novel deep learning methods for accurate detection and identification of professionally unreported drug side effects using widely available public data (open data). Utilizing a manually-labelled dataset of 10,000 reviews gathered from WebMD and Drugs.com, this research proposes a deep learning-based approach utilizing Bidirectional Encoder Representations from Transformers (BERT) based models for ADE detection and extraction and compares results to standard deep learning models and current state-of-the-art extraction models. By utilizing a hybrid of transfer learning from pre-trained BERT representations and sentence embeddings, the proposed model achieves an AUC score of 0.94 for ADE detection and an F1 score of 0.97 for ADE extraction. Previous state of the art deep learning approach achieves an AUC of 0.85 in ADE detection and an F1 of 0.82 in ADE extraction on our dataset of review texts. The results show that a BERT-based model achieves new state-of-the-art results on both the ADE detection and extraction task. This approach can be applied to multiple healthcare and information extraction tasks and used to help solve the problem that doctors face when prescribing drugs. Overall, this research introduces a novel dataset utilizing social media health forum data and shows the viability and capability of using deep learning techniques in ADE detection and extraction as well as information extraction as a whole. The model proposed in this paper achieves state-of-the-art results and can be applied to multiple other healthcare and information extraction tasks including medical entity extraction and entity recognition.
ER  - 
TY  - JOUR
T1  - Novel multiobjective TLBO algorithms for the feature subset selection problem
A1  - Kiziloz, Hakan Ezgi
A1  - Deniz, Ayça
A1  - Dokeroglu, Tansel
A1  - Cosar, Ahmet
Y1  - 2018///
KW  - Multiobjective feature selection
KW  - Supervised learning
KW  - Teaching learning based optimization
JF  - Neurocomputing
VL  - 306
SP  - 94
EP  - 107
DO  - https://doi.org/10.1016/j.neucom.2018.04.020
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218304442
N2  - Teaching Learning Based Optimization (TLBO) is a new metaheuristic that has been successfully applied to several intractable optimization problems in recent years. In this study, we propose a set of novel multiobjective TLBO algorithms combined with supervised machine learning techniques for the solution of Feature Subset Selection (FSS) in Binary Classification Problems (FSS-BCP). Selecting the minimum number of features while not compromising the accuracy of the results in FSS-BCP is a multiobjective optimization problem. We propose TLBO as a FSS mechanism and utilize its algorithm-specific parameterless concept that does not require any parameters to be tuned during the optimization. Most of the classical metaheuristics such as Genetic and Particle Swarm Optimization algorithms need additional efforts for tuning their parameters (crossover ratio, mutation ratio, velocity of particle, inertia weight, etc.), which may have an adverse influence on their performance. Comprehensive experiments are carried out on the well-known machine learning datasets of UCI Machine Learning Repository and significant improvements have been observed when the proposed multiobjective TLBO algorithms are compared with state-of-the-art NSGA-II, Particle Swarm Optimization, Tabu Search, Greedy Search, and Scatter Search algorithms.
ER  - 
TY  - JOUR
T1  - An automated snoring sound classification method based on local dual octal pattern and iterative hybrid feature selector
A1  - Tuncer, Turker
A1  - Akbal, Erhan
A1  - Dogan, Sengul
Y1  - 2021///
KW  -  Discrete wavelet transform
KW  -  ReliefF and iterative NCA
KW  -  Snoring sound classification
KW  -  Sound analysis
KW  - Local Dual Octal Pattern
JF  - Biomedical Signal Processing and Control
VL  - 63
SP  - 102173
EP  - 102173
DO  - https://doi.org/10.1016/j.bspc.2020.102173
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420303128
N2  - In this research, a novel snoring sound classification (SSC) method is presented by proposing a new feature generation function to yield a high classification rate. The proposed feature extractor is named as Local Dual Octal Pattern (LDOP). A novel LDOP based SSC method is presented to solve the low success rate problems for Munich-Passau Snore Sound Corpus (MPSSC) dataset. Multilevel discrete wavelet transform (DWT) decomposition and the LDOP based feature generation, informative features selection with ReliefF and iterative neighborhood component analysis (RFINCA), and classification using k nearest neighbors (kNN) are fundamental phases of the proposed SSC method. Seven leveled DWT transform, and LDOP are used together to generate low, medium, and high levels features. This feature generation network extracts 4096 features in total. RFINCA selects 95 the most discriminative and informative ones of these 4096 features. In the classification phase, kNN with leave one out cross-validation (LOOCV) is used. 95.53% classification accuracy and 94.65% unweighted average recall (UAR) have been achieved using this method. The proposed LDOP based SSC method reaches 22% better result than the best of the other state-of-the-art machine learning and deep learning-based methods. These results clearly denote the success of the proposed SSC method.
ER  - 
TY  - JOUR
T1  - A personalized hashtag recommendation approach using LDA-based topic model in microblog environment
A1  - Zhao, Feng
A1  - Zhu, Yajun
A1  - Jin, Hai
A1  - Yang, Laurence T
Y1  - 2016///
KW  -  Hashtag
KW  -  LDA
KW  -  Recommendation
KW  -  Topic model
KW  - Microblog
JF  - Future Generation Computer Systems
VL  - 65
SP  - 196
EP  - 206
DO  - https://doi.org/10.1016/j.future.2015.10.012
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X15003258
N1  - Special Issue on Big Data in the Cloud
N2  - With wide use of cloud computing technologies, microblog is used more widely for services providing more personal communities by user information sharing, dissemination and acquisition. In Microblog environment, hashtag is used to find messages with a specific theme or content, which can greatly facilitate information diffusion, microblog searching, event detection and topic analysis, etc. Recommending relevant hashtags to users in the cloud is challenging, because hashtags are created at tremendous speed alongside microblogs, and scattered in micro-blogging systems without a systematic organization. In this paper, a personalized hashtag recommendation approach is proposed according to the latent topical information in microblogs. With users represented by user-topics distribution, the proposed approach finds top-k similar users, then computes all hashtags’ frequencies appeared in these users, and finally the most relevant hashtags are recommended to user. In order to excavate latent topical information, a Latent Dirichlet Allocation (LDA)-based topic model is also proposed, named Hashtag-LDA, which can greatly enhance the influence of hashtags on latent topics’ generation by jointly modeling hashtags and words in microblogs. Hashtag-LDA can not only find meaningful latent topics, but also find global hashtags and the relationships between topics and hashtags. The experimental results on real Twitter dataset show that the proposed recommendation approach outperforms the related methods and Hashtag-LDA is effective.
ER  - 
TY  - JOUR
T1  - A statistical analysis based recommender model for heart disease patients
A1  - Mustaqeem, Anam
A1  - Anwar, Syed Muhammad
A1  - Khan, Abdul Rashid
A1  - Majid, Muhammad
Y1  - 2017///
KW  -  E-health
KW  -  Heart disease
KW  -  Machine learning
KW  -  Risk analysis
KW  - Medical recommendations
JF  - International Journal of Medical Informatics
VL  - 108
SP  - 134
EP  - 145
DO  - https://doi.org/10.1016/j.ijmedinf.2017.10.008
UR  - https://www.sciencedirect.com/science/article/pii/S1386505617303684
N2  - Objectives
An intelligent information technology based system could have a positive impact on the life-style of patients suffering from chronic diseases by providing useful health recommendations. In this paper, we have proposed a hybrid model that provides disease prediction and medical recommendations to cardiac patients. The first part aims at implementing a prediction model, that can identify the disease of a patient and classify it into one of the four output classes i.e., non-cardiac chest pain, silent ischemia, angina, and myocardial infarction. Following the disease prediction, the second part of the model provides general medical recommendations to patients.
Methods
The recommendations are generated by assessing the severity of clinical features of patients, estimating the risk associated with clinical features and disease, and calculating the probability of occurrence of disease. The purpose of this model is to build an intelligent and adaptive recommender system for heart disease patients. The experiments for the proposed recommender system are conducted on a clinical data set collected and labelled in consultation with medical experts from a known hospital.
Results
The performance of the proposed prediction model is evaluated using accuracy and kappa statistics as evaluation measures. The medical recommendations are generated based on information collected from a knowledge base created with the help of physicians. The results of the recommendation model are evaluated using confusion matrix and gives an accuracy of 97.8%.
Conclusion
The proposed system exhibits good prediction and recommendation accuracies and promises to be a useful contribution in the field of e-health and medical informatics.
ER  - 
TY  - JOUR
T1  - Customization in a unified framework for summarizing medical literature
A1  - Elhadad, N
A1  - Kan, M.-Y.
A1  - Klavans, J L
A1  - McKeown, K R
Y1  - 2005///
KW  -  Clinical information system
KW  -  Multi-document information extraction
KW  -  Multi-document summarization
KW  -  User modeling
KW  - Medical digital library
JF  - Artificial Intelligence in Medicine
VL  - 33
IS  - 2
SP  - 179
EP  - 198
DO  - https://doi.org/10.1016/j.artmed.2004.07.018
UR  - https://www.sciencedirect.com/science/article/pii/S0933365704001332
N1  - Information Extraction and Summarization from Medical Documents
N2  - Summary
Objective:
We present the summarization system in the PErsonalized Retrieval and Summarization of Images, Video and Language (PERSIVAL) medical digital library. Although we discuss the context of our summarization research within the PERSIVAL platform, the primary focus of this article is on strategies to define and generate customized summaries.
Methods and material:
Our summarizer employs a unified user model to create a tailored summary of relevant documents for either a physician or lay person. The approach takes advantage of regularities in medical literature text structure and content to fulfill identified user needs.
Results:
The resulting summaries combine both machine-generated text and extracted text that comes from multiple input documents. Customization includes both group-based modeling for two classes of users, physician and lay person, and individually driven models based on a patient record.
Conclusions:
Our research shows that customization is feasible in a medical digital library.
ER  - 
TY  - JOUR
T1  - Effect of Diagnosis on Variability of ICU Patients in Insulin Sensitivity
A1  - Ferenci, Tamás
A1  - Kovács, Levente
A1  - Benyó, Balázs
A1  - Le Compte, Aaron
A1  - Shaw, Geoffrey M
A1  - Chase, Geoffrey J
Y1  - 2012///
KW  - Insulin sensitivity
KW  - Patient variability
KW  - Statistical Analysis
KW  - Tight glycemic control
JF  - IFAC Proceedings Volumes
VL  - 45
IS  - 18
SP  - 462
EP  - 466
DO  - https://doi.org/10.3182/20120829-3-HU-2029.00108
UR  - https://www.sciencedirect.com/science/article/pii/S1474667016321425
N1  - 8th IFAC Symposium on Biological and Medical Systems
N2  - Tight glycemic control (TGC) in intensive care unit (ICU) patients represents an active research field as it has been proved its mortality and cost reduction effects. Previous works demonstrated that insulin sensitivity plays an important role in this question. The paper investigates by two defined metrics patient's variability in insulin sensitivity based on a previously introduced (ICING) glucose-insulin model. These metrics grasp the deviations of the actual insulin sensitivity data from their predictions; hence, characterizing a patient's variability at a given time. We also introduce and examine a way to characterize variability across longer time periods and across different patients. Investigations are applied to an actual longitudinal database consisting of n = 261 patients with 47,836 hours of measurement in total, with patients grouped according to diagnosis groups formed from their Apache III codes. Data was also segregated according to time spent in ICU (in days). Kruskal–Wallis-test was then employed (separately for different days) to assess whether patients in different diagnosis groups exhibit significantly different variability. Differences were further analyzed with Tukey HSD post-hoc testing. Results show that insulin sensitivity decreases with time and that differences between diagnosis groups diminish. However, there are significant differences on the first two days of stay in the ICU according to one of the metrics, with cardiac patients being more variable and gastric patients (especially non-operative ones) being less variable.
ER  - 
TY  - JOUR
T1  - Improving the accuracy of prediction of heart disease risk based on ensemble classification techniques
A1  - Latha, C Beulah Christalin
A1  - Jeeva, S Carolin
Y1  - 2019///
KW  -  Ensemble classifier
KW  -  Machine learning
KW  -  Prediction model
KW  - Heart disease
JF  - Informatics in Medicine Unlocked
VL  - 16
SP  - 100203
EP  - 100203
DO  - https://doi.org/10.1016/j.imu.2019.100203
UR  - https://www.sciencedirect.com/science/article/pii/S235291481830217X
N2  - Machine learning involves artificial intelligence, and it is used in solving many problems in data science. One common application of machine learning is the prediction of an outcome based upon existing data. The machine learns patterns from the existing dataset, and then applies them to an unknown dataset in order to predict the outcome. Classification is a powerful machine learning technique that is commonly used for prediction. Some classification algorithms predict with satisfactory accuracy, whereas others exhibit a limited accuracy. This paper investigates a method termed ensemble classification, which is used for improving the accuracy of weak algorithms by combining multiple classifiers. Experiments with this tool were performed using a heart disease dataset. A comparative analytical approach was done to determine how the ensemble technique can be applied for improving prediction accuracy in heart disease. The focus of this paper is not only on increasing the accuracy of weak classification algorithms, but also on the implementation of the algorithm with a medical dataset, to show its utility to predict disease at an early stage. The results of the study indicate that ensemble techniques, such as bagging and boosting, are effective in improving the prediction accuracy of weak classifiers, and exhibit satisfactory performance in identifying risk of heart disease. A maximum increase of 7% accuracy for weak classifiers was achieved with the help of ensemble classification. The performance of the process was further enhanced with a feature selection implementation, and the results showed significant improvement in prediction accuracy.
ER  - 
TY  - JOUR
T1  - Heterogeneous recurrence analysis of heartbeat dynamics for the identification of sleep apnea events
A1  - Cheng, Changqing
A1  - Kan, Chen
A1  - Yang, Hui
Y1  - 2016///
KW  -  Fractal representation
KW  -  Heterogeneous recurrence quantification
KW  -  Nonlinear dynamics
KW  -  Obstructive sleep apnea
KW  - Electrocardiogram
JF  - Computers in Biology and Medicine
VL  - 75
SP  - 10
EP  - 18
DO  - https://doi.org/10.1016/j.compbiomed.2016.05.006
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516301159
N2  - Obstructive sleep apnea (OSA) is a common sleep disorder that affects 24% of adult men and 9% of adult women. It occurs due to the occlusion of the upper airway during sleep, thereby leading to a decrease of blood oxygen level that triggers arousals and sleep fragmentation. OSA significantly impacts the quality of sleep and it is known to be responsible for a number of health complications, such as high blood pressure and type 2 diabetes. Traditional diagnosis of OSA relies on polysomnography, which is expensive, time-consuming and inaccessible to the general population. Recent advancement of sensing provides an unprecedented opportunity for the screening of OSA events using single-channel electrocardiogram (ECG). However, existing approaches are limited in their ability to characterize nonlinear dynamics underlying ECG signals. As such, hidden patterns of OSA-altered cardiac electrical activity cannot be fully revealed and understood. This paper presents a new heterogeneous recurrence model to characterize the heart rate variability for the identification of OSA. A nonlinear state space is firstly reconstructed from a time series of RR intervals that are extracted from single-channel ECGs. Further, the state space is recursively partitioned into a hierarchical structure of local recurrence regions. A new fractal representation is designed to efficiently characterize state transitions among segmented sub-regions. Statistical measures are then developed to quantify heterogeneous recurrence patterns. In addition, we integrate classification models with heterogeneous recurrence features to differentiate healthy subjects from OSA patients. Experimental results show that the proposed approach captures heterogeneous recurrence patterns in the transformed space and provides an effective tool to detect OSA using one-lead ECG signals.
ER  - 
TY  - JOUR
T1  - Evolutionary tuning of multiple SVM parameters
A1  - Friedrichs, Frauke
A1  - Igel, Christian
Y1  - 2005///
KW  -  Evolutionary algorithms
KW  -  Model selection
KW  - Support vector machines
JF  - Neurocomputing
VL  - 64
SP  - 107
EP  - 117
DO  - https://doi.org/10.1016/j.neucom.2004.11.022
UR  - https://www.sciencedirect.com/science/article/pii/S0925231204005223
N1  - Trends in Neurocomputing: 12th European Symposium on Artificial Neural Networks 2004
N2  - The problem of model selection for support vector machines (SVMs) is considered. We propose an evolutionary approach to determine multiple SVM hyperparameters: The covariance matrix adaptation evolution strategy (CMA-ES) is used to determine the kernel from a parameterized kernel space and to control the regularization. Our method is applicable to optimize non-differentiable kernel functions and arbitrary model selection criteria. We demonstrate on benchmark datasets that the CMA-ES improves the results achieved by grid search already when applied to few hyperparameters. Further, we show that the CMA-ES is able to handle much more kernel parameters compared to grid-search and that tuning of the scaling and the rotation of Gaussian kernels can lead to better results in comparison to standard Gaussian kernels with a single bandwidth parameter. In particular, more flexibility of the kernel can reduce the number of support vectors.
ER  - 
TY  - JOUR
T1  - A new version of the deterministic dendritic cell algorithm based on numerical differential and immune response
A1  - Zhou, Wen
A1  - Liang, Yiwen
Y1  - 2021///
KW  -  Anomaly detection
KW  -  Immune nonlinear model
KW  -  Immune response
KW  -  dDCA
KW  - Numerical differential
JF  - Applied Soft Computing
VL  - 102
SP  - 107055
EP  - 107055
DO  - https://doi.org/10.1016/j.asoc.2020.107055
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620309935
N2  - The efficiency of the promising deterministic dendritic cell algorithm (dDCA) in terms of anomaly detection has been demonstrated in large number of studies. However, the results of these studies also show that the signal calculation phase of dDCA depends on artificial experience, while the algorithm also lacks adaptability. To overcome these limitations, we propose a new approach for anomaly detection based on dDCA, in which numerical differential and immune responses are applied. The numerical differential is introduced to provide a new method of calculating the signal values that is independent of artificial experience. In the immune response method, moreover, the immune nonlinear model is adopted to adjust the weight signal matrix of dDCA. The experimental results show that the proposed algorithm has significant advantages over the compared immune anomaly detection approaches in terms of signal calculation and adaptability.
ER  - 
TY  - JOUR
T1  - Pattern-based Mining in Electronic Health Records for Complex Clinical Process Analysis
A1  - Metsker, Oleg
A1  - Bolgova, Ekaterina
A1  - Yakovlev, Alexey
A1  - Funkner, Anastasia
A1  - Kovalchuk, Sergey
Y1  - 2017///
KW  -  EHR
KW  -  cardiology
KW  -  complex process modeling
KW  -  event data
KW  -  modeling
KW  -  process mining
KW  - text mining
JF  - Procedia Computer Science
VL  - 119
SP  - 197
EP  - 206
DO  - https://doi.org/10.1016/j.procs.2017.11.177
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917323876
N1  - 6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland
N2  - This paper presents the application of text mining methods to the texts in electronic health records (EHR). It is shown in an experimental study how to raise the data possibility to reflect the real medical processes for process modeling tasks. The method is based on the patterns identified in the analysis of medical databases with the physician assistant. EHR is characterized by the gap between common semantic structure and syntactic structure what is important for complex processes modeling. This study aimed at the solution of the problem of knowledge retrieval from EHR by identifying the specifics of their semantic structure and the development of algorithms for interpretation of medical records using the text mining. The medical tests description, surgery protocols, and other medical documents contain many extremely important items for the process analysis. By automating the retrieval of significant data from EHR can be also used for knowledge bases filling. Moreover, the proposed method is developed during the study of actual Russian language medical data of Acute coronary syndrome (ACS) patients from the current specialized medical center which also valuable. The efficiency of this method is demonstrated in the course of correlation analysis of comorbidities on the treatment duration of ACS and in the case of extracted data using to develop process models with complexity metrics at the control-flow perspective of process mining techniques.
ER  - 
TY  - JOUR
T1  - A modular cluster based collaborative recommender system for cardiac patients
A1  - Mustaqeem, Anam
A1  - Anwar, Syed Muhammad
A1  - Majid, Muhammad
Y1  - 2020///
KW  -  Cardiovascular disease
KW  -  Clustering
KW  -  Collaborative filtering
KW  -  Decision support
KW  - Recommender system
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101761
EP  - 101761
DO  - https://doi.org/10.1016/j.artmed.2019.101761
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718305402
N2  - In the last few years, hospitals have been collecting a large amount of health related digital data for patients. This includes clinical test reports, treatment updates and disease diagnosis. The information extracted from this data is used for clinical decisions and treatment recommendations. Among health recommender systems, collaborative filtering technique has gained a significant success. However, traditional collaborative filtering algorithms are facing challenges such as data sparsity and scalability, which leads to a reduction in system accuracy and efficiency. In a clinical setting, the recommendations should be accurate and timely. In this paper, an improvised collaborative filtering technique is proposed, which is based on clustering and sub-clustering. The proposed methodology is applied on a supervised set of data for four different types of cardiovascular diseases including angina, non-cardiac chest pain, silent ischemia, and myocardial infarction. The patient data is partitioned with respect to their corresponding disease class, which is followed by k-mean clustering, applied separately on each disease partition. A query patient once directed to the correct disease partition requires to get similarity scores from a reduced sub-cluster, thereby improving the efficiency of the system. Each disease partition has a separate process for recommendation, which gives rise to modularization and helps in improving scalability of the system. The experimental results demonstrate that the proposed modular clustering based recommender system reduces the spatial search domain for a query patient and the time required for providing accurate recommendations. The proposed system improves upon the accuracy of recommendations as demonstrated by the precision and recall values. This is significant for health recommendation systems particularly for those related to cardiovascular diseases.
ER  - 
TY  - JOUR
T1  - Analysis of high-order SNP barcodes in mitochondrial D-loop for chronic dialysis susceptibility
A1  - Yang, Cheng-Hong
A1  - Lin, Yu-Da
A1  - Chuang, Li-Yeh
A1  - Chang, Hsueh-Wei
Y1  - 2016///
KW  -  Differential evolution
KW  -  SNP barcode
KW  - Single nucleotide polymorphism
JF  - Journal of Biomedical Informatics
VL  - 63
SP  - 112
EP  - 119
DO  - https://doi.org/10.1016/j.jbi.2016.08.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416300867
N2  - Objectives
Positively identifying disease-associated single nucleotide polymorphism (SNP) markers in genome-wide studies entails the complex association analysis of a huge number of SNPs. Such large numbers of SNP barcode (SNP/genotype combinations) continue to pose serious computational challenges, especially for high-dimensional data.
Methods
We propose a novel exploiting SNP barcode method based on differential evolution, termed IDE (improved differential evolution). IDE uses a “top combination strategy” to improve the ability of differential evolution to explore high-order SNP barcodes in high-dimensional data.
Results
We simulate disease data and use real chronic dialysis data to test four global optimization algorithms. In 48 simulated disease models, we show that IDE outperforms existing global optimization algorithms in terms of exploring ability and power to detect the specific SNP/genotype combinations with a maximum difference between cases and controls. In real data, we show that IDE can be used to evaluate the relative effects of each individual SNP on disease susceptibility.
Conclusion
IDE generated significant SNP barcode with less computational complexity than the other algorithms, making IDE ideally suited for analysis of high-order SNP barcodes.
ER  - 
TY  - JOUR
T1  - An innovative neural network framework to classify blood vessels and tubules based on Haralick features evaluated in histological images of kidney biopsy
A1  - Bevilacqua, Vitoantonio
A1  - Pietroleonardo, Nicola
A1  - Triggiani, Vito
A1  - Brunetti, Antonio
A1  - Di Palma, Anna Maria
A1  - Rossini, Michele
A1  - Gesualdo, Loreto
Y1  - 2017///
KW  - Computer Aided Diagnosis
KW  - Haralick features
KW  - Histological Image
KW  - Image processing and segmentation
KW  - Supervised artificial neural network
KW  - Vessels detection
JF  - Neurocomputing
VL  - 228
SP  - 143
EP  - 153
DO  - https://doi.org/10.1016/j.neucom.2016.09.091
UR  - https://www.sciencedirect.com/science/article/pii/S0925231216312796
N1  - Advanced Intelligent Computing: Theory and Applications
N2  - Introduction and objective: Computer Aided Diagnosis (CAD) systems based on Medical Imaging could support physicians in several fields and recently are also applied in histopathology. The aim of this work is to discuss in detail the design and testing of a CAD system for segmentation and discrimination of blood vessels versus tubules from biopsies in the kidney tissue through the elaboration of histological images. Materials and methods: Materials consist in 10 Kidney Biopsy Slides (KBS) with Periodic Acid Schiff (PAS) staining. The Regions Of Interest (ROI) identified by expert are in total 221:71 vessels and 150 tubules. KBS preparation and digital acquisition have been conducted by expert technicians at the Department of Emergency and Organ Transplantation (DETO) of the University of Bari Aldo Moro (Italy). Each slice is a Red Green Blue (RGB) format image with a resolution of 0.50µm/pixel. Starting from KBS images, several techniques were tested for ROI's segmentation and classification. In particular, we formerly describe the innovative preliminary step to segment regions of interest, the procedure to extract significant features from them and finally discuss the supervised Artificial Neural Networks (ANNs) architecture based on error back propagation training algorithm. All the training sets were builts by using vessels and non vessels (tubules) ROI samples, whose dimensions were correlated to the vessels to be detected. Results: The performance of the best ANN architecture, trained by using a training set of 35 vessels among the 71 available vessels in dataset, were evaluated in terms of False Positives (FPs) and False Negatives (FNs). On an initial reduced dataset, it reveals good performance and robustness in terms of FPs reduction. Conclusion: Tests determined that the supervised ANN approach is consistent and reveals good performance, after a training phase based on vessels and non-vessels (tubules) samples. Moreover, our method could be improved by using a larger dataset diagnosed by expert nephropathologists.
ER  - 
TY  - JOUR
T1  - Profiling instances in noise reduction
A1  - Delany, Sarah Jane
A1  - Segata, Nicola
A1  - Mac Namee, Brian
Y1  - 2012///
KW  -  Case-based editing
KW  -  Noise reduction
KW  -  Profiling
KW  -  Visualisation
KW  - Instance based learning
JF  - Knowledge-Based Systems
VL  - 31
SP  - 28
EP  - 40
DO  - https://doi.org/10.1016/j.knosys.2012.01.015
UR  - https://www.sciencedirect.com/science/article/pii/S0950705112000329
N2  - The dependency on the quality of the training data has led to significant work in noise reduction for instance-based learning algorithms. This paper presents an empirical evaluation of current noise reduction techniques, not just from the perspective of their comparative performance, but from the perspective of investigating the types of instances that they focus on for removal. A novel instance profiling technique known as RDCL profiling allows the structure of a training set to be analysed at the instance level categorising each instance based on modelling their local competence properties. This profiling approach offers the opportunity of investigating the types of instances removed by the noise reduction techniques that are currently in use in instance-based learning. The paper also considers the effect of removing instances with specific profiles from a dataset and shows that a very simple approach of removing instances that are misclassified by the training set and cause other instances in the dataset to be misclassified is an effective noise reduction technique.
ER  - 
TY  - JOUR
T1  - Predicting dementia with routine care EMR data
A1  - Ben Miled, Zina
A1  - Haas, Kyle
A1  - Black, Christopher M
A1  - Khandker, Rezaul Karim
A1  - Chandrasekaran, Vasu
A1  - Lipton, Richard
A1  - Boustani, Malaz A
Y1  - 2020///
KW  -  EMR
KW  -  Machine learning
KW  -  Prediction
KW  -  Random forest
KW  - Dementia
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101771
EP  - 101771
DO  - https://doi.org/10.1016/j.artmed.2019.101771
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718306481
N2  - Our aim is to develop a machine learning (ML) model that can predict dementia in a general patient population from multiple health care institutions one year and three years prior to the onset of the disease without any additional monitoring or screening. The purpose of the model is to automate the cost-effective, non-invasive, digital pre-screening of patients at risk for dementia. Towards this purpose, routine care data, which is widely available through Electronic Medical Record (EMR) systems is used as a data source. These data embody a rich knowledge and make related medical applications easy to deploy at scale in a cost-effective manner. Specifically, the model is trained by using structured and unstructured data from three EMR data sets: diagnosis, prescriptions, and medical notes. Each of these three data sets is used to construct an individual model along with a combined model which is derived by using all three data sets. Human-interpretable data processing and ML techniques are selected in order to facilitate adoption of the proposed model by health care providers from multiple institutions. The results show that the combined model is generalizable across multiple institutions and is able to predict dementia within one year of its onset with an accuracy of nearly 80% despite the fact that it was trained using routine care data. Moreover, the analysis of the models identified important predictors for dementia. Some of these predictors (e.g., age and hypertensive disorders) are already confirmed by the literature while others, especially the ones derived from the unstructured medical notes, require further clinical analysis.
ER  - 
TY  - JOUR
T1  - Hyper-parameter optimization for support vector machines using stochastic gradient descent and dual coordinate descent
A1  - Jiang, W.e.i.
A1  - Siddiqui, Sauleh
Y1  - 2020///
KW  -  Gradient-based methods
KW  -  Support vector machines
KW  - Hyper-parameter
JF  - EURO Journal on Computational Optimization
VL  - 8
IS  - 1
SP  - 85
EP  - 101
DO  - https://doi.org/10.1007/s13675-019-00115-7
UR  - https://www.sciencedirect.com/science/article/pii/S2192440621000083
N2  - We developed a gradient-based method to optimize the regularization hyper-parameter, C, for support vector machines in a bilevel optimization framework. On the upper level, we optimized the hyper-parameter C to minimize the prediction loss on validation data using stochastic gradient descent. On the lower level, we used dual coordinate descent to optimize the parameters of support vector machines to minimize the loss on training data. The gradient of the loss function on the upper level with respect to the hyper-parameter, C, was computed using the implicit function theorem combined with the optimality condition of the lower-level problem, i.e., the dual problem of support vector machines. We compared our method with the existing gradient-based method in the literature on several datasets. Numerical results showed that our method converges faster to the optimal solution and achieves better prediction accuracy for large-scale support vector machine problems.
ER  - 
TY  - JOUR
T1  - The Catch data warehouse: support for community health care decision-making
A1  - Berndt, Donald J
A1  - Hevner, Alan R
A1  - Studnicki, James
Y1  - 2003///
KW  -  Community decision-making
KW  -  Data quality
KW  -  Data staging
KW  -  Data warehousing
KW  -  Decision support systems
KW  -  Online analytic processing (OLAP)
KW  - Health care information systems
JF  - Decision Support Systems
VL  - 35
IS  - 3
SP  - 367
EP  - 384
DO  - https://doi.org/10.1016/S0167-9236(02)00114-8
UR  - https://www.sciencedirect.com/science/article/pii/S0167923602001148
N2  - The measurement and assessment of health status in communities throughout the world is a massive information technology challenge. Comprehensive Assessment for Tracking Community Health (CATCH) provides systematic methods for community-level assessment that is invaluable for resource allocation and health care policy formulation. CATCH is based on health status indicators from multiple data sources, using an innovative comparative framework and weighted evaluation process to produce a rank-ordered list of critical community health care challenges. The community-level focus is intended to empower local decision-makers by providing a clear methodology for organizing and interpreting relevant health care data. Extensive field experience with the CATCH methods, in combination with expertise in data warehousing technology, has led to an innovative application of information technology in the health care arena. The data warehouse allows a core set of reports to be produced at a reasonable cost for community use. In addition, online analytic processing (OLAP) functionality can be used to gain a deeper understanding of specific health care issues. The data warehouse in conjunction with Web-enabled dissemination methods allows the information to be presented in a variety of formats and to be distributed more widely in the decision-making community. In this paper, we focus on the technical challenges of designing and implementing an effective data warehouse for health care information. Illustrations of actual data designs and reporting formats from the CATCH data warehouse are used throughout the discussion. Ongoing research directions in health care data warehousing and community health care decision-making conclude the paper.
ER  - 
TY  - JOUR
T1  - Pediatric population health analysis of southern and central Illinois region: A cross sectional retrospective study using association rule mining and multiple logistic regression
A1  - Buxton, Elham Khorasani
A1  - Vohra, Sameer
A1  - Guo, Yanhui
A1  - Fogleman, Amanda
A1  - Patel, Rushabh
Y1  - 2019///
KW  -  Data mining
KW  -  Medical billing database
KW  -  Pediatric diseases risk factors
KW  - Population health analysis
JF  - Computer Methods and Programs in Biomedicine
VL  - 178
SP  - 145
EP  - 153
DO  - https://doi.org/10.1016/j.cmpb.2019.06.020
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718312914
N2  - Background
Southern Illinois University School of Medicine (SIUSOM) collects large amounts of data every day. SIUSOM and other similar healthcare systems are always looking for better ways to use the data to understand and address population level problems. The purpose of this study is to analyze the administrative dataset for pediatric patients served by Southern Illinois University School of Medicine (SIUSOM) to uncover patterns that correlate specific demographic information to diagnoses of pediatric diseases. The study uses a cross-sectional database of medical billing information for all pediatric patients served by SIUSOM between June 2013 and December 2016. The dataset consists of about 980.9K clinical visits for 65.4K unique patients and includes patient demographic identifiers such as their sex, date of birth, race, anonymous zipcode and primary and secondary insurance plan as well as the related pediatric diagnosis codes. The goal is to find unknown correlations in this database.
Method
We proposed a two step methodology to derive unknown correlations in SIUSOM administrative database. First, Class association rule mining was used as a well-established data mining method to generate hypothesis and derive associations of the form D → M, where D is diagnosis code of a pediatric disease and M is a patient demographic identifier (age,sex, anonymous zipcode, insurance plan, or race). The resulting associations were pruned and filtered using measures such as lift, odds ratio, relative risk, and confidence. The final associations were selected by a pediatric doctor based on their clinical significance. Second,each association rule in the final set was further validated and adjusted odds ratios were obtained using multiple logistic regression.
Results
Several associations were found correlating specific patients’ residential zip codes with the diagnosis codes for viral hepatitis carrier, exposure to communicable diseases, screening for mental and developmental disorder in childhood, history allergy to medications, disturbance of emotions specific to childhood, and acute sinusitis. In addition, the results show that African American patients are more likely to be screened for mental and developmental disorders compared to White patients for SIUSOM pediatric population (Odds Ratio (OR):3.56, 95% Confidence Interval (CI):[3.29,3.85]).
Conclusion
Class association rule mining is an effective method for detecting signals in a large patient administrative database and generating hypotheses which correlate patients’ demographics with diagnosis of pediatric diseases. A post processing of the hypotheses generated by this method is necessary to prune spurious associations and select a set of clinically relevant hypotheses.
ER  - 
TY  - JOUR
T1  - Artificial intelligence techniques applied to the development of a decision–support system for diagnosing celiac disease
A1  - Tenório, Josceli Maria
A1  - Hummel, Anderson Diniz
A1  - Cohrs, Frederico Molina
A1  - Sdepanian, Vera Lucia
A1  - Pisa, Ivan Torres
A1  - de Fátima Marin, Heimar
Y1  - 2011///
KW  -  Artificial intelligence
KW  -  Celiac disease
KW  -  clinical
KW  - Decision support systems
JF  - International Journal of Medical Informatics
VL  - 80
IS  - 11
SP  - 793
EP  - 802
DO  - https://doi.org/10.1016/j.ijmedinf.2011.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S1386505611001651
N2  - Background
Celiac disease (CD) is a difficult-to-diagnose condition because of its multiple clinical presentations and symptoms shared with other diseases. Gold-standard diagnostic confirmation of suspected CD is achieved by biopsying the small intestine.
Objective
To develop a clinical decision–support system (CDSS) integrated with an automated classifier to recognize CD cases, by selecting from experimental models developed using intelligence artificial techniques.
Methods
A web-based system was designed for constructing a retrospective database that included 178 clinical cases for training. Tests were run on 270 automated classifiers available in Weka 3.6.1 using five artificial intelligence techniques, namely decision trees, Bayesian inference, k-nearest neighbor algorithm, support vector machines and artificial neural networks. The parameters evaluated were accuracy, sensitivity, specificity and area under the ROC curve (AUC). AUC was used as a criterion for selecting the CDSS algorithm. A testing database was constructed including 38 clinical CD cases for CDSS evaluation. The diagnoses suggested by CDSS were compared with those made by physicians during patient consultations.
Results
The most accurate method during the training phase was the averaged one-dependence estimator (AODE) algorithm (a Bayesian classifier), which showed accuracy 80.0%, sensitivity 0.78, specificity 0.80 and AUC 0.84. This classifier was integrated into the web-based decision–support system. The gold-standard validation of CDSS achieved accuracy of 84.2% and k=0.68 (p<0.0001) with good agreement. The same accuracy was achieved in the comparison between the physician's diagnostic impression and the gold standard k=0. 64 (p<0.0001). There was moderate agreement between the physician's diagnostic impression and CDSS k=0.46 (p=0.0008).
Conclusions
The study results suggest that CDSS could be used to help in diagnosing CD, since the algorithm tested achieved excellent accuracy in differentiating possible positive from negative CD diagnoses. This study may contribute towards developing of a computer-assisted environment to support CD diagnosis.
ER  - 
TY  - JOUR
T1  - An evolutionary multi-objective approach to learn from positive and unlabeled data
A1  - Qiu, Jianfeng
A1  - Cai, Xiaoqiang
A1  - Zhang, Xingyi
A1  - Cheng, Fan
A1  - Yuan, Shenzhi
A1  - Fu, Guanglong
Y1  - 2021///
KW  -  Elite label
KW  -  Evolutionary algorithm
KW  -  Multi-objective problem
KW  -  PU similarity
KW  - PU learning
JF  - Applied Soft Computing
VL  - 101
SP  - 106986
EP  - 106986
DO  - https://doi.org/10.1016/j.asoc.2020.106986
UR  - https://www.sciencedirect.com/science/article/pii/S156849462030925X
N2  - Positive and unlabeled (PU) learning has attracted increasing interests in recent years. Despite that a number of PU learning algorithms have been proposed, most of them are subject to some assumptions about unlabeled sample distribution and objective functions, which makes them difficult to be adopted for real applications. To this end, in this paper, an evolutionary multi-objective approach, namely MOEA-PUL, is suggested for PU learning, whose aim is to build a PU classifier without any prior assumption for data distribution and objective functions. To be specific, the PU learning is formulated as a bi-objective optimization problem, where the true positive rate (TPR) and a new metric, termed unlabeled accuracy rate (UAR) are used as the two objectives. A multi-objective evolutionary algorithm is proposed to solve this bi-objective optimization problem, under the framework of NSGA-II, where a PU similarity based initialization strategy and an elite label based learning strategy are developed. Empirical studies on 12 datasets demonstrate the superiority of MOEA-PUL over the existing PU learning algorithms.
ER  - 
TY  - JOUR
T1  - An interactive human centered data science approach towards crime pattern analysis
A1  - Qazi, Nadeem
A1  - Wong, B L William
Y1  - 2019///
KW  -  Crime matching
KW  -  Data science
KW  -  Human-centred searching
KW  -  Knowledge graph
KW  -  Linkage analysis
KW  -  Network visualization
KW  -  Text mining
KW  - Interactive clustering
JF  - Information Processing & Management
VL  - 56
IS  - 6
SP  - 102066
EP  - 102066
DO  - https://doi.org/10.1016/j.ipm.2019.102066
UR  - https://www.sciencedirect.com/science/article/pii/S0306457318302942
N2  - The traditional machine learning systems lack a pathway for a human to integrate their domain knowledge into the underlying machine learning algorithms. The utilization of such systems, for domains where decisions can have serious consequences (e.g. medical decision-making and crime analysis), requires the incorporation of human experts' domain knowledge. The challenge, however, is how to effectively incorporate domain expert knowledge with machine learning algorithms to develop effective models for better decision making. In crime analysis, the key challenge is to identify plausible linkages in unstructured crime reports for the hypothesis formulation. Crime analysts painstakingly perform time-consuming searches of many different structured and unstructured databases to collate these associations without any proper visualization. To tackle these challenges and aiming towards facilitating the crime analysis, in this paper, we examine unstructured crime reports through text mining to extract plausible associations. Specifically, we present associative questioning based searching model to elicit multi-level associations among crime entities. We coupled this model with partition clustering to develop an interactive, human-assisted knowledge discovery and data mining scheme. The proposed human-centered knowledge discovery and data mining scheme for crime text mining is able to extract plausible associations between crimes, identifying crime pattern, grouping similar crimes, eliciting co-offender network and suspect list based on spatial-temporal and behavioral similarity. These similarities are quantified through calculating Cosine, Jacquard, and Euclidean distances. Additionally, each suspect is also ranked by a similarity score in the plausible suspect list. These associations are then visualized through creating a two-dimensional re-configurable crime cluster space along with a bipartite knowledge graph. This proposed scheme also inspects the grand challenge of integrating effective human interaction with the machine learning algorithms through a visualization feedback loop. It allows the analyst to feed his/her domain knowledge including choosing of similarity functions for identifying associations, dynamic feature selection for interactive clustering of crimes and assigning weights to each component of the crime pattern to rank suspects for an unsolved crime. We demonstrate the proposed scheme through a case study using the Anonymized burglary dataset. The scheme is found to facilitate human reasoning and analytic discourse for intelligence analysis.
ER  - 
TY  - JOUR
T1  - Learner excellence biased by data set selection: A case for data characterisation and artificial data sets
A1  - Macià, Núria
A1  - Bernadó-Mansilla, Ester
A1  - Orriols-Puig, Albert
A1  - Kam Ho, Tin
Y1  - 2013///
KW  -  Data complexity
KW  -  Learner assessment
KW  - Supervised learning
JF  - Pattern Recognition
VL  - 46
IS  - 3
SP  - 1054
EP  - 1066
DO  - https://doi.org/10.1016/j.patcog.2012.09.022
UR  - https://www.sciencedirect.com/science/article/pii/S0031320312004281
N2  - The excellence of a given learner is usually claimed through a performance comparison with other learners over a collection of data sets. Too often, researchers are not aware of the impact of their data selection on the results. Their test beds are small, and the selection of the data sets is not supported by any previous data analysis. Conclusions drawn on such test beds cannot be generalised, because particular data characteristics may favour certain learners unnoticeably. This work raises these issues and proposes the characterisation of data sets using complexity measures, which can be helpful for both guiding experimental design and explaining the behaviour of learners.
ER  - 
TY  - JOUR
T1  - A survey on feature selection methods
A1  - Chandrashekar, Girish
A1  - Sahin, Ferat
Y1  - 2014///
JF  - Computers & Electrical Engineering
VL  - 40
IS  - 1
SP  - 16
EP  - 28
DO  - https://doi.org/10.1016/j.compeleceng.2013.11.024
UR  - https://www.sciencedirect.com/science/article/pii/S0045790613003066
N1  - 40th-year commemorative issue
N2  - Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.
ER  - 
TY  - JOUR
T1  - Learning representative exemplars using one-class Gaussian process regression
A1  - Son, Youngdoo
A1  - Lee, Sujee
A1  - Park, Saerom
A1  - Lee, Jaewook
Y1  - 2018///
KW  -  Automatic relevance determination
KW  -  Kernel methods
KW  -  One class Gaussian process regression
KW  -  Support-based clustering
KW  - Representative exemplars
JF  - Pattern Recognition
VL  - 74
SP  - 185
EP  - 197
DO  - https://doi.org/10.1016/j.patcog.2017.09.002
UR  - https://www.sciencedirect.com/science/article/pii/S0031320317303461
N2  - An exemplar is an observation that represents a group of similar observations. Exemplars from data are examined to divide entire heterogeneous data into several homogeneous subgroups, wherein each subgroup is represented by an exemplar. With its inherent sparsity, an exemplar-based learning model provides a parsimonious model to represent or cluster large-scale data. A novel exemplar learning method using one-class Gaussian process (GP) regression is proposed in this study. The proposed method constructs data distribution support from one-class GP regression using automatic relevance determination prior and heterogeneous GP noise. Exemplars that correspond to the basis vectors of the constructed support function are then automatically located during the training process. The proposed method is applied to various data sets to examine its operability, characteristics of data representation, and cluster analysis. The exemplars of some real data generated by the proposed method are also reported.
ER  - 
TY  - JOUR
T1  - Absolute approximation of Tukey depth: Theory and experiments
A1  - Chen, Dan
A1  - Morin, Pat
A1  - Wagner, Uli
Y1  - 2013///
KW  -  Computational geometry
KW  - Tukey depth
JF  - Computational Geometry
VL  - 46
IS  - 5
SP  - 566
EP  - 573
DO  - https://doi.org/10.1016/j.comgeo.2012.03.001
UR  - https://www.sciencedirect.com/science/article/pii/S0925772112000594
N1  - Geometry and Optimization
N2  - A Monte Carlo approximation algorithm for the Tukey depth problem in high dimensions is introduced. The algorithm is a generalization of an algorithm presented by Rousseeuw and Struyf (1998) [20]. The performance of this algorithm is studied both analytically and experimentally.
ER  - 
TY  - JOUR
T1  - Bin loss for hard exudates segmentation in fundus images
A1  - Guo, Song
A1  - Wang, Kai
A1  - Kang, Hong
A1  - Liu, Teng
A1  - Gao, Yingqi
A1  - Li, Tao
Y1  - 2020///
KW  -  Bin loss
KW  -  Deep learning
KW  -  Diabetic retinopathy
KW  -  Fundus image
KW  - Hard exudates segmentation
JF  - Neurocomputing
VL  - 392
SP  - 314
EP  - 324
DO  - https://doi.org/10.1016/j.neucom.2018.10.103
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219304783
N2  - Diabetic retinopathy is one of the leading reasons that causes blindness. And the segmentation of hard exudates in color fundus images is crucial for early diagnosis of diabetic retinopathy, which is a difficult task due to its uncertainty in size, shape and contrast. Class-balanced cross entropy (CBCE) loss is the most popular objective function for image segmentation task to solve the class-unbalance problem. However, we show that background pixels tend to be misclassified to hard exudates in CBCE since the loss for a misclassified background pixels is much smaller than that for a misclassified hard exudate pixel, which is called loss-unbalance problem here. A top-k loss is proposed in this paper, which considers the cases of both class-unbalance and loss-unbalance by focusing more over the hard-to-classify pixels. Moreover, a fast version of the top-k loss, named bin loss, is implemented for efficiency, which reduces the time complexity from O(nlog n) of top-k loss to O(n), where n is the number of background pixels. We evaluated the proposed bin loss over two public datasets for hard exudates segmentation task, including e-ophtha EX and IDRiD. Furthermore, three popular models for image segmentation, HED, DeepLab v2, and FCRN, were used to evaluate the versatility of bin loss. Extensive experiments show that each model with the proposed bin loss performs better than that with CBCE loss, which demonstrates bin loss is versatile so that it can be applied to different models for performance improvement. Specially, for DeepLab over e-ophtha EX, the F-score increases 5.2 percentage points, and the area under the SE-PPV curve (AUC) increases 10.6 percentage points. Moreover, the AUC increases more than 4 percentage points over IDRiD dataset for both DeepLab and FCRN. The source code of bin loss is available at: https://github.com/guomugong/bin_loss.
ER  - 
TY  - JOUR
T1  - Extract critical factors affecting the length of hospital stay of pneumonia patient by data mining (case study: an Iranian hospital)
A1  - Khajehali, Naghmeh
A1  - Alizadeh, Somayeh
Y1  - 2017///
KW  -  Ensemble methods
KW  -  Medical data mining
KW  -  Patients
KW  -  Pneumonia
KW  - Length of stay (LOS)
JF  - Artificial Intelligence in Medicine
VL  - 83
SP  - 2
EP  - 13
DO  - https://doi.org/10.1016/j.artmed.2017.06.010
UR  - https://www.sciencedirect.com/science/article/pii/S0933365717300350
N1  - Machine Learning and Graph Analytics in Computational Biomedicine
N2  - Motivation
Pneumonia is a prevalent infection of lower respiratory tract caused by infected lungs. Length of stay (LOS) in hospital is one of the simplest and most important indicators in hospital activity that is used for different purposes. The aim of this study is to explore the important factors affecting the LOS of patients with pneumonia in hospitals.
Methods
The clinical data set for the study were collected from 387 patients in a specialized hospital in Iran between 2009 and 2015. Patients discharge summary includes their demographic details, reasons for admission, prescribed medications for the patient, the result of laboratory tests, and length of treatment.
Results and conclusions
The proposed model in the study demonstrates the way various scenarios of data processing impact on the scale efficiency model, which points to the significance of the pre-processing in data mining. In this article, some methods were utilized; it is noteworthy that Bayesian boosting method led to better results in identifying the factors affecting LOS (accuracy 95.17%). In addition, it was found that 58% of patients younger than 15 years old and 74% of the elderly within the age range of 74–88 were more vulnerable to pneumonia disease. Also, it was found that the Meropenem is a relatively more effective medicine compared to other antibiotics which are used to treat pneumonia in the majority of age groups. Regardless of the impact of various laboratory findings (including CRP, ESR, WBC, NA, K), the patients LOS decreased as a result of Meropenem.
ER  - 
TY  - JOUR
T1  - Quantitative and temporal approach to utilising electronic medical records from general practices in mental health prediction
A1  - Półchłopek, Olga
A1  - Koning, Nynke R
A1  - Büchner, Frederike L
A1  - Crone, Mathilde R
A1  - Numans, Mattijs E
A1  - Hoogendoorn, Mark
Y1  - 2020///
KW  -  General practice
KW  -  Mental health classification
KW  -  Pattern recognition
KW  -  Temporal pattern mining
KW  - Electronic medical records
JF  - Computers in Biology and Medicine
VL  - 125
SP  - 103973
EP  - 103973
DO  - https://doi.org/10.1016/j.compbiomed.2020.103973
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520303048
N2  - This study proposes a framework for mining temporal patterns from Electronic Medical Records. A new scoring scheme based on the Wilson interval is provided to obtain frequent and predictive patterns, as well as to accelerate the mining process by reducing the number of patterns mined. This is combined with a case study using data from general practices in the Netherlands to identify children at risk of suffering from mental disorders. To develop an accurate model, feature engineering methods such as one hot encoding and frequency transformation are proposed, and the pattern selection is tailored to this type of clinical data. Six machine learning models are trained on five age groups, with XGBoost achieving the highest AUC values (0.75–0.79) with sensitivity and specificity above 0.7 and 0.6 respectively. An improvement is demonstrated by the models learning from patterns in addition to non-temporal features.
ER  - 
TY  - JOUR
T1  - Ellipsoidal data description
A1  - Wang, Kunzhe
A1  - Xiao, Huaitie
Y1  - 2017///
KW  -  Kernel methods
KW  -  Mahalanobis distance
KW  -  Minimum volume enclosing ellipsoid
KW  -  Rademacher complexity
KW  -  Support vector machines
KW  - Novelty detection
JF  - Neurocomputing
VL  - 238
SP  - 328
EP  - 339
DO  - https://doi.org/10.1016/j.neucom.2017.01.070
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217301911
N2  - Support vector data description (SVDD) is a leading classification method for novelty detection, which minimizes the volume of a spherically shaped decision boundary around the normal class. While SVDD has achieved promising performance, it will lead to a loose boundary for multivariate datasets of which the input dimensions are usually correlated. Inspired by the relationship between kernel principal component analysis (kernel PCA) and the best-fit ellipsoid for a dataset, this study proposes the ellipsoidal data description (ELPDD) which considers feature variance of each dimension adaptively. A minimum volume enclosing ellipsoid (MVEE) is constructed around the target data in the kernel PCA subspace which can be learned via a SVM-like objective function with log-determinant penalty. We also provide the Rademacher complexity bound for our model. Some relating problems are investigated in detail. Experiments on artificial and real-world datasets validate the effectiveness of our method.
ER  - 
TY  - JOUR
T1  - Optimization-based k-anonymity algorithms
A1  - Liang, Yuting
A1  - Samavi, Reza
Y1  - 2020///
KW  -  Mixed Integer Linear Program
KW  -  Optimization
KW  -  Privacy
KW  -  Security
KW  - Anonymization
JF  - Computers & Security
VL  - 93
SP  - 101753
EP  - 101753
DO  - https://doi.org/10.1016/j.cose.2020.101753
UR  - https://www.sciencedirect.com/science/article/pii/S0167404820300377
N2  - In this paper we present a formulation of k-anonymity as a mathematical optimization problem. In solving this formulated problem, k-anonymity is achieved while maximizing the utility of the resulting dataset. Our formulation has the advantage of incorporating different weights for attributes in order to achieve customized utility to suit different research purposes. The resulting formulation is a Mixed Integer Linear Program (MILP), which is NP-complete in general. Recognizing the complexity of the problem, we propose two practical algorithms which can provide near-optimal utility. Our experimental evaluation confirms that our algorithms are scalable when used for datasets containing large numbers of records.
ER  - 
TY  - JOUR
T1  - A new wrapper feature selection approach using neural network
A1  - Monirul Kabir, Md.
A1  - Monirul Islam, Md.
A1  - Murase, Kazuyuki
Y1  - 2010///
KW  -  Correlation information
KW  -  Neural networks
KW  -  Wrapper approach
KW  - Feature selection
JF  - Neurocomputing
VL  - 73
IS  - 16
SP  - 3273
EP  - 3283
DO  - https://doi.org/10.1016/j.neucom.2010.04.003
UR  - https://www.sciencedirect.com/science/article/pii/S0925231210001979
N1  - 10th Brazilian Symposium on Neural Networks (SBRN2008)
N2  - This paper presents a new feature selection (FS) algorithm based on the wrapper approach using neural networks (NNs). The vital aspect of this algorithm is the automatic determination of NN architectures during the FS process. Our algorithm uses a constructive approach involving correlation information in selecting features and determining NN architectures. We call this algorithm as constructive approach for FS (CAFS). The aim of using correlation information in CAFS is to encourage the search strategy for selecting less correlated (distinct) features if they enhance accuracy of NNs. Such an encouragement will reduce redundancy of information resulting in compact NN architectures. We evaluate the performance of CAFS on eight benchmark classification problems. The experimental results show the essence of CAFS in selecting features with compact NN architectures.
ER  - 
TY  - JOUR
T1  - Automatic classification of RDoC positive valence severity with a neural network
A1  - Clark, Cheryl
A1  - Wellner, Ben
A1  - Davis, Rachel
A1  - Aberdeen, John
A1  - Hirschman, Lynette
Y1  - 2017///
KW  -  Machine learning
KW  -  Mental disorder severity
KW  -  Research domain criteria (RDoC)
KW  -  Text classification
KW  - Positive valance
JF  - Journal of Biomedical Informatics
VL  - 75
SP  - S120
EP  - S128
DO  - https://doi.org/10.1016/j.jbi.2017.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417301612
N1  - Supplement: A Natural Language Processing Challenge for Clinical Records: Research Domains Criteria (RDoC) for Psychiatry
N2  - Objective
Our objective was to develop a machine learning-based system to determine the severity of Positive Valance symptoms for a patient, based on information included in their initial psychiatric evaluation. Severity was rated on an ordinal scale of 0–3 as follows: 0 (absent=no symptoms), 1 (mild=modest significance), 2 (moderate=requires treatment), 3 (severe=causes substantial impairment) by experts.
Materials and methods
We treated the task of assigning Positive Valence severity as a text classification problem. During development, we experimented with regularized multinomial logistic regression classifiers, gradient boosted trees, and feedforward, fully-connected neural networks. We found both regularization and feature selection via mutual information to be very important in preventing models from overfitting the data. Our best configuration was a neural network with three fully connected hidden layers with rectified linear unit activations.
Results
Our best performing system achieved a score of 77.86%. The evaluation metric is an inverse normalization of the Mean Absolute Error presented as a percentage number between 0 and 100, where 100 means the highest performance. Error analysis showed that 90% of the system errors involved neighboring severity categories.
Conclusion
Machine learning text classification techniques with feature selection can be trained to recognize broad differences in Positive Valence symptom severity with a modest amount of training data (in this case 600 documents, 167 of which were unannotated). An increase in the amount of annotated data can increase accuracy of symptom severity classification by several percentage points. Additional features and/or a larger training corpus may further improve accuracy.
ER  - 
TY  - JOUR
T1  - HCLS 2.0/3.0: Health care and life sciences data mashup using Web 2.0/3.0
A1  - Cheung, Kei-Hoi
A1  - Yip, Kevin Y
A1  - Townsend, Jeffrey P
A1  - Scotch, Matthew
Y1  - 2008///
KW  -  Bioinformatics
KW  -  Biomedical informatics
KW  -  Health care
KW  -  Integration
KW  -  Life sciences
KW  -  Mashup
KW  -  Public health
KW  -  Semantic Web
KW  - Web 2.0
JF  - Journal of Biomedical Informatics
VL  - 41
IS  - 5
SP  - 694
EP  - 705
DO  - https://doi.org/10.1016/j.jbi.2008.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046408000518
N1  - Semantic Mashup of Biomedical Data
N2  - We describe the potential of current Web 2.0 technologies to achieve data mashup in the health care and life sciences (HCLS) domains, and compare that potential to the nascent trend of performing semantic mashup. After providing an overview of Web 2.0, we demonstrate two scenarios of data mashup, facilitated by the following Web 2.0 tools and sites: Yahoo! Pipes, Dapper, Google Maps and GeoCommons. In the first scenario, we exploited Dapper and Yahoo! Pipes to implement a challenging data integration task in the context of DNA microarray research. In the second scenario, we exploited Yahoo! Pipes, Google Maps, and GeoCommons to create a geographic information system (GIS) interface that allows visualization and integration of diverse categories of public health data, including cancer incidence and pollution prevalence data. Based on these two scenarios, we discuss the strengths and weaknesses of these Web 2.0 mashup technologies. We then describe Semantic Web, the mainstream Web 3.0 technology that enables more powerful data integration over the Web. We discuss the areas of intersection of Web 2.0 and Semantic Web, and describe the potential benefits that can be brought to HCLS research by combining these two sets of technologies.
ER  - 
TY  - JOUR
T1  - Hybridization strategies for continuous ant colony optimization and particle swarm optimization applied to data clustering
A1  - Huang, Cheng-Lung
A1  - Huang, Wen-Chen
A1  - Chang, Hung-Yi
A1  - Yeh, Yi-Chun
A1  - Tsai, Cheng-Yi
Y1  - 2013///
KW  -  Ant colony optimization
KW  -  Data clustering
KW  -  Hybrid systems
KW  -  Particle swarm optimization
KW  - Swarm intelligence
JF  - Applied Soft Computing
VL  - 13
IS  - 9
SP  - 3864
EP  - 3872
DO  - https://doi.org/10.1016/j.asoc.2013.05.003
UR  - https://www.sciencedirect.com/science/article/pii/S1568494613001580
N2  - Ant colony optimization (ACO) and particle swarm optimization (PSO) are two popular algorithms in swarm intelligence. Recently, a continuous ACO named ACOR was developed to solve the continuous optimization problems. This study incorporated ACOR with PSO to improve the search ability, investigating four types of hybridization as follows: (1) sequence approach, (2) parallel approach, (3) sequence approach with an enlarged pheromone-particle table, and (4) global best exchange. These hybrid systems were applied to data clustering. The experimental results utilizing public UCI datasets show that the performances of the proposed hybrid systems are superior compared to those of the K-mean, standalone PSO, and standalone ACOR. Among the four strategies of hybridization, the sequence approach with the enlarged pheromone table is superior to the other approaches because the enlarged pheromone table diversifies the generation of new solutions of ACOR and PSO, which prevents traps into the local optimum.
ER  - 
TY  - JOUR
T1  - Modified Grasshopper Optimization Algorithm for detection of Autism Spectrum Disorder
A1  - Goel, Nikita
A1  - Grover, Bhavya
A1  - Anuj
A1  - Gupta, Deepak
A1  - Khanna, Ashish
A1  - Sharma, Moolchand
Y1  - 2020///
KW  -  Grasshopper Optimization Algorithm
KW  -  Machine learning
KW  -  Random Forest
KW  - Autism Spectrum Disorders
JF  - Physical Communication
VL  - 41
SP  - 101115
EP  - 101115
DO  - https://doi.org/10.1016/j.phycom.2020.101115
UR  - https://www.sciencedirect.com/science/article/pii/S1874490720301920
N2  - Autism Spectrum Disorder (ASD) is a disorder of neurodevelopment whose delayed diagnosis has been posing a barrier in alleviating the severity of the conditions of the sufferers. ASD patients experience difficulties with social communication and interaction. They also show restricted and repetitive patterns of behavior. Approximately 62 million people are diagnosed with ASD globally. Males are about 3–4 times more likely to suffer from ASD than females. Statistically, ASD can be detected between the age of one to two, but some cases may remain undetected for a substantial period. It is crucial to detect ASD precisely and at the nascent stage to remediate the disease. The presented paper proposes an algorithm, namely Modified Grasshopper Optimization Algorithm (MGOA), capable of detecting Autism Spectrum Disorder at all stages of life. GOA is a nature-inspired algorithm that has the potential to explore and exploit the search space effectively. Through this paper, we have attempted to overcome the shortcomings of the traditional GOA, resulting in early diagnosis of the disease. The algorithm is used on the three ASD screening datasets targeting different age groups, namely children, adolescents, and adults, are used for numerical experimentation, and the results are contrasted with the state-of-the-art algorithms. The proposed algorithm with the Random Forest classifier predicted ASD with an approximate accuracy of 100% with specificity and sensitivity as 100% at all stages of life.
ER  - 
TY  - JOUR
T1  - Inferring disease causing genes and their pathways: GpRr method
A1  - Devasia, Jeethu V
A1  - Chandran, Priya
Y1  - 2018///
KW  - Approximation algorithm
KW  - Causal genes
KW  - Dysregulated pathway
KW  - Graph pruning
KW  - Molecular interaction network
KW  - Randomized rounding
JF  - Journal of Computational Science
VL  - 26
SP  - 108
EP  - 117
DO  - https://doi.org/10.1016/j.jocs.2018.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S1877750317309821
N2  - Background and objective
A system level view of cellular processes for human and several organisms can be captured by analyzing molecular interaction networks. A molecular interaction network formed of differentially expressed genes and their interactions helps to understand key players behind disease development. If the functions of these genes could be blocked by altering their interactions, it would be a significant step towards controlling the disease. Due to these, promising solutions to the problem of inferring disease causing genes and their pathways has achieved a crucial research significance in computational biology. However, considering the huge size of interaction networks, executing computations for making inferences is costly. Review of literature shows that the methods proposed for finding the set of disease causing genes could be assessed in terms of their accuracy, expressed as a fraction of the size of the set of genes a perfect algorithm would find. Along with accuracy, the time complexity of the method is also important, as high time complexities would limit the number of pathways that could be found within a pragmatic time interval.
Methods and results
Aiming to get more accurate results in lesser execution time, we propose an algorithm that augments graph theoretical approaches with approximation for inferring causal genes and dysregulated pathways. We demonstrated more accurate and computationally time efficient results than existing algorithms based on experimentation on multiple benchmark datasets. We also analyzed the biological relevance of these results.
Conclusions
Based on the computational approaches on biological data, we identified genes whose involvement in the disease states is not yet proven and corresponding pathways for multiple disease cases. The proposed method would have a remarkable contribution in areas like drug development and gene therapy, once these research results are corroborated with biological experiments.
ER  - 
TY  - JOUR
T1  - Missing value imputation affects the performance of machine learning: A review and analysis of the literature (2010–2021)
A1  - Hasan, Md. Kamrul
A1  - Alam, Md. Ashraful
A1  - Roy, Shidhartho
A1  - Dutta, Aishwariya
A1  - Jawad, Md. Tasnim
A1  - Das, Sunanda
Y1  - 2021///
KW  -  Imputation methods and evaluations
KW  -  Machine learning classifiers and evaluations
KW  -  PRISMA technique
KW  - Incomplete datasets
JF  - Informatics in Medicine Unlocked
VL  - 27
SP  - 100799
EP  - 100799
DO  - https://doi.org/10.1016/j.imu.2021.100799
UR  - https://www.sciencedirect.com/science/article/pii/S2352914821002653
N2  - Recently, numerous studies have been conducted on Missing Value Imputation (MVI), intending the primary solution scheme for the datasets containing one or more missing attribute’s values. The incorporation of MVI reinforces the Machine Learning (ML) models’ performance and necessitates a systematic review of MVI methodologies employed for different tasks and datasets. It will aid beginners as guidance towards composing an effective ML-based decision-making system in various fields of applications. This article aims to conduct a rigorous review and analysis of the state-of-the-art MVI methods in the literature published in the last decade. Altogether, 191 articles, published from 2010 to August 2021, are selected for review using the well-known Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique. We summarize those articles with relevant definitions, theories, and analyses to provide essential information for building a precise decision-making framework. In addition, the evaluation metrics employed for MVI methods and ML-based classification models are also discussed and explored. Remarkably, the trends for the MVI method and its evaluation are also scrutinized from the last twelve years’ data. To come up with the conclusion, several ML-based pipelines, where the MVI schemes are incorporated for performance enhancement, are investigated and reviewed for many different datasets. In the end, informative observations and recommendations are addressed for future research directions and trends in related fields of interest.
ER  - 
TY  - JOUR
T1  - Extraction and optimization of classification rules for temporal sequences: Application to hospital data
A1  - Vandromme, M
A1  - Jacques, J
A1  - Taillard, J
A1  - Hansske, A
A1  - Jourdan, L
A1  - Dhaenens, C
Y1  - 2017///
KW  -  Classification
KW  -  Optimization
KW  -  Temporal data
KW  - Data mining
JF  - Knowledge-Based Systems
VL  - 122
SP  - 148
EP  - 158
DO  - https://doi.org/10.1016/j.knosys.2017.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S0950705117300588
N2  - This study focuses on the problem of supervised classification on heterogeneous temporal data featuring a mixture of attribute types (numeric, binary, symbolic, temporal). We present a model for classification rules designed to use both non-temporal attributes and sequences of temporal events as predicates. We also propose an efficient local search-based metaheuristic algorithm to mine such rules in large scale, real-life data sets extracted from a hospital’s information system. The proposed algorithm, MOSC (Multi-Objective Sequence Classifier), is compared to standard classifiers and previous works on these real data sets and exhibits noticeably better classification performance. While designed with medical applications in mind, the proposed approach is generic and can be used for problems from other application domains.
ER  - 
TY  - JOUR
T1  - PAIP 2019: Liver cancer segmentation challenge
A1  - Kim, Yoo Jung
A1  - Jang, Hyungjoon
A1  - Lee, Kyoungbun
A1  - Park, Seongkeun
A1  - Min, Sung-Gyu
A1  - Hong, Choyeon
A1  - Park, Jeong Hwan
A1  - Lee, Kanggeun
A1  - Kim, Jisoo
A1  - Hong, Wonjae
A1  - Jung, Hyun
A1  - Liu, Yanling
A1  - Rajkumar, Haran
A1  - Khened, Mahendra
A1  - Krishnamurthi, Ganapathy
A1  - Yang, Sen
A1  - Wang, Xiyue
A1  - Han, Chang Hee
A1  - Kwak, Jin Tae
A1  - Ma, Jianqiang
A1  - Tang, Zhe
A1  - Marami, Bahram
A1  - Zeineh, Jack
A1  - Zhao, Zixu
A1  - Heng, Pheng-Ann
A1  - Schmitz, Rüdiger
A1  - Madesta, Frederic
A1  - Rösch, Thomas
A1  - Werner, Rene
A1  - Tian, Jie
A1  - Puybareau, Elodie
A1  - Bovio, Matteo
A1  - Zhang, Xiufeng
A1  - Zhu, Yifeng
A1  - Chun, Se Young
A1  - Jeong, Won-Ki
A1  - Park, Peom
A1  - Choi, Jinwook
Y1  - 2021///
KW  - Challenge
KW  - Digital pathology
KW  - Liver cancer
KW  - Segmentation
KW  - Tumor burden
JF  - Medical Image Analysis
VL  - 67
SP  - 101854
EP  - 101854
DO  - https://doi.org/10.1016/j.media.2020.101854
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520302188
N2  - Pathology Artificial Intelligence Platform (PAIP) is a free research platform in support of pathological artificial intelligence (AI). The main goal of the platform is to construct a high-quality pathology learning data set that will allow greater accessibility. The PAIP Liver Cancer Segmentation Challenge, organized in conjunction with the Medical Image Computing and Computer Assisted Intervention Society (MICCAI 2019), is the first image analysis challenge to apply PAIP datasets. The goal of the challenge was to evaluate new and existing algorithms for automated detection of liver cancer in whole-slide images (WSIs). Additionally, the PAIP of this year attempted to address potential future problems of AI applicability in clinical settings. In the challenge, participants were asked to use analytical data and statistical metrics to evaluate the performance of automated algorithms in two different tasks. The participants were given the two different tasks: Task 1 involved investigating Liver Cancer Segmentation and Task 2 involved investigating Viable Tumor Burden Estimation. There was a strong correlation between high performance of teams on both tasks, in which teams that performed well on Task 1 also performed well on Task 2. After evaluation, we summarized the top 11 team’s algorithms. We then gave pathological implications on the easily predicted images for cancer segmentation and the challenging images for viable tumor burden estimation. Out of the 231 participants of the PAIP challenge datasets, a total of 64 were submitted from 28 team participants. The submitted algorithms predicted the automatic segmentation on the liver cancer with WSIs to an accuracy of a score estimation of 0.78. The PAIP challenge was created in an effort to combat the lack of research that has been done to address Liver cancer using digital pathology. It remains unclear of how the applicability of AI algorithms created during the challenge can affect clinical diagnoses. However, the results of this dataset and evaluation metric provided has the potential to aid the development and benchmarking of cancer diagnosis and segmentation.
ER  - 
TY  - JOUR
T1  - Abdominal adipose tissues extraction using multi-scale deep neural network
A1  - Jiang, Fei
A1  - Li, Huating
A1  - Hou, Xuhong
A1  - Sheng, Bin
A1  - Shen, Ruimin
A1  - Liu, Xiao-Yang
A1  - Jia, Weiping
A1  - Li, Ping
A1  - Fang, Ruogu
Y1  - 2017///
KW  -  Coarse-to-fine segmentation
KW  -  Internal boundary
KW  -  Multi-scale deep neural network
KW  - Abdominal adipose tissues segmentation
JF  - Neurocomputing
VL  - 229
SP  - 23
EP  - 33
DO  - https://doi.org/10.1016/j.neucom.2016.07.059
UR  - https://www.sciencedirect.com/science/article/pii/S0925231216313480
N1  - Advances in computing techniques for big medical image data
N2  - Segmentation of abdominal adipose tissues (AAT) into subcutaneous adipose tissues (SAT) and visceral adipose tissues (VAT) is of crucial interest for managing the obesity. Previous methods with raw or hand-crafted features rarely work well on large-scale subject cohorts, because of the inhomogeneous image intensities, artifacts and the diverse distributions of VAT. In this paper, we propose a novel two-stage coarse-to-fine algorithm for AAT segmentation. In the first stage, we formulate the AAT segmentation task as a pixel-wise classification problem. First, three types of features, intensity, spatial and contextual features, are extracted. Second, a new type of deep neural network, named multi-scale deep neural network (MSDNN), is provided to extract high-level features. In the second stage, to improve the segmentation accuracy, we refine coarse segmentation results by determining the internal boundaries of SAT based on coarse segmentation results and the continuous of SAT internal boundaries. Finally, we demonstrate the efficacy of our algorithm for both 2D and 3D cases on a wide population range. Compared with other algorithms, our method is not only more suitable for large-scale dataset, but also achieves better segmentation results. Furthermore, our system takes about 2s to segment an abdominal image, which implies potential clinical applications.
ER  - 
TY  - JOUR
T1  - Regularized margin-based conditional log-likelihood loss for prototype learning
A1  - Jin, Xiao-Bo
A1  - Liu, Cheng-Lin
A1  - Hou, Xinwen
Y1  - 2010///
KW  -  Conditional log-likelihood loss
KW  -  Distance metric learning
KW  -  Log-likelihood of margin (LOGM)
KW  -  Regularization
KW  - Prototype learning
JF  - Pattern Recognition
VL  - 43
IS  - 7
SP  - 2428
EP  - 2438
DO  - https://doi.org/10.1016/j.patcog.2010.01.013
UR  - https://www.sciencedirect.com/science/article/pii/S0031320310000464
N2  - The classification performance of nearest prototype classifiers largely relies on the prototype learning algorithm. The minimum classification error (MCE) method and the soft nearest prototype classifier (SNPC) method are two important algorithms using misclassification loss. This paper proposes a new prototype learning algorithm based on the conditional log-likelihood loss (CLL), which is based on the discriminative model called log-likelihood of margin (LOGM). A regularization term is added to avoid over-fitting in training as well as to maximize the hypothesis margin. The CLL in the LOGM algorithm is a convex function of margin, and so, shows better convergence than the MCE. In addition, we show the effects of distance metric learning with both prototype-dependent weighting and prototype-independent weighting. Our empirical study on the benchmark datasets demonstrates that the LOGM algorithm yields higher classification accuracies than the MCE, generalized learning vector quantization (GLVQ), soft nearest prototype classifier (SNPC) and the robust soft learning vector quantization (RSLVQ), and moreover, the LOGM with prototype-dependent weighting achieves comparable accuracies to the support vector machine (SVM) classifier.
ER  - 
TY  - JOUR
T1  - Explainable deep learning ensemble for food image analysis on edge devices
A1  - Tahir, Ghalib Ahmed
A1  - Loo, Chu Kiong
Y1  - 2021///
KW  -  Data augmentation
KW  -  Deep learning
KW  -  Ensemble learning
KW  -  Explainable AI
KW  -  Food recognition
KW  -  Mobile application
KW  -  Neural network
KW  - User-centred explainable AI
JF  - Computers in Biology and Medicine
VL  - 139
SP  - 104972
EP  - 104972
DO  - https://doi.org/10.1016/j.compbiomed.2021.104972
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521007666
N2  - Food recognition systems recently garnered much research attention in the relevant field due to their ability to obtain objective measurements for dietary intake. This feature contributes to the management of various chronic conditions. Challenges such as inter and intraclass variations alongside the practical applications of smart glasses, wearable cameras, and mobile devices require resource-efficient food recognition models with high classification performance. Furthermore, explainable AI is also crucial in health-related domains as it characterizes model performance, enhancing its transparency and objectivity. Our proposed architecture attempts to address these challenges by drawing on the strengths of the transfer learning technique upon initializing MobiletNetV3 with weights from a pre-trained model of ImageNet. The MobileNetV3 achieves superior performance using the squeeze and excitation strategy, providing unequal weight to different input channels and contrasting equal weights in other variants. Despite being fast and efficient, there is a high possibility for it to be stuck in the local optima like other deep neural networks, reducing the desired classification performance of the model. Thus, we overcome this issue by applying the snapshot ensemble approach as it enables the M model in a single training process without any increase in the required training time. As a result, each snapshot in the ensemble visits different local minima before converging to the final solution which enhances recognition performance. On overcoming the challenge of explainability, we argue that explanations cannot be monolithic, since each stakeholder perceive the results’, explanations based on different objectives and aims. Thus, we proposed a user-centered explainable artificial intelligence (AI) framework to increase the trust of the involved parties by inferencing and rationalizing the results according to needs and user profile. Our framework is comprehensive in terms of a dietary assessment app as it detects Food/Non-Food, food categories, and ingredients. Experimental results on the standard food benchmarks and newly contributed Malaysian food dataset for ingredient detection demonstrated superior performance on an integrated set of measures over other methodologies.
ER  - 
TY  - JOUR
T1  - On visual BMI analysis from facial images
A1  - Jiang, Min
A1  - Shang, Yuanyuan
A1  - Guo, Guodong
Y1  - 2019///
KW  -  FIW-BMI database
KW  -  Facial images
KW  -  Facial representations
KW  - Visual BMI estimation
JF  - Image and Vision Computing
VL  - 89
SP  - 183
EP  - 196
DO  - https://doi.org/10.1016/j.imavis.2019.07.003
UR  - https://www.sciencedirect.com/science/article/pii/S0262885619301027
N2  - Automatically assessing body mass index (BMI) from facial images is an interesting and challenging problem in computer vision. Facial feature extraction is an important step for visual BMI estimation. This work studies the visual BMI estimation problem based on the characteristics and performance of different facial representations, which has not been well studied yet. Various facial representations, including geometry based representations and deep learning based, are comprehensively evaluated and analyzed from three perspectives: the overall performance on visual BMI prediction, the redundancy in facial representations and the sensitivity to head pose changes. The experiments are conducted on two databases: a new dataset we collected, called the FIW-BMI and an existing large dataset Morph II. Our studies provide some deep insights into the facial representations for visual BMI analysis: 1) The deep model based methods perform better than geometry based methods. Among them, the VGG-Face and Arcface show more robustness than others in most cases; 2) Removing the redundancy in VGG-Face representation can increase the accuracy and efficiency in BMI estimation; 3) Large head poses lead to low performance for BMI estimation. The Arcface, VGG-Face and PIGF are more robust than the others to head pose variations.
ER  - 
TY  - JOUR
T1  - ECG-based monitoring of blood potassium concentration: Periodic versus principal component as lead transformation for biomarker robustness
A1  - Palmieri, Flavio
A1  - Gomis, Pedro
A1  - Ruiz, José Esteban
A1  - Ferreira, Dina
A1  - Martín-Yebra, Alba
A1  - Pueyo, Esther
A1  - Martínez, Juan Pablo
A1  - Ramírez, Julia
A1  - Laguna, Pablo
Y1  - 2021///
KW  -  Lead space reduction
KW  -  Non-invasive potassium monitoring
KW  -  Periodic component analysis
KW  -  Principal component analysis
KW  -  T-wave morphology
KW  -  Time-warping
KW  - Electrocardiogram
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102719
EP  - 102719
DO  - https://doi.org/10.1016/j.bspc.2021.102719
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421003165
N2  - Objective
The aim of this study is to compare the performance of two electrocardiogram (ECG) lead-space reduction (LSR) techniques in generating a transformed ECG lead from which T-wave morphology markers can be reliably derived to non-invasively monitor blood potassium concentration ([K+]) in end-stage renal disease (ESRD) patients undergoing hemodialysis (HD). These LSR techniques are: (1) principal component analysis (PCA), learned on the T wave, and (2) periodic component analysis (πCA), either learned on the whole QRST complex (πCB) or on the T wave (πCT). We hypothesized πCA is less sensitive to non-periodic disturbances, like noise and body position changes (BPC), than PCA, thus leading to more reliable T wave morphology markers.
Methods
We compared the ability of T wave morphology markers obtained from PCA, πCB and πCT in tracking [K+] in an ESRD-HD dataset, including 29 patients, during and after HD (evaluated by correlation and residual fitting error analysis). We also studied their robustness to BPC using an annotated database, including 20 healthy individuals, as well as to different levels of noise using a simulation set-up (assessed by means of Mann–Whitney U test and relative error, respectively).
Results
The performance of both πCB and πCT-based markers in following [K+]-variations during HD was comparable, and superior to that from PCA-based markers. Moreover, πCT-based markers showed superior robustness against BPC and noise.
Conclusion
Both πCB and πCT outperform PCA in terms of monitoring [K+] in ESRD-HD patients, as well as of robustness against BPC and low SNR, with πCT showing the highest stability for continuous post-HD monitoring.
Significance
The usage of πCA (i) increases the accuracy in monitoring dynamic [K+] variations in ESRD-HD patients and (ii) reduces the sensitivity to BPC and noise in deriving T wave morphology markers.
ER  - 
TY  - JOUR
T1  - gHRV: Heart rate variability analysis made easy
A1  - Rodríguez-Liñares, L
A1  - Lado, M J
A1  - Vila, X A
A1  - Méndez, A J
A1  - Cuesta, P
Y1  - 2014///
KW  -  Open source
KW  -  Signal processing
KW  - Heart rate variability
JF  - Computer Methods and Programs in Biomedicine
VL  - 116
IS  - 1
SP  - 26
EP  - 38
DO  - https://doi.org/10.1016/j.cmpb.2014.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260714001461
N2  - In this paper, the gHRV software tool is presented. It is a simple, free and portable tool developed in python for analysing heart rate variability. It includes a graphical user interface and it can import files in multiple formats, analyse time intervals in the signal, test statistical significance and export the results. This paper also contains, as an example of use, a clinical analysis performed with the gHRV tool, namely to determine whether the heart rate variability indexes change across different stages of sleep. Results from tests completed by researchers who have tried gHRV are also explained: in general the application was positively valued and results reflect a high level of satisfaction. gHRV is in continuous development and new versions will include suggestions made by testers.
ER  - 
TY  - JOUR
T1  - Quantification of haemodynamic response to auditory stimulus in intensive care
A1  - Korhonen, Ilkka
A1  - Karhu, Jari
A1  - Mainardi, Luca
A1  - Jakob, Stephan M
Y1  - 2000///
KW  -  Auditory stimulus
KW  -  Haemodynamic response
KW  -  Heart rate variability
KW  - Autonomous nervous system
JF  - Computer Methods and Programs in Biomedicine
VL  - 63
IS  - 3
SP  - 211
EP  - 218
DO  - https://doi.org/10.1016/S0169-2607(00)00111-5
UR  - https://www.sciencedirect.com/science/article/pii/S0169260700001115
N2  - Measuring effects of sensory stimuli on haemodynamics could provide information about the interplay between central and autonomous nervous system (ANS). However, ANS response to sensory stimulus has received little attention. In this paper we present a signal processing scheme to extract the responses of heart rate and systemic arterial pressure on auditory stimulus in intensive care patients (N=5). In short, the effect of mechanical ventilation is rejected by optimal linear modelling. Other disturbances are attenuated by filtering and efficient rejection of outlying sweeps of data. The results show identifiable responses on three out of five cases. The response characteristics may be explained by synchronisation of spontaneous variability in systemic arterial pressure to auditory stimulus.
ER  - 
TY  - JOUR
T1  - Segmentation of retinal vessels by means of directional response vector similarity and region growing
A1  - Lázár, István
A1  - Hajdu, András
Y1  - 2015///
KW  -  Directional filters
KW  -  Directional response similarity
KW  -  Multiscale matched filtering
KW  -  Vessel segmentation
KW  - Retinal image analysis
JF  - Computers in Biology and Medicine
VL  - 66
SP  - 209
EP  - 221
DO  - https://doi.org/10.1016/j.compbiomed.2015.09.008
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515003169
N2  - This paper presents a novel retinal vessel segmentation method. Opposed to the general approach in similar directional methods, where only the maximal or summed responses of a pixel are used, here, the directional responses of a pixel are considered as a vector. The segmentation method is a unique region growing procedure which combines a hysteresis thresholding scheme with the response vector similarity of adjacent pixels. A vessel score map is constructed as the combination of the statistical measures of the response vectors and its local maxima to provide the seeds for the region growing procedure. A nearest neighbor classifier based on a rotation invariant response vector similarity measure is used to filter the seed points. Many techniques in the literature that capture the Gaussian-like cross-section of vessels suffer from the drawback of giving false high responses to the steep intensity transitions at the boundary of the optic disc and bright lesions. To overcome this issue, we also propose a symmetry constrained multiscale matched filtering technique. The proposed vessel segmentation method has been tested on three publicly available image sets, where its performance proved to be competitive with the state-of-the-art and comparable to the accuracy of a human observer, as well.
ER  - 
TY  - JOUR
T1  - Ocular fundus reference images from optical coherence tomography
A1  - Guimarães, Pedro
A1  - Rodrigues, Pedro
A1  - Lobo, Conceição
A1  - Leal, Sérgio
A1  - Figueira, João
A1  - Serranho, Pedro
A1  - Bernardes, Rui
Y1  - 2014///
KW  -  Fundus images
KW  -  Ocular fundus
KW  -  Retina
KW  -  Vascular network
KW  - Optical coherence tomography
JF  - Computerized Medical Imaging and Graphics
VL  - 38
IS  - 5
SP  - 381
EP  - 389
DO  - https://doi.org/10.1016/j.compmedimag.2014.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0895611114000329
N2  - Two-dimensional images computed from three-dimensional optical coherence tomography (OCT) data are intrinsically aligned with it, allowing to accurately position a retinal OCT scan within the ocular fundus. In this work, we aim to compute an OCT fundus reference image with improved retinal vasculature extension and contrast over traditional approaches. Based on the shadow casted by hemoglobin on the outer layers of the retina, we compute three independent images from the OCT volumetric data (including the traditional fundus reference image). Combining these images, a fourth one is created that is able to outperform the other three, both quantitatively and qualitatively (as evaluated by retina specialists). The vascular network extension provided by this method was also compared with widely used fundus imaging modalities, showing that it is similar to that achieved with color fundus photography. In this way, the proposed method is an important starting point to the segmentation of the vascular tree and provides users with a detailed fundus reference image.
ER  - 
TY  - JOUR
T1  - ExplAIn: Explanatory artificial intelligence for diabetic retinopathy diagnosis
A1  - Quellec, Gwenolé
A1  - Al Hajj, Hassan
A1  - Lamard, Mathieu
A1  - Conze, Pierre-Henri
A1  - Massin, Pascale
A1  - Cochener, Béatrice
Y1  - 2021///
KW  -  Diabetic retinopathy diagnosis
KW  -  Self-supervised learning
KW  - Explanatory artificial intelligence
JF  - Medical Image Analysis
VL  - 72
SP  - 102118
EP  - 102118
DO  - https://doi.org/10.1016/j.media.2021.102118
UR  - https://www.sciencedirect.com/science/article/pii/S136184152100164X
N2  - In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the “black-box” nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.
ER  - 
TY  - JOUR
T1  - Variability of the autoregulation index decreases after removing the effect of the very low frequency band
A1  - Elting, J W
A1  - Maurits, N M
A1  - Aries, M J H
Y1  - 2014///
KW  - Autoregulation index
KW  - Dynamic autoregulation
KW  - High pass filtering
KW  - Stroke
KW  - Transfer function
JF  - Medical Engineering & Physics
VL  - 36
IS  - 5
SP  - 601
EP  - 606
DO  - https://doi.org/10.1016/j.medengphy.2013.10.009
UR  - https://www.sciencedirect.com/science/article/pii/S1350453313002300
N1  - Special Issue on Cerebral Autoregulation: Measurement and Modelling
N2  - Dynamic cerebral autoregulation (dCA) estimates show large between and within subject variability. Sources of variability include low coherence and influence of CO2 in the very low frequency (VLF) band, where dCA is active. This may lead to unreliable transfer function and autoregulation index (ARI) estimates. We tested whether variability of the ARI could be decreased by suppressing the effect of the VLF band through filtering. We also evaluated whether filtering had any effect on mean group differences between healthy subjects and acute stroke patients. Data from a recent mobilization stroke study were re-analyzed. Middle cerebral artery cerebral blood flow velocity (MCA-CBFV), mean arterial blood pressure (MABP) and end tidal PCO2 (PetCO2) were obtained in 16 healthy subjects and 27 acute ischemic stroke patients in the supine position. The ARI index was calculated from the transfer function (TF) by using spontaneous BP fluctuations. Three different filtering strategies were compared; no filtering (NF), a high pass filter at 0.04Hz (Time Domain Filtering: TDF) and a high pass Transfer Function Filter (TFF) at 0.04Hz. In addition, a simulation study was done to obtain further insight into the effects of the applied filters. The variability of the ARI index decreased significantly only with TFF in healthy subjects (standard deviation (left vs. right) after NF 2.28 vs. 2.36, after TDF 2.13 vs. 2.31 after TFF 1.09 vs. 1.19, p<0.001). Variability was not significantly reduced in stroke patients. The mean ARI was significantly lower in stroke patients compared to healthy subjects after TFF (affected hemisphere 5.85±1.96 vs. 7.13±1.09, non-affected hemisphere 5.96±1.64 vs. 7.31±1.19, p<0.01 for both hemispheres), but not after NF or TDF. The simulation study showed that TFF results in an overestimation of the ARI index at low ARI levels (0–3), but in correct estimates at higher ARI levels. Removing the effect of the VLF band with TFF results in less ARI variability in healthy subjects, and in more pronounced group differences between stroke patients and healthy subjects. This will improve diagnostic properties when using TFA for ARI calculation.
ER  - 
TY  - JOUR
T1  - Internet of Things applications: A systematic review
A1  - Asghari, Parvaneh
A1  - Rahmani, Amir Masoud
A1  - Javadi, Hamid Haj Seyyed
Y1  - 2019///
KW  -  Internet of things
KW  -  Quality of service
KW  -  Smart objects
KW  -  Systematic literature review
KW  - Application-based services
JF  - Computer Networks
VL  - 148
SP  - 241
EP  - 261
DO  - https://doi.org/10.1016/j.comnet.2018.12.008
UR  - https://www.sciencedirect.com/science/article/pii/S1389128618305127
N2  - Internet of Things (IoT) is considered as an ecosystem that contains smart objects equipped with sensors, networking and processing technologies integrating and working together to provide an environment in which smart services are taken to the end users. The IoT is leading numerous benefits into the human life through the environment wherein smart services are provided to utilize every activity anywhere and anytime. All these facilities and services are conveyed through the diverse applications which are performed in the IoT environment. The most important utilities that are achieved by the IoT applications are monitoring and consequently immediate decision making for efficient management. In this paper, we intend to survey in divers IoT application domains to comprehend the different approaches in IoT applications which have been recently presented based on the Systematic Literature Review (SLR) method. The aim of this paper is to categorize analytically and statistically, and analyze the current research techniques on IoT applications approaches published from 2011 to 2018. A technical taxonomy is presented for the IoT applications approaches according to the content of current studies that are selected with SLR process in this study including health care, environmental monitoring, smart city, commercial, industrial and general aspects in IoT applications. IoT applications are compared with each other according to some technical features such as Quality of Service (QoS), proposed case study and evaluation environments. The achievements and disadvantages of each study is discussed as well as presenting some hints for addressing their weaknesses and highlighting the future research challenges and open issues in IoT applications.
ER  - 
TY  - JOUR
T1  - Pattern classification by concurrently determined piecewise linear and convex discriminant functions
A1  - Ryoo, Hong Seo
Y1  - 2006///
KW  -  Classification
KW  -  Data mining
KW  -  Mixed integer and linear programming
KW  - Machine learning
JF  - Computers & Industrial Engineering
VL  - 51
IS  - 1
SP  - 79
EP  - 89
DO  - https://doi.org/10.1016/j.cie.2006.06.015
UR  - https://www.sciencedirect.com/science/article/pii/S0360835206000659
N1  - Special Issue on Computational Intelligence and Information Technology: Applications to Industrial Engineering
N2  - This paper develops a new methodology for pattern classification by concurrently determined k piecewise linear and convex discriminant functions. Toward the end, we design a new l1-norm distance metric for measuring misclassification errors and use it to develop a mixed 0–1 integer and linear program (MILP) for the k piecewise linear and convex separation of data. The proposed model is meritorious in that it considers the synergy as well as the individual role of the k hyperplanes in constructing a decision surface and exploits the advances in theory and algorithms and the advent of powerful softwares for MILP for its solution. With artificially created data, we illustrate pros and cons of pattern classification by the proposed methodology. With six benchmark classification datasets, we demonstrate that the proposed approach is effective and competitive with well-established learning methods. In summary, the classifiers constructed by the proposed approach obtain the best prediction rates on three of the six datasets and the second best records for two of the remaining three datasets.
ER  - 
TY  - JOUR
T1  - Active subgroup mining: a case study in coronary heart disease risk group detection
A1  - Gamberger, Dragan
A1  - Lavrač, Nada
A1  - Krstačić, Goran
Y1  - 2003///
KW  -  Active mining
KW  -  Machine learning
KW  -  Non-invasive cardiovascular tests
KW  -  Risk group detection
KW  -  Subgroup discovery
KW  - Coronary heart disease
JF  - Artificial Intelligence in Medicine
VL  - 28
IS  - 1
SP  - 27
EP  - 57
DO  - https://doi.org/10.1016/S0933-3657(03)00034-4
UR  - https://www.sciencedirect.com/science/article/pii/S0933365703000344
N2  - This paper presents an approach to active mining of patient records aimed at discovering patient groups at high risk for coronary heart disease (CHD). The approach proposes active expert involvement in the following steps of the knowledge discovery process: data gathering, cleaning and transformation, subgroup discovery, statistical characterization of induced subgroups, their interpretation, and the evaluation of results. As in the discovery and characterization of risk subgroups, the main risk factors are made explicit, the proposed methodology has high potential for patient screening and early detection of patient groups at risk for CHD.
ER  - 
TY  - JOUR
T1  - Biomedical engineering education and research activity in Hungary
A1  - Benyó, Zoltáan
A1  - Benyó, Balázs
A1  - Várady, Péter
A1  - Szilágyi, László
A1  - Paláncz, Béla
A1  - Szlávecz, Ákos
A1  - Micsik, Tamás
A1  - Bongár, Szabolcs
A1  - Fördös, Gergely
A1  - Várallyay, György
Y1  - 2003///
KW  -  biomedical systems
KW  -  detection algorithms
KW  -  image analysis
KW  -  physiological models
KW  -  signal processing
KW  - education
JF  - IFAC Proceedings Volumes
VL  - 36
IS  - 15
SP  - 51
EP  - 55
DO  - https://doi.org/10.1016/S1474-6670(17)33471-7
UR  - https://www.sciencedirect.com/science/article/pii/S1474667017334717
N1  - 5th IFAC Symposium on Modelling and Control in Biomedical Systems 2003, Melbourne, Australia, 21-23 August 2003
N2  - Biomedical Engineering is a relatively new interdisciplinary science. This paper presents the biomedical engineering activity, which is carried out at Budapest University of Technology and Economics and its partner institutes. In the first part the main goals and the curriculum of the Biomedical Engineering Education Program (BMEEP) is presented. The second part of the paper summarizes the most important biomedical engineering researches carried out mostly in the Biomedical Engineering Laboratory of our university.
ER  - 
TY  - JOUR
T1  - An electronic nose-based assistive diagnostic prototype for lung cancer detection with conformal prediction
A1  - Zhan, Xianghao
A1  - Wang, Zhan
A1  - Yang, Meng
A1  - Luo, Zhiyuan
A1  - Wang, You
A1  - Li, Guang
Y1  - 2020///
KW  -  Electronic nose
KW  -  Lung cancer
KW  -  Online prediction
KW  -  Reliability
KW  - Conformal prediction
JF  - Measurement
VL  - 158
SP  - 107588
EP  - 107588
DO  - https://doi.org/10.1016/j.measurement.2020.107588
UR  - https://www.sciencedirect.com/science/article/pii/S0263224120301251
N2  - Lung cancer leads to high mortalities in various countries while the reliability of cancer diagnosis has not been paid enough attention. In this work, a novel application of conformal prediction in lung cancer diagnosis with electronic nose is introduced. The nonconformity measurement is based on k-nearest neighbors. In offline prediction, accuracies of 87.5% and 83.33% have been achieved by conformal predictors based on 1NN and 3NN respectively, outperforming those of simple k-nearest neighbor predictors. Additionally, conformal predictors provides confidence and credibility information of each prediction that could inform the patients of diagnostic risks. In online prediction, with increasing number of samples, the frequency of errors given by conformal predictions can gradually be limited by the significance level set by users. This project manifests that electronic nose promises to be an applicable cheaper analytic tool in assisting lung cancer diagnosis and conformal prediction provides a promising method to ensure reliability.
ER  - 
TY  - JOUR
T1  - Associative cellular learning automata and its applications
A1  - Ahangaran, Meysam
A1  - Taghizadeh, Nasrin
A1  - Beigy, Hamid
Y1  - 2017///
KW  -  Cellular automata
KW  -  Classification
KW  -  Clustering
KW  -  External input
KW  -  Image segmentation
KW  -  Learning automata
KW  -  Self-organizing map
KW  - Cellular learning automata
JF  - Applied Soft Computing
VL  - 53
SP  - 1
EP  - 18
DO  - https://doi.org/10.1016/j.asoc.2016.12.006
UR  - https://www.sciencedirect.com/science/article/pii/S156849461630624X
N2  - Cellular learning automata (CLA) is a distributed computational model which was introduced in the last decade. This model combines the computational power of the cellular automata with the learning power of the learning automata. Cellular learning automata is composed from a lattice of cells working together to accomplish their computational task; in which each cell is equipped with some learning automata. Wide range of applications utilizes CLA such as image processing, wireless networks, evolutionary computation and cellular networks. However, the only input to this model is a reinforcement signal and so it cannot receive another input such as the state of the environment. In this paper, we introduce a new model of CLA such that each cell receives extra information from the environment in addition to the reinforcement signal. The ability of getting an extra input from the environment increases the computational power and flexibility of the model. We have designed some new algorithms for solving famous problems in pattern recognition and machine learning such as classification, clustering and image segmentation. All of them are based on the proposed CLA. We investigated performance of these algorithms through several computer simulations. Results of the new clustering algorithm shows acceptable performance on various data sets. CLA-based classification algorithm gets average precision 84% on eight data sets in comparison with SVM, KNN and Naive Bayes with average precision 88%, 84% and 75%, respectively. Similar results are obtained for semi-supervised classification based on the proposed CLA.
ER  - 
TY  - JOUR
T1  - Fast instance selection for speeding up support vector machines
A1  - Chen, Jingnian
A1  - Zhang, Caiming
A1  - Xue, Xiaoping
A1  - Liu, Cheng-Lin
Y1  - 2013///
KW  -  Classification
KW  -  Clustering
KW  -  Instance selection
KW  -  Multi-class
KW  - SVM
JF  - Knowledge-Based Systems
VL  - 45
SP  - 1
EP  - 7
DO  - https://doi.org/10.1016/j.knosys.2013.01.031
UR  - https://www.sciencedirect.com/science/article/pii/S0950705113000518
N2  - Support vector machine (SVM) has shown prominent performance for binary classification. How to effectively apply it to massive datasets with large number of classes and instances is still a serious challenge. Instance selection methods have been proposed and shown significant efficacy for reducing the training complexity of SVM, but more or less trade off the generalization performance. This paper presents an instance selection method especially for multi-class problems. With cluster centers of positive class as reference points instances are selected for each one-versus-rest SVM model. The purpose of clustering here is to improve the efficiency of instance selection, other than to select instances directly from clusters as previous methods did. Experiments on a wide variety of datasets demonstrate that the proposed method selects fewer instances than most competitive algorithms and keeps the highest classification accuracy on most datasets. Additionally, experimental results show that this method also performs superiorly for binary problems.
ER  - 
TY  - JOUR
T1  - Deep learning of smartphone sensor data for personal health assistance
A1  - Li, Honggui
A1  - Trocan, Maria
Y1  - 2019///
KW  -  Deep learning
KW  -  Personal health
KW  -  Smartphone
KW  -  Stacked autoencoders
KW  - Sensor data
JF  - Microelectronics Journal
VL  - 88
SP  - 164
EP  - 172
DO  - https://doi.org/10.1016/j.mejo.2018.01.015
UR  - https://www.sciencedirect.com/science/article/pii/S0026269217302525
N2  - This paper strives to construct the relevance between smartphone sensor data and personal health via a deep learning method. Firstly, the data captured by smartphone sensors is categorized into groups by deep stacked autoencoders (SAE), which is a traditional deep learning architecture with multi-layer sparse autoencoders for feature extraction and a softmax layer for classification. For instance, accelerometer data from smartphones can be classified in accordance with different user motion states such as sitting, standing, walking, and running. Secondly, the quantitative correlation between divided sensor data and individual health is built. For example, walking and running for a suitable amount of time are beneficial to human health. Finally, a simulation experiment is devised to verify the performance of the proposed method. A 0.15 percent gain on the overall classification accuracy for automatic feature extraction on the human action recognition (HAR) dataset has been achieved compared with the state of the art methods and 0.5 percent gain for manual feature extraction. It is also demonstrated by the experimental results that smartphone sensor data can reveal the health status of the user to some degree and smartphone can serve as an assistant for human health.
ER  - 
TY  - JOUR
T1  - Biosensors for in vivo glucose measurement: can we cross the experimental stage
A1  - Abel, P U
A1  - von Woedtke, T
Y1  - 2002///
KW  -  Biocompatibility
KW  -  Biostability
KW  -  Diffusion membrane
KW  -  Enzyme fixation
KW  -  In vivo studies
KW  -  Sterilization
KW  - Biosensor
JF  - Biosensors and Bioelectronics
VL  - 17
IS  - 11
SP  - 1059
EP  - 1070
DO  - https://doi.org/10.1016/S0956-5663(02)00099-4
UR  - https://www.sciencedirect.com/science/article/pii/S0956566302000994
N2  - The development of in vivo working glucose sensors needs two decades, so far. The availability of long term functional implantable biosensors for continuous glucose measurings is a basic preriquisite for the individualized optimum insulin treatment of diabetics. Enzymatic electrochemical sensors are described which realize a functional stability over more than 2 years in vitro, however their function in vivo is limited due to certain bioincompatibility expressed by inflammation of the surrounding tissue, exsudates, and immun reactions. The paper reflects an overview concerning different sensor covering materials used as more or less suitable diffusion membranes. From experimental studies in animals and human volunteers conclusions are drawn for further developmental steps of biosensors for in vivo use and for the appliability of glucose sensors for transient diagnostic purposes and as a basis for glucose controlled therapeutic measures. The results demonstrate that further progress aimed at long term biostability of implanted biosensors needs to solve technological problems and the serial production of sensors with really comparable qualities as a preriquisite for clinical trials.
ER  - 
TY  - JOUR
T1  - Congestive heart failure detection using random forest classifier
A1  - Masetic, Zerina
A1  - Subasi, Abdulhamit
Y1  - 2016///
KW  -  Autoregressive (AR) modeling
KW  -  Congestive heart failure (CHF)
KW  -  Machine learning
KW  -  Random forest
KW  - Electrocardiogram (ECG)
JF  - Computer Methods and Programs in Biomedicine
VL  - 130
SP  - 54
EP  - 64
DO  - https://doi.org/10.1016/j.cmpb.2016.03.020
UR  - https://www.sciencedirect.com/science/article/pii/S0169260715303369
N2  - Background and objectives
Automatic electrocardiogram (ECG) heartbeat classification is substantial for diagnosing heart failure. The aim of this paper is to evaluate the effect of machine learning methods in creating the model which classifies normal and congestive heart failure (CHF) on the long-term ECG time series.
Methods
The study was performed in two phases: feature extraction and classification phase. In feature extraction phase, autoregressive (AR) Burg method is applied for extracting features. In classification phase, five different classifiers are examined namely, C4.5 decision tree, k-nearest neighbor, support vector machine, artificial neural networks and random forest classifier. The ECG signals were acquired from BIDMC Congestive Heart Failure and PTB Diagnostic ECG databases and classified by applying various experiments.
Results
The experimental results are evaluated in several statistical measures (sensitivity, specificity, accuracy, F-measure and ROC curve) and showed that the random forest method gives 100% classification accuracy.
Conclusions
Impressive performance of random forest method proves that it plays significant role in detecting congestive heart failure (CHF) and can be valuable in expressing knowledge useful in medicine.
ER  - 
TY  - JOUR
T1  - Comprehensible evaluation of prognostic factors and prediction of wound healing
A1  - Robnik-Šikonja, Marko
A1  - Cukjati, David
A1  - Kononenko, Igor
Y1  - 2003///
KW  -  Comprehensibility of attribute evaluation
KW  -  Electric stimulation
KW  -  Machine learning
KW  -  Relief algorithms
KW  - Wound healing prediction
JF  - Artificial Intelligence in Medicine
VL  - 29
IS  - 1
SP  - 25
EP  - 38
DO  - https://doi.org/10.1016/S0933-3657(03)00044-7
UR  - https://www.sciencedirect.com/science/article/pii/S0933365703000447
N1  - Artificial Intelligence in Medicine Europe AIME '01
N2  - We analyzed the data of a controlled clinical study of the chronic wound healing acceleration as a result of electrical stimulation. The study involved a conventional conservative treatment, sham treatment, biphasic pulsed current, and direct current electrical stimulation. Data was collected over 10 years and suffices for an analysis with machine learning methods. So far, only a limited number of studies have investigated the wound and patient attributes which affect the chronic wound healing. There is none to our knowledge to include treatment attributes. The aims of our study are to determine effects of the wound, patient and treatment attributes on the wound healing process and to propose a system for prediction of the wound healing rate. First we analyzed which wound and patient attributes play a predominant role in the wound healing process and investigated a possibility to predict the wound healing rate at the beginning of the treatment based on the initial wound, patient and treatment attributes. Later we tried to enhance the wound healing rate prediction accuracy by predicting it after a few weeks of the wound healing follow-up. Using the attribute estimation algorithms ReliefF and RReliefF we obtained a ranking of the prognostic factors which was comprehensible to experts. We used regression and classification trees to build models for prediction of the wound healing rate. The obtained results are encouraging and may form a basis for an expert system for the chronic wound healing rate prediction. If the wound healing rate is known, then the provided information can help to formulate the appropriate treatment decisions and orient resources towards individuals with poor prognosis.
ER  - 
TY  - JOUR
T1  - Health of Humans and Machines in a Common Perspective
A1  - Rahhal, Mohamad
A1  - Adda, Mehdi
A1  - Atieh, Mirna
A1  - Ibrahim, Hussein
Y1  - 2020///
KW  -  Ambient Assisted Living
KW  -  Industry 4.0
KW  -  Prognostic Health Management
KW  -  Smart Industry
KW  - Internet of Things
JF  - Procedia Computer Science
VL  - 177
SP  - 415
EP  - 422
DO  - https://doi.org/10.1016/j.procs.2020.10.055
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920323231
N1  - The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2020) / The 10th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2020) / Affiliated Workshops
N2  - The health domain is one of the most researched domains, simply because maintaining good health for humans is an essential aspect for life. But the health domain is not limited to humans, maintaining health for machines is essential if we want to achieve a consistent productive industry. There are different ways to monitor and maintain health for humans and other ways for machines. Internet of Things (IOT) is a new paradigm shift that can be used to monitor both subdomains at the same time. IOT is already being used in Ambient Assisted Living (AAL) and Prognostic Health Management (PHM) systems to achieve certain separate goals. The aim of this paper is to give a brief review of both subdomains showing their technology and how to achieve these systems, aiming to merge these two subdomains into a one big frame work to achieve the full potential of Industry 4.0.
ER  - 
TY  - JOUR
T1  - Gaussian kernels for incomplete data
A1  - Mesquita, Diego P P
A1  - Gomes, João P P
A1  - Corona, Francesco
A1  - Souza, Amauri H
A1  - Nobre, Juvêncio S
Y1  - 2019///
KW  -  Gaussian kernel
KW  -  Gaussian mixture model
KW  -  Kernel estimation
KW  - Incomplete data
JF  - Applied Soft Computing
VL  - 77
SP  - 356
EP  - 365
DO  - https://doi.org/10.1016/j.asoc.2019.01.022
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619300262
N2  - This paper discusses a method to estimate the expected value of the Gaussian kernel in the presence of incomplete data. We show how, under the general assumption of a missing-at-random mechanism, the expected value of the Gaussian kernel function has a simple closed-form solution. Such a solution depends only on the parameters of the Gamma distribution which is assumed to represent squared distances. Furthermore, we show how the parameters governing the Gamma distribution depend only on the non-central moments of the kernel arguments, via the second-order moments of their squared distance, and can be estimated by making use of any parametric density estimation model of the data distribution. We approximate the data distribution with the maximum likelihood estimate of a Gaussian mixture distribution. The validity of the method is empirically assessed, under a range of conditions, on synthetic and real problems and the results compared to existing methods. For comparison, we consider methods that indirectly estimate a Gaussian kernel function by either estimating squared distances or by imputing missing values and then computing distances. Based on the experimental results, the proposed method consistently proves itself an accurate technique that further extends the use of Gaussian kernels with incomplete data.
ER  - 
TY  - JOUR
T1  - RESKO: Repositioning drugs by using side effects and knowledge from ontologies
A1  - McGarry, Ken
A1  - Graham, Yitka
A1  - McDonald, Sharon
A1  - Rashid, Anuam
Y1  - 2018///
KW  -  Graph theory
KW  -  Ontologies
KW  -  Pattern matching
KW  -  Protein targets
KW  - Side-effects
JF  - Knowledge-Based Systems
VL  - 160
SP  - 34
EP  - 48
DO  - https://doi.org/10.1016/j.knosys.2018.06.017
UR  - https://www.sciencedirect.com/science/article/pii/S0950705118303332
N2  - The objective of drug repositioning is to apply existing drugs to different diseases or medical conditions than the original target, and thus alleviate to a certain extent the time and cost expended in drug development. Our system RESKO, REpositioning drugs using Side Effects and Knowledge from Ontologies, identifies drugs with similar side-effects which are potential candidates for use elsewhere, the supposition is that similar side-effects may be caused by drugs targeting similar proteins and pathways. RESKO, integrates drug chemical data, protein interaction and ontological knowledge. The novel aspects of our system include a high level of biological knowledge through the use of pathway and biological ontology integration. This provides a explanation facility lacking in most of the existing methods and improves the repositioning process. We evaluate the shared side effects from the eight conventional Alzheimer drugs, from which sixty-seven candidate drugs based on a side-effect commonality were identified. The top 25 drugs on the list were further investigated in depth for their suitability to be repositioned, the literature revealed that many of the candidate drugs appear to have been trialed for Alzheimer’s disease. Thus verifying the accuracy of our system, we also compare our technique with several competing systems found in the literature.
ER  - 
TY  - JOUR
T1  - An efficient monitoring of eclamptic seizures in wireless sensors networks
A1  - Haider, Daniyal
A1  - Ren, Aifeng
A1  - Fan, Dou
A1  - Zhao, Nan
A1  - Yang, Xiaodong
A1  - Shah, Syed Aziz
A1  - Hu, Fangming
A1  - Abbasi, Qammer H
Y1  - 2019///
KW  -  Internet of thing (IoT)
KW  -  K-nearest neighbor (KNN)
KW  -  Random forest (RF) and K-mean
KW  -  Support vector machine (SVM)
KW  -  Wireless channel information (WCI)
KW  - C-band
JF  - Computers & Electrical Engineering
VL  - 75
SP  - 16
EP  - 30
DO  - https://doi.org/10.1016/j.compeleceng.2019.02.011
UR  - https://www.sciencedirect.com/science/article/pii/S0045790618316112
N2  - This paper presents the application of wireless sensing at C-band operating at 4.8 GHz technology (a potential Chinese 5G band). A wireless transceiver is used in the indoor environment to monitor different body motions of a woman experiencing an eclamptic seizure. The body movement shows unique wireless data which carries the wireless channel information. The results indicate that using higher features increases the accuracy from 3% to 4% for classifying data from different body motions. All of the four classifiers are compared by using six performance metrics such as accuracy, recall, precession, specificity, F-measure and Kappa. The values from these metrics indicate the better performance of SVM as compared to other three classifiers, the results indicate that the eclamptic seizures are easily differentiated from other body movements by applying the aforementioned classifiers.
ER  - 
TY  - JOUR
T1  - A Review of Dimensionality Reduction Techniques for Efficient Computation
A1  - Velliangiri, S
A1  - Alagumuthukrishnan, S
A1  - Thankumar joseph, S Iwin
Y1  - 2019///
KW  -  Feature Extraction
KW  -  Feature Reduction
KW  -  Feature Selection
KW  - Dimensionality reduction
JF  - Procedia Computer Science
VL  - 165
SP  - 104
EP  - 111
DO  - https://doi.org/10.1016/j.procs.2020.01.079
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920300879
N1  - 2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019
N2  - Dimensionality Reduction (DR) is the pre-processing step to remove redundant features, noisy and irrelevant data, in order to improve learning feature accuracy and reduce the training time. Dimensionality reductions techniques have been proposed and implemented by using feature selection and extraction method. Principal Component Analysis (PCA) one of the Dimensions reduction techniques which give reduced computation time for the learning process. In this paper presents most widely used feature extraction techniques such as EMD, PCA, and feature selection techniques such as correlation, LDA, forward selection have been analyzed based on high performance and accuracy. These techniques are highly applied in Deep Neural Network for medical image diagnosis and used to improve the classification accuracy. Further, we discussed how dimension reduction is made in deep learning.
ER  - 
TY  - JOUR
T1  - Leveraging Twitter data to understand public sentiment for the COVID‐19 outbreak in Singapore
A1  - Mohamed Ridhwan, Khairiyah
A1  - Hargreaves, Carol Anne
Y1  - 2021///
KW  -  Gibbs Sampling Dirichlet Multinomial Mixture (GSDMM)
KW  -  Latent Dirichlet Allocation (LDA)
KW  -  Sentiment analysis
KW  -  Singapore
KW  -  Social media
KW  -  Topic modeling
KW  -  VADER classifier
KW  - COVID-19
JF  - International Journal of Information Management Data Insights
VL  - 1
IS  - 2
SP  - 100021
EP  - 100021
DO  - https://doi.org/10.1016/j.jjimei.2021.100021
UR  - https://www.sciencedirect.com/science/article/pii/S2667096821000148
N2  - Microblogging has become one of the most useful tools for sharing everyday life events and news and for expressing opinions about those events. As Twitter posts are short and constantly being generated, they are a great source for providing public sentiment towards events that occurred throughout the COVID-19 period in Singapore. In this project, we perform sentiment analysis and topic modeling on the tweets about COVID-19 in Singapore, from 1 February 2020 to 31 August 2020. We accomplished this by collecting tweets discussing about COVID-19 and geolocated as ‘Singapore’, using the Python library ‘SNSCRAPE’. We used the sentiments returned from the VADER lexicon-based classifier and emotions from pre-trained recurrent neural networks to find correlations between real-life events and sentiment changes throughout the whole period. From our analysis, we discovered an increase in tweets about COVID-19 during key periods such as the circuit breaker and found that the overall sentiment polarity was dominantly positive. However, emotion analysis revealed that there were changes in the prevalence of fear and joy emotions over time, due to real-life COVID-19 developments in Singapore. Additionally, sentiment polarity was found to differ from topic to topic.
ER  - 
TY  - JOUR
T1  - Essie: A Concept-based Search Engine for Structured Biomedical Text
A1  - Ide, Nicholas C
A1  - Loane, Russell F
A1  - Demner-Fushman, Dina
Y1  - 2007///
JF  - Journal of the American Medical Informatics Association
VL  - 14
IS  - 3
SP  - 253
EP  - 263
DO  - https://doi.org/10.1197/jamia.M2233
UR  - https://www.sciencedirect.com/science/article/pii/S1067502707000473
N2  - This article describes the algorithms implemented in the Essie search engine that is currently serving several Web sites at the National Library of Medicine. Essie is a phrase-based search engine with term and concept query expansion and probabilistic relevancy ranking. Essie’s design is motivated by an observation that query terms are often conceptually related to terms in a document, without actually occurring in the document text. Essie’s performance was evaluated using data and standard evaluation methods from the 2003 and 2006 Text REtrieval Conference (TREC) Genomics track. Essie was the best-performing search engine in the 2003 TREC Genomics track and achieved results comparable to those of the highest-ranking systems on the 2006 TREC Genomics track task. Essie shows that a judicious combination of exploiting document structure, phrase searching, and concept based query expansion is a useful approach for information retrieval in the biomedical domain.
ER  - 
TY  - JOUR
T1  - Support vector machine with parameter optimization by a novel hybrid method and its application to fault diagnosis
A1  - Zhang, Xiaoyuan
A1  - Qiu, Daoyin
A1  - Chen, Fuan
Y1  - 2015///
KW  -  Bare bones differential evolution
KW  -  Differential evolution
KW  -  Fault diagnosis
KW  -  Parameter optimization
KW  -  Particle swarm optimization
KW  - Support vector machine
JF  - Neurocomputing
VL  - 149
SP  - 641
EP  - 651
DO  - https://doi.org/10.1016/j.neucom.2014.08.010
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214010248
N2  - The performance of support vector machine (SVM) heavily depends on its parameters. The parameter optimization for SVM is still an ongoing research issue. The current parameter optimization methods either are easy to fall into local optimal solution, or are time consuming. Moreover, some optimization methods depend also on the choice of parameters for them, provoking thus a vicious circle. In view of this, a new hybrid method is proposed to optimize the parameters of SVM in this paper. It uses the inter-cluster distance in the feature space (ICDF) to determine a small and effective search interval from a larger kernel parameter search space, while a hybrid of the barebones particle swarm optimization and differential evolution (BBDE) is used to search the optimal parameter combination in the new search space. The ICDF shows the degree the classes are separated. The BBDE is a new, almost parameter-free optimization algorithm. Some benchmark datasets are used to evaluate the proposed algorithm. Furthermore, the proposed method is used to diagnose the faults of rolling element bearings. Experiments and engineering application show that the proposed method outperforms other methods both mentioned in this paper and published in other literature.
ER  - 
TY  - JOUR
T1  - A case-based reasoning system for recommendation of data cleaning algorithms in classification and regression tasks
A1  - Corrales, David Camilo
A1  - Ledezma, Agapito
A1  - Corrales, Juan Carlos
Y1  - 2020///
KW  -  Classification
KW  -  Regression
KW  - Case-based reasoning
JF  - Applied Soft Computing
VL  - 90
SP  - 106180
EP  - 106180
DO  - https://doi.org/10.1016/j.asoc.2020.106180
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620301204
N2  - Recently, advances in Information Technologies (social networks, mobile applications, Internet of Things, etc.) generate a deluge of digital data; but to convert these data into useful information for business decisions is a growing challenge. Exploiting the massive amount of data through knowledge discovery (KD) process includes identifying valid, novel, potentially useful and understandable patterns from a huge volume of data. However, to prepare the data is a non-trivial refinement task that requires technical expertise in methods and algorithms for data cleaning. Consequently, the use of a suitable data analysis technique is a headache for inexpert users. To address these problems, we propose a case-based reasoning system (CBR) to recommend data cleaning algorithms for classification and regression tasks. In our approach, we represent the problem space by the meta-features of the dataset, its attributes, and the target variable. The solution space contains the algorithms of data cleaning used for each dataset. We represent the cases through a Data Cleaning Ontology. The case retrieval mechanism is composed of a filter and similarity phases. In the first phase, we defined two filter approaches based on clustering and quartile analysis. These filters retrieve a reduced number of relevant cases. The second phase computes a ranking of the retrieved cases by filter approaches, and it scores a similarity between a new case and the retrieved cases. The retrieval mechanism proposed was evaluated through a set of judges. The panel of judges scores the similarity between a query case against all cases of the case-base (ground truth). The results of the retrieval mechanism reach an average precision on judges ranking of 94.5% in top 3 (P@3), for top 7 (P@7) 84.55%, while in top 10 (P@10) 78.35%.
ER  - 
TY  - JOUR
T1  - Deep learning for healthcare applications based on physiological signals: A review
A1  - Faust, Oliver
A1  - Hagiwara, Yuki
A1  - Hong, Tan Jen
A1  - Lih, Oh Shu
A1  - Acharya, U Rajendra
Y1  - 2018///
KW  -  Electrocardiogram
KW  -  Electroencephalogram
KW  -  Electromyogram
KW  -  Electrooculogram
KW  -  Physiological signals
KW  - Deep learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 161
SP  - 1
EP  - 13
DO  - https://doi.org/10.1016/j.cmpb.2018.04.005
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718301226
N2  - Background and objective: We have cast the net into the ocean of knowledge to retrieve the latest scientific research on deep learning methods for physiological signals. We found 53 research papers on this topic, published from 01.01.2008 to 31.12.2017.
Methods
An initial bibliometric analysis shows that the reviewed papers focused on Electromyogram(EMG), Electroencephalogram(EEG), Electrocardiogram(ECG), and Electrooculogram(EOG). These four categories were used to structure the subsequent content review.
Results
During the content review, we understood that deep learning performs better for big and varied datasets than classic analysis and machine classification methods. Deep learning algorithms try to develop the model by using all the available input.
Conclusions
This review paper depicts the application of various deep learning algorithms used till recently, but in future it will be used for more healthcare areas to improve the quality of diagnosis.
ER  - 
TY  - JOUR
T1  - Latent topic ensemble learning for hospital readmission cost optimization
A1  - Baechle, Christopher
A1  - Huang, C Derrick
A1  - Agarwal, Ankur
A1  - Behara, Ravi S
A1  - Goo, Jahyun
Y1  - 2020///
KW  - Analytics
KW  - Ensemble learning
KW  - Hospital readmission
KW  - Natural language processing
KW  - Predictive analysis
JF  - European Journal of Operational Research
VL  - 281
IS  - 3
SP  - 517
EP  - 531
DO  - https://doi.org/10.1016/j.ejor.2019.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S0377221719304102
N1  - Featured Cluster: Business Analytics: Defining the field and identifying a research agenda
N2  - Unplanned hospital readmission is a costly problem in the United States, and in 2013 the U.S. federal government began to reduce payments to hospitals with preventable patient readmissions. Predictive modeling using machine learning and data analytics can be a useful decision support tool to help identify patients most likely to be readmitted. However, current systems have several shortcomings, such as difficulties in utilizing unstructured data and combining data from multiple hospitals. In this paper, we propose Latent Topic Ensemble Learning, which uses an ensemble of topic specific models to leverage data from multiple hospitals, as key data analytic algorithm for predicting hospital readmission. Models are built and evaluated incorporating federal financial penalties and tested using dataset containing data collected from 16 regional hospitals. It is found that LTEL significantly outperforms the best performing baseline method for readmission cost optimization.
ER  - 
TY  - JOUR
T1  - Ensembles of decision trees based on imprecise probabilities and uncertainty measures
A1  - Abellán, Joaquín
Y1  - 2013///
KW  -  Credal sets
KW  -  Decision trees
KW  -  Ensemble methods
KW  -  Supervised classification
KW  -  Uncertainty measures
KW  - Imprecise probabilities
JF  - Information Fusion
VL  - 14
IS  - 4
SP  - 423
EP  - 430
DO  - https://doi.org/10.1016/j.inffus.2012.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S1566253512000358
N2  - In this paper, we present an experimental comparison among different strategies for combining decision trees built by means of imprecise probabilities and uncertainty measures. It has been proven that the combination or fusion of the information obtained from several classifiers can improve the final process of the classification. We use previously developed schemes, known as Bagging and Boosting, along with a new one based on the variation of the root node via the information rank of each feature of the class variable. To this end, we applied two different approaches to deal with missing data and continuous variables. We use a set of tests on the performance of the methods analyzed here, to show that, with the appropriate approach, the Boosting scheme constitutes an excellent way to combine this type of decision tree. It should be noted that it provides good results, even compared with a standard Random Forest classifier, a successful procedure very commonly used in the literature.
ER  - 
TY  - JOUR
T1  - A discrete particle swarm optimization method for feature selection in binary classification problems
A1  - Unler, Alper
A1  - Murat, Alper
Y1  - 2010///
KW  -  Binary classification
KW  -  Logistic regression
KW  -  Metaheuristics
KW  -  Particle swarm optimization
KW  - Feature selection
JF  - European Journal of Operational Research
VL  - 206
IS  - 3
SP  - 528
EP  - 539
DO  - https://doi.org/10.1016/j.ejor.2010.02.032
UR  - https://www.sciencedirect.com/science/article/pii/S0377221710001633
N2  - This paper investigates the feature subset selection problem for the binary classification problem using logistic regression model. We developed a modified discrete particle swarm optimization (PSO) algorithm for the feature subset selection problem. This approach embodies an adaptive feature selection procedure which dynamically accounts for the relevance and dependence of the features included the feature subset. We compare the proposed methodology with the tabu search and scatter search algorithms using publicly available datasets. The results show that the proposed discrete PSO algorithm is competitive in terms of both classification accuracy and computational performance.
ER  - 
TY  - JOUR
T1  - Automated problem list generation and physicians perspective from a pilot study
A1  - Devarakonda, Murthy V
A1  - Mehta, Neil
A1  - Tsou, Ching-Huei
A1  - Liang, Jennifer J
A1  - Nowacki, Amy S
A1  - Jelovsek, John Eric
Y1  - 2017///
KW  -  IBM Watson
KW  -  Longitudinal patient records
KW  -  Machine learning
KW  -  Natural language processing
KW  -  Problem list
KW  - Electronic health records
JF  - International Journal of Medical Informatics
VL  - 105
SP  - 121
EP  - 129
DO  - https://doi.org/10.1016/j.ijmedinf.2017.05.015
UR  - https://www.sciencedirect.com/science/article/pii/S1386505617301648
N2  - Objective
An accurate, comprehensive and up-to-date problem list can help clinicians provide patient-centered care. Unfortunately, problem lists created and maintained in electronic health records by providers tend to be inaccurate, duplicative and out of date. With advances in machine learning and natural language processing, it is possible to automatically generate a problem list from the data in the EHR and keep it current. In this paper, we describe an automated problem list generation method and report on insights from a pilot study of physicians’ assessment of the generated problem lists compared to existing providers-curated problem lists in an institution’s EHR system.
Materials and methods
The natural language processing and machine learning-based Watson11Watson, mentioned here, refers to new methods developed for patient record text analytics, including the automated problem list generation, based on the core Watson text processing tools for sentence segmentation, parsing, and named entity linking. method models clinical thinking in identifying a patient’s problem list using clinical notes and structured data. This pilot study assessed the Watson method and included 15 randomly selected, de-identified patient records from a large healthcare system that were each planned to be reviewed by at least two internal medicine physicians. The physicians created their own problem lists, and then evaluated the overall usefulness of their own problem lists (P), Watson generated problem lists (W), and the existing EHR problem lists (E) on a 10-point scale. The primary outcome was pairwise comparisons of P, W, and E.
Results
Six out of the 10 invited physicians completed 27 assessments of P, W, and E, and in process evaluated 732 Watson generated problems and 444 problems in the EHR system. As expected, physicians rated their own lists, P, highest. However, W was rated higher than E. Among 89% of assessments, Watson identified at least one important problem that physicians missed.
Conclusion
Cognitive computing systems like this Watson system hold the potential for accurate, problem-list-centered summarization of patient records, potentially leading to increased efficiency, better clinical decision support, and improved quality of patient care.
ER  - 
TY  - JOUR
T1  - Comparison between neural networks and multiple logistic regression to predict acute coronary syndrome in the emergency room
A1  - Green, Michael
A1  - Björk, Jonas
A1  - Forberg, Jakob
A1  - Ekelund, Ulf
A1  - Edenbrandt, Lars
A1  - Ohlsson, Mattias
Y1  - 2006///
KW  -  Acute coronary syndrome
KW  -  Acute myocardial infarction
KW  -  Clinical decision support
KW  -  Ensemble methods
KW  -  Logistic regression
KW  - Artificial neural networks
JF  - Artificial Intelligence in Medicine
VL  - 38
IS  - 3
SP  - 305
EP  - 318
DO  - https://doi.org/10.1016/j.artmed.2006.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0933365706001059
N2  - Summary
Objective
Patients with suspicion of acute coronary syndrome (ACS) are difficult to diagnose and they represent a very heterogeneous group. Some require immediate treatment while others, with only minor disorders, may be sent home. Detecting ACS patients using a machine learning approach would be advantageous in many situations.
Methods and materials
Artificial neural network (ANN) ensembles and logistic regression models were trained on data from 634 patients presenting an emergency department with chest pain. Only data immediately available at patient presentation were used, including electrocardiogram (ECG) data. The models were analyzed using receiver operating characteristics (ROC) curve analysis, calibration assessments, inter- and intra-method variations. Effective odds ratios for the ANN ensembles were compared with the odds ratios obtained from the logistic model.
Results
The ANN ensemble approach together with ECG data preprocessed using principal component analysis resulted in an area under the ROC curve of 80%. At the sensitivity of 95% the specificity was 41%, corresponding to a negative predictive value of 97%, given the ACS prevalence of 21%. Adding clinical data available at presentation did not improve the ANN ensemble performance. Using the area under the ROC curve and model calibration as measures of performance we found an advantage using the ANN ensemble models compared to the logistic regression models.
Conclusion
Clinically, a prediction model of the present type, combined with the judgment of trained emergency department personnel, could be useful for the early discharge of chest pain patients in populations with a low prevalence of ACS.
ER  - 
TY  - JOUR
T1  - Expert-validated estimation of diagnostic uncertainty for deep neural networks in diabetic retinopathy detection
A1  - Ayhan, Murat Seçkin
A1  - Kühlewein, Laura
A1  - Aliyeva, Gulnar
A1  - Inhoffen, Werner
A1  - Ziemssen, Focke
A1  - Berens, Philipp
Y1  - 2020///
KW  -  Calibration
KW  -  Diabetic retinopathy
KW  -  Uncertainty
KW  - Deep neural networks
JF  - Medical Image Analysis
VL  - 64
SP  - 101724
EP  - 101724
DO  - https://doi.org/10.1016/j.media.2020.101724
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520300888
N2  - Deep learning-based systems can achieve a diagnostic performance comparable to physicians in a variety of medical use cases including the diagnosis of diabetic retinopathy. To be useful in clinical practice, it is necessary to have well calibrated measures of the uncertainty with which these systems report their decisions. However, deep neural networks (DNNs) are being often overconfident in their predictions, and are not amenable to a straightforward probabilistic treatment. Here, we describe an intuitive framework based on test-time data augmentation for quantifying the diagnostic uncertainty of a state-of-the-art DNN for diagnosing diabetic retinopathy. We show that the derived measure of uncertainty is well-calibrated and that experienced physicians likewise find cases with uncertain diagnosis difficult to evaluate. This paves the way for an integrated treatment of uncertainty in DNN-based diagnostic systems.
ER  - 
TY  - JOUR
T1  - Cost-sensitive meta-learning classifiers: MEPAR-miner and DIFACONN-miner
A1  - Kulluk, Sinem
A1  - Özbakır, Lale
A1  - Tapkan, Pınar Zarif
A1  - Baykasoğlu, Adil
Y1  - 2016///
KW  -  DIFACONN-miner
KW  -  MEPAR-miner
KW  -  Meta-learning
KW  - Cost-sensitive classification
JF  - Knowledge-Based Systems
VL  - 98
SP  - 148
EP  - 161
DO  - https://doi.org/10.1016/j.knosys.2016.01.025
UR  - https://www.sciencedirect.com/science/article/pii/S0950705116000459
N2  - Cost-sensitive learning which deals with classification problems that have non-uniform costs has attracted great attention from the machine learning and data mining communities in recent years. In this study, a rescaling based meta-learning scheme is applied to cost-insensitive MEPAR-miner and DIFACONN-miner algorithms which were previously developed by the authors in order to make the algorithms cost-sensitive. Rescaling is realized in two ways by means of oversampling and undersampling by resampling the training instances in proportion to their costs. The proposed algorithms can extract rules for both binary and n-ary classification problems and also handle data sets that have missing values. An extensive computational study is performed on different types of classification benchmark problems with the aim of testing the performances of the algorithms. Comparisons with traditional cost-sensitive meta-learning algorithms and cost-insensitive MEPAR-miner and DIFACONN-miner algorithms show that the proposed cost-sensitive algorithms are competitive meta-learning algorithms and able to produce accurate and effective classification rules with low misclassification costs.
ER  - 
TY  - JOUR
T1  - The woven fiber organic electrochemical transistors based on polypyrrole nanowires/reduced graphene oxide composites for glucose sensing
A1  - Wang, Yuedan
A1  - Qing, Xing
A1  - Zhou, Quan
A1  - Zhang, Yang
A1  - Liu, Qiongzhen
A1  - Liu, Ke
A1  - Wang, Wenwen
A1  - Li, Mufang
A1  - Lu, Zhentan
A1  - Chen, Yuanli
A1  - Wang, Dong
Y1  - 2017///
KW  -  Fiber
KW  -  Glucose sensor
KW  -  Polypyrrole nanowires
KW  -  Reduced graphene oxide
KW  - Organic electrochemical transistors
JF  - Biosensors and Bioelectronics
VL  - 95
SP  - 138
EP  - 145
DO  - https://doi.org/10.1016/j.bios.2017.04.018
UR  - https://www.sciencedirect.com/science/article/pii/S0956566317302658
N2  - Novel woven fiber organic electrochemical transistors based on polypyrrole (PPy) nanowires and reduced graphene oxide (rGO) have been prepared. SEM revealed that the introduction of rGO nanosheets could induce the growth and increase the amount of PPy nanowires. Moreover, it could enhance the electrical performance of fiber transistors. The hybrid transistors showed high on/off ratio of 102, fast switch speed, and long cycling stability. The glucose sensors based on the fiber organic electrochemical transistors have also been investigated, which exhibited outstanding sensitivity, as high as 0.773 NCR/decade, with a response time as fast as 0.5s, a linear range of 1nM to 5μM, a low detection concentration as well as good repeatability. In addition, the glucose could be selectively detected in the presence of ascorbic acid and uric acid interferences. The reliability of the proposed glucose sensor was evaluated in real samples of rabbit blood. All the results indicate that the novel fiber transistors pave the way for portable and wearable electronics devices, which have a promising future for healthcare and biological applications.
ER  - 
TY  - JOUR
T1  - A review of biosensor technologies for blood biomarkers toward monitoring cardiovascular diseases at the point-of-care
A1  - Ouyang, Mengxing
A1  - Tu, Dandan
A1  - Tong, Lin
A1  - Sarwar, Mehenur
A1  - Bhimaraj, Arvind
A1  - Li, Chenzhong
A1  - Coté, Gerard L
A1  - Di Carlo, Dino
Y1  - 2021///
KW  -  Cardiac biomarkers
KW  -  Cardiovascular disease
KW  -  Commercial diagnostic devices
KW  -  Multiplexed biomarker panel
KW  -  Point-of-care monitoring
KW  - Biosensors
JF  - Biosensors and Bioelectronics
VL  - 171
SP  - 112621
EP  - 112621
DO  - https://doi.org/10.1016/j.bios.2020.112621
UR  - https://www.sciencedirect.com/science/article/pii/S0956566320306114
N2  - Cardiovascular diseases (CVDs) cause significant mortality globally. Notably, CVDs disproportionately negatively impact underserved populations, such as those that are economically disadvantaged and often located in remote regions. Devices to measure cardiac biomarkers have traditionally been focused on large instruments in a central laboratory but the development of affordable, portable devices that measure multiple cardiac biomarkers at the point-of-care (POC) are needed to improve clinical outcomes for patients, especially in underserved populations. Considering the enormity of the global CVD problem, complexity of CVDs, and the large candidate pool of biomarkers, it is of great interest to evaluate and compare biomarker performance and identify potential multiplexed panels that can be used in combination with affordable and robust biosensors at the POC toward improved patient care. This review focuses on describing the known and emerging CVD biosensing technologies for analysis of cardiac biomarkers from blood. Initially, the global burden of CVDs and the standard of care for the primary CVD categories, namely heart failure (HF) and acute coronary syndrome (ACS) including myocardial infarction (MI) are discussed. The latest United States, Canadian and European society guidelines recommended standalone, emerging, and add-on cardiac biomarkers, as well as their combinations are then described for the prognosis, diagnosis, and risk stratification of CVDs. Finally, both commercial in vitro biosensing devices and recent state-of-art techniques for detection of cardiac biomarkers are reviewed that leverage single and multiplexed panels of cardiac biomarkers with a view toward affordable, compact devices with excellent performance for POC diagnosis and monitoring.
ER  - 
TY  - JOUR
T1  - Stacked ensemble combined with fuzzy matching for biomedical named entity recognition of diseases
A1  - Bhasuran, Balu
A1  - Murugesan, Gurusamy
A1  - Abdulkadhar, Sabenabanu
A1  - Natarajan, Jeyakumar
Y1  - 2016///
KW  -  Biomedical named entity recognition
KW  -  Fuzzy matching
KW  -  Machine learning
KW  -  Stacked ensemble
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 64
SP  - 1
EP  - 9
DO  - https://doi.org/10.1016/j.jbi.2016.09.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416301216
N2  - Biomedical Named Entity Recognition (Bio-NER) is the crucial initial step in the information extraction process and a majorly focused research area in biomedical text mining. In the past years, several models and methodologies have been proposed for the recognition of semantic types related to gene, protein, chemical, drug and other biological relevant named entities. In this paper, we implemented a stacked ensemble approach combined with fuzzy matching for biomedical named entity recognition of disease names. The underlying concept of stacked generalization is to combine the outputs of base-level classifiers using a second-level meta-classifier in an ensemble. We used Conditional Random Field (CRF) as the underlying classification method that makes use of a diverse set of features, mostly based on domain specific, and are orthographic and morphologically relevant. In addition, we used fuzzy string matching to tag rare disease names from our in-house disease dictionary. For fuzzy matching, we incorporated two best fuzzy search algorithms Rabin Karp and Tuned Boyer Moore. Our proposed approach shows promised result of 94.66%, 89.12%, 84.10%, and 76.71% of F-measure while on evaluating training and testing set of both NCBI disease and BioCreative V CDR Corpora.
ER  - 
TY  - JOUR
T1  - A framework for healthcare support in the rural and low income areas of the developing world
A1  - Bagula, A
A1  - Mandava, M
A1  - Bagula, H
Y1  - 2018///
KW  -  Artificial intelligence
KW  -  Cyber physical healthcare system
KW  -  Cyber-healthcare
KW  -  Machine learning
KW  -  Public health
KW  - Fog computing
JF  - Journal of Network and Computer Applications
VL  - 120
SP  - 17
EP  - 29
DO  - https://doi.org/10.1016/j.jnca.2018.06.010
UR  - https://www.sciencedirect.com/science/article/pii/S1084804518302182
N2  - Cyber-Healthcare is an emerging field of the healthcare domain that builds upon cyber physical health systems (CPHSs) to provide pervasive access to medical services any time and from anywhere in the world where medical expertise is available. It is expected to change the way healthcare is delivered in the developing world and enable both its rural and urban settings to leapfrog from poorly equipped to medically prepared environments capable of tackling some of its most challenging health issues. However, owing to their infancy stage in the developing world, CPHSs require substantial research and practical work to move from their theoretical boundaries into the development, deployment and exploitation phase. This paper proposes a Cyber-Healthcare framework and its implementation as a fog-based CPHS infrastructure using low-cost lightweight devices to achieve patients' condition recognition as a first step towards the implementation of digital healthcare support systems in the developing world. We propose a multi-layer architecture for the framework and consider a patients' condition recognition system that uses machine learning techniques as a key component of the framework. We present experimental results that reveal i) the relative efficiency of different machine learning algorithms used for patient condition recognition and ii) the storage and processing overheads incurred by two popular lightweight embedded devices when used as fog computing devices in the CPHS.
ER  - 
TY  - JOUR
T1  - Extreme learning machine based transfer learning algorithms: A survey
A1  - Salaken, Syed Moshfeq
A1  - Khosravi, Abbas
A1  - Nguyen, Thanh
A1  - Nahavandi, Saeid
Y1  - 2017///
KW  -  Extreme learning machine
KW  - Transfer learning
JF  - Neurocomputing
VL  - 267
SP  - 516
EP  - 524
DO  - https://doi.org/10.1016/j.neucom.2017.06.037
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217311657
N2  - Extreme learning machine (ELM) has been increasingly popular in the field of transfer learning (TL) due to its simplicity, training speed and ease of use in online sequential learning process. This paper critically examines transfer learning algorithms formulated with ELM technique and provides state of the art knowledge to expedite the learning process ELM based TL algorithms. As this article discusses available ELM based TL algorithm in detail, it provides a holistic overview of current literature, serves as a starting point for new researchers in ELM based TL algorithms and facilitates identification of future research direction in concise manner.
ER  - 
TY  - JOUR
T1  - Clustering clinical trials with similar eligibility criteria features
A1  - Hao, Tianyong
A1  - Rusanov, Alexander
A1  - Boland, Mary Regina
A1  - Weng, Chunhua
Y1  - 2014///
KW  -  Clinical trial
KW  -  Cluster analysis
KW  - Medical informatics
JF  - Journal of Biomedical Informatics
VL  - 52
SP  - 112
EP  - 120
DO  - https://doi.org/10.1016/j.jbi.2014.01.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414000112
N1  - Special Section: Methods in Clinical Research Informatics
N2  - Objectives
To automatically identify and cluster clinical trials with similar eligibility features.
Methods
Using the public repository ClinicalTrials.gov as the data source, we extracted semantic features from the eligibility criteria text of all clinical trials and constructed a trial-feature matrix. We calculated the pairwise similarities for all clinical trials based on their eligibility features. For all trials, by selecting one trial as the center each time, we identified trials whose similarities to the central trial were greater than or equal to a predefined threshold and constructed center-based clusters. Then we identified unique trial sets with distinctive trial membership compositions from center-based clusters by disregarding their structural information.
Results
From the 145,745 clinical trials on ClinicalTrials.gov, we extracted 5,508,491 semantic features. Of these, 459,936 were unique and 160,951 were shared by at least one pair of trials. Crowdsourcing the cluster evaluation using Amazon Mechanical Turk (MTurk), we identified the optimal similarity threshold, 0.9. Using this threshold, we generated 8806 center-based clusters. Evaluation of a sample of the clusters by MTurk resulted in a mean score 4.331±0.796 on a scale of 1–5 (5 indicating “strongly agree that the trials in the cluster are similar”).
Conclusions
We contribute an automated approach to clustering clinical trials with similar eligibility features. This approach can be potentially useful for investigating knowledge reuse patterns in clinical trial eligibility criteria designs and for improving clinical trial recruitment. We also contribute an effective crowdsourcing method for evaluating informatics interventions.
ER  - 
TY  - JOUR
T1  - Pharmacovigilance from social media: An improved random subspace method for identifying adverse drug events
A1  - Liu, Jing
A1  - Wang, Gang
Y1  - 2018///
KW  - Adverse drug event identification
KW  - Random subspace
KW  - Social media
KW  - Stratified sampling
JF  - International Journal of Medical Informatics
VL  - 117
SP  - 33
EP  - 43
DO  - https://doi.org/10.1016/j.ijmedinf.2018.06.008
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618304416
N2  - Objective
Recent advances in Web 2.0 technologies have seen significant strides towards utilizing patient-generated content for pharmacovigilance. Social media-based pharmacovigilance has great potential to augment current efforts and provide regulatory authorities with valuable decision aids. Among various pharmacovigilance activities, identifying adverse drug events (ADEs) is very important for patient safety. However, in health-related discussion forums, ADEs may confound with drug indications and beneficial effects, etc. Therefore, the focus of this study is to develop a strategy to identify ADEs from other semantic types, and meanwhile to determine the drug that an ADE is associated with.
Materials and methods
In this study, two groups of features, i.e., shallow linguistic features and semantic features, are explored. Moreover, motivated and inspired by the characteristics of explored two feature categories for social media-based ADE identification, an improved random subspace method, called Stratified Sampling-based Random Subspace (SSRS), is proposed. Unlike conventional random subspace method that applies random sampling for subspace selection, SSRS adopts stratified sampling-based subspace selection strategy.
Results
A case study on heart disease discussion forums is performed to evaluate the effectiveness of the SSRS method. Experimental results reveal that the proposed SSRS method significantly outperforms other compared ensemble methods and existing approaches for ADE identification.
Discussion and conclusion
Our proposed method is easy to implement since it is based on two feature sets that can be naturally derived, and therefore, can omit artificial stratum generation efforts. Moreover, SSRS has great potential of being applied to deal with other high-dimensional problems that can represent original data from two different aspects.
ER  - 
TY  - JOUR
T1  - Conceptual-driven classification for coding advise in health insurance reimbursement
A1  - Li, Sheng-Tun
A1  - Chen, Chih-Chuan
A1  - Huang, Fernando
Y1  - 2011///
KW  -  Fuzzy formal concept analysis
KW  -  Health insurance
KW  -  ICD code
KW  -  Information retrieval
KW  -  Text mining
KW  - Knowledge discovery
JF  - Artificial Intelligence in Medicine
VL  - 51
IS  - 1
SP  - 27
EP  - 41
DO  - https://doi.org/10.1016/j.artmed.2010.10.003
UR  - https://www.sciencedirect.com/science/article/pii/S0933365710001223
N2  - Objective
With the non-stop increases in medical treatment fees, the economic survival of a hospital in Taiwan relies on the reimbursements received from the Bureau of National Health Insurance, which in turn depend on the accuracy and completeness of the content of the discharge summaries as well as the correctness of their International Classification of Diseases (ICD) codes. The purpose of this research is to enforce the entire disease classification framework by supporting disease classification specialists in the coding process.
Methodology
This study developed an ICD code advisory system (ICD-AS) that performed knowledge discovery from discharge summaries and suggested ICD codes. Natural language processing and information retrieval techniques based on Zipf's Law were applied to process the content of discharge summaries, and fuzzy formal concept analysis was used to analyze and represent the relationships between the medical terms identified by MeSH. In addition, a certainty factor used as reference during the coding process was calculated to account for uncertainty and strengthen the credibility of the outcome.
Results
Two sets of 360 and 2579 textual discharge summaries of patients suffering from cerebrovascular disease was processed to build up ICD-AS and to evaluate the prediction performance. A number of experiments were conducted to investigate the impact of system parameters on accuracy and compare the proposed model to traditional classification techniques including linear-kernel support vector machines. The comparison results showed that the proposed system achieves the better overall performance in terms of several measures. In addition, some useful implication rules were obtained, which improve comprehension of the field of cerebrovascular disease and give insights to the relationships between relevant medical terms.
Conclusion
Our system contributes valuable guidance to disease classification specialists in the process of coding discharge summaries, which consequently brings benefits in aspects of patient, hospital, and healthcare system.
ER  - 
TY  - JOUR
T1  - Cognitive gravitation model for classification on small noisy data
A1  - Wen, Guihua
A1  - Wei, Jia
A1  - Wang, Jiabing
A1  - Zhou, Tiangang
A1  - Chen, L
Y1  - 2013///
KW  -  Classification
KW  -  Gravitation model
KW  -  Nearest neighbors
KW  - Cognitive laws
JF  - Neurocomputing
VL  - 118
SP  - 245
EP  - 252
DO  - https://doi.org/10.1016/j.neucom.2013.02.033
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213003317
N2  - When performing the classification on the high dimensional, the sparse, or the noisy data, many approaches easily lead to the dramatic performance degradation. To deal with this issue from the different perspective, this paper proposes a cognitive gravitation model (CGM) based on both the law of gravitation in physics and the cognitive laws, where the self-information of each sample instead of mass is applied. Subsequently, a new classifier is designed which utilizes CGM to find k nearest neighbors from each class for the query sample and then classifies this query sample to the class whose cognitive gravitation is largest. The cognitive gravitation of the class is defined as the sum of the cognitive gravitation between its each nearest neighbor and the query sample. The advantage of our approach is that it has a firm and simple mathematical basis while it has good classification performance. The conducted experiments on challenging benchmark data sets validate the proposed model and the classification approach.
ER  - 
TY  - JOUR
T1  - Learning kernel logistic regression in the presence of class label noise
A1  - Bootkrajang, Jakramate
A1  - Kabán, Ata
Y1  - 2014///
KW  -  Label noise
KW  -  Model selection
KW  -  Multiple Kernel Learning
KW  - Classification
JF  - Pattern Recognition
VL  - 47
IS  - 11
SP  - 3641
EP  - 3655
DO  - https://doi.org/10.1016/j.patcog.2014.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S0031320314001927
N2  - The classical machinery of supervised learning machines relies on a correct set of training labels. Unfortunately, there is no guarantee that all of the labels are correct. Labelling errors are increasingly noticeable in today׳s classification tasks, as the scale and difficulty of these tasks increases so much that perfect label assignment becomes nearly impossible. Several algorithms have been proposed to alleviate the problem of which a robust Kernel Fisher Discriminant is a successful example. However, for classification, discriminative models are of primary interest, and rather curiously, the very few existing label-robust discriminative classifiers are limited to linear problems. In this paper, we build on the widely used and successful kernelising technique to introduce a label-noise robust Kernel Logistic Regression classifier. The main difficulty that we need to bypass is how to determine the model complexity parameters when no trusted validation set is available. We propose to adapt the Multiple Kernel Learning approach for this new purpose, together with a Bayesian regularisation scheme. Empirical results on 13 benchmark data sets and two real-world applications demonstrate the success of our approach.
ER  - 
TY  - JOUR
T1  - A GMM-IG framework for selecting genes as expression panel biomarkers
A1  - Wang, Mingyi
A1  - Chen, Jake Y
Y1  - 2010///
KW  -  Data integration
KW  -  Gaussian mixture model
KW  -  Information gain
KW  -  Lung cancer
KW  -  Microarray data
KW  - Gene selection
JF  - Artificial Intelligence in Medicine
VL  - 48
IS  - 2
SP  - 75
EP  - 82
DO  - https://doi.org/10.1016/j.artmed.2009.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0933365709000979
N1  - Artificial Intelligence in Biomedical Engineering and Informatics
N2  - Objective
The limitation of small sample size of functional genomics experiments has made it necessary to integrate DNA microarray experimental data from different sources. However, experimentation noises and biases of different microarray platforms have made integrated data analysis challenging. In this work, we propose an integrative computational framework to identify candidate biomarker genes from publicly available functional genomics studies.
Methods
We developed a new framework, Gaussian Mixture Modeling-Coupled Information Gain (GMM-IG). In this framework, we first apply a two-component Gaussian mixture model (GMM) to estimate the conditional probability distributions of gene expression data between two different types of samples, for example, normal versus cancer. An expectation-maximization algorithm is then used to estimate the maximum likelihood parameters of a mixture of two Gaussian models in the feature space and determine the underlying expression levels of genes. Gene expression results from different studies are discretized, based on GMM estimations and then unified. Significantly differentially-expressed genes are filtered and assessed with information gain (IG) measures.
Results
DNA microarray experimental data for lung cancers from three different prior studies was processed using the new GMM-IG method. Target gene markers from a gene expression panel were selected and compared with several conventional computational biomarker data analysis methods. GMM-IG showed consistently high accuracy for several classification assessments. A high reproducibility of gene selection results was also determined from statistical validations. Our study shows that the GMM-IG framework can overcome poor reliability issues from single-study DNA microarray experiment while maintaining high accuracies by combining true signals from multiple studies.
Conclusions
We present a conceptually simple framework that enables reliable integration of true differential gene expression signals from multiple microarray experiments. This novel computational method has been shown to generate interesting biomarker panels for lung cancer studies. It is promising as a general strategy for future panel biomarker development, especially for applications that requires integrating experimental results generated from different research centers or with different technology platforms.
ER  - 
TY  - JOUR
T1  - Heteroscedastic linear feature extraction based on sufficiency conditions
A1  - Mahanta, Mohammad Shahin
A1  - S. Aghaei, Amirhossein
A1  - Plataniotis, Konstantinos N
A1  - Pasupathy, Subbarayan
Y1  - 2012///
KW  -  Dimension reduction
KW  -  Discriminant analysis
KW  -  Gaussianity
KW  -  Heteroscedastic data
KW  -  Quadratic classifier
KW  -  Sufficient statistic
KW  - Feature extraction
JF  - Pattern Recognition
VL  - 45
IS  - 2
SP  - 821
EP  - 830
DO  - https://doi.org/10.1016/j.patcog.2011.07.024
UR  - https://www.sciencedirect.com/science/article/pii/S0031320311003128
N2  - Classification of high-dimensional data typically requires extraction of discriminant features. This paper proposes a linear feature extractor, called whitened linear sufficient statistic (WLSS), which is based on the sufficiency conditions for heteroscedastic Gaussian distributions. WLSS approximates, in the least squares sense, an operator providing a sufficient statistic. The proposed method retains covariance discriminance in heteroscedastic data, while it reduces to the commonly used linear discriminant analysis (LDA) in the homoscedastic case. Compared to similar heteroscedastic methods, WLSS imposes a low computational complexity, and is highly generalizable as confirmed by its consistent competence over various data sets.
ER  - 
TY  - JOUR
T1  - Deep learning approaches for automatic detection of sleep apnea events from an electrocardiogram
A1  - Erdenebayar, Urtnasan
A1  - Kim, Yoon Ji
A1  - Park, Jong-Uk
A1  - Joo, Eun Yeon
A1  - Lee, Kyoung-Joung
Y1  - 2019///
KW  -  Convolutional neural network
KW  -  Deep learning
KW  -  Gated-recurrent unit
KW  -  Long short-term memory
KW  -  Recurrent neural network
KW  - Sleep apnea
JF  - Computer Methods and Programs in Biomedicine
VL  - 180
SP  - 105001
EP  - 105001
DO  - https://doi.org/10.1016/j.cmpb.2019.105001
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719303086
N2  - Background and Objective
This study demonstrates deep learning approaches with an aim to find the optimal method to automatically detect sleep apnea (SA) events from an electrocardiogram (ECG) signal.
Methods
Six deep learning approaches were designed and implemented for automatic detection of SA events including deep neural network (DNN), one-dimensional (1D) convolutional neural networks (CNN), two-dimensional (2D) CNN, recurrent neural networks (RNN), long short-term memory, and gated-recurrent unit (GRU). Designed deep learning models were analyzed and compared in the performances. The ECG signal was pre-processed, normalized, and segmented into 10 s intervals. Subsequently, the signal was converted into a 2D form for analysis in the 2D CNN model. A dataset collected from 86 patients with SA was used. The training set comprised data from 69 of the patients, while the test set contained data from the remaining 17 patients.
Results
The accuracy of the best-performing model was 99.0%, and the 1D CNN and GRU models had 99.0% recall rates.
Conclusions
The designed deep learning approaches performed better than those developed and tested in previous studies in terms of detecting SA events, and they could distinguish between apnea and hypopnea events using an ECG signal. The deep learning approaches such as 1D CNN and GRU can be helpful tools to automatically detect SA in sleep apnea screening and related studies.
ER  - 
TY  - JOUR
T1  - A general framework for time series data mining based on event analysis: Application to the medical domains of electroencephalography and stabilometry
A1  - Lara, Juan A
A1  - Lizcano, David
A1  - Pérez, Aurora
A1  - Valente, Juan P
Y1  - 2014///
KW  -  Classification
KW  -  Electroencephalography
KW  -  Event
KW  -  Stabilometry
KW  -  Time series analysis
KW  - Medical data mining
JF  - Journal of Biomedical Informatics
VL  - 51
SP  - 219
EP  - 241
DO  - https://doi.org/10.1016/j.jbi.2014.06.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414001415
N2  - There are now domains where information is recorded over a period of time, leading to sequences of data known as time series. In many domains, like medicine, time series analysis requires to focus on certain regions of interest, known as events, rather than analyzing the whole time series. In this paper, we propose a framework for knowledge discovery in both one-dimensional and multidimensional time series containing events. We show how our approach can be used to classify medical time series by means of a process that identifies events in time series, generates time series reference models of representative events and compares two time series by analyzing the events they have in common. We have applied our framework on time series generated in the areas of electroencephalography (EEG) and stabilometry. Framework performance was evaluated in terms of classification accuracy, and the results confirmed that the proposed schema has potential for classifying EEG and stabilometric signals. The proposed framework is useful for discovering knowledge from medical time series containing events, such as stabilometric and electroencephalographic time series. These results would be equally applicable to other medical domains generating iconographic time series, such as, for example, electrocardiography (ECG).
ER  - 
TY  - JOUR
T1  - A hybrid of whale optimization and late acceptance hill climbing based imputation to enhance classification performance in electronic health records
A1  - Nagarajan, Gayathri
A1  - Dhinesh Babu, L D
Y1  - 2019///
KW  -  Electronic health records
KW  -  Late acceptance hill climbing
KW  -  Machine learning
KW  -  Whale optimization
KW  - Missing data imputation
JF  - Journal of Biomedical Informatics
VL  - 94
SP  - 103190
EP  - 103190
DO  - https://doi.org/10.1016/j.jbi.2019.103190
UR  - https://www.sciencedirect.com/science/article/pii/S153204641930108X
N2  - Electronic health records (EHR) are a major source of information in biomedical informatics. Yet, missing values are prominent characteristics of EHR. Prediction on dataset with missing values results in inaccurate inferences. Nearest neighbour imputation based on lazy learning approach is a proven technique for missing data imputation and is recognized as one among the top ten data mining algorithms due to its simplicity and understandability. But its performance is deteriorated due to the curse of dimensionality as unimportant features are likely to dominate. We address this problem by proposing a novel approach for feature weighting based on a hybrid of metaheuristic whale optimization algorithm (WOA) and local search late acceptance hill climbing algorithm (LAHCA) on nearest neighbour imputation method. Our proposed approach Metaheuristic and Local Search based Feature Weighted Nearest Neighbour Imputation (kNN+LAHCAWOA) also learns different k values for different test points. Our approach is tested on benchmark EHR datasets with three proven classifiers Support Vector Machines(SVM), Random forest(RF) and Deep neural networks(DNN). The results prove that kNN+LAHCAWOA is an effective imputation strategy and aids in improving the classification performance when compared with its competitor methods.
ER  - 
TY  - JOUR
T1  - Spectral and spatiotemporal variability ECG parameters linked to catheter ablation outcome in persistent atrial fibrillation
A1  - Hidalgo-Muñoz, Antonio R
A1  - Latcu, Decebal G
A1  - Meo, Marianna
A1  - Meste, Olivier
A1  - Popescu, Irina
A1  - Saoudi, Nadir
A1  - Zarzoso, Vicente
Y1  - 2017///
KW  -  Catheter ablation
KW  -  ECG
KW  -  Logistic regression
KW  -  Predictive model
KW  -  Spatiotemporal variability
KW  -  Spectral feature
KW  - Atrial fibrillation
JF  - Computers in Biology and Medicine
VL  - 88
SP  - 126
EP  - 131
DO  - https://doi.org/10.1016/j.compbiomed.2017.07.004
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517302287
N2  - With the increasing prevalence of atrial fibrillation (AF), there is a strong clinical interest in determining whether a patient suffering from persistent AF will benefit from catheter ablation (CA) therapy at long term. This work presents several regression models based on noninvasive measures automatically computed from the standard 12-lead electrocardiogram (ECG) such as AF dominant frequency (DF), spectral concentration and spatiotemporal variability (STV). Sixty-two AF patients referred to CA were enrolled in this study. Forty-seven of them had no recurrence after CA during an average follow-up of 14 ± 8 months. The ECG features were extracted from an ECG recorded before the CA intervention and they were combined by means of logistic regression. The combination of DF and STV values from different precordial leads reached AUC = 0.939, outperforming the best results by using only one kind of features, such as DF (AUC = 0.801), and yielding a global accuracy of 93.5% for discriminating the best long-term responders to CA. These results point out the need to take into consideration the spatial variation of spectral ECG parameters to build predictive models dealing with AF.
ER  - 
TY  - JOUR
T1  - Improving accuracy for identifying related PubMed queries by an integrated approach
A1  - Lu, Zhiyong
A1  - Wilbur, W John
Y1  - 2009///
KW  -  Contextual similarity
KW  -  Lexical similarity
KW  -  PubMed query log
KW  -  Related query
KW  -  Session segmentation
KW  - PubMed distance
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 5
SP  - 831
EP  - 838
DO  - https://doi.org/10.1016/j.jbi.2008.12.006
UR  - https://www.sciencedirect.com/science/article/pii/S153204640800155X
N1  - Biomedical Natural Language Processing
N2  - PubMed is the most widely used tool for searching biomedical literature online. As with many other online search tools, a user often types a series of multiple related queries before retrieving satisfactory results to fulfill a single information need. Meanwhile, it is also a common phenomenon to see a user type queries on unrelated topics in a single session. In order to study PubMed users’ search strategies, it is necessary to be able to automatically separate unrelated queries and group together related queries. Here, we report a novel approach combining both lexical and contextual analyses for segmenting PubMed query sessions and identifying related queries and compare its performance with the previous approach based solely on concept mapping. We experimented with our integrated approach on sample data consisting of 1539 pairs of consecutive user queries in 351 user sessions. The prediction results of 1396 pairs agreed with the gold-standard annotations, achieving an overall accuracy of 90.7%. This demonstrates that our approach is significantly better than the previously published method. By applying this approach to a one day query log of PubMed, we found that a significant proportion of information needs involved more than one PubMed query, and that most of the consecutive queries for the same information need are lexically related. Finally, the proposed PubMed distance is shown to be an accurate and meaningful measure for determining the contextual similarity between biological terms. The integrated approach can play a critical role in handling real-world PubMed query log data as is demonstrated in our experiments.
ER  - 
TY  - JOUR
T1  - Unsupervised machine learning for the discovery of latent disease clusters and patient subgroups using electronic health records
A1  - Wang, Yanshan
A1  - Zhao, Yiqing
A1  - Therneau, Terry M
A1  - Atkinson, Elizabeth J
A1  - Tafti, Ahmad P
A1  - Zhang, Nan
A1  - Amin, Shreyasee
A1  - Limper, Andrew H
A1  - Khosla, Sundeep
A1  - Liu, Hongfang
Y1  - 2020///
KW  -  Aging
KW  -  Artificial intelligence
KW  -  Electronic health records
KW  -  Epidemiology
KW  - Unsupervised Machine learning
JF  - Journal of Biomedical Informatics
VL  - 102
SP  - 103364
EP  - 103364
DO  - https://doi.org/10.1016/j.jbi.2019.103364
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419302849
N2  - Machine learning has become ubiquitous and a key technology on mining electronic health records (EHRs) for facilitating clinical research and practice. Unsupervised machine learning, as opposed to supervised learning, has shown promise in identifying novel patterns and relations from EHRs without using human created labels. In this paper, we investigate the application of unsupervised machine learning models in discovering latent disease clusters and patient subgroups based on EHRs. We utilized Latent Dirichlet Allocation (LDA), a generative probabilistic model, and proposed a novel model named Poisson Dirichlet Model (PDM), which extends the LDA approach using a Poisson distribution to model patients’ disease diagnoses and to alleviate age and sex factors by considering both observed and expected observations. In the empirical experiments, we evaluated LDA and PDM on three patient cohorts, namely Osteoporosis, Delirium/Dementia, and Chronic Obstructive Pulmonary Disease (COPD)/Bronchiectasis Cohorts, with their EHR data retrieved from the Rochester Epidemiology Project (REP) medical records linkage system, for the discovery of latent disease clusters and patient subgroups. We compared the effectiveness of LDA and PDM in identifying disease clusters through the visualization of disease representations. We tested the performance of LDA and PDM in differentiating patient subgroups through survival analysis, as well as statistical analysis of demographics and Elixhauser Comorbidity Index (ECI) scores in those subgroups. The experimental results show that the proposed PDM could effectively identify distinguished disease clusters based on the latent patterns hidden in the EHR data by alleviating the impact of age and sex, and that LDA could stratify patients into differentiable subgroups with larger p-values than PDM. However, those subgroups identified by LDA are highly associated with patients’ age and sex. The subgroups discovered by PDM might imply the underlying patterns of diseases of greater interest in epidemiology research due to the alleviation of age and sex. Both unsupervised machine learning approaches could be leveraged to discover patient subgroups using EHRs but with different foci.
ER  - 
TY  - JOUR
T1  - Multiswarm heterogeneous binary PSO using win-win approach for improved feature selection in liver and kidney disease diagnosis
A1  - Gunasundari, S
A1  - Janakiraman, S
A1  - Meenambal, S
Y1  - 2018///
KW  -  Boolean particle swarm optimization
KW  -  Feature selection
KW  -  Kidney cancer
KW  -  Liver cancer
KW  -  Swarm intelligence
KW  - Binary Particle Swarm Optimization
JF  - Computerized Medical Imaging and Graphics
VL  - 70
SP  - 135
EP  - 154
DO  - https://doi.org/10.1016/j.compmedimag.2018.10.003
UR  - https://www.sciencedirect.com/science/article/pii/S0895611118305779
N2  - Feature selection is a significant preprocessing method in the classification part of an expert system. We propose a new Multiswarm Heterogeneous Binary Particle Swarm Optimization algorithm (MHBPSO) using a Win-Win approach to improve the performance of Binary Particle Swarm Optimization algorithm (BPSO) for feature selection. MHBPSO is a cooperation algorithm, which includes BPSO and its three variants such as Boolean PSO (BoPSO), Self Adjusted Hierarchical Boolean PSO (SAHBoPSO), and Catfish Self Adjusted Hierarchical Boolean PSO (CSAHBoPSO). It performs heterogeneous search on the entire solution space using four different algorithms. Each algorithm shares global best information to another algorithm to select the preeminent global best position. A variant of BoPSO, SAHBoPSO is proposed, in which leaders are identified based on the fitness values for guiding remaining particles and thus forms the hierarchical structure of leaders and its followers. Meanwhile leaders and followers are changed dynamically and consequently changed the hierarchical structure. CSAHBoPSO, which is the version of SAHBoPSO, is also proposed to avoid stagnation in the subsequent iterations. To assess the performance of the proposed algorithms CEC 2013 benchmark functions are employed. In order to validate the proposed algorithms, comparative study with BPSO, BoPSO, VPSO (Mirjalili and Lewis, 2013), HBPSOGA (Wang et al., 2018) and CatfishBPSO (Chuang et al., 2011a) is provided. Experimental results show that SAHBoPSO and CSAHBoPSO algorithm based on BoPSO are promising and significantly better than BPSO, BoPSO, and VPSO. MHBPSO shows the superior improvement in the search ability. In addition, proposed algorithms are tested in the feature selection phase of intelligent liver and kidney cancer diagnostic systems to select elite features from the liver and kidney cancer data. Findings show that the proposed system is proficient in selecting the elite features to classify the tumor as benign or malignant with minimum error rates.
ER  - 
TY  - JOUR
T1  - A new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients
A1  - Santos, Miriam Seoane
A1  - Abreu, Pedro Henriques
A1  - García-Laencina, Pedro J
A1  - Simão, Adélia
A1  - Carvalho, Armando
Y1  - 2015///
KW  -  Clustering
KW  -  K-means
KW  -  Oversampling
KW  -  SMOTE
KW  -  Survival prediction
KW  - Hepatocellular Carcinoma (HCC)
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - 49
EP  - 59
DO  - https://doi.org/10.1016/j.jbi.2015.09.012
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415002063
N2  - Liver cancer is the sixth most frequently diagnosed cancer and, particularly, Hepatocellular Carcinoma (HCC) represents more than 90% of primary liver cancers. Clinicians assess each patient’s treatment on the basis of evidence-based medicine, which may not always apply to a specific patient, given the biological variability among individuals. Over the years, and for the particular case of Hepatocellular Carcinoma, some research studies have been developing strategies for assisting clinicians in decision making, using computational methods (e.g. machine learning techniques) to extract knowledge from the clinical data. However, these studies have some limitations that have not yet been addressed: some do not focus entirely on Hepatocellular Carcinoma patients, others have strict application boundaries, and none considers the heterogeneity between patients nor the presence of missing data, a common drawback in healthcare contexts. In this work, a real complex Hepatocellular Carcinoma database composed of heterogeneous clinical features is studied. We propose a new cluster-based oversampling approach robust to small and imbalanced datasets, which accounts for the heterogeneity of patients with Hepatocellular Carcinoma. The preprocessing procedures of this work are based on data imputation considering appropriate distance metrics for both heterogeneous and missing data (HEOM) and clustering studies to assess the underlying patient groups in the studied dataset (K-means). The final approach is applied in order to diminish the impact of underlying patient profiles with reduced sizes on survival prediction. It is based on K-means clustering and the SMOTE algorithm to build a representative dataset and use it as training example for different machine learning procedures (logistic regression and neural networks). The results are evaluated in terms of survival prediction and compared across baseline approaches that do not consider clustering and/or oversampling using the Friedman rank test. Our proposed methodology coupled with neural networks outperformed all others, suggesting an improvement over the classical approaches currently used in Hepatocellular Carcinoma prediction models.
ER  - 
TY  - JOUR
T1  - Repolarization alternans heterogeneity in healthy subjects and acute myocardial infarction patients
A1  - Burattini, Laura
A1  - Bini, Silvia
A1  - Burattini, Roberto
Y1  - 2012///
KW  -  ECG signal processing
KW  -  T-wave alternans
KW  - Repolarization variability
JF  - Medical Engineering & Physics
VL  - 34
IS  - 3
SP  - 305
EP  - 312
DO  - https://doi.org/10.1016/j.medengphy.2011.07.019
UR  - https://www.sciencedirect.com/science/article/pii/S1350453311001846
N2  - An association between heterogeneity of repolarization alternans (RA) and cardiac electrical instability has been reported. Characterization of RA in health and identification of physiological RA heterogeneity may help discrimination of abnormal RA cases more likely associated to arrhythmic events. Thus, aim of the present study was the identification of a physiological RA region in terms of mean temporal location (MRAD) with respect to the T apex, and mean amplitude (MRAA), by application of our heart-rate adaptive match filter method to clinical ECG recordings from 51 control healthy (CH) subjects and 43 acute myocardial infarction (AMI) patients. Results indicate that RA occurring within the first half of the T wave is dominant in both CH and AMI populations (74.5% and 53.5% of cases, respectively; P<0.05). Definition of physiological RA region in the MRAD vs. MRAA plane (−83ms≤MRAD≤23ms, 0≤MRAA≤30μV) provided 0% and 32.6% abnormal RA cases among the CH subjects and AMI patients, respectively. We conclude that myocardial infarction may associate with an RA occurring early (MRAD<−83ms) or late (MRAD>23ms) along the JT segment, in addition or in alternative to an abnormally high RA amplitude (MRAA>30μV).
ER  - 
TY  - JOUR
T1  - Utilizing multiple pheromones in an ant-based algorithm for continuous-attribute classification rule discovery
A1  - Salama, Khalid M
A1  - Abdelbar, Ashraf M
A1  - Otero, Fernando E B
A1  - Freitas, Alex A
Y1  - 2013///
KW  -  Ant Colony Optimization
KW  -  Biologically inspired computing
KW  - Classification rule discovery
JF  - Applied Soft Computing
VL  - 13
IS  - 1
SP  - 667
EP  - 675
DO  - https://doi.org/10.1016/j.asoc.2012.07.026
UR  - https://www.sciencedirect.com/science/article/pii/S1568494612003456
N2  - The cAnt-Miner algorithm is an Ant Colony Optimization (ACO) based technique for classification rule discovery in problem domains which include continuous attributes. In this paper, we propose several extensions to cAnt-Miner. The main extension is based on the use of multiple pheromone types, one for each class value to be predicted. In the proposed μcAnt-Miner algorithm, an ant first selects a class value to be the consequent of a rule and the terms in the antecedent are selected based on the pheromone levels of the selected class value; pheromone update occurs on the corresponding pheromone type of the class value. The pre-selection of a class value also allows the use of more precise measures for the heuristic function and the dynamic discretization of continuous attributes, and further allows for the use of a rule quality measure that directly takes into account the confidence of the rule. Experimental results on 20 benchmark datasets show that our proposed extension improves classification accuracy to a statistically significant extent compared to cAnt-Miner, and has classification accuracy similar to the well-known Ripper and PART rule induction algorithms.
ER  - 
TY  - JOUR
T1  - IOT based sustainable diabetic retinopathy diagnosis system
A1  - Jemima Jebaseeli, T
A1  - Anand Deva Durai, C
A1  - Dinesh Peter, J
Y1  - 2020///
KW  -  Diabetic retinopathy
KW  -  Health care
KW  -  Image segmentation
KW  -  Internet of things
KW  -  Smart devices
KW  - Sustainable computing
JF  - Sustainable Computing: Informatics and Systems
VL  - 28
SP  - 100272
EP  - 100272
DO  - https://doi.org/10.1016/j.suscom.2018.08.004
UR  - https://www.sciencedirect.com/science/article/pii/S2210537918301872
N2  - Sustainable computing provides a remote access to the diagnosis system for easy and fast implementation. The proposed approach measures glucose level in the blood through Dexcom G4 Plantinum sensors on diabetic patients. Based on the readings, Internet of Things (IoT) platform offer a sustainable solution to Diabetic Retinopathy. The purpose of this research is to save the life of the patient from vision loss. The process starts from the devices themselves which securely transfer information with IoT platform and pledge the common language for the mobile apps to collaborate with each other. This platform constantly gathers thousands of information from the device and store in a secure database. It incorporates the data received from IoT devices and applies analytics to predict valuable data to address clinical needs. The results exhibited by the implementation of the proposed approaches are comparable with the sophisticated systems in relations of accuracy, specificity and sensitivity. The proposed technique performs better than other techniques by achieving an average 99.58% Accuracy, 72.51% Sensitivity and 99.83% Specificity in the experimental setup.
ER  - 
TY  - JOUR
T1  - Modeling and recognition of smart grid faults by a combined approach of dissimilarity learning and one-class classification
A1  - De Santis, Enrico
A1  - Livi, Lorenzo
A1  - Sadeghian, Alireza
A1  - Rizzi, Antonello
Y1  - 2015///
KW  -  Clustering
KW  -  Dissimilarity measure learning
KW  -  Genetic algorithm
KW  -  Localized fault recognition
KW  -  One-class classification
KW  - Smart grid
JF  - Neurocomputing
VL  - 170
SP  - 368
EP  - 383
DO  - https://doi.org/10.1016/j.neucom.2015.05.112
UR  - https://www.sciencedirect.com/science/article/pii/S0925231215008632
N1  - Advances on Biological Rhythmic Pattern Generation: Experiments, Algorithms and Applications Selected Papers from the 2013 International Conference on Intelligence Science and Big Data Engineering (IScIDE 2013) Computational Energy Management in Smart Grids
N2  - Detecting faults in electrical power grids is of paramount importance, both from the electricity operator and consumer point of view. Modern electric power grids (smart grids) are equipped with smart sensors that allow to gather real-time information regarding the physical status of all components belonging to the whole infrastructure (e.g., cables and related insulation, transformers, and breakers). In real-world smart grid systems, usually, additional information that are related to the operational status of the grid are collected, such as meteorological information. Designing an efficient recognition model to discriminate faults in real-world smart grid system is hence a challenging task. This follows from the heterogeneity of the information that actually determine a typical fault condition. In this paper, we deal with the problem of modeling and recognizing faults in a real-world smart grid system, which supplies the entire city of Rome, Italy. Recognition of faults is addressed by following a combined approach of dissimilarity measures learning and one-class classification techniques. We provide here an in-depth study related to the available data and to the models based on the proposed one-class classification approach. Furthermore, we perform a comprehensive analysis of the fault recognition results by exploiting a fuzzy set based decision rule.
ER  - 
TY  - JOUR
T1  - Learning personalized ADL recognition models from few raw data
A1  - Compagnon, Paul
A1  - Lefebvre, Grégoire
A1  - Duffner, Stefan
A1  - Garcia, Christophe
Y1  - 2020///
KW  -  Activity of daily living
KW  -  EHealth
KW  -  Gated recurrent units
KW  -  Inertial measurement unit
KW  -  Matching networks
KW  - Few-shot learning
JF  - Artificial Intelligence in Medicine
VL  - 107
SP  - 101916
EP  - 101916
DO  - https://doi.org/10.1016/j.artmed.2020.101916
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719311376
N2  - Recognition of activities of daily living (ADL) is an essential component of assisted living systems based on actigraphy. This task can nowadays be performed by machine learning models which are able to automatically extract and learn relevant features but, most of time, need to be trained with large amounts of data collected on several users. In this paper, we propose an approach to learn personalized ADL recognition models from few raw data based on a specific type of neural network called matching network. The interest of this few-shot learning approach is three-fold. Firstly, people perform activities their own way and general models may average out important individual characteristics unlike personalized models that could thus achieve better performance. Secondly, gathering large quantities of annotated data from one user is time-consuming and threatens privacy in a medical context. Thirdly, matching networks are by nature weakly dependent on the classes they are trained on and can generalize easily to new activities without needing extra training, thus making them very versatile for real applications. Our results show the effectiveness of the proposed approach compared to general neural network models, even in situations with few training data.
ER  - 
TY  - JOUR
T1  - Mixture of Gaussians for distance estimation with missing data
A1  - Eirola, Emil
A1  - Lendasse, Amaury
A1  - Vandewalle, Vincent
A1  - Biernacki, Christophe
Y1  - 2014///
KW  -  Distance estimation
KW  -  Mixture model
KW  - Missing data
JF  - Neurocomputing
VL  - 131
SP  - 32
EP  - 42
DO  - https://doi.org/10.1016/j.neucom.2013.07.050
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213010990
N2  - Many data sets have missing values in practical application contexts, but the majority of commonly studied machine learning methods cannot be applied directly when there are incomplete samples. However, most such methods only depend on the relative differences between samples instead of their particular values, and thus one useful approach is to directly estimate the pairwise distances between all samples in the data set. This is accomplished by fitting a Gaussian mixture model to the data, and using it to derive estimates for the distances. A variant of the model for high-dimensional data with missing values is also studied. Experimental simulations confirm that the proposed method provides accurate estimates compared to alternative methods for estimating distances. In particular, using the mixture model for estimating distances is on average more accurate than using the same model to impute any missing values and then calculating distances. The experimental evaluation additionally shows that more accurately estimating distances lead to improved prediction performance for classification and regression tasks when used as inputs for a neural network.
ER  - 
TY  - JOUR
T1  - Application of attention network test and demographic information to detect mild cognitive impairment via combining feature selection with support vector machine
A1  - Lv, Shipin
A1  - Wang, Xiukun
A1  - Cui, Yifen
A1  - Jin, Jue
A1  - Sun, Yan
A1  - Tang, Yiyuan
A1  - Bai, Ying
A1  - Wang, Yan
A1  - Zhou, Li
Y1  - 2010///
KW  -  Attention
KW  -  Mild cognitive impairment
KW  -  Support vector machine
KW  - Feature selection
JF  - Computer Methods and Programs in Biomedicine
VL  - 97
IS  - 1
SP  - 11
EP  - 18
DO  - https://doi.org/10.1016/j.cmpb.2009.05.003
UR  - https://www.sciencedirect.com/science/article/pii/S016926070900162X
N2  - Mild cognitive impairment (MCI) is now thought as the prodromal phase of Alzheimer's disease (AD), and the usual method for diagnosing the disease would be a battery of neuropsychological assessment. The present study proposes to integrate a feature selection scheme with support vector machine (SVM) to identify patients with MCI by using attention network test (ANT) and demographic data. Forty-two patients with MCI and forty-five normal individuals underwent ANT recording, and the reaction time and accuracy of ANT and demographics (age, gender, and educational level) were selected as original features. To select features, we first introduced some random variables as probe features in the original data, then ranked all the features according to their influence on the support vector machine decision function, and finally selected those features that had an influence higher than that of the probes. Initially 18 different features were reduced to only four features by our method. SVM classifier created by using these four features gave an 85% classification accuracy with a sensitivity of 85% and a specificity of 86%. And the area under the curve obtained by receiver operating characteristics analysis was 0.918. The experimental results demonstrate that the proposed method is a good potential use to assist identifying patients with MCI objectively and efficiently.
ER  - 
TY  - JOUR
T1  - Automated macular pathology diagnosis in retinal OCT images using multi-scale spatial pyramid and local binary patterns in texture and shape encoding
A1  - Liu, Yu-Ying
A1  - Chen, Mei
A1  - Ishikawa, Hiroshi
A1  - Wollstein, Gadi
A1  - Schuman, Joel S
A1  - Rehg, James M
Y1  - 2011///
KW  -  Local binary patterns (LBP)
KW  -  Macular pathology
KW  -  Multi-scale spatial pyramid (MSSP)
KW  -  Optical Coherence Tomography (OCT)
KW  - Computer-aided diagnosis (CAD)
JF  - Medical Image Analysis
VL  - 15
IS  - 5
SP  - 748
EP  - 759
DO  - https://doi.org/10.1016/j.media.2011.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S1361841511000922
N1  - Special Issue on the 2010 Conference on Medical Image Computing and Computer-Assisted Intervention
N2  - We address a novel problem domain in the analysis of optical coherence tomography (OCT) images: the diagnosis of multiple macular pathologies in retinal OCT images. The goal is to identify the presence of normal macula and each of three types of macular pathologies, namely, macular edema, macular hole, and age-related macular degeneration, in the OCT slice centered at the fovea. We use a machine learning approach based on global image descriptors formed from a multi-scale spatial pyramid. Our local features are dimension-reduced local binary pattern histograms, which are capable of encoding texture and shape information in retinal OCT images and their edge maps, respectively. Our representation operates at multiple spatial scales and granularities, leading to robust performance. We use 2-class support vector machine classifiers to identify the presence of normal macula and each of the three pathologies. To further discriminate sub-types within a pathology, we also build a classifier to differentiate full-thickness holes from pseudo-holes within the macular hole category. We conduct extensive experiments on a large dataset of 326 OCT scans from 136 subjects. The results show that the proposed method is very effective (all AUC>0.93).
ER  - 
TY  - JOUR
T1  - Explainable artificial intelligence in high-throughput drug repositioning for subgroup stratifications with interventionable potential
A1  - Al-Taie, Zainab
A1  - Liu, Danlu
A1  - Mitchem, Jonathan B
A1  - Papageorgiou, Christos
A1  - Kaifi, Jussuf T
A1  - Warren, Wesley C
A1  - Shyu, Chi-Ren
Y1  - 2021///
KW  -  Data mining
KW  -  Explainable AI
KW  -  Network analysis
KW  -  Subgroup stratifications
KW  - Drug repositioning
JF  - Journal of Biomedical Informatics
VL  - 118
SP  - 103792
EP  - 103792
DO  - https://doi.org/10.1016/j.jbi.2021.103792
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421001210
N2  - Enabling precision medicine requires developing robust patient stratification methods as well as drugs tailored to homogeneous subgroups of patients from a heterogeneous population. Developing de novo drugs is expensive and time consuming with an ultimately low FDA approval rate. These limitations make developing new drugs for a small portion of a disease population unfeasible. Therefore, drug repositioning is an essential alternative for developing new drugs for a disease subpopulation. This shows the importance of developing data-driven approaches that find druggable homogeneous subgroups within the disease population and reposition the drugs for these subgroups. In this study, we developed an explainable AI approach for patient stratification and drug repositioning. Contrast pattern mining and network analysis were used to discover homogeneous subgroups within a disease population. For each subgroup, a biomedical network analysis was done to find the drugs that are most relevant to a given subgroup of patients. The set of candidate drugs for each subgroup was ranked using an aggregated drug score assigned to each drug. The proposed method represents a human-in-the-loop framework, where medical experts use the data-driven results to generate hypotheses and obtain insights into potential therapeutic candidates for patients who belong to a subgroup. Colorectal cancer (CRC) was used as a case study. Patients' phenotypic and genotypic data was utilized with a heterogeneous knowledge base because it gives a multi-view perspective for finding new indications for drugs outside of their original use. Our analysis of the top candidate drugs for the subgroups identified by medical experts showed that most of these drugs are cancer-related, and most of them have the potential to be a CRC regimen based on studies in the literature.
ER  - 
TY  - JOUR
T1  - Tandem cyclic alignment
A1  - Benson, Gary
Y1  - 2005///
KW  -  Cyclic alignment
KW  -  Wraparound dynamic programming
KW  - Tandem repeats
JF  - Discrete Applied Mathematics
VL  - 146
IS  - 2
SP  - 124
EP  - 133
DO  - https://doi.org/10.1016/j.dam.2004.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S0166218X04003439
N1  - 12th Annual Symposium on Combinatorial Pattern Matching
N2  - We present a solution for the following problem. Given two sequences X=x1x2⋯xn and Y=y1y2⋯ym, n⩽m, find the best scoring alignment of X′=Xk[i] vs. Y over all possible pairs (k,i), for k=1,2,… and 1⩽i⩽n, where X[i] is the cyclic permutation of X starting at xi, Xk[i] is the concatenation of k complete copies of X[i] (k tandem copies), and the alignment must include all of Y and all of X′. Our algorithm allows any alignment scoring scheme with additive gap costs and uses O(nmlogn) time and O(nm) space. We use it to identify related tandem repeats in the C. elegans genome as part of the development of a multi-genome database of tandem repeats.
ER  - 
TY  - JOUR
T1  - Optimized model tuning in medical systems
A1  - Kléma, Jiří
A1  - Kubalík, Jiří
A1  - Lhotská, Lenka
Y1  - 2005///
KW  -  Genetic algorithm
KW  -  Predictive model: Medical decision support system
KW  -  ROC curve
KW  -  Tuning of model parameters
KW  - Instance-based learning
JF  - Computer Methods and Programs in Biomedicine
VL  - 80
SP  - S17
EP  - S28
DO  - https://doi.org/10.1016/S0169-2607(05)80003-3
UR  - https://www.sciencedirect.com/science/article/pii/S0169260705800033
N2  - Summary
In medical systems it is often advantageous to utilize specific problem situations (cases) in addition to or instead of a general model. Decisions are then based on relevant past cases retrieved from a case memory. The reliability of such decisions depends directly on the ability to identify cases of practical relevance to the current situation. This paper discusses issues of automated tuning in order to obtain a proper definition of mutual case similarity in a specific medical domain. The main focus is on a reasonably time-consuming optimization of the parameters that determine case retrieval and further utilization in decision making/ prediction. The two case studies - mortality prediction after cardiological intervention, and resource allocation at a spa - document that the optimization process is influenced by various characteristics of the problem domain.
ER  - 
TY  - JOUR
T1  - Small margin ensembles can be robust to class-label noise
A1  - Sabzevari, Maryam
A1  - Martínez-Muñoz, Gonzalo
A1  - Suárez, Alberto
Y1  - 2015///
KW  -  Bagging
KW  -  Bootstrap sampling
KW  -  Small margin classifiers
KW  - Label noise
JF  - Neurocomputing
VL  - 160
SP  - 18
EP  - 33
DO  - https://doi.org/10.1016/j.neucom.2014.12.086
UR  - https://www.sciencedirect.com/science/article/pii/S092523121500123X
N2  - Subsampling is used to generate bagging ensembles that are accurate and robust to class-label noise. The effect of using smaller bootstrap samples to train the base learners is to make the ensemble more diverse. As a result, the classification margins tend to decrease. In spite of having small margins, these ensembles can be robust to class-label noise. The validity of these observations is illustrated in a wide range of synthetic and real-world classification tasks. In the problems investigated, subsampling significantly outperforms standard bagging for different amounts of class-label noise. By contrast, the effectiveness of subsampling in random forest is problem dependent. In these types of ensembles the best overall accuracy is obtained when the random trees are built on bootstrap samples of the same size as the original training data. Nevertheless, subsampling becomes more effective as the amount of class-label noise increases.
ER  - 
TY  - JOUR
T1  - DMAKit: A user-friendly web platform for bringing state-of-the-art data analysis techniques to non-specific users
A1  - Medina-Ortiz, David
A1  - Contreras, Sebastián
A1  - Quiroz, Cristofer
A1  - Asenjo, Juan A
A1  - Olivera-Nappa, Álvaro
Y1  - 2020///
KW  -  Data mining
KW  -  Pattern recognition
KW  -  Statistics
KW  -  User-friendly web platform
KW  - Machine learning
JF  - Information Systems
VL  - 93
SP  - 101557
EP  - 101557
DO  - https://doi.org/10.1016/j.is.2020.101557
UR  - https://www.sciencedirect.com/science/article/pii/S0306437920300533
N2  - Tremendous advances in different areas of knowledge are producing vast volumes of data, a quantity so large that it has made necessary the development of new computational algorithms. Among the algorithms developed, we find Machine Learning models and specific data mining techniques that might be useful for all areas of knowledge. The use of computational tools for data analysis is increasingly required, given the need to extract meaningful information from such large volumes of data. However, there are no free access libraries, modules, or web services that comprise a vast array of analytical techniques in a user-friendly environment for non-specific users. Those that exist raise high usability barriers for those untrained in the field as they usually have specific installation requirements and require in-depth programming knowledge, or may result expensive. As an alternative, we have developed DMAKit, a user-friendly web platform powered by DMAKit-lib, a new library implemented in Python, which facilitates the analysis of data of different kind and origins. Our tool implements a wide array of state-of-the-art data mining and pattern recognition techniques, allowing the user to quickly implement classification, prediction or clustering models, statistical evaluation, and feature analysis of different attributes in diverse datasets without requiring any specific programming knowledge. DMAKit is especially useful for users who have large volumes of data to be analyzed but do not have the informatics, mathematical, or statistical knowledge to implement models. We expect this platform to provide a way to extract information and analyze patterns through data mining techniques for anyone interested in applying them with no specific knowledge required. Particularly, we present several cases of study in the areas of biology, biotechnology, and biomedicine, where we highlight the applicability of our tool to ease the labor of non-specialist users to apply data analysis and pattern recognition techniques. DMAKit is available for non-commercial use as an open-access library, licensed under the GNU General Public License, version GPL 3.0. The web platform is publicly available at https://pesb2.cl/dmakitWeb. Demonstrative and tutorial videos for the web platform are available in https://pesb2.cl/dmakittutorials/. Complete urls for relevant content are listed in the Data Availability section.
ER  - 
TY  - JOUR
T1  - Identifying potential adverse effects using the web: A new approach to medical hypothesis generation
A1  - Benton, Adrian
A1  - Ungar, Lyle
A1  - Hill, Shawndra
A1  - Hennessy, Sean
A1  - Mao, Jun
A1  - Chung, Annie
A1  - Leonard, Charles E
A1  - Holmes, John H
Y1  - 2011///
KW  -  Drug adverse effect
KW  -  Information extraction
KW  -  Medical message board
KW  - Data mining
JF  - Journal of Biomedical Informatics
VL  - 44
IS  - 6
SP  - 989
EP  - 996
DO  - https://doi.org/10.1016/j.jbi.2011.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S1532046411001237
N2  - Medical message boards are online resources where users with a particular condition exchange information, some of which they might not otherwise share with medical providers. Many of these boards contain a large number of posts and contain patient opinions and experiences that would be potentially useful to clinicians and researchers. We present an approach that is able to collect a corpus of medical message board posts, de-identify the corpus, and extract information on potential adverse drug effects discussed by users. Using a corpus of posts to breast cancer message boards, we identified drug event pairs using co-occurrence statistics. We then compared the identified drug event pairs with adverse effects listed on the package labels of tamoxifen, anastrozole, exemestane, and letrozole. Of the pairs identified by our system, 75–80% were documented on the drug labels. Some of the undocumented pairs may represent previously unidentified adverse drug effects.
ER  - 
TY  - JOUR
T1  - Concept embedding to measure semantic relatedness for biomedical information ontologies
A1  - Park, Junseok
A1  - Kim, Kwangmin
A1  - Hwang, Woochang
A1  - Lee, Doheon
Y1  - 2019///
KW  -  Embedding
KW  -  NLP
KW  -  Paragraph vector
KW  -  Similarity
KW  -  Wikipedia
KW  - UMLS
JF  - Journal of Biomedical Informatics
VL  - 94
SP  - 103182
EP  - 103182
DO  - https://doi.org/10.1016/j.jbi.2019.103182
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419301005
N2  - There have been many attempts to identify relationships among concepts corresponding to terms from biomedical information ontologies such as the Unified Medical Language System (UMLS). In particular, vector representation of such concepts using information from UMLS definition texts is widely used to measure the relatedness between two biological concepts. However, conventional relatedness measures have a limited range of applicable word coverage, which limits the performance of these models. In this paper, we propose a concept-embedding model of a UMLS semantic relatedness measure to overcome the limitations of earlier models. We obtained context texts of biological concepts that are not defined in UMLS by utilizing Wikipedia as an external knowledgebase. Concept vector representations were then derived from the context texts of the biological concepts. The degree of relatedness between two concepts was defined as the cosine similarity between corresponding concept vectors. As a result, we validated that our method provides higher coverage and better performance than the conventional method.
ER  - 
TY  - JOUR
T1  - A new nonlinear classifier with a penalized signed fuzzy measure using effective genetic algorithm
A1  - Fang, Hua
A1  - Rizzo, Maria L
A1  - Wang, Honggang
A1  - Espy, Kimberly Andrews
A1  - Wang, Zhenyuan
Y1  - 2010///
KW  -  Classification
KW  -  Genetic algorithm
KW  -  Optimization
KW  -  Signed fuzzy measure
KW  - Choquet integral
JF  - Pattern Recognition
VL  - 43
IS  - 4
SP  - 1393
EP  - 1401
DO  - https://doi.org/10.1016/j.patcog.2009.10.006
UR  - https://www.sciencedirect.com/science/article/pii/S0031320309003793
N2  - This paper proposes a new nonlinear classifier based on a generalized Choquet integral with signed fuzzy measures to enhance the classification accuracy and power by capturing all possible interactions among two or more attributes. This generalized approach was developed to address unsolved Choquet-integral classification issues such as allowing for flexible location of projection lines in n-dimensional space, automatic search for the least misclassification rate based on Choquet distance, and penalty on misclassified points. A special genetic algorithm is designed to implement this classification optimization with fast convergence. Both the numerical experiment and empirical case studies show that this generalized approach improves and extends the functionality of this Choquet nonlinear classification in more real-world multi-class multi-dimensional situations.
ER  - 
TY  - JOUR
T1  - A variance maximization criterion for active learning
A1  - Yang, Yazhou
A1  - Loog, Marco
Y1  - 2018///
KW  -  Retraining information matrix
KW  -  Variance maximization
KW  - Active learning
JF  - Pattern Recognition
VL  - 78
SP  - 358
EP  - 370
DO  - https://doi.org/10.1016/j.patcog.2018.01.017
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318300256
N2  - Active learning aims to train a classifier as fast as possible with as few labels as possible. The core element in virtually any active learning strategy is the criterion that measures the usefulness of the unlabeled data based on which new points to be labeled are picked. We propose a novel approach which we refer to as maximizing variance for active learning or MVAL for short. MVAL measures the value of unlabeled instances by evaluating the rate of change of output variables caused by changes in the next sample to be queried and its potential labelling. In a sense, this criterion measures how unstable the classifier’s output is for the unlabeled data points under perturbations of the training data. MVAL maintains, what we refer to as, retraining information matrices to keep track of these output scores and exploits two kinds of variance to measure the informativeness and representativeness, respectively. By fusing these variances, MVAL is able to select the instances which are both informative and representative. We employ our technique both in combination with logistic regression and support vector machines and demonstrate that MVAL achieves state-of-the-art performance in experiments on a large number of standard benchmark datasets.
ER  - 
TY  - JOUR
T1  - Biosensor analysis of natural and artificial sweeteners in intact taste epithelium
A1  - Zhang, Fenni
A1  - Zhang, Qian
A1  - Zhang, Diming
A1  - Lu, Yanli
A1  - Liu, Qingjun
A1  - Wang, Ping
Y1  - 2014///
KW  -  Artificial sweetener
KW  -  Microelectrodes
KW  -  Natural sugar
KW  -  Taste epithelium
KW  - Biosensor
JF  - Biosensors and Bioelectronics
VL  - 54
SP  - 385
EP  - 392
DO  - https://doi.org/10.1016/j.bios.2013.11.020
UR  - https://www.sciencedirect.com/science/article/pii/S0956566313007987
N2  - Sweeteners are commonly used as food additives in our daily life, which, however, have been causing a number of undesirable diseases since the last century. Therefore, the detection and quantification of sweeteners are of great value for food safety. In this study, we used a taste biosensor to measure and analyze different sweeteners, both natural and artificial sweeteners included. Electrophysiological activities from taste epithelium were detected by the multi-channel biosensors and analyzed with spatiotemporal methods. The longtime signal result showed different temporal-frequency properties with stimulations of individual sweeteners such as glucose, sucrose, saccharin, and cyclamate, while the multi-channel results in our study revealed the spatial expression of taste epithelium to sweet stimuli. Furthermore, in the analysis of sweetener with different concentrations, the result showed obvious dose-dependent increases in signal responses of the taste epithelium, which indicated promising applications in sweetness evaluation. Besides, the mixture experiment of two natural sweeteners with a similar functional unit (glucose and sucrose) presented two signal patterns, which turned out to be similar with responses of each individual stimulus involved. The biosensor analysis of common sweeteners provided new approaches for both natural and artificial sweeteners evaluation.
ER  - 
TY  - JOUR
T1  - Balanites aegyptiaca (L.) Del. for dermatophytoses: Ascertaining the efficacy and mode of action through experimental and computational approaches
A1  - Mohamed Hussain, Syed Abuthakir
A1  - Velusamy, Sharmila
A1  - Muthusamy, Jeyam
Y1  - 2019///
KW  -  
KW  -  IFD
KW  -  docking
KW  - Dermatophyte
JF  - Informatics in Medicine Unlocked
VL  - 15
SP  - 100177
EP  - 100177
DO  - https://doi.org/10.1016/j.imu.2019.100177
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819300280
N2  - Dermatophytoses, also known as tinea or ringworm, are superficial infections caused by a group of fungi, the dermatophytes, affecting skin, nail, hair, and wet areas of the body. Microsporum, Trichophyton and Epidermophyton are the three genera classified as dermatophytes. Balanites aegyptiaca, the “Desert date”, is an important medicinal plant. It is a spiny shrub or short tree with sweet fruits, stone-like seeds, and oily kernels. Various parts of this plant are used in treating many diseases in humans, especially skin disease, jaundice, intestinal worm infection, wounds, malaria, epilepsy, dysentery, hemorrhoid, stomachaches, and asthma. In the present study, different parts of the plant, viz. the aerial part, fruit pulp, epicarp, and kernel portions were separated and shade dried, and extracts of these portions were prepared using the Soxhlet apparatus. The antidermatophytic activity on M.gypseum and T.rubrum was tested using the poison food technique with five different concentrations (1,2,3,4,5 mg/ml) of each extract. The in vitro results were compared with the standard drug Ketaconazole used to treat dermatophytosis. From the results of the present study, the methanol extract of the pulp portion was found to inhibit the growth of M.gypseum and T.rubrum completely from the concentration of 3 mg/ml onwards. LC-MS was performed for fruit pulp, and the identified compounds were docked with the proteins involved in ergosterol biosynthesis, protein synthesis, and the cell wall of the pathogen, and compared with the specific drugs using Glide module of Schrödinger. Cyanidin-3-O-rhamnoside, Taurocholic acid, PRZ-M382, Papaverine and Mebeverine gave better interactions and dock score than the drug, and are available in the fruit mesocarp, which is edible. This study reveals that the active compounds from the fruit pulp of Balanites aegyptiaca may have potential activity against the growth of dermatophytes, and the edible part may be validated in vivo.
ER  - 
TY  - JOUR
T1  - Distributed machine learning cloud teleophthalmology IoT for predicting AMD disease progression
A1  - Das, Arun
A1  - Rad, Paul
A1  - Choo, Kim-Kwang Raymond
A1  - Nouhi, Babak
A1  - Lish, Jonathan
A1  - Martel, James
Y1  - 2019///
KW  -  Deep learning
KW  -  Macular degeneration
KW  -  Mobile-cloud teleophthalmology
KW  -  Telemedicine
KW  -  Teleophthalmology
KW  -  Wearable IoMT
KW  - Internet of Medical Things (IoMT)
JF  - Future Generation Computer Systems
VL  - 93
SP  - 486
EP  - 498
DO  - https://doi.org/10.1016/j.future.2018.10.050
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18317941
N2  - The ability to perform screening of potential vision-impairing diseases remotely with an Ophthalmologist-in-the-loop is crucial in serving the Medically Underserved Areas/Population (MUA/P) and in acute medical settings, such as emergency departments. With an estimated 217 million individuals affected by moderate to severe vision-impairing diseases worldwide and an increasing number of new patients with such diseases, the need for access to faster (or real-time) diagnosis on a large scale is imperative. It is evident that early diagnosis of chronic diseases such as diabetic retinopathy and age-related macular degeneration (AMD) could better prevent vision loss. In this paper, a scalable cloud based teleophthalmology architecture via the Internet of Medical Things (IoMT) for diagnosis of AMD is presented. In the proposed architecture, patients wear a head-mounted camera (OphthoAI IoMT headset) to send their retinal fundus images to their secure and private cloud drive storage for personalized disease severity detection and predictive progression analysis. A proposed AMD-ResNet convolution neural network with 152 layers will then analyze the images to identify and determine AMD disease severity. The algorithm is trained with AREDS (age related eye disease study) images from the National Institute of Health (NIH) with over 130,000 fundus images captured over 12 years, and for determining AMD severity, we achieve a sensitivity and specificity of 94.97 ± 0.5% and 98.32 ± 0.1% respectively. A temporal Long–Short Term Memory (LSTM) deep neural network for precision medicine and AMD predictive progression is also proposed. Patient personalization allows better targeted care, lesser side effects, and a greater likelihood of responding to treatments by tailoring healthcare on a per-patient basis.
ER  - 
TY  - JOUR
T1  - A review of multimodal human activity recognition with special emphasis on classification, applications, challenges and future directions
A1  - Yadav, Santosh Kumar
A1  - Tiwari, Kamlesh
A1  - Pandey, Hari Mohan
A1  - Akbar, Shaik Ali
Y1  - 2021///
KW  -  Computer vision
KW  -  Fusion of vision and inertial sensors
KW  -  Multimodality
KW  -  Smart-shoes
KW  -  Wearable sensors
KW  - Activity recognition
JF  - Knowledge-Based Systems
VL  - 223
SP  - 106970
EP  - 106970
DO  - https://doi.org/10.1016/j.knosys.2021.106970
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121002331
N2  - Human activity recognition (HAR) is one of the most important and challenging problems in the computer vision. It has critical application in wide variety of tasks including gaming, human–robot interaction, rehabilitation, sports, health monitoring, video surveillance, and robotics. HAR is challenging due to the complex posture made by the human and multiple people interaction. Various artifacts that commonly appears in the scene such as illuminations variations, clutter, occlusions, background diversity further adds the complexity to HAR. Sensors for multiple modalities could be used to overcome some of these inherent challenges. Such sensors could include an RGB-D camera, infrared sensors, thermal cameras, inertial sensors, etc. This article introduces a comprehensive review of different multimodal human activity recognition methods where different types of sensors being used along with their analytical approaches and fusion methods. Further, this article presents classification and discussion of existing work within seven rational aspects: (a) what are the applications of HAR; (b) what are the single and multi-modality sensing for HAR; (c) what are different vision based approaches for HAR; (d) what and how wearable sensors based system contributes to the HAR; (e) what are different multimodal HAR methods; (f) how a combination of vision and wearable inertial sensors based system contributes to the HAR; and (g) challenges and future directions in HAR. With a more and comprehensive understanding of multimodal human activity recognition, more research in this direction can be motivated and refined.
ER  - 
TY  - JOUR
T1  - Linear classifier combination via multiple potential functions
A1  - Trajdos, Pawel
A1  - Burduk, Robert
Y1  - 2021///
KW  -  Ensemble of classifiers
KW  -  Potential function
KW  -  Score function
KW  - Linear classifier
JF  - Pattern Recognition
VL  - 111
SP  - 107681
EP  - 107681
DO  - https://doi.org/10.1016/j.patcog.2020.107681
UR  - https://www.sciencedirect.com/science/article/pii/S0031320320304842
N2  - A vital aspect of the classification based model construction process is the calibration of the scoring function. One of the weaknesses of the calibration process is that it does not take into account the information about the relative positions of the recognized objects in the feature space. To alleviate this limitation, in this paper, we propose a novel concept of calculating a scoring function based on the distance of the object from the decision boundary and its distance to the class centroid. An important property is that the proposed score function has the same nature for all linear base classifiers, which means that outputs of these classifiers are equally represented and have the same meaning. The proposed approach is compared with other ensemble algorithms and experiments on multiple Keel datasets demonstrate the effectiveness of our method. To discuss the results of our experiments, we use multiple classification performance measures and statistical analysis.
ER  - 
TY  - JOUR
T1  - Ontology-guided feature engineering for clinical text classification
A1  - Garla, Vijay N
A1  - Brandt, Cynthia
Y1  - 2012///
KW  -  Feature selection
KW  -  Information content
KW  -  Information gain
KW  -  Kernel methods
KW  -  Semantic similarity
KW  - Natural language processing
JF  - Journal of Biomedical Informatics
VL  - 45
IS  - 5
SP  - 992
EP  - 998
DO  - https://doi.org/10.1016/j.jbi.2012.04.010
UR  - https://www.sciencedirect.com/science/article/pii/S1532046412000639
N1  - Text Mining and Natural Language Processing in Pharmacogenomics
N2  - In this study we present novel feature engineering techniques that leverage the biomedical domain knowledge encoded in the Unified Medical Language System (UMLS) to improve machine-learning based clinical text classification. Critical steps in clinical text classification include identification of features and passages relevant to the classification task, and representation of clinical text to enable discrimination between documents of different classes. We developed novel information-theoretic techniques that utilize the taxonomical structure of the Unified Medical Language System (UMLS) to improve feature ranking, and we developed a semantic similarity measure that projects clinical text into a feature space that improves classification. We evaluated these methods on the 2008 Integrating Informatics with Biology and the Bedside (I2B2) obesity challenge. The methods we developed improve upon the results of this challenge’s top machine-learning based system, and may improve the performance of other machine-learning based clinical text classification systems. We have released all tools developed as part of this study as open source, available at http://code.google.com/p/ytex.
ER  - 
TY  - JOUR
T1  - A decision analytic approach to predicting quality of life for lung transplant recipients: A hybrid genetic algorithms-based methodology
A1  - Oztekin, Asil
A1  - Al-Ebbini, Lina
A1  - Sevkli, Zulal
A1  - Delen, Dursun
Y1  - 2018///
KW  -  Feature selection
KW  -  Genetic algorithms
KW  -  OR in medicine
KW  -  Quality of life
KW  - UNOS lung allocation
JF  - European Journal of Operational Research
VL  - 266
IS  - 2
SP  - 639
EP  - 651
DO  - https://doi.org/10.1016/j.ejor.2017.09.034
UR  - https://www.sciencedirect.com/science/article/pii/S037722171730855X
N2  - Feature selection, a critical pre-processing step for data mining, is aimed at determining representative variables/predictors from a large and feature-rich dataset for development of an effective prediction model. The purpose of this paper is to develop a hybrid methodology for feature selection using genetic algorithms to identify such representative features (input variables) and thereby to ensure the development of the best possible analytic model to predict and explain the target variable, quality of life (QoL), for patients undergoing a lung transplant overseen by the United Network for Organ Sharing (UNOS). The evaluation of three classification models, GA-kNN, GA-SVM, and GA-ANN, demonstrated that performance of the lung transplantation process has significantly improved via the GA-SVM approach, although the other two models have also yielded considerably high prediction accuracies. This study is unique in that it proposes a hybrid GA-based feature selection methodology along with design and development of several highly accurate classification algorithms to identify the most important features in the large and feature rich UNOS transplant dataset for lung transplantation.
ER  - 
TY  - JOUR
T1  - Towards energy-aware fog-enabled cloud of things for healthcare
A1  - Mahmoud, Mukhtar M E
A1  - Rodrigues, Joel J P C
A1  - Saleem, Kashif
A1  - Al-Muhtadi, Jalal
A1  - Kumar, Neeraj
A1  - Korotaev, Valery
Y1  - 2018///
KW  -  Cloud of things
KW  -  Energy efficiency
KW  -  Fog Computing
KW  -  Healthcare
KW  -  Internet of Things
KW  - Application allocation
JF  - Computers & Electrical Engineering
VL  - 67
SP  - 58
EP  - 69
DO  - https://doi.org/10.1016/j.compeleceng.2018.02.047
UR  - https://www.sciencedirect.com/science/article/pii/S0045790618300399
N2  - The Internet-of-Things (IoT) represents the next groundbreaking change in information and communication technology (ICT) after the Internet. IoT is concerned with making everything connected and accessible through the Internet. However, IoT objects (things) are characterized by constrained computing and storage resources. Therefore, the Cloud of Things (CoT) paradigm that integrates the Cloud with IoT is proposed to meet the IoT requirements. In CoT, the IoT capabilities (e.g., sensing) are provisioned as services. Unfortunately, the two-tier CoT model is not efficient in the use cases sensitive to delays and energy consumption (e.g., in healthcare). Consequently, Fog Computing is proposed to support such IoT services and applications. This paper reviews the most relevant Fog-enabled CoT system models and proposes an energy-aware allocation strategy for placing application modules (tasks) on Fog devices. Finally, the performance of the proposed strategy is evaluated in comparison with the default allocation and Cloud-only policies, using the iFogSim simulator. The proposed solution was observed to be more energy-efficient, saving approximately 2.72% of the energy compared to Cloud-only and approximately 1.6% of the energy compared to the Fog-default.
ER  - 
TY  - JOUR
T1  - An autonomous GP-based system for regression and classification problems
A1  - Oltean, Mihai
A1  - Dioşan, Laura
Y1  - 2009///
KW  -  Adaptive strategies
KW  -  Autonomous systems
KW  -  Classification
KW  -  Symbolic regression
KW  - Genetic Programming
JF  - Applied Soft Computing
VL  - 9
IS  - 1
SP  - 49
EP  - 60
DO  - https://doi.org/10.1016/j.asoc.2008.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S156849460800032X
N2  - The aim of this research is to develop an autonomous system for solving data analysis problems. The system, called Genetic Programming-Autonomous Solver (GP-AS) contains most of the features required by an autonomous software: it decides if it knows or not how to solve a particular problem, it can construct solutions for new problems, it can store the created solutions for later use, it can improve the existing solutions in the idle-time it can efficiently manage the computer resources for fast running speed and it can detect and handle failure cases. The generator of solutions for new problems is based on an adaptive variant of Genetic Programming. We have tested this part by solving some well-known problems in the field of symbolic regression and classification. Numerical experiments show that the GP-AS system is able to perform very well on the considered test problems being able to successfully compete with standard GP having manually set parameters.
ER  - 
TY  - JOUR
T1  - A deep neural networks based model for uninterrupted marine environment monitoring
A1  - G., Thippa Reddy
A1  - R.M., Swarna Priya
A1  - M., Parimala
A1  - Chowdhary, Chiranji Lal
A1  - M., Praveen Kumar Reddy
A1  - Hakak, Saqib
A1  - Khan, Wazir Zada
Y1  - 2020///
KW  -  Deep neural network (DNN)
KW  -  Linear regression
KW  -  One-hot encoding
KW  -  Principal component analysis (PCA)
KW  -  Standardscaler
KW  -  XGBoost
KW  - Internet of things (IoT)
JF  - Computer Communications
VL  - 157
SP  - 64
EP  - 75
DO  - https://doi.org/10.1016/j.comcom.2020.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S0140366420300542
N2  - In the last few decades, there is a massive increase in population and hence increase in societal development. Concerning environmental change as a result of development in society and the economy, the marine environment plays a significant role in global climatic change. Hence recent Information and Communication Technologists are attracted towards monitoring the marine environment. Various marine monitoring systems are developed in the past few years. Out of these, the Internet of Things (IoT) plays a significant role. In IoT based Marine monitoring systems, various sensors are deployed in the real-time environment for monitoring and measuring various physical parameters. These sensors work on battery power. When the battery drains, there is a possibility of interruption in the monitoring activity until the battery is replaced. This research paper focuses on developing a prediction model for predicting the life of battery well ahead and alert the technologists so that the monitoring will not be interrupted using Principal Component Analysis (PCA) and Deep Neural Network (DNN). The model is evaluated using raw data collected from a real-time marine monitoring system which is deployed at Chicago Park District along the beach water. The results obtained are compared and analyzed with the widely used state of art techniques namely Linear Regression and XGBoost. The results show that the proposed PCA based DNN Prediction Model outperforms the other techniques by an increase of 12% in accuracy and 30% in reduction of time complexity.
ER  - 
TY  - JOUR
T1  - Online multi-label streaming feature selection based on neighborhood rough set
A1  - Liu, Jinghua
A1  - Lin, Yaojin
A1  - Li, Yuwen
A1  - Weng, Wei
A1  - Wu, Shunxiang
Y1  - 2018///
KW  -  Granularity
KW  -  Multi-label learning
KW  -  Neighborhood rough set
KW  - Online feature selection
JF  - Pattern Recognition
VL  - 84
SP  - 273
EP  - 287
DO  - https://doi.org/10.1016/j.patcog.2018.07.021
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318302553
N2  - Multi-label feature selection has grabbed intensive attention in many big data applications. However, traditional multi-label feature selection methods generally ignore a real-world scenario, i.e., the features constantly flow into the model one by one over time. To address this problem, we develop a novel online multi-label streaming feature selection method based on neighborhood rough set to select a feature subset which contains strongly relevant and non-redundant features. The main motivation is that data mining based on neighborhood rough set does not require any priori knowledge of the feature space structure. Moreover, neighborhood rough set deals with mixed data without breaking the neighborhood and order structure of data. In this paper, we first introduce the maximum-nearest-neighbor of instance to granulate all instances which can solve the problem of granularity selection in neighborhood rough set, and then generalize neighborhood rough set in single-label to fit multi-label learning. Meanwhile, an online multi-label streaming feature selection framework, which includes online importance selection and online redundancy update, is presented. Under this framework, we propose a criterion to select the important features relative to the currently selected features, and design a bound on pairwise correlations between features under label set to filter out redundant features. An empirical study using a series of benchmark datasets demonstrates that the proposed method outperforms other state-of-the-art multi-label feature selection methods.
ER  - 
TY  - JOUR
T1  - Teaching Computational Modeling in the Data Science Era
A1  - Giabbanelli, Philippe J
A1  - Mago, Vijay K
Y1  - 2016///
KW  - Course content
KW  - Data analytics
KW  - Simulations
JF  - Procedia Computer Science
VL  - 80
SP  - 1968
EP  - 1977
DO  - https://doi.org/10.1016/j.procs.2016.05.517
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916310055
N1  - International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA
N2  - Integrating data and models is an important and still challenging goal in science. Computational modeling has been taught for decades and regularly revised, for example in the 2000s where it became more inclusive of data mining. As we are now in the ‘data science’ era, we have the occasion (and often the incentive) to teach in an integrative manner computational modeling and data science. In this paper, we reviewed the content of courses and programs on computational modeling and/or data science. From this review and our teaching experience, we formed a set of design principles for an integrative course. We independently implemented these principles in two public research universities, in Canada and the US, for a course targeting graduate students and upper-division undergraduates. We discuss and contrast these implementations, and suggest ways in which the teaching of computational science can continue to be revised going forward.
ER  - 
TY  - JOUR
T1  - Knowledge discovery in traditional Chinese medicine: State of the art and perspectives
A1  - Feng, Yi
A1  - Wu, Zhaohui
A1  - Zhou, Xuezhong
A1  - Zhou, Zhongmei
A1  - Fan, Weiyu
Y1  - 2006///
KW  -  Data mining
KW  -  Knowledge discovery
KW  - Traditional Chinese medicine
JF  - Artificial Intelligence in Medicine
VL  - 38
IS  - 3
SP  - 219
EP  - 236
DO  - https://doi.org/10.1016/j.artmed.2006.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S0933365706001047
N2  - Summary
Objective
As a complementary medical system to Western medicine, traditional Chinese medicine (TCM) provides a unique theoretical and practical approach to the treatment of diseases over thousands of years. Confronted with the increasing popularity of TCM and the huge volume of TCM data, historically accumulated and recently obtained, there is an urgent need to explore these resources effectively by the techniques of knowledge discovery in database (KDD). This paper aims at providing an overview of recent KDD studies in TCM field.
Methods
A literature search was conducted in both English and Chinese publications, and major studies of knowledge discovery in TCM (KDTCM) reported in these materials were identified. Based on an introduction to the state of the art of TCM data resources, a review of four subfields of KDTCM research was presented, including KDD for the research of Chinese medical formula, KDD for the research of Chinese herbal medicine, KDD for TCM syndrome research, and KDD for TCM clinical diagnosis. Furthermore, the current state and main problems in each subfield were summarized based on a discussion of existing studies, and future directions for each subfield were also proposed accordingly.
Results
A series of KDD methods are used in existing KDTCM researches, ranging from conventional frequent itemset mining to state of the art latent structure model. Considerable interesting discoveries are obtained by these methods, such as novel TCM paired drugs discovered by frequent itemset analysis, functional community of related genes discovered under syndrome perspective by text mining, the high proportion of toxic plants in the botanical family Ranunculaceae disclosed by statistical analysis, the association between M-cholinoceptor blocking drug and Solanaceae revealed by association rule mining, etc. It is particularly inspiring to see some studies connecting TCM with biomedicine, which provide a novel top–down view for functional genomics research. However, further developments of KDD methods are still expected to better adapt to the features of TCM.
Conclusions
Existing studies demonstrate that KDTCM is effective in obtaining medical discoveries. However, much more work needs to be done in order to discover real diamonds from TCM domain. The usage and development of KDTCM in the future will substantially contribute to the TCM community, as well as modern life science.
ER  - 
TY  - JOUR
T1  - Correlates of the shift in heart rate variability with postures and walking by time–frequency analysis
A1  - Chan, Hsiao-Lung
A1  - Lin, Ming-An
A1  - Chao, Pei-Kuang
A1  - Lin, Chun-Hsien
Y1  - 2007///
KW  - Autonomic nervous system
KW  - Heart rate variability
KW  - Personal digital assistant
KW  - Physical-activity
JF  - Computer Methods and Programs in Biomedicine
VL  - 86
IS  - 2
SP  - 124
EP  - 130
DO  - https://doi.org/10.1016/j.cmpb.2007.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0169260707000302
N2  - Heart rate (HR) variability derived from electrocardiogram (ECG) can be used to assess the function of the autonomic nervous system. HR exhibits various characteristics during different physical activities attributed to the altered autonomic mediation, where it is also beneficial to reveal the autonomic shift in response to physical-activity change. In this paper, the physical-activity-related HR behaviors were delineated using a portable ECG and body acceleration recorder based on a personal digital assistant and the smoothed pseudo Wigner–Ville distribution. The results based upon eighteen subjects performing four sequential 5-min physical activities (supine, sitting, standing and spontaneous walking) showed that the high-frequency heartbeat fluctuations during supine and sitting were significantly larger than during standing, and that the ratio of low- to high-frequency fluctuation during standing was significantly higher than during supine and sitting. This could be linked with the parasympathetic predominance during supine and sitting, and a shift to sympathetic dominance while standing. During spontaneous walking, the high-frequency fluctuation was significant lower than during supine. The low- to high-frequency ratio decreased significantly from standing to spontaneous walking, which may imply an increased vagal predominance (autonomic effect) or an increased respiratory activity (mechanical effect).
ER  - 
TY  - JOUR
T1  - EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning
A1  - Zhao, Chao
A1  - Jiang, Jingchi
A1  - Guan, Yi
A1  - Guo, Xitong
A1  - He, Bin
Y1  - 2018///
KW  -  Clinical decision support
KW  -  Distributed representation
KW  -  Markov random field
KW  -  Medical knowledge network
KW  - Electronic medical record
JF  - Artificial Intelligence in Medicine
VL  - 87
SP  - 49
EP  - 59
DO  - https://doi.org/10.1016/j.artmed.2018.03.005
UR  - https://www.sciencedirect.com/science/article/pii/S0933365717302385
N2  - Objective
Electronic medical records (EMRs) contain medical knowledge that can be used for clinical decision support (CDS). Our objective is to develop a general system that can extract and represent knowledge contained in EMRs to support three CDS tasks—test recommendation, initial diagnosis, and treatment plan recommendation—given the condition of a patient.
Methods
We extracted four kinds of medical entities from records and constructed an EMR-based medical knowledge network (EMKN), in which nodes are entities and edges reflect their co-occurrence in a record. Three bipartite subgraphs (bigraphs) were extracted from the EMKN, one to support each task. One part of the bigraph was the given condition (e.g., symptoms), and the other was the condition to be inferred (e.g., diseases). Each bigraph was regarded as a Markov random field (MRF) to support the inference. We proposed three graph-based energy functions and three likelihood-based energy functions. Two of these functions are based on knowledge representation learning and can provide distributed representations of medical entities. Two EMR datasets and three metrics were utilized to evaluate the performance.
Results
As a whole, the evaluation results indicate that the proposed system outperformed the baseline methods. The distributed representation of medical entities does reflect similarity relationships with respect to knowledge level.
Conclusion
Combining EMKN and MRF is an effective approach for general medical knowledge representation and inference. Different tasks, however, require individually designed energy functions.
ER  - 
TY  - JOUR
T1  - Classifier Approaches for Liver Steatosis using Ultrasound Images
A1  - Andrade, Andreia
A1  - Silva, José Silvestre
A1  - Santos, Jaime
A1  - Belo-Soares, Pedro
Y1  - 2012///
KW  -  Feature Extraction
KW  -  Liver Steatosis
KW  -  Ultrasound
KW  - Classifier
JF  - Procedia Technology
VL  - 5
SP  - 763
EP  - 770
DO  - https://doi.org/10.1016/j.protcy.2012.09.084
UR  - https://www.sciencedirect.com/science/article/pii/S2212017312005154
N1  - 4th Conference of ENTERprise Information Systems – aligning technology, organizations and people (CENTERIS 2012)
N2  - This paper presents a semi-automatic classification approach to evaluate steatotic liver tissues using B-scan ultrasound images. Several features have been extracted and used in three different classifiers, such as Artificial Neural Networks (ANN), Support Vector Machines (SVM) and k-Nearest Neighbors (kNN). The classifiers were trained using the 10-cross validation method. A feature selection method based on stepwise regression was also exploited resulting in better accuracy predictions. The results showed that the SVM have a slightly higher performance than the kNN and the ANN, appearing as the most relevant one to be applied to the discrimination of pathologic tissues in clinical practice.
ER  - 
TY  - JOUR
T1  - Automated sleep breath disorders detection utilizing patient sound analysis
A1  - Doukas, Charalampos
A1  - Petsatodis, Theodoros
A1  - Boukis, Christos
A1  - Maglogiannis, Ilias
Y1  - 2012///
KW  -  Mobile sound processing
KW  -  Sleep apnea detection
KW  -  Snore signals
KW  -  Voice activity detection
KW  - Sleep breath disorder detection
JF  - Biomedical Signal Processing and Control
VL  - 7
IS  - 3
SP  - 256
EP  - 264
DO  - https://doi.org/10.1016/j.bspc.2012.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S1746809412000213
N1  - BioSignal Processing for Engineering and Computing: the MEDICON conference case
N2  - Results of clinical studies suggest that there is a relationship between breathing-related sleep disorders and behavioral disorder and health effects. Apnea is considered one of the major sleep disorders with great accession in population and significant impact on patient's health. Symptoms include disruption of oxygenation, snoring, choking sensations, apneic episodes, poor concentration, memory loss, and daytime somnolence. Diagnosis of apnea and breath disorders involves monitoring patient's biosignals and breath during sleep in specialized clinics requiring expensive equipment and technical personnel. This paper discusses the design and technical details of an integrated low-cost system capable for preliminary detection of sleep breath disorders at patient's home utilizing patient sound signals. The paper describes the proposed architecture and the corresponding HW and SW modules, along with a preliminary evaluation.
ER  - 
TY  - JOUR
T1  - A biological continuum based approach for efficient clinical classification
A1  - Tay, Darwin
A1  - Poh, Chueh Loo
A1  - Goh, Carolyn
A1  - Kitney, Richard I
Y1  - 2014///
KW  -  Dimensionality reduction
KW  -  Etiological network
KW  -  Feature selection
KW  -  Genetic algorithm
KW  -  Support vector machine
KW  - Classification
JF  - Journal of Biomedical Informatics
VL  - 47
SP  - 28
EP  - 38
DO  - https://doi.org/10.1016/j.jbi.2013.09.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046413001433
N2  - Clinical feature selection problem is the task of selecting and identifying a subset of informative clinical features that are useful for promoting accurate clinical diagnosis. This is a significant task of pragmatic value in the clinical settings as each clinical test is associated with a different financial cost, diagnostic value, and risk for obtaining the measurement. Moreover, with continual introduction of new clinical features, the need to repeat the feature selection task can be very time consuming. Therefore to address this issue, we propose a novel feature selection technique for diagnosis of myocardial infarction – one of the leading causes of morbidity and mortality in many high-income countries. This method adopts the conceptual framework of biological continuum, the optimization capability of genetic algorithm for performing feature selection and the classification ability of support vector machine. Together, a network of clinical risk factors, called the biological continuum based etiological network (BCEN), was constructed. Evaluation of the proposed methods was carried out using the cardiovascular heart study (CHS) dataset. Results demonstrate a significant speedup of 4.73-fold can be achieved for the development of MI classification model. The key advantage of this methodology is the provision of a reusable (feature subset) paradigm for efficient development of up-to-date and efficacious clinical classification models.
ER  - 
TY  - JOUR
T1  - Coupling different methods for overcoming the class imbalance problem
A1  - Nanni, Loris
A1  - Fantozzi, Carlo
A1  - Lazzarini, Nicola
Y1  - 2015///
KW  -  Downsampling/oversampling
KW  -  Ensemble of classifiers
KW  -  Support vector machine
KW  - Imbalanced dataset
JF  - Neurocomputing
VL  - 158
SP  - 48
EP  - 61
DO  - https://doi.org/10.1016/j.neucom.2015.01.068
UR  - https://www.sciencedirect.com/science/article/pii/S0925231215001411
N2  - Many classification problems must deal with imbalanced datasets where one class – the majority class – outnumbers the other classes. Standard classification methods do not provide accurate predictions in this setting since classification is generally biased towards the majority class. The minority classes are oftentimes the ones of interest (e.g., when they are associated with pathological conditions in patients), so methods for handling imbalanced datasets are critical. Using several different datasets, this paper evaluates the performance of state-of-the-art classification methods for handling the imbalance problem in both binary and multi-class datasets. Different strategies are considered, including the one-class and dimension reduction approaches, as well as their fusions. Moreover, some ensembles of classifiers are tested, in addition to stand-alone classifiers, to assess the effectiveness of ensembles in the presence of imbalance. Finally, a novel ensemble of ensembles is designed specifically to tackle the problem of class imbalance: the proposed ensemble does not need to be tuned separately for each dataset and outperforms all the other tested approaches. To validate our classifiers we resort to the KEEL-dataset repository, whose data partitions (training/test) are publicly available and have already been used in the open literature: as a consequence, it is possible to report a fair comparison among different approaches in the literature. Our best approach (MATLAB code and datasets not easily accessible elsewhere) will be available at https://www.dei.unipd.it/node/2357.
ER  - 
TY  - JOUR
T1  - Learning a single-hidden layer feedforward neural network using a rank correlation-based strategy with application to high dimensional gene expression and proteomic spectra datasets in cancer detection
A1  - Belciug, Smaranda
A1  - Gorunescu, Florin
Y1  - 2018///
KW  -  Adaptive hidden nodes initialization
KW  -  Automated cancer detection
KW  -  Mass spectrometry
KW  -  Microarray
KW  -  Single-hidden layer feedforward neural network
KW  - Extreme learning machine
JF  - Journal of Biomedical Informatics
VL  - 83
SP  - 159
EP  - 166
DO  - https://doi.org/10.1016/j.jbi.2018.06.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418301096
N2  - Methods based on microarrays (MA), mass spectrometry (MS), and machine learning (ML) algorithms have evolved rapidly in recent years, allowing for early detection of several types of cancer. A pitfall of these approaches, however, is the overfitting of data due to large number of attributes and small number of instances -- a phenomenon known as the 'curse of dimensionality'. A potentially fruitful idea to avoid this drawback is to develop algorithms that combine fast computation with a filtering module for the attributes. The goal of this paper is to propose a statistical strategy to initiate the hidden nodes of a single-hidden layer feedforward neural network (SLFN) by using both the knowledge embedded in data and a filtering mechanism for attribute relevance. In order to attest its feasibility, the proposed model has been tested on five publicly available high-dimensional datasets: breast, lung, colon, and ovarian cancer regarding gene expression and proteomic spectra provided by cDNA arrays, DNA microarray, and MS. The novel algorithm, called adaptive SLFN (aSLFN), has been compared with four major classification algorithms: traditional ELM, radial basis function network (RBF), single-hidden layer feedforward neural network trained by backpropagation algorithm (BP-SLFN), and support vector-machine (SVM). Experimental results showed that the classification performance of aSLFN is competitive with the comparison models.
ER  - 
TY  - JOUR
T1  - Harnessing social media for health information management
A1  - Zhou, Lina
A1  - Zhang, Dongsong
A1  - Yang, Christopher C
A1  - Wang, Yu
Y1  - 2018///
KW  -  Data analytics
KW  -  Health information management
KW  -  Social media
KW  - Conceptual framework
JF  - Electronic Commerce Research and Applications
VL  - 27
SP  - 139
EP  - 151
DO  - https://doi.org/10.1016/j.elerap.2017.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S1567422317300960
N2  - The remarkable upsurge of social media has dramatic impacts on health care research and practice. Social media are reshaping health information management in a variety of ways, ranging from providing cost-effective ways to improve clinician-patient communication and exchange health-related information and experience, to enabling the discovery of new medical knowledge and information. Despite some demonstrated initial success, social media use and analytics for improving health as a research field is still at its infancy. Information systems researchers can potentially play a key role in advancing the field. This study proposes a conceptual framework for social media-based health information management by drawing on multi-disciplinary research. With the guidance of the framework, this paper presents related research challenges, identifies important yet under-explored research issues, and discusses promising directions for future research.
ER  - 
TY  - JOUR
T1  - Automated detection of sleep apnea using sparse residual entropy features with various dictionaries extracted from heart rate and EDR signals
A1  - Viswabhargav, ChS.S.S.
A1  - Tripathy, R K
A1  - Acharya, U Rajendra
Y1  - 2019///
KW  -  EDR and HRV signals dictionaries
KW  -  SVM classifier
KW  -  Sparse residual entropy
KW  - Sleep apnea detection
JF  - Computers in Biology and Medicine
VL  - 108
SP  - 20
EP  - 30
DO  - https://doi.org/10.1016/j.compbiomed.2019.03.016
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519300897
N2  - Sleep is a prominent physiological activity in our daily life. Sleep apnea is the category of sleep disorder during which the breathing of the person diminishes causing the alternation in the upper airway resistance. The electrocardiogram derived respiration (EDR) and heart rate (RR-time-series) signals are normally used for the detection of sleep apnea as these two signals capture cardio-pulmonary activity information. Hence, the analysis of these two signals provides vital information about sleep apnea. In this paper, we propose the novel sparse residual entropy (SRE) features for the automated detection of sleep apnea using EDR and heart rate signals. The features required for the automated detection of sleep apnea are extracted in three steps: (i) atomic decomposition based residual estimation from both EDR and heart rate signals using orthogonal matching pursuit (OMP) with different dictionaries, (ii) estimation of probabilities from each sparse residual, and (iii) calculation of the entropy features. The proposed SRE features are fed to the combination of fuzzy K-means clustering and support vector machine (SVM) to pick the best performing classifier. The experimental results demonstrate that the proposed SRE features with radial basis function (RBF) kernel-based SVM classifier yielded higher performance with accuracy, sensitivity and specificity values of 78.07%, 78.01%, and 78.13%, respectively with Fourier dictionary and 10-fold cross-validation. For subject-specific or leave-one-out validation case, the SVM classifier has sensitivity and specificity of 85.43% and 92.60%, respectively using SRE features with Fourier dictionary (FD).
ER  - 
TY  - JOUR
T1  - Microarray cancer feature selection: Review, challenges and research directions
A1  - Hambali, Moshood A
A1  - Oladele, Tinuke O
A1  - Adewole, Kayode S
Y1  - 2020///
KW  -  Embedded
KW  -  Filter
KW  -  Microarray data
KW  -  Microarray technology
KW  -  Wrapper
KW  - Feature selection
JF  - International Journal of Cognitive Computing in Engineering
VL  - 1
SP  - 78
EP  - 97
DO  - https://doi.org/10.1016/j.ijcce.2020.11.001
UR  - https://www.sciencedirect.com/science/article/pii/S2666307420300085
N2  - Microarray technology has become an emerging trend in the domain of genetic research in which many researchers employ to study and investigate the levels of genes’ expression in a given organism. Microarray experiments have lots of application areas in the health sector such as diseases prediction and diagnosis, cancer study and soon. The enormous quantity of raw gene expression data usually results in analytical and computational complexities which include feature selection and classification of the datasets into the correct class or group. To achieve satisfactory cancer classification accuracy with the complete set of genes remains a great challenge, due to the high dimensions, small sample size, and presence of noise in gene expression data. Feature reduction is critical and sensitive in the classification task. Therefore, this paper presents a comprehensive survey of studies on microarray cancer classification with a focus on feature selection methods. In this paper, the taxonomy of the various feature selection methods used for microarray cancer classification and open research issues have been extensively discussed.
ER  - 
TY  - JOUR
T1  - Creating an invalid defect classification model using text mining on server development
A1  - Su, Yihsiung
A1  - Luarn, Pin
A1  - Lee, Yue-Shi
A1  - Yen, Show-Jane
Y1  - 2017///
KW  -  BIOS
KW  -  Classification
KW  -  Project management
KW  -  Server development
KW  -  Text mining
KW  - Invalid defect
JF  - Journal of Systems and Software
VL  - 125
SP  - 197
EP  - 206
DO  - https://doi.org/10.1016/j.jss.2016.12.005
UR  - https://www.sciencedirect.com/science/article/pii/S0164121216302527
N2  - Invalid defects, which are often overlooked, reduce development productivity and efficiency. This study used exploratory study and text mining to answer three research questions related to invalid defects in two research stages. In the first stage, we filtered 231 invalid BIOS (basic input/output system) defects from the 3347 defects of three server projects. These defects were from numerous function areas owned by virtual teams located in Taiwan, China, and the United States. Results indicated that BIOS firmware demonstrates the maximum number of defects and invalid defects. This firmware accounted for 43.3% defects and 33% invalid defects in server development. Results determined that invalid defect classification that includes four types, namely, working as designed (WAD), user error, duplicate, and others. All of these types can be grouped under the term WUDO. WAD accounts for the maximum of 45% of invalid defects in the WUDO classification. In the second stage, this study determined a stable classification algorithm, namely, decision tree C4.5, to classify the invalid defect types. This study helps project teams for information technology products to classify the different invalid defect types that developers and testers face. Results can improve project team productivity and mitigate project risks in project management.
ER  - 
TY  - JOUR
T1  - Automated estimation of choroidal thickness distribution and volume based on OCT images of posterior visual section
A1  - Vupparaboina, Kiran Kumar
A1  - Nizampatnam, Srinath
A1  - Chhablani, Jay
A1  - Richhariya, Ashutosh
A1  - Jana, Soumya
Y1  - 2015///
KW  -  Choroidal thickness
KW  -  Choroidal volume
KW  -  Structural similarity (SSIM) index
KW  -  Tensor voting
KW  - Optical coherence tomography (OCT)
JF  - Computerized Medical Imaging and Graphics
VL  - 46
SP  - 315
EP  - 327
DO  - https://doi.org/10.1016/j.compmedimag.2015.09.008
UR  - https://www.sciencedirect.com/science/article/pii/S089561111500141X
N2  - A variety of vision ailments are indicated by anomalies in the choroid layer of the posterior visual section. Consequently, choroidal thickness and volume measurements, usually performed by experts based on optical coherence tomography (OCT) images, have assumed diagnostic significance. Now, to save precious expert time, it has become imperative to develop automated methods. To this end, one requires choroid outer boundary (COB) detection as a crucial step, where difficulty arises as the COB divides the choroidal granularity and the scleral uniformity only notionally, without marked brightness variation. In this backdrop, we measure the structural dissimilarity between choroid and sclera by structural similarity (SSIM) index, and hence estimate the COB by thresholding. Subsequently, smooth COB estimates, mimicking manual delineation, are obtained using tensor voting. On five datasets, each consisting of 97 adult OCT B-scans, automated and manual segmentation results agree visually. We also demonstrate close statistical match (greater than 99.6% correlation) between choroidal thickness distributions obtained algorithmically and manually. Further, quantitative superiority of our method is established over existing results by respective factors of 27.67% and 76.04% in two quotient measures defined relative to observer repeatability. Finally, automated choroidal volume estimation, being attempted for the first time, also yields results in close agreement with that of manual methods.
ER  - 
TY  - JOUR
T1  - SemPathFinder: Semantic path analysis for discovering publicly unknown knowledge
A1  - Song, Min
A1  - Heo, Go Eun
A1  - Ding, Ying
Y1  - 2015///
KW  -  Named entity recognition
KW  -  Relation extraction
KW  -  Semantic path analysis
KW  -  Semantic relatedness score
KW  - Literature based discovery
JF  - Journal of Informetrics
VL  - 9
IS  - 4
SP  - 686
EP  - 703
DO  - https://doi.org/10.1016/j.joi.2015.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S1751157715200533
N2  - The enormous amount of biomedicine's natural-language texts creates a daunting challenge to discover novel and interesting patterns embedded in the text corpora that help biomedical professionals find new drugs and treatments. These patterns constitute entities such as genes, compounds, treatments, and side effects and their associations that spread across publications in different biomedical specialties. This paper proposes SemPathFinder to discover previously unknown relations in biomedical text. SemPathFinder overcomes the problems of Swanson's ABC model by using semantic path analysis to tell a story about plausible connections between biological terms. Storytelling-based semantic path analysis can be viewed as relation navigation for bio-entities that are semantically close to each other, and reveals insight into how a series of entity pairs is organized, and how it can be harnessed to explain seemingly unrelated connections. We apply SemPathFinder for two well-known use cases of Swanson's ABC model, and the experimental results show that SemPathFinder detects all intermediate terms except for one and also infers several interesting new hypotheses.
ER  - 
TY  - JOUR
T1  - Automated myocardial infarction identification based on interbeat variability analysis of the photoplethysmographic data
A1  - Chakraborty, Abhishek
A1  - Sadhukhan, Deboleena
A1  - Pal, Saurabh
A1  - Mitra, Madhuchhanda
Y1  - 2020///
KW  -  Expert health systems
KW  -  Feature extraction
KW  -  Myocardial infarction (MI)
KW  -  PPG derivatives
KW  -  Photoplethysmography (PPG)
KW  -  Variability analysis
KW  - Automated diagnosis
JF  - Biomedical Signal Processing and Control
VL  - 57
SP  - 101747
EP  - 101747
DO  - https://doi.org/10.1016/j.bspc.2019.101747
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419303283
N2  - Background and objective
Myocardial infarction (MI) remains a major cause of mortality around the world for decades. Timely detection followed by instant medical intervention is strongly recommended to minimize MI related death threats. Generally, electrocardiogram (ECG) based automated techniques are preferred to ensure early diagnosis of MI. Recently, Photoplethysmogram (PPG) signal is evolving as a promising diagnostic tool for several cardiac monitoring applications because of its low cost, reliable and easy acquisition technology. However, the use of PPG for MI detection has not been much explored till date. Hence, in the present work, a method for MI detection is proposed based on the use of the PPG signal only.
Method
The pathophysiological alteration due to MI induces a beat-to-beat variation in the PPG beat morphology. Different time-plane parameters from the PPG signal and its derivatives are extracted to represent the individual beat morphologies and their variations are then analyzed as features over the cardiac cycles. By using different feature optimization techniques, finally five features are selected that presents discriminating variability to identify MI.
Results
The proposed method is evaluated with PPG records collected from 52 hospitalized MI patients and 52 normal subjects. The optimized five feature set, representing the inter beat morphological variations exhibits significant performance along with SVM (linear) classification technique with an average sensitivity, positive predictivity and detection accuracy of 92.70 %, 98.10 %, and 95.40 %, respectively.
Conclusions
Compared to other related works, use of PPG signal for MI detection is studied in this research for the first time. The promising result obtained establishes the utility of PPG signal for MI detection with a potential of implementation in the personal healthcare systems.
ER  - 
TY  - JOUR
T1  - Speech stress recognition using semi-eager learning
A1  - Yerigeri, Vaijanath V
A1  - Ragha, L K
Y1  - 2021///
KW  -  Gammatone Frequency Cepstral coefficient (GFCC)
KW  -  Perceptual Linear Predictive Cepstrum (PLPC)
KW  -  Revised Perceptual Linear Prediction Coefficient’s (RPLP)
KW  -  SemiEager
KW  - Speech emotion
JF  - Cognitive Systems Research
VL  - 65
SP  - 79
EP  - 97
DO  - https://doi.org/10.1016/j.cogsys.2020.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S1389041720300735
N2  - Homo-sapiens suffer from psychogenic pain due to current day lifestyle. According to psychologists, stress is the most destructive form of psychalgia and it is a vicious companion for this species. Immoderate levels of stress may lead to the death of many individuals. Normally, the presence of stress gives rise to certain emotions which can be detected to predict stress levels of a person. This paper proposes the development of mechanized and efficient Speech Emotion Recognition (SER) for stress level analysis. The paper investigates the performance of perceptual based speech features like Revised Perceptual Linear Prediction Coefficients, Bark Frequency Cepstral Coefficients, Perceptual Linear Predictive Cepstrum, Gammatone Frequency Cepstral coefficient, Mel Frequency Cepstral Coefficient, Gammatone Wavelet Cepstral Coefficient and Inverted Mel Frequency Cepstral Coefficients on SER. The novelty of this work involves application of a SemiEager (SemiE) learning algorithm for evaluating auditory cues. SemiE offers advantages over eager and lazy based learning by reducing the computational cost. Stress level recognition being the main objective, the Speech Under Simulated and Actual Stress (SUSAS) benchmark database is used for performance analysis. A comparative analysis is presented to demonstrate the improvement in the SED performance. An overall accuracy of 90.66% recognition of stress related emotions is achieved.
ER  - 
TY  - JOUR
T1  - Hyperdisk based large margin classifier
A1  - Cevikalp, Hakan
A1  - Triggs, Bill
Y1  - 2013///
KW  -  Classification
KW  -  Convex approximation
KW  -  Hyperdisk
KW  -  Kernel method
KW  -  Support Vector Machine
KW  - Large margin classifier
JF  - Pattern Recognition
VL  - 46
IS  - 6
SP  - 1523
EP  - 1531
DO  - https://doi.org/10.1016/j.patcog.2012.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S0031320312004657
N2  - We introduce a large margin linear binary classification framework that approximates each class with a hyperdisk – the intersection of the affine support and the bounding hypersphere of its training samples in feature space – and then finds the linear classifier that maximizes the margin separating the two hyperdisks. We contrast this with Support Vector Machines (SVMs), which find the maximum-margin separator of the pointwise convex hulls of the training samples, arguing that replacing convex hulls with looser convex class models such as hyperdisks provides safer margin estimates that improve the accuracy on some problems. Both the hyperdisks and their separators are found by solving simple quadratic programs. The method is extended to nonlinear feature spaces using the kernel trick, and multi-class problems are dealt with by combining binary classifiers in the same ways as for SVMs. Experiments on a range of data sets show that the method compares favourably with other popular large margin classifiers.
ER  - 
TY  - JOUR
T1  - Machine assessment of neonatal facial expressions of acute pain
A1  - Brahnam, Sheryl
A1  - Chuang, Chao-Fa
A1  - Sexton, Randall S
A1  - Shih, Frank Y
Y1  - 2007///
KW  -  Linear discriminant analysis
KW  -  Medical face classification
KW  -  Neural network simultaneous optimization algorith
KW  -  Principal component analysis
KW  -  Support vector machines
KW  - Neonate pain recognition
JF  - Decision Support Systems
VL  - 43
IS  - 4
SP  - 1242
EP  - 1254
DO  - https://doi.org/10.1016/j.dss.2006.02.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167923606000261
N1  - Special Issue Clusters
N2  - We propose that a machine assessment system of neonatal expressions of pain be developed to assist clinicians in diagnosing pain. The facial expressions of 26 neonates (age 18–72h) were photographed experiencing the acute pain of a heel lance and three nonpain stressors. Four algorithms were evaluated on out-of-sample observations: PCA, LDA, SVMs and NNSOA. NNSOA provided the best classification rate of pain versus nonpain (90.20%), followed by SVM with linear kernel (82.35%). We believe these results indicate a high potential for developing a decision support system for diagnosing neonatal pain from images of neonatal facial displays.
ER  - 
TY  - JOUR
T1  - The data complexity index to construct an efficient cross-validation method
A1  - Li, Der-Chiang
A1  - Fang, Yao-Hwei
A1  - Fang, Y M Frank
Y1  - 2010///
KW  -  Cross-validation
KW  -  Data complexity
KW  - Binary classification problem
JF  - Decision Support Systems
VL  - 50
IS  - 1
SP  - 93
EP  - 102
DO  - https://doi.org/10.1016/j.dss.2010.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S0167923610001119
N2  - Cross-validation is a widely used model evaluation method in data mining applications. However, it usually takes a lot of effort to determine the appropriate parameter values, such as training data size and the number of experiment runs, to implement a validated evaluation. This study develops an efficient cross-validation method called Complexity-based Efficient (CBE) cross-validation for binary classification problems. CBE cross-validation establishes a complexity index, called the CBE index, by exploring the geometric structure and noise of data. The CBE index is used to calculate the optimal training data size and the number of experiment runs to reduce model evaluation time when dealing with computationally expensive classification data sets. A simulated and three real data sets are employed to validate the performance of the proposed method in the study, while the validation methods compared are repeated random sub-sampling validation and K-fold cross-validation. The results show that CBE cross-validation, repeated random sub-sampling validation and K-fold cross-validation have similar validation performance, except that the training time required for CBE cross-validation is indeed lower than that for the other two methods.
ER  - 
TY  - JOUR
T1  - Predicting severe clinical events by learning about life-saving actions and outcomes using distant supervision
A1  - Lee, Dae Hyun
A1  - Yetisgen, Meliha
A1  - Vanderwende, Lucy
A1  - Horvitz, Eric
Y1  - 2020///
KW  -  Acute organ failure prediction
KW  -  Distant supervision
KW  -  Early warning scores
KW  -  Failure to rescue
KW  - Machine learning
JF  - Journal of Biomedical Informatics
VL  - 107
SP  - 103425
EP  - 103425
DO  - https://doi.org/10.1016/j.jbi.2020.103425
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300538
N2  - Medical error is a leading cause of patient death in the United States. Among the different types of medical errors, harm to patients caused by doctors missing early signs of deterioration is especially challenging to address due to the heterogeneity of patients’ physiological patterns. In this study, we implemented risk prediction models using the gradient boosted tree method to derive risk estimates for acute onset diseases in the near future. The prediction model uses physiological variables as input signals and the time of the administration of outcome-related interventions and discharge diagnoses as labels. We examine four categories of acute onset illness: acute heart failure (AHF), acute lung injury (ALI), acute kidney injury (AKI), and acute liver failure (ALF). To develop and test the model, we consider data from two sources: 23,578 admissions to the Intensive Care Unit (ICU) from the MIMIC-3 dataset (Beth-Israel Hospital) and 16,612 ICU admissions on hospitals affiliated with our institution (University of Washington Medical Center and Harborview Medical Center, the UW-CDR dataset). We systematically identify outcome-related interventions for each acute organ failure, then use them, along with discharge diagnoses, to label proxy events to train gradient boosted trees. The trained models achieve the highest F1 score with a value of 0.6018 when predicting the need for life-saving interventions for ALI within the next 24 h in the MIMIC-3 dataset while showing a median F1 score of 0.3850 from all acute organ failures in both datasets. The approach also achieves the highest F1 score of 0.6301 when classifying a patient’s ALI status at the time of discharge from the MIMIC-3 dataset, with a median F1 score of 0.4307 in both datasets. This study shows the potential for using the time of outcome-related intervention administrations and discharge diagnoses as labels to train supervised machine learning models that predict the risk of acute onset illnesses.
ER  - 
TY  - JOUR
T1  - Multiple empirical kernel learning with locality preserving constraint
A1  - Fan, Qi
A1  - Gao, Daqi
A1  - Wang, Zhe
Y1  - 2016///
KW  -  Empirical kernel mapping
KW  -  Locality preserving projection
KW  -  Machine learning
KW  -  Pattern recognition
KW  - Multiple kernel learning
JF  - Knowledge-Based Systems
VL  - 105
SP  - 107
EP  - 118
DO  - https://doi.org/10.1016/j.knosys.2016.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S095070511630096X
N2  - Multiple Kernel Learning (MKL) is flexible in dealing with problems involving multiple and heterogeneous data sources. However, the necessity of inner-product form restricts its application since to kernelize the algorithms unsatisfying the inner-product formulation is pretty difficult. To overcome this problem, Multiple Empirical Kernel Learning (MEKL) is proposed by explicitly mapping input samples to feature spaces, in which the mapped feature vectors are explicitly presented. Most existed MEKLs optimize the learning framework by minimizing empirical risk, regularization risk and the loss term of multiple feature spaces. As little attention is paid to preserving local structure among training samples, the learned classifier might lack of locality similarity preserving property, which might result in unfavorable performance. Inspired by Locality Preserving Projection (LPP) which is to seek the optimal projection by preserving the local property of input samples, we introduce the locality preserving constraint into the learning framework to propose a novel Multiple Empirical Kernel Learning with Locality Preserving Constraint (MEKL-LPC). MEKL-LPC shows lower generalization error bound than both the Modification of Ho–Kashyap algorithm with Squared approximation of the misclassification error (MHKS) and Multi-Kernel MHKS (MultiK-MHKS) in terms of Rademacher complexity. Experiments on several real-world datasets demonstrate that MEKL-LPC outperforms the compared algorithms. The contributions of this work are: (i) originally integrating locality preserving constraint into MEKL, (ii) proposing a lower generalization error bound algorithm, i.e. MEKL-LPC.
ER  - 
TY  - JOUR
T1  - Developing an automated mechanism to identify medical articles from wikipedia for knowledge extraction
A1  - Yu, Lishan
A1  - Yu, Sheng
Y1  - 2020///
KW  -  Crawling classification
KW  -  False discovery control
KW  -  Text classification
KW  - Wikipedia
JF  - International Journal of Medical Informatics
VL  - 141
SP  - 104234
EP  - 104234
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104234
UR  - https://www.sciencedirect.com/science/article/pii/S1386505620302070
N2  - Wikipedia contains rich biomedical information that can support medical informatics studies and applications. Identifying the subset of medical articles of Wikipedia has many benefits, such as facilitating medical knowledge extraction, serving as a corpus for language modeling, or simply making the size of data easy to work with. However, due to the extremely low prevalence of medical articles in the entire Wikipedia, articles identified by generic text classifiers would be bloated by irrelevant pages. To control the false discovery rate while maintaining a high recall, we developed a mechanism that leverages the rich page elements and the connected nature of Wikipedia and uses a crawling classification strategy to achieve accurate classification. Structured assertional knowledge in Infoboxes and Wikidata items associated with the identified medical articles were also extracted. This automatic mechanism is aimed to run periodically to update the results and share them with the informatics community.
ER  - 
TY  - JOUR
T1  - Case Based Reasoning in the Detection of Retinal Abnormalities Using Decision Trees
A1  - Banerjee, Sreeparna
A1  - Chowdhury, Amrita Roy
Y1  - 2015///
KW  -  decision Trees
KW  -  retinal Images;
KW  -  similarity based retrieval
KW  - CBR
JF  - Procedia Computer Science
VL  - 46
SP  - 402
EP  - 408
DO  - https://doi.org/10.1016/j.procs.2015.02.037
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915001015
N1  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
N2  - The most common abnormalities in retina images occur due to age related macular degeneration and diabetic retinopathy. In this paper a decision support system is proposed to classify these abnormalities. The process involves the combination of contextual information with images obtained from a database. Decisions trees are proposed for this purpose as they can combine contextual information with images. Images are pre- processed and segmented to obtain the regions of interest for the individual manifestations in each of these diseases. Matching of candidate segmented images with prototype segmented images is performed along with the matching of the associated contextual information.
ER  - 
TY  - JOUR
T1  - A comparison of evaluation metrics for biomedical journals, articles, and websites in terms of sensitivity to topic
A1  - Fu, Lawrence D
A1  - Aphinyanaphongs, Yindalon
A1  - Wang, Lily
A1  - Aliferis, Constantin F
Y1  - 2011///
KW  -  Bibliometrics
KW  -  Journal impact factor
KW  -  Machine learning
KW  -  PageRank
KW  -  Topic-sensitivity
KW  - Information retrieval
JF  - Journal of Biomedical Informatics
VL  - 44
IS  - 4
SP  - 587
EP  - 594
DO  - https://doi.org/10.1016/j.jbi.2011.03.006
UR  - https://www.sciencedirect.com/science/article/pii/S1532046411000578
N2  - Evaluating the biomedical literature and health-related websites for quality are challenging information retrieval tasks. Current commonly used methods include impact factor for journals, PubMed’s clinical query filters and machine learning-based filter models for articles, and PageRank for websites. Previous work has focused on the average performance of these methods without considering the topic, and it is unknown how performance varies for specific topics or focused searches. Clinicians, researchers, and users should be aware when expected performance is not achieved for specific topics. The present work analyzes the behavior of these methods for a variety of topics. Impact factor, clinical query filters, and PageRank vary widely across different topics while a topic-specific impact factor and machine learning-based filter models are more stable. The results demonstrate that a method may perform excellently on average but struggle when used on a number of narrower topics. Topic-adjusted metrics and other topic robust methods have an advantage in such situations. Users of traditional topic-sensitive metrics should be aware of their limitations.
ER  - 
TY  - JOUR
T1  - Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review
A1  - Kreimeyer, Kory
A1  - Foster, Matthew
A1  - Pandey, Abhishek
A1  - Arya, Nina
A1  - Halford, Gwendolyn
A1  - Jones, Sandra F
A1  - Forshee, Richard
A1  - Walderhaug, Mark
A1  - Botsis, Taxiarchis
Y1  - 2017///
KW  -  Common data elements
KW  -  Natural language processing
KW  -  Systematic
KW  - Review
JF  - Journal of Biomedical Informatics
VL  - 73
SP  - 14
EP  - 29
DO  - https://doi.org/10.1016/j.jbi.2017.07.012
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417301685
N2  - We followed a systematic approach based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses to identify existing clinical natural language processing (NLP) systems that generate structured information from unstructured free text. Seven literature databases were searched with a query combining the concepts of natural language processing and structured data capture. Two reviewers screened all records for relevance during two screening phases, and information about clinical NLP systems was collected from the final set of papers. A total of 7149 records (after removing duplicates) were retrieved and screened, and 86 were determined to fit the review criteria. These papers contained information about 71 different clinical NLP systems, which were then analyzed. The NLP systems address a wide variety of important clinical and research tasks. Certain tasks are well addressed by the existing systems, while others remain as open challenges that only a small number of systems attempt, such as extraction of temporal information or normalization of concepts to standard terminologies. This review has identified many NLP systems capable of processing clinical free text and generating structured output, and the information collected and evaluated here will be important for prioritizing development of new approaches for clinical NLP.
ER  - 
TY  - JOUR
T1  - Phosphorus magnetic resonance spectroscopy and imaging (31P-MRS/MRSI) as a window to brain and muscle metabolism: A review of the methods
A1  - Santos-Díaz, Alejandro
A1  - Noseworthy, Michael D
Y1  - 2020///
KW  -  Brain
KW  -  Energy metabolism
KW  -  MRSI
KW  -  Skeletal muscle
KW  - Phosphorus MRS
JF  - Biomedical Signal Processing and Control
VL  - 60
SP  - 101967
EP  - 101967
DO  - https://doi.org/10.1016/j.bspc.2020.101967
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420301233
N2  - Phosphorus magnetic resonance spectroscopy and spectroscopic imaging (31P-MRS/MRSI) non-invasively provide very important information regarding energy metabolism as they can detect high-energy metabolites and membrane phospholipids in vivo. They have repeatedly proven their utility in the study of healthy and disease conditions in skeletal muscle and brain tissue, as many disorders are related to imbalances in bioenergetics processes. However, they are not often used in a clinic setting, as there are technical challenges imposed by the low sensitivity and low concentration of metabolites leading to low signal to noise ratio (SNR), coarse spatial resolution and very long acquisition times. This paper presents an overview of the main techniques used for the acquisition, data reconstruction and processing of 31P-MRS/MRSI experiments with emphasis in methodological aspects, as well as some of their main applications in the study of skeletal muscle and brain tissue. Also, recent advances in the development of accelerated methods for acquisition of 31P-MR data are discussed.
ER  - 
TY  - JOUR
T1  - SODE: Self-Adaptive One-Dependence Estimators for classification
A1  - Wu, Jia
A1  - Pan, Shirui
A1  - Zhu, Xingquan
A1  - Zhang, Peng
A1  - Zhang, Chengqi
Y1  - 2016///
KW  -  Artificial immune systems
KW  -  Classification
KW  -  Evolutionary machine learning
KW  -  Naive Bayes
KW  -  Self-adaptive
KW  - Attribute weighting
JF  - Pattern Recognition
VL  - 51
SP  - 358
EP  - 377
DO  - https://doi.org/10.1016/j.patcog.2015.08.023
UR  - https://www.sciencedirect.com/science/article/pii/S0031320315003118
N2  - SuperParent-One-Dependence Estimators (SPODEs) represent a family of semi-naive Bayesian classifiers which relax the attribute independence assumption of Naive Bayes (NB) to allow each attribute to depend on a common single attribute (superparent). SPODEs can effectively handle data with attribute dependency but still inherent NB׳s key advantages such as computational efficiency and robustness for high dimensional data. In reality, determining an optimal superparent for SPODEs is difficult. One common approach is to use weighted combinations of multiple SPODEs, each having a different superparent with a properly assigned weight value (i.e., a weight value is assigned to each attribute). In this paper, we propose a self-adaptive SPODEs, namely SODE, which uses immunity theory in artificial immune systems to automatically and self-adaptively select the weight for each single SPODE. SODE does not need to know the importance of individual SPODE nor the relevance among SPODEs, and can flexibly and efficiently search optimal weight values for each SPODE during the learning process. Extensive experiments and comparisons on 56 benchmark data sets, and validations on image and text classification, demonstrate that SODE outperforms state-of-the-art weighted SPODE algorithms and is suitable for a wide range of learning tasks. Results also confirm that SODE provides an appropriate balance between runtime efficiency and accuracy.
ER  - 
TY  - JOUR
T1  - Shearlet and contourlet transforms for analysis of electrocardiogram signals
A1  - Amorim, Paulo
A1  - Moraes, Thiago
A1  - Fazanaro, Dalton
A1  - Silva, Jorge
A1  - Pedrini, Helio
Y1  - 2018///
KW  -  Contourlets
KW  -  Heart rhythm
KW  -  Shearlets
KW  -  Signal classification
KW  - Electrocardiogram signals
JF  - Computer Methods and Programs in Biomedicine
VL  - 161
SP  - 125
EP  - 132
DO  - https://doi.org/10.1016/j.cmpb.2018.04.021
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717309975
N2  - Background and Objective: Cardiac arrhythmia is an abnormal variation in the heart electrical activity that affects millions of people worldwide. Electrocardiogram (ECG) signals have been widely used to assess and diagnose cardiac abnormalities. Methods: A novel methodology based on shearlet and contourlet transforms for automatically classify an input ECG signal into different heart beat types is proposed and evaluated in this work. Classifiers are trained through a set of features extracted from these time-frequency coefficients. Results: Tests are conducted on MIT-BIH data set to demonstrate the effectiveness of the proposed classification method. The shearlet and contourlet transforms achieved high classification accuracy rates. Conclusions: The developed system can help cardiologists obtain structural and functional information of the heart by means of ECG patterns, improving their diagnostic tasks.
ER  - 
TY  - JOUR
T1  - Using a similarity measure for credible classification
A1  - Subasi, M
A1  - Subasi, E
A1  - Anthony, M
A1  - Hammer, P L
Y1  - 2009///
KW  -  Boolean similarity measure
KW  -  Classification
KW  - Boolean functions
JF  - Discrete Applied Mathematics
VL  - 157
IS  - 5
SP  - 1104
EP  - 1112
DO  - https://doi.org/10.1016/j.dam.2008.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S0166218X08001790
N2  - This paper concerns classification by Boolean functions. We investigate the classification accuracy obtained by standard classification techniques on unseen points (elements of the domain, {0,1}n, for some n) that are similar, in particular senses, to the points that have been observed as training observations. Explicitly, we use a new measure of how similar a point x∈{0,1}n is to a set of such points to restrict the domain of points on which we offer a classification. For points sufficiently dissimilar, no classification is given. We report on experimental results which indicate that the classification accuracies obtained on the resulting restricted domains are better than those obtained without restriction. These experiments involve a number of standard data-sets and classification techniques. We also compare the classification accuracies with those obtained by restricting the domain on which classification is given by using the Hamming distance.
ER  - 
TY  - JOUR
T1  - Evaluation of classification performance in human lower limb jump phases of signal correlation information and LSTM models
A1  - Lu, Yanzheng
A1  - Wang, Hong
A1  - Qi, Yangyang
A1  - Xi, Hailong
Y1  - 2021///
KW  -  Complex network
KW  -  IMU
KW  -  Jump phase recognition
KW  -  Long–short term memory network
KW  -  Pearson correlation coefficient
KW  - EMG
JF  - Biomedical Signal Processing and Control
VL  - 64
SP  - 102279
EP  - 102279
DO  - https://doi.org/10.1016/j.bspc.2020.102279
UR  - https://www.sciencedirect.com/science/article/pii/S1746809420304018
N2  - The recognition of the human lower limb jump phases plays an important role in measuring the degree of rehabilitation and the control of the exoskeleton. However, one of the challenges is that the recognition accuracy using sEMG signal is low. In this paper, we propose two types of long–short term memory network (LSTM) models for offline and online recognition of jump sequences. The recognition accuracies of bidirectional LSTM and convolutional LSTM (ConvLSTM) for sEMG reach 97.84% and 97.44%, respectively. When the offline analysis model is used with sEMG sequence of the jump process, the misclassification only occurs in the adjacent phases. From the Pearson correlation coefficients (PCCs) of sEMG and IMU signals, the complex network of muscles and kinematics is built to analyze the coupling of muscle and motion in the jumping process. Taking the sequence composed of PCC matrix with sensor information confusion as the input, ConvLSTM model can acquire spatiotemporal features and the accuracy of the online model can reach 98.13%. In this paper, the number and length of analysis windows that influence the model performance are studied. The synthesis method of Euler angle signals facilitates the recognition of human movement intention.
ER  - 
TY  - JOUR
T1  - Linear dimensionality reduction based on Hybrid structure preserving projections
A1  - Zhang, Yupei
A1  - Xiang, Ming
A1  - Yang, Bo
Y1  - 2016///
KW  -  Hybrid data structure
KW  -  Learning discriminative features
KW  -  Neighborhood representation
KW  -  Sparse representation
KW  - Dimensionality reduction
JF  - Neurocomputing
VL  - 173
SP  - 518
EP  - 529
DO  - https://doi.org/10.1016/j.neucom.2015.07.011
UR  - https://www.sciencedirect.com/science/article/pii/S0925231215009856
N2  - Recent advances have shown the methods based on local structure preserving projections can effectively learn discriminative features. The two attractive approaches for characterizing such data structure are: the classical nearest neighbor strategy for neighborhood structure and the sparse coding algorithm for sparsity structure. Motivated by the intuitive analysis of the relationship between the two structures, in this paper, we take both of them into account and propose two integrated approaches for dimensionality reduction. Concretely, we for achieving improvement directly integrate two available objectives, utilizing neighborhood structure and based on sparsity structure, to construct the combined method, briefly called CSNP. However, such rough strategy often results in its degradation in practice. Instead of the superficial combination, we exploit a hybrid structure by intergrading the two structures and then propose the Sparsity and Neighborhood Preserving Projections, dubbed SNPP, by preserving the hybrid structure into reduced subspace. The resulting optimization problems can be also interpreted as an instance of the general graph embedding framework and can reduce to the generalized eigenvalue decomposition problem. Finally, we conduct extensive experiments on publicly available data sets to verify the efficacy of our algorithms. From the experimental results, we roughly draw the conclusion that neighborhood structure is more important for low-dimensional data while sparsity structure is more useful for high-dimensional data.
ER  - 
TY  - JOUR
T1  - Data mining issues and opportunities for building nursing knowledge
A1  - Goodwin, Linda
A1  - VanDyne, Michele
A1  - Lin, Simon
A1  - Talbert, Steven
Y1  - 2003///
KW  -  Clinical information systems
KW  -  Data mining issues
KW  -  Expert nurses
KW  -  KDD
KW  -  Knowledge base
KW  -  Knowledge discovery in data
KW  -  Nurse knowledge workers
KW  -  Nursing knowledge
KW  - Data mining
JF  - Journal of Biomedical Informatics
VL  - 36
IS  - 4
SP  - 379
EP  - 388
DO  - https://doi.org/10.1016/j.jbi.2003.09.020
UR  - https://www.sciencedirect.com/science/article/pii/S1532046403001011
N1  - Building Nursing Knowledge through Informatics: From Concept Representation to Data Mining
N2  - Health care information systems tend to capture data for nursing tasks, and have little basis in nursing knowledge. Opportunity lies in an important issue where the knowledge used by expert nurses (nursing knowledge workers) in caring for patients is undervalued in the health care system. The complexity of nursing’s knowledge base remains poorly articulated and inadequately represented in contemporary information systems. There is opportunity for data mining methods to assist with discovering important linkages between clinical data, nursing interventions, and patient outcomes. Following a brief overview of relevant data mining techniques, a preterm risk prediction case study illustrates the opportunities and describes typical data mining issues in the nontrivial task of building knowledge. Building knowledge in nursing, using data mining or any other method, will make progress only if important data that capture expert nurses’ contributions are available in clinical information systems configurations.
ER  - 
TY  - JOUR
T1  - Principal Association Mining: An efficient classification approach
A1  - Chen, Fuzan
A1  - Wang, Yanlan
A1  - Li, Minqiang
A1  - Wu, Harris
A1  - Tian, Jin
Y1  - 2014///
KW  -  Association rule
KW  -  Associative classification
KW  -  Classification
KW  -  Knowledge discovery
KW  - Data mining
JF  - Knowledge-Based Systems
VL  - 67
SP  - 16
EP  - 25
DO  - https://doi.org/10.1016/j.knosys.2014.06.013
UR  - https://www.sciencedirect.com/science/article/pii/S0950705114002342
N2  - Classification is one of the key tasks in business intelligence, decision science, and machine learning. Associative classification has aroused significant research interest in recent years due to its superior accuracy. Traditional association rule mining algorithms often yield many redundant and sometimes conflicting class association rules. This paper presents a new, efficient associative classification approach. This new approach produces a compact classifier with a small number of association rules, yet with good classification performance. This approach is based on a novel rule quality metric, named as Principality, which measures an association rule’s classification accuracy and coverage for a specific class. Heuristic methods utilizing the Principality metric are applied to rule pruning and associative classifier construction to produce a compact classifier. This Principal Association Mining (PAM) approach is confirmed to be effective at improving classification accuracy as well as decreasing classifier size by experiments conducted on 17 datasets.
ER  - 
TY  - JOUR
T1  - Automated identification of insomnia using optimal bi-orthogonal wavelet transform technique with single-channel EEG signals
A1  - Sharma, Manish
A1  - Patel, Virendra
A1  - Acharya, U Rajendra
Y1  - 2021///
KW  -  Classification
KW  -  EBDT (ensemble bagged decision trees)
KW  -  Insomnia
KW  -  Machine learning
KW  -  PSG (polysomnographic)
KW  -  Sleep disorders
KW  -  Sleep stages
KW  - EEG (electroencephalography)
JF  - Knowledge-Based Systems
VL  - 224
SP  - 107078
EP  - 107078
DO  - https://doi.org/10.1016/j.knosys.2021.107078
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121003415
N2  - Nowadays, sleep studies have gained a lot of attention from researchers due to the immense importance of quality sleep. Human beings spend nearly one-third of their lives in sleep. Therefore, adequate quality sleep is indispensable for a healthy life. The sleep pattern may not be the same for every individual as one may either suffer from various sleep ailments such as insomnia, apnea, bruxism, epilepsy, narcolepsy, or maybe healthy with no sleep disorder. Insomnia is a prevalent sleep disorder that can lead to many health-related issues in human beings. Usually, polysomnogram (PSG) signals are used to detect the sleep stages and sleep disorders. The PSG signals are difficult to handle, time-consuming, and not convenient for patients. Hence, in this work, we have used single-channel electroencephalogram (EEG) signals to detect insomnia automatically. To the best of our knowledge, this is the first study on automated insomnia identification using the CAP database and EEG alone. We have used the single-channel EEG channel and created eight different subsets based on sleep-stages annotations according to American Academy of Sleep Medicine (AASM) guidelines for sleep stage scoring. The classification task is performed on each subset for the automated identification of insomnia. A new class of an optimal bi-orthogonal filter bank is used for wavelet decomposition. The wavelet-based norm features are extracted using the optimal filter bank. Then these features are fed to various machine learning algorithms. The proposed model has attained the highest classification performance with the area under receivers’ operating characteristic curve (AROC) of 0.97, F1-score of 0.9645, the accuracy of 95.60%, and Cohen’s Kappa value of 0.9067 using an ensemble bagged decision trees (EBDT) classifier. Our developed model can be used to detect insomnia using sleep EEG signals accurately and provide early treatment. The method is simple and computationally fast. The proposed system can be used at home as well as at sleep labs to monitor insomnia.
ER  - 
TY  - JOUR
T1  - Automatic white matter lesion segmentation using contrast enhanced FLAIR intensity and Markov Random Field
A1  - Roy, Pallab Kanti
A1  - Bhuiyan, Alauddin
A1  - Janke, Andrew
A1  - Desmond, Patricia M
A1  - Wong, Tien Yin
A1  - Abhayaratna, Walter P
A1  - Storey, Elsdon
A1  - Ramamohanarao, Kotagiri
Y1  - 2015///
KW  -  Cerebrovascular diseases
KW  -  Magnetic resonance imaging (MRI)
KW  -  Markov Random Field (MRF)
KW  -  Random forest (RF)
KW  - White matter lesions (WMLs)
JF  - Computerized Medical Imaging and Graphics
VL  - 45
SP  - 102
EP  - 111
DO  - https://doi.org/10.1016/j.compmedimag.2015.08.005
UR  - https://www.sciencedirect.com/science/article/pii/S0895611115001214
N2  - White matter lesions (WMLs) are small groups of dead cells that clump together in the white matter of brain. In this paper, we propose a reliable method to automatically segment WMLs. Our method uses a novel filter to enhance the intensity of WMLs. Then a feature set containing enhanced intensity, anatomical and spatial information is used to train a random forest classifier for the initial segmentation of WMLs. Following that a reliable and robust edge potential function based Markov Random Field (MRF) is proposed to obtain the final segmentation by removing false positive WMLs. Quantitative evaluation of the proposed method is performed on 24 subjects of ENVISion study. The segmentation results are validated against the manual segmentation, performed under the supervision of an expert neuroradiologist. The results show a dice similarity index of 0.76 for severe lesion load, 0.73 for moderate lesion load and 0.61 for mild lesion load. In addition to that we have compared our method with three state of the art methods on 20 subjects of Medical Image Computing and Computer Aided Intervention Society's (MICCAI's) MS lesion challenge dataset, where our method shows better segmentation accuracy compare to the state of the art methods. These results indicate that the proposed method can assist the neuroradiologists in assessing the WMLs in clinical practice.
ER  - 
TY  - JOUR
T1  - Bayesian distance metric learning for discriminative fuzzy c-means clustering
A1  - Heidari, Negar
A1  - Moslehi, Zahra
A1  - Mirzaei, Abdolreza
A1  - Safayani, Mehran
Y1  - 2018///
KW  -  Bayesian inference
KW  -  Fuzzy clustering
KW  -  Markov chain monte carlo
KW  -  Probabilistic graphical models
KW  -  Unsupervised learning
KW  - Distance metric learning
JF  - Neurocomputing
VL  - 319
SP  - 21
EP  - 33
DO  - https://doi.org/10.1016/j.neucom.2018.08.071
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218310348
N2  - A great number of machine learning algorithms strongly depend on the underlying distance metric for representing the important correlations of input data. Distance metric learning is defined as learning an appropriate similarity or distance metric for all input data pairs. Metric learning algorithms are of supervised and unsupervised categories with different deterministic and probabilistic approaches. One of the objectives of unsupervised metric learning is to project data points into a new space in such a way that high clustering accuracy is provided. This is obtainable by maximizing between-clusters separation. There exist some deterministic metric learning methods to serve this purpose. In this article, a probabilistic method for unsupervised distance metric learning is proposed which aims to maximize the separability among different clusters in the projected space. In this proposed method, distance metric learning and fuzzy c-means clustering are jointly formulated in a sense that FCM provides clusters, and distance metric learning algorithm applies the obtained clusters to materialize the maximum separability among all; moreover, Markov Chain Monte Carlo (MCMC) algorithm is applied to infer the latent variables. This proposed method, not only can obtain a low dimensional projection with specified number of dimensions, but also it can learn the proper number of reduced dimensions for each dataset in an automated sense. The experimental results reveal the out-performance of this method on different real-world datasets against its counterparts.
ER  - 
TY  - JOUR
T1  - A resource elasticity framework for QoS-aware execution of cloud applications
A1  - Kaur, Pankaj Deep
A1  - Chana, Inderveer
Y1  - 2014///
KW  -  Application behavior
KW  -  Quality of service
KW  -  Resource elasticity
KW  - Cloud computing
JF  - Future Generation Computer Systems
VL  - 37
SP  - 14
EP  - 25
DO  - https://doi.org/10.1016/j.future.2014.02.018
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X14000430
N1  - Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications
N2  - Cloud infrastructures consisting of heterogeneous resources are increasingly being utilized for hosting large-scale distributed applications from diverse users with discrete needs. The multifarious cloud applications impose varied demands for computational resources along with multitude of performance implications. Successful hosting of cloud applications necessitates service providers to take into account the heterogeneity existing in the behavior of users, applications and system resources while respecting the user’s agreed Quality of Service (QoS) criteria. In this work, we propose a QoS-Aware Resource Elasticity (QRE) framework that allows service providers to make an assessment of the application behavior and develop mechanisms that enable dynamic scalability of cloud resources hosting the application components. Experimental results conducted on the Amazon EC2 cloud clearly demonstrate the effectiveness of our approach while complying with the agreed QoS attributes of users.
ER  - 
TY  - JOUR
T1  - An efficient integration and indexing method based on feature patterns and semantic analysis for big data
A1  - Nashipudimath, Madhu Mahesh
A1  - Shinde, Subhash K
A1  - Jain, Jayshree
Y1  - 2020///
KW  -  Feature patterns
KW  -  Indexing
KW  -  Integration
KW  -  Semantic analysis
KW  - Big data
JF  - Array
VL  - 7
SP  - 100033
EP  - 100033
DO  - https://doi.org/10.1016/j.array.2020.100033
UR  - https://www.sciencedirect.com/science/article/pii/S2590005620300187
N2  - Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.
ER  - 
TY  - JOUR
T1  - Model-based physiomarkers of cerebral hemodynamics in patients with mild cognitive impairment
A1  - Marmarelis, V Z
A1  - Shin, D C
A1  - Orme, M E
A1  - Zhang, R
Y1  - 2014///
KW  -  Alzheimer's disease
KW  -  Cerebral hemodynamics
KW  -  Closed-loop PDM-based model
KW  -  Mild cognitive impairment
KW  -  Model-based physiomarkers
KW  -  Nonlinear modeling
KW  -  Principal dynamic modes
KW  - Modeling of cerebral autoregulation
JF  - Medical Engineering & Physics
VL  - 36
IS  - 5
SP  - 628
EP  - 637
DO  - https://doi.org/10.1016/j.medengphy.2014.02.025
UR  - https://www.sciencedirect.com/science/article/pii/S1350453314000551
N1  - Special Issue on Cerebral Autoregulation: Measurement and Modelling
N2  - In our previous studies, we have introduced model-based “functional biomarkers” or “physiomarkers” of cerebral hemodynamics that hold promise for improved diagnosis of early-stage Alzheimer's disease (AD). The advocated methodology utilizes subject-specific data-based dynamic nonlinear models of cerebral hemodynamics to compute indices (serving as possible diagnostic physiomarkers) that quantify the state of cerebral blood flow autoregulation to pressure-changes (CFAP) and cerebral CO2 vasomotor reactivity (CVMR) in each subject. The model is estimated from beat-to-beat measurements of mean arterial blood pressure, mean cerebral blood flow velocity and end-tidal CO2, which can be made reliably and non-invasively under resting conditions. In a previous study, it was found that a CVMR index quantifying the impairment in CO2 vasomotor reactivity correlates with clinical indications of early AD, offering the prospect of a potentially useful diagnostic tool. In this paper, we explore the use of the same model-based indices for patients with amnestic Mild Cognitive Impairment (MCI), a preclinical stage of AD, relative to a control subjects and clinical cognitive assessments. It was found that the model-based CVMR values were lower for MCI patients relative to the control subjects.
ER  - 
TY  - JOUR
T1  - A deep learning–based, unsupervised method to impute missing values in electronic health records for improved patient management
A1  - Xu, Da
A1  - Hu, Paul Jen-Hwa
A1  - Huang, Ting-Shuo
A1  - Fang, Xiao
A1  - Hsu, Chih-Chin
Y1  - 2020///
KW  -  Clinical decision support
KW  -  Clinical predictive analytics
KW  -  Deep learning
KW  -  Electronic health records
KW  -  Machine learning
KW  - Missing value imputation
JF  - Journal of Biomedical Informatics
VL  - 111
SP  - 103576
EP  - 103576
DO  - https://doi.org/10.1016/j.jbi.2020.103576
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420302045
N2  - Electronic health records (EHRs) often suffer missing values, for which recent advances in deep learning offer a promising remedy. We develop a deep learning–based, unsupervised method to impute missing values in patient records, then examine its imputation effectiveness and predictive efficacy for peritonitis patient management. Our method builds on a deep autoencoder framework, incorporates missing patterns, accounts for essential relationships in patient data, considers temporal patterns common to patient records, and employs a novel loss function for error calculation and regularization. Using a data set of 27,327 patient records, we perform a comparative evaluation of the proposed method and several prevalent benchmark techniques. The results indicate the greater imputation performance of our method relative to all the benchmark techniques, recording 5.3–15.5% lower imputation errors. Furthermore, the data imputed by the proposed method better predict readmission, length of stay, and mortality than those obtained from any benchmark techniques, achieving 2.7–11.5% improvements in predictive efficacy. The illustrated evaluation indicates the proposed method’s viability, imputation effectiveness, and clinical decision support utilities. Overall, our method can reduce imputation biases and be applied to various missing value scenarios clinically, thereby empowering physicians and researchers to better analyze and utilize EHRs for improved patient management.
ER  - 
TY  - JOUR
T1  - Bioelectronic tongues: New trends and applications in water and food analysis
A1  - Cetó, Xavier
A1  - Voelcker, Nicolas H
A1  - Prieto-Simón, Beatriz
Y1  - 2016///
KW  -  Biosensor
KW  -  Food
KW  -  Sensor array
KW  -  Taste sensor
KW  -  Water
KW  - Bioelectronic tongue
JF  - Biosensors and Bioelectronics
VL  - 79
SP  - 608
EP  - 626
DO  - https://doi.org/10.1016/j.bios.2015.12.075
UR  - https://www.sciencedirect.com/science/article/pii/S0956566315307272
N2  - Over the last years, there has been an increasing demand for fast, highly sensitive and selective methods of analysis to meet new challenges in environmental monitoring, food safety and public health. In response to this demand, biosensors have arisen as a promising tool, which offers accurate chemical data in a timely and cost-effective manner. However, the difficulty to obtain sensors with appropriate selectivity and sensitivity for a given analyte, and to solve analytical problems which do not require the quantification of a certain analyte, but an overall effect on a biological system (e.g. toxicity, quality indices, provenance, freshness, etc.), led to the concept of electronic tongues as a new strategy to tackle these problems. In this direction, to improve the performance of electronic tongues, and thus to spawn new application fields, biosensors have recently been incorporated to electronic tongue arrays, leading to what is known as bioelectronic tongues. Bioelectronic tongues provide superior performance by combining the capabilities of electronic tongues to derive meaning from complex or imprecise data, and the high selectivity and specificity of biosensors. The result is postulated as a tool that exploits chemometrics to solve biosensors’ interference problems, and biosensors to solve electronic tongues’ selectivity problems. The review presented herein aims to illustrate the capabilities of bioelectronic tongues as analytical tools, especially suited for screening analysis, with particular emphasis in water analysis and the characterization of food and beverages. After briefly reviewing the key concepts related to the design and principles of electronic tongues, we provide an overview of significant contributions to the field of bioelectronic tongues and their future perspectives.
ER  - 
TY  - JOUR
T1  - Patients’ involvement in e-health services quality assessment: A system for the automatic interpretation of SMS-based patients’ feedback
A1  - Rubrichi, Stefania
A1  - Battistotti, Andrea
A1  - Quaglini, Silvana
Y1  - 2014///
KW  -  Conditional random fields
KW  -  Health service assessment
KW  -  Information extraction
KW  -  Patients’ feedback
KW  -  SMS
KW  - e-Health
JF  - Journal of Biomedical Informatics
VL  - 51
SP  - 41
EP  - 48
DO  - https://doi.org/10.1016/j.jbi.2014.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414000604
N2  - Purpose
Effective communication between patients and health services providers is a key aspect for optimizing and maintaining these services. This work describes a system for the automatic evaluation of users’ perception of the quality of SmsCup, a reminder system for outpatient visits based on short message service (SMS). The final purpose is the creation of a closed-loop control system for the outpatient service, where patients’ complaints and comments represent a feedback that can be used for a better implementation of the service itself.
Methods
SmsCup was adopted since about eight years by an Italian healthcare organization, with very good results in reducing the no-show (missing visits) phenomenon. During these years, a number of citizens, even if not required, sent a message back, with comments about the service. The automatic interpretation of the content of those SMS may be useful for monitoring and improving service performances.Yet, due to the complex nature of SMS language, their interpretation represents an ongoing challenge. The proposed system uses conditional random fields as the information extraction method for classifying messages into several semantic categories. The categories refer to appreciation of the service or complaints of various types. Then, the system analyzes the extracted content and provides feedback to the service providers, making them learning and acting on this basis.
Results
At each step, the content of the messages reveals the actual state of the service as well as the efficacy of corrective actions previously undertaken. Our evaluations showed that: (i) the SMS classification system has achieved good overall performance with an average F1-measure and an overall accuracy of about 92%; (ii) the notification of the patients’ feedbacks to service providers showed a positive impact on service functioning.
Conclusions
Our study proposed an interactive patient-centered system for continuous monitoring of the service quality. It has demonstrated the feasibility of a tool for the analysis and notification of the patients’ feedback on their service experiences, which would support a more regular access to the service.
ER  - 
TY  - JOUR
T1  - A support vector machine-based ensemble algorithm for breast cancer diagnosis
A1  - Wang, Haifeng
A1  - Zheng, Bichen
A1  - Yoon, Sang Won
A1  - Ko, Hoo Sang
Y1  - 2018///
KW  -  Cancer diagnoses
KW  -  Ensemble learning
KW  -  Support vector machine
KW  -  Variance reduction
KW  - Analytics
JF  - European Journal of Operational Research
VL  - 267
IS  - 2
SP  - 687
EP  - 699
DO  - https://doi.org/10.1016/j.ejor.2017.12.001
UR  - https://www.sciencedirect.com/science/article/pii/S0377221717310810
N2  - This research studies a support vector machine (SVM)-based ensemble learning algorithm for breast cancer diagnosis. Illness diagnosis plays a critical role in designating treatment strategies, which are highly related to patient safety. Nowadays, numerous classification models in data mining domains are adapted to breast cancer diagnosis based on patients’ historical medical records. However, the performance of each algorithm depends on various model configurations, such as input feature types and model parameters. To tackle the limitation of individual model performance, this research focuses on breast cancer diagnosis that uses an SVM-based ensemble learning algorithm to reduce the diagnosis variance and increase diagnosis accuracy. Twelve different SVMs, based on the proposed Weighted Area Under the Receiver Operating Characteristic Curve Ensemble (WAUCE) approach, are hybridized. To evaluate the performance of the proposed model, Wisconsin Breast Cancer, Wisconsin Diagnostic Breast Cancer, and the U.S. National Cancer Institute’s Surveillance, Epidemiology, and End Results (SEER) program breast cancer datasets have been studied. The experimental results show that the WAUCE model achieves a higher accuracy with a significantly lower variance for breast cancer diagnosis compared to five other ensemble mechanisms and two common ensemble models, i.e., adaptive boosting and bagging classification tree. The proposed WAUCE model reduces the variance by 97.89% and increases accuracy by 33.34%, compared to the best single SVM model on the SEER dataset. In practice, the proposed methodology can be further applied to other illness diagnoses, which offers an alternative to a safer, more reliable, and more robust illness diagnosis process.
ER  - 
TY  - JOUR
T1  - Analysis of brain NMR images for age estimation with deep learning
A1  - Rossi, Alberto
A1  - Vannuccini, Gioele
A1  - Andreini, Paolo
A1  - Bonechi, Simone
A1  - Giacomini, Giorgia
A1  - Scarselli, Franco
A1  - Bianchini, Monica
Y1  - 2019///
KW  -  Convolutional Neural Networks
KW  -  Magnetic Resonance Imaging
KW  -  Medical Image Analysis
KW  - Brain Age Estimation
JF  - Procedia Computer Science
VL  - 159
SP  - 981
EP  - 989
DO  - https://doi.org/10.1016/j.procs.2019.09.265
UR  - https://www.sciencedirect.com/science/article/pii/S1877050919314619
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019
N2  - During the last decade, deep learning and Convolutional Neural Networks (CNNs) have produced a devastating impact on computer vision, yielding exceptional results on a variety of problems, including analysis of medical images. Recently, these techniques have been extended to 3D images with the downside of a large increase in the computational load. In particular, state-of-the-art CNNs have been used for brain Nuclear Magnetic Resonance (NMR) imaging, with the aim of estimating the patients’ age. In fact, a large discrepancy between the real and the estimated age is a clear alarm for the onset of neurodegenerative diseases, such as some types of early dementia and Alzheimer’s disease. In this paper, we propose an effective alternative to 3D convolutions that guarantees a significant reduction of the computational requirements for this kind of analysis. The proposed architectures achieve comparable results with the competitor 3D methods, requiring only a fraction of the training time and GPU memory.
ER  - 
TY  - JOUR
T1  - Classification of carotid artery Doppler signals in the early phase of atherosclerosis using complex-valued artificial neural network
A1  - Ceylan, Murat
A1  - Ceylan, Rahime
A1  - Dirgenali, Fatma
A1  - Kara, Sadık
A1  - Özbay, Yüksel
Y1  - 2007///
KW  -  Carotid artery
KW  -  Complex valued ANN
KW  -  Doppler
KW  - Atherosclerosis
JF  - Computers in Biology and Medicine
VL  - 37
IS  - 1
SP  - 28
EP  - 36
DO  - https://doi.org/10.1016/j.compbiomed.2005.08.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482505001186
N2  - In this study, carotid arterial Doppler ultrasound signals were acquired from left carotid arteries of 38 patients and 40 healthy volunteers. The patient group had an established diagnosis of the early phase of atherosclerosis through coronary or aortofemoropopliteal angiographies. Results were classified using complex-valued artificial neural network (CVANN). Principal component analysis (PCA) and fuzzy c-means clustering (FCM) algorithm were used to make a CVANN system more effective. For this aim, before classifying with CVANN, PCA method was used for feature extraction in PCA-CVANN architecture and FCM algorithm was used for data set reduction in FCM-CVANN architecture. Training and test data were selected randomly using 10-fold cross validation. PCA-CVANN and FCM-CVANN architectures classified healthy and unhealthy subjects for training and test data with about 100% correct classification rate. These results shown that PCA-CVANN and FCM-CVANN classified Doppler signals successfully.
ER  - 
TY  - JOUR
T1  - ECG based human identification using Second Order Difference Plots
A1  - Altan, Gokhan
A1  - Kutlu, Yakup
A1  - Yeniad, Mustafa
Y1  - 2019///
KW  -  Biometric
KW  -  Identification
KW  -  PQRST complex
KW  -  SODP
KW  -  Second Order Difference Plot
KW  - ECG
JF  - Computer Methods and Programs in Biomedicine
VL  - 170
SP  - 81
EP  - 93
DO  - https://doi.org/10.1016/j.cmpb.2019.01.010
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717301530
N2  - Background and objective
ECG is one of the biometric signals that has been studied in peer-reviewed over past years. The developments on the signal analysis methods show that the studies on the ECG would continue unabatedly. It has a common use on cardiac diseases with high rates of classification performances by integrating it with signal analysis methods. The aim of the study is to utilize the ECG for human identification.
Methods
Second Order Difference Plot (SODP) is a non-linear time-series analysis method that allows determining the features using the statistical analysis of the wave distributions. The SODP features were extracted using different quantification methods for ECG-based human identification. A new quantification approach has been proposed on the SODP for ECG-based human identification. The proposed method, Logarithmic Grid Analysis, was compared with the existing quantification methods on the SODP. The region of the SODP was divided into sub-regions with logarithmically increasing distances and the numbers of data points in each logarithmic sub regions were calculated in the proposed method. Three different databases were used to test the validity of the method. These records have been tested with the conventional feature extraction methods on the SODP. The long-term ECG signals were divided into 5-s short-term ECG signals.
Results
The Logarithmic Grid Analysis features that were counted from short-time ECG signals were classified with k-Nearest Neighbor algorithm using 10-fold cross validation, and the identification performance of the proposed model was evaluated. Consequently, high accuracy rates of 91.96%, 99.86% and 95.12% were achieved on ECG-based human identification using the Logarithmic Grid Analysis method on the SODP.
Conclusions
The density score of data points at the center of the SODP is too high. This case increases the importance of the regions close the center in order to find the detailed and significant features from the SODP. The number of data points at the center has been extracted in more detail and the vertex areas of the major axes of the SODP can be interpreted in the aggregate sub-regions by using logarithmically increasing distances with a small number of feature size.
ER  - 
TY  - JOUR
T1  - A decision support system for Acute Leukaemia classification based on digital microscopic images
A1  - Negm, Ahmed S
A1  - Hassan, Osama A
A1  - Kandil, Ahmed H
Y1  - 2018///
KW  -  Classification
KW  -  Image processing
KW  -  Leukaemia
KW  - Decision support system
JF  - Alexandria Engineering Journal
VL  - 57
IS  - 4
SP  - 2319
EP  - 2332
DO  - https://doi.org/10.1016/j.aej.2017.08.025
UR  - https://www.sciencedirect.com/science/article/pii/S1110016817302831
N2  - In the era of digital microscopic imaging, Image Processing, data analysis, classification, decision support systems have emerged as one of the most important tools for diagnostic research. Physicians can observe cellular internal structures abnormalities by visualizing and analyzing images. Leukemia is a malignant disease characterized by the uncontrolled accumulation of abnormal white blood cells. The recognition of acute leukemia blast cells in colored microscopic images is a challenging task. The first important step in the automatic recognition of this disease, image segmentation, is considered to be the most critical step. In this study, we present a decision support system that includes the panel selection, segmentation using K-means clustering to identify the leukemia cells and features extraction, and image refinement. After the decision support system successfully identifies the cells and its internal structure, the cells are classified according to their morphological features of this analysis the decision support system was tested using a public dataset designed to test segmentation techniques for identifying specific cells, and the results of this analysis were compared with those of other techniques, which were suggested by other researchers, applied to the same data. The algorithm was then applied to another dataset, extracted under the supervision by an expert pathologist, from a local hospital; the total dataset consisted of 757 images gathered from two datasets. The images of the datasets are labeled with three different labels, which represents three types of leukemia cells: blast, myelocyte, and segmented cells. The process of labeling of these images was revised by the expert pathologist. The algorithm testing using this dataset demonstrated an overall accuracy of 99.517%, the sensitivity of 99.348%, and specificity of 99.529%. Therefore, this algorithm yielded promising results and warrants further research.
ER  - 
TY  - JOUR
T1  - Shared farthest neighbor approach to clustering of high dimensionality, low cardinality data
A1  - Rovetta, Stefano
A1  - Masulli, Francesco
Y1  - 2006///
KW  -  DNA microarray data analysis
KW  -  High-dimensional data analysis
KW  -  Similarity-based clustering
KW  - Data clustering
JF  - Pattern Recognition
VL  - 39
IS  - 12
SP  - 2415
EP  - 2425
DO  - https://doi.org/10.1016/j.patcog.2006.06.021
UR  - https://www.sciencedirect.com/science/article/pii/S0031320306003001
N1  - Bioinformatics
N2  - Clustering algorithms are routinely used in biomedical disciplines, and are a basic tool in bioinformatics. Depending on the task at hand, there are two most popular options, the central partitional techniques and the agglomerative hierarchical clustering techniques and their derivatives. These methods are well studied and well established. However, both categories have some drawbacks related to data dimensionality (for partitional algorithms) and to the bottom-up structure (for hierarchical agglomerative algorithms). To overcome these limitations, motivated by the problem of gene expression analysis with DNA microarrays, we present a hierarchical clustering algorithm based on a completely different principle, which is the analysis of shared farthest neighbors. We present a framework for clustering using ranks and indexes, and introduce the shared farthest neighbors (SFN) clustering criterion. We illustrate the properties of the method and present experimental results on different data sets, using the strategy of evaluating data clustering by extrinsic knowledge given by class labels.
ER  - 
TY  - JOUR
T1  - Finding optimal model parameters by deterministic and annealed focused grid search
A1  - Barbero Jiménez, Álvaro
A1  - López Lázaro, Jorge
A1  - Dorronsoro, José R
Y1  - 2009///
KW  -  Annealing
KW  -  CMA-ES
KW  -  Evolutive algorithms
KW  -  Optimal parameters
KW  - Grid search
JF  - Neurocomputing
VL  - 72
IS  - 13
SP  - 2824
EP  - 2832
DO  - https://doi.org/10.1016/j.neucom.2008.09.024
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209001222
N1  - Hybrid Learning Machines (HAIS 2007) / Recent Developments in Natural Computation (ICNC 2007)
N2  - Optimal parameter model finding is usually a crucial task in engineering applications of classification and modelling. The exponential cost of linear search on a parameter grid of a given precision rules it out in all but the simplest problems and random algorithms such as uniform design or the covariance matrix adaptation-evolution strategy (CMA-ES) are usually applied. In this work we shall present two focused grid search (FGS) alternatives in which one repeatedly zooms into more concentrated sets of discrete grid points in the parameter search space. The first one, deterministic FGS (DFGS), is much faster than standard search although still too costly in problems with a large number of parameters. The second one, annealed FGS (AFGS), is a random version of DFGS where a fixed fraction of grid points is randomly selected and examined. As we shall numerically see over several classification problems for multilayer perceptrons and support vector machines, DFGS and AFGS are competitive with respect to CMA-ES, one of the most successful evolutive black-box optimizers. The choice of a concrete technique may thus rest in other facts, and the simplicity and basically parameter-free nature of both DFGS and AFGS may make them worthwile alternatives to the thorough theoretical and experimental background of CMA-ES.
ER  - 
TY  - JOUR
T1  - Improving performance of neural classifiers via selective reduction of target levels
A1  - Mora-Jiménez, I
A1  - Figueiras-Vidal, A R
Y1  - 2009///
KW  -  Classification
KW  -  Learning algorithm
KW  -  Reduced target level
KW  -  Sample selection
KW  - Artificial neural networks
JF  - Neurocomputing
VL  - 72
IS  - 13
SP  - 3020
EP  - 3027
DO  - https://doi.org/10.1016/j.neucom.2009.04.002
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209001465
N1  - Hybrid Learning Machines (HAIS 2007) / Recent Developments in Natural Computation (ICNC 2007)
N2  - Reducing the level of the targets corresponding to training samples for a machine classifier using the outputs of an auxiliary classifier is interesting because it allows to save expressive power unnecessarily dedicated to increase the output level of well-classified samples. In this paper we propose an iterative form of this selective reduction of target levels with a simple linear reduction schedule. Extensive simulations show that the proposed method has not only a performance better than or equal to conventional training or using static versions of the reduction, but also with respect to support vector machines (SVM). This potential advantage is accompanied by a smaller size and a design effort not much higher than the corresponding SVM, thus making the proposed method very attractive for practical applications.
ER  - 
TY  - JOUR
T1  - Machine-learning for biopharmaceutical batch process monitoring with limited data
A1  - Tulsyan, Aditya
A1  - Garvin, Christopher
A1  - Undey, Cenk
Y1  - 2018///
KW  -  Biopharmaceutical manufacturing
KW  -  Low-N problem
KW  - Process monitoring
JF  - IFAC-PapersOnLine
VL  - 51
IS  - 18
SP  - 126
EP  - 131
DO  - https://doi.org/10.1016/j.ifacol.2018.09.287
UR  - https://www.sciencedirect.com/science/article/pii/S2405896318319645
N1  - 10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018
N2  - Commercial biopharmaceutical manufacturing comprises of multiple distinct processing steps that require effective and efficient monitoring of many variables simultaneously in real-time. This article addresses the problem of real-time statistical batch process monitoring (BPM) for biopharmaceutical processes with limited production history; herein, referred to as the ‘Low-N’ problem. In this article, we propose an approach to transition from a Low-N scenario to a Large-N scenario by generating an arbitrarily large number of in silico batch data sets. The proposed method is a combination of hardware exploitation and algorithm development. To this effect, we propose a Bayesian non-parametric approach to model a batch process, and then use probabilistic programming to generate an arbitrarily large number of dynamic in silico campaign data sets. The efficacy of the proposed solution is elucidated on an industrial process.
ER  - 
TY  - JOUR
T1  - Predicting the graft survival for heart–lung transplantation patients: An integrated data mining methodology
A1  - Oztekin, Asil
A1  - Delen, Dursun
A1  - Kong, Zhenyu (James)
Y1  - 2009///
KW  -  Classification
KW  -  Combined heart–lung transplantation
KW  -  Cox proportional hazards models
KW  -  Data mining
KW  - Survival analysis
JF  - International Journal of Medical Informatics
VL  - 78
IS  - 12
SP  - e84
EP  - e96
DO  - https://doi.org/10.1016/j.ijmedinf.2009.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S1386505609000707
N1  - Mining of Clinical and Biomedical Text and Data Special Issue
N2  - Background
Predicting the survival of heart–lung transplant patients has the potential to play a critical role in understanding and improving the matching procedure between the recipient and graft. Although voluminous data related to the transplantation procedures is being collected and stored, only a small subset of the predictive factors has been used in modeling heart–lung transplantation outcomes. The previous studies have mainly focused on applying statistical techniques to a small set of factors selected by the domain-experts in order to reveal the simple linear relationships between the factors and survival. The collection of methods known as ‘data mining’ offers significant advantages over conventional statistical techniques in dealing with the latter's limitations such as normality assumption of observations, independence of observations from each other, and linearity of the relationship between the observations and the output measure(s). There are statistical methods that overcome these limitations. Yet, they are computationally more expensive and do not provide fast and flexible solutions as do data mining techniques in large datasets.
Purpose
The main objective of this study is to improve the prediction of outcomes following combined heart–lung transplantation by proposing an integrated data-mining methodology.
Methods
A large and feature-rich dataset (16,604 cases with 283 variables) is used to (1) develop machine learning based predictive models and (2) extract the most important predictive factors. Then, using three different variable selection methods, namely, (i) machine learning methods driven variables—using decision trees, neural networks, logistic regression, (ii) the literature review-based expert-defined variables, and (iii) common sense-based interaction variables, a consolidated set of factors is generated and used to develop Cox regression models for heart–lung graft survival.
Results
The predictive models’ performance in terms of 10-fold cross-validation accuracy rates for two multi-imputed datasets ranged from 79% to 86% for neural networks, from 78% to 86% for logistic regression, and from 71% to 79% for decision trees. The results indicate that the proposed integrated data mining methodology using Cox hazard models better predicted the graft survival with different variables than the conventional approaches commonly used in the literature. This result is validated by the comparison of the corresponding Gains charts for our proposed methodology and the literature review based Cox results, and by the comparison of Akaike information criteria (AIC) values received from each.
Conclusions
Data mining-based methodology proposed in this study reveals that there are undiscovered relationships (i.e. interactions of the existing variables) among the survival-related variables, which helps better predict the survival of the heart–lung transplants. It also brings a different set of variables into the scene to be evaluated by the domain-experts and be considered prior to the organ transplantation.
ER  - 
TY  - JOUR
T1  - Detection of Glomerulosclerosis in Diabetic Nephropathy Using Contour-based Segmentation
A1  - Ravi, M
A1  - Hegadi, Ravindra S
Y1  - 2015///
KW  -  Active contour
KW  -  Glomerulosclerosis and Segmentation.
KW  -  Nephropathy
KW  - Detection
JF  - Procedia Computer Science
VL  - 45
SP  - 244
EP  - 249
DO  - https://doi.org/10.1016/j.procs.2015.03.129
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915003658
N1  - International Conference on Advanced Computing Technologies and Applications (ICACTA)
N2  - We have proposed a method for detection of glomerulosclerosis in diabetic nephropathy using Contour-Based Segmentation. Pathological images of the glomerulosclerosis were acquired from various patients. It is a challenging task as 100% detection of Nephropathy disease with regular pathological procedure is not possible. We propose a solution to the problem of segmentation of the glomerulosclerosis images for the analysis of abnormalities. The proposed method is the modification of the original Chan-Vese algorithm, with the varied values of parameters. The proposed method achieved better segmentation and the results are encouraging.
ER  - 
TY  - JOUR
T1  - Fully automated diabetic retinopathy screening using morphological component analysis
A1  - Imani, Elaheh
A1  - Pourreza, Hamid-Reza
A1  - Banaee, Touka
Y1  - 2015///
KW  -  Morphological component analysis (MCA) algorithm
KW  -  Retinal image quality assessment
KW  - Diabetic retinopathy screening
JF  - Computerized Medical Imaging and Graphics
VL  - 43
SP  - 78
EP  - 88
DO  - https://doi.org/10.1016/j.compmedimag.2015.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S0895611115000646
N2  - Diabetic retinopathy is the major cause of blindness in the world. It has been shown that early diagnosis can play a major role in prevention of visual loss and blindness. This diagnosis can be made through regular screening and timely treatment. Besides, automation of this process can significantly reduce the work of ophthalmologists and alleviate inter and intra observer variability. This paper provides a fully automated diabetic retinopathy screening system with the ability of retinal image quality assessment. The novelty of the proposed method lies in the use of Morphological Component Analysis (MCA) algorithm to discriminate between normal and pathological retinal structures. To this end, first a pre-screening algorithm is used to assess the quality of retinal images. If the quality of the image is not satisfactory, it is examined by an ophthalmologist and must be recaptured if necessary. Otherwise, the image is processed for diabetic retinopathy detection. In this stage, normal and pathological structures of the retinal image are separated by MCA algorithm. Finally, the normal and abnormal retinal images are distinguished by statistical features of the retinal lesions. Our proposed system achieved 92.01% sensitivity and 95.45% specificity on the Messidor dataset which is a remarkable result in comparison with previous work.
ER  - 
TY  - JOUR
T1  - Feature definition, analysis and selection for cystoid region characterization in Optical Coherence Tomography
A1  - de Moura, Joaquim
A1  - Vidal, Plácido L
A1  - Novo, Jorge
A1  - Rouco, José
A1  - Ortega, Marcos
Y1  - 2017///
KW  -  Optical Coherence Tomography
KW  -  classification
KW  -  feature selection
KW  -  intraretinal cystoid region characterization
KW  -  retinal imaging
KW  - Computer-aided diagnosis
JF  - Procedia Computer Science
VL  - 112
SP  - 1369
EP  - 1377
DO  - https://doi.org/10.1016/j.procs.2017.08.043
UR  - https://www.sciencedirect.com/science/article/pii/S1877050917313844
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France
N2  - Optical Coherence Tomography (OCT) is, nowadays, a clinical standard imaging technique in opthalmology as it provides more information than other classical modalities as can be, for instance, retinographies. OCT scans show a 3D representation of the real layout of the eye fundus in a non-invasive way, letting clinicians inspect deeply the retinal layers in a cross-sectional visualization. For that reason, OCT scans are commonly used in the study of the retinal morphology and the identification of pathological structures. Among them, an appropriate identification and analysis of any present intraretinal cystoid region is crucial to perform an adequate diagnosis of the exudative macular disease, one of the main causes of blindness in developed countries. In this work, we analyzed and characterized the intraretinal cystoid regions in OCT images by the definition of a complete and heterogeneous set of 326 intensity and texture-based features. Relief-F and L0 feature selectors were used in order to identify the optimal feature subsets that provide the best discriminative power. Representative classifiers, as the Linear Bayes Normal Classifier (LDC), Quadratic Bayes Normal Classifier (QDC) and K-Nearest Neighbor Classifier (KNN) were finally used to evaluate the potential of identification of the feature subsets. The method was validated using 51 OCT images. From them, 363 and 360 samples of cystoid and non-cystoid regions were selected, respectively. The best results were offered by the LDC classifier that, using a feature subset identified by the L0 selector, provided an accuracy of 0.9060.
ER  - 
TY  - JOUR
T1  - Automatic Ontology-based Annotation of Food, Nutrition and Health Arabic Web Content
A1  - Albukhitan, Saeed
A1  - Helmy, Tarek
Y1  - 2013///
KW  -  Annotation
KW  -  Arabic Language
KW  -  Name Entity Recognition
KW  - Semantic Web
JF  - Procedia Computer Science
VL  - 19
SP  - 461
EP  - 469
DO  - https://doi.org/10.1016/j.procs.2013.06.062
UR  - https://www.sciencedirect.com/science/article/pii/S1877050913006704
N1  - The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)
N2  - To have a successful semantic Web, it is critically required to have sufficient amount of relevant semantic and high-quality Web content. One way to produce such content is through the semantic annotation of the Web sources. Semantic annotation is the process of adding machine-readable content to the natural language textual content of the Web sources. Annotating Web content in Arabic language has received less attention compared to Latin Languages especially for content related to specific domains such as food, nutrition and health. Considering the huge amount of emerging Web content, semantic annotation of their contents by hand is neither practicable nor scalable. In this paper, we present an automatic annotation of the Arabic Web resources related to food, nutrition and health domains. The proposed method makes use of developed Arabic OWL ontologies related to those domains. It uses linguistic patterns to discover relevant relationships between the named entities in the Arabic Web resources. The extracted information is then associated to the corresponding concepts and object properties of the developed ontology to produce the RDF metadata for the corresponding Web resources. Empirical evaluations of the proposed method show promising precision and recall. As a contribution, the produced RDF triples could be utilized by semantic Web searching application to retrieve intelligent and relevant answers to end user's quires.
ER  - 
TY  - JOUR
T1  - VOS: A new outlier detection model using virtual graph
A1  - Wang, Chao
A1  - Liu, Zhen
A1  - Gao, Hui
A1  - Fu, Yan
Y1  - 2019///
KW  -  Graph-based outlier detection
KW  -  Markov random walk
KW  -  Neighborhood information graph
KW  -  Outlier detection
KW  -  Virtual graph
KW  - Anomaly detection
JF  - Knowledge-Based Systems
VL  - 185
SP  - 104907
EP  - 104907
DO  - https://doi.org/10.1016/j.knosys.2019.104907
UR  - https://www.sciencedirect.com/science/article/pii/S0950705119303648
N2  - Outlier detection has been well studied due to its wide applications in both academia and industry, among which graph-based methods have drawn extensive attention in recent years because of their robust expressiveness for various types of datasets. Combining the local information with the implicit connections in the graph representation of the original dataset, in this study, we propose a new outlier detection model named Virtual Outlier Score (VOS). The proposed model constructs a similarity graph by using the top-k similar neighbors of each object, and introduces a virtual node coupling with a collection of virtual edges to generate a k-virtual graph. A tailored Markov random walk process is then performed on the strongly connected virtual graph under the principle that a potential outlier should get more weight to be visited. After reaching equilibrium, the stationary distribution vector is utilized to deduce the virtual outlier score. Furthermore, we provide a theoretical analysis of the proposed VOS model. Experiments on both synthetic and real-world datasets showed that the proposed model obtains an improvement over eight state-of-the-art algorithms under the measures of ROC AUC as well as the outlier discovery curve.
ER  - 
TY  - JOUR
T1  - Robust multiobjective evolutionary feature subset selection algorithm for binary classification using machine learning techniques
A1  - Deniz, Ayça
A1  - Kiziloz, Hakan Ezgi
A1  - Dokeroglu, Tansel
A1  - Cosar, Ahmet
Y1  - 2017///
KW  -  Binary classification
KW  -  Evolutionary algorithm
KW  -  Supervised/unsupervised machine learning
KW  - Multiobjective feature selection
JF  - Neurocomputing
VL  - 241
SP  - 128
EP  - 146
DO  - https://doi.org/10.1016/j.neucom.2017.02.033
UR  - https://www.sciencedirect.com/science/article/pii/S092523121730303X
N2  - This study investigates the success of a multiobjective genetic algorithm (GA) combined with state-of-the-art machine learning (ML) techniques for the feature subset selection (FSS) in binary classification problem (BCP). Recent studies have focused on improving the accuracy of BCP by including all of the features, neglecting to determine the best performing subset of features. However, for some problems, the number of features may reach thousands, which will cause too much computation power to be consumed during the feature evaluation and classification phases, also possibly reducing the accuracy of the results. Therefore, selecting the minimum number of features while preserving and/or increasing the accuracy of the results at a high level becomes an important issue for achieving fast and accurate binary classification. Our multiobjective evolutionary algorithm includes two phases, FSS using a GA and applying ML techniques for the BCP. Since exhaustively investigating all of the feature subsets is intractable, a GA is preferred for the first phase of the algorithm for intelligently detecting the most appropriate feature subset. The GA uses multiobjective crossover and mutation operators to improve a population of individuals (each representing a selected feature subset) and obtain (near-) optimal solutions through generations. In the second phase of the algorithms, the fitness of the selected subset is decided by using state-of-the-art ML techniques; Logistic Regression, Support Vector Machines, Extreme Learning Machine, K-means, and Affinity Propagation. The performance of the multiobjective evolutionary algorithm (and the ML techniques) is evaluated with comprehensive experiments and compared with state-of-the-art algorithms, Greedy Search, Particle Swarm Optimization, Tabu Search, and Scatter Search. The proposed algorithm was observed to be robust and it performed better than the existing methods on most of the datasets.
ER  - 
TY  - JOUR
T1  - ONE-STEP AHEAD PREDICTION FOR PARAMETER ESTIMATION IN PHYSIOLOGICAL HYBRID MODELS
A1  - van Riel, Natal A W
A1  - Juloski, Aleksandar Lj.
A1  - op den Buijs, Jorn
Y1  - 2005///
KW  -  identification algorithms
KW  -  least-squares identification
KW  -  parameter estimation
KW  -  physiological models
KW  -  prediction error methods
KW  - hybrid models
JF  - IFAC Proceedings Volumes
VL  - 38
IS  - 1
SP  - 43
EP  - 48
DO  - https://doi.org/10.3182/20050703-6-CZ-1902.02121
UR  - https://www.sciencedirect.com/science/article/pii/S1474667016381332
N1  - 16th IFAC World Congress
N2  - Identification of hybrid systems is complex because it is not a priori known which data is generated by which discrete mode of the system. A novel procedure is presented for the simultaneous determination of the model parameters and classification of the data. Initial parameter estimates are obtained based on a priori knowledge of the modes of the system. Next, the data points are subsequently classified and with each new classified data point the parameter estimates are refined. The proposed procedure is applied in the estimation of parameters in a hybrid description of calcium cycling in the intact heart.
ER  - 
TY  - JOUR
T1  - A methodology for customizing clinical tests for esophageal cancer based on patient preferences
A1  - Roy, Asis
A1  - Bhattacharya, Sourangshu
A1  - Guin, Kalyan
Y1  - 2019///
KW  -  Classification with costs
KW  -  Electronic health record (EHR)
KW  -  Electronic medical record (EMR)
KW  -  Esophageal cancer
KW  -  Personalized test selection
KW  -  Unbalanced classification
KW  - Personalized diagnosis
JF  - Artificial Intelligence in Medicine
VL  - 95
SP  - 16
EP  - 26
DO  - https://doi.org/10.1016/j.artmed.2018.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365716305449
N2  - Background
Clinical tests for diagnosis of any disease may be expensive, uncomfortable, time consuming and can have side effects e.g. barium swallow test for esophageal cancer. Although we can predict non-existence of esophageal cancer with near 100% certainty just using demographics, lifestyle, medical history information, and a few basic clinical tests but our objective is to devise a general methodology for customizing tests with user preferences to avoid expensive or uncomfortable tests.
Method
We propose to use classifiers trained from electronic medical records (EMR) for selection of tests. The key idea is to design classifiers with 100% false normal rates, possibly at the cost of higher false abnormal. We find kernel logistic regression to be most suitable for the task. We propose an algorithm for finding the best probability threshold for kernel LR, based on test set accuracy tuning with help of a validation data set. Using the proposed algorithm, we describe schemes for selecting tests, which appear as features in the automatic classification algorithm, using preferences on costs and discomfort of the users i.e the proposed method is able to detect almost all true patients in the population even with user preferred clinical tests.
Result
We test our methodology with EMRs collected for more than 3000 patients, as a part of project carried out by a reputed hospital in Mumbai, India. We found that kernel SVM and kernel LR with a polynomial kernel of degree 3, yields an accuracy of 99.18% and sensitivity 100% using only demographic, lifestyle, patient history, and basic clinical tests. We demonstrate our test selection algorithm using two case studies, one using cost of clinical tests, and other using “discomfort” values for clinical tests. We compute the test sets corresponding to the lowest false abnormals for each criterion described above, using exhaustive enumeration of 12 and 15 clinical tests respectively. The sets turn out to be different, substantiating our claim that one can customize test sets based on user preferences.
ER  - 
TY  - JOUR
T1  - Nearly homogeneous multi-partitioning with a deterministic generator
A1  - Aupetit, Michaël
Y1  - 2009///
KW  -  Deterministic sampling
KW  -  Distribution
KW  -  Divergence
KW  -  Homogeneity measure
KW  -  Multi-partition
KW  -  Nearest neighbor
KW  -  Random sampling
KW  -  Reproducibility
KW  -  Seriation
KW  - Homogeneous partition
JF  - Neurocomputing
VL  - 72
IS  - 7
SP  - 1379
EP  - 1389
DO  - https://doi.org/10.1016/j.neucom.2008.12.024
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209000046
N1  - Advances in Machine Learning and Computational Intelligence
N2  - The need for homogeneous partitions, where all parts have the same distribution, is ubiquitous in machine learning and in other fields of scientific studies. Especially when only few partitions can be generated. In that case, validation sets need to be distributed the same way as training sets to get good estimates of models’ complexities. And when standard data analysis tools cannot deal with too large data sets, the analysis could be performed onto a smaller subset, as far as its homogeneity to the larger one is good enough to get relevant results. However, pseudo-random generators may generate partitions whose parts have very different distributions because the geometry of the data is ignored. In this work, we propose an algorithm which deterministically generates partitions whose parts have empirically greater homogeneity on average than parts arising from pseudo-random partitions. The data to partition are seriated based on a nearest neighbor rule, and assigned to a part of the partition according to their rank in this seriation. We demonstrate the efficiency of this algorithm on toys and real data sets. Since this algorithm is deterministic, it also provides a way to make reproducible machine learning experiments usually based on pseudo-random partitions.
ER  - 
TY  - JOUR
T1  - Recent advances in wearable sensors for animal health management
A1  - Neethirajan, Suresh
Y1  - 2017///
KW  -  Animal health diagnostics
KW  -  Microfluidics
KW  -  Nanotechnology
KW  -  On-farm disease surveillance
KW  -  Precision livestock farming (PLF)
KW  -  Serodiagnosis
KW  -  Stress detection
KW  -  Sweat sensing
KW  -  Wearable technology
KW  - Biosensor
JF  - Sensing and Bio-Sensing Research
VL  - 12
SP  - 15
EP  - 29
DO  - https://doi.org/10.1016/j.sbsr.2016.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S2214180416301350
N2  - Biosensors, as an application for animal health management, are an emerging market that is quickly gaining recognition in the global market. Globally, a number of sensors being produced for animal health management are at various stages of commercialization. Some technologies for producing an accurate health status and disease diagnosis are applicable only for humans, with few modifications or testing in animal models. Now, these innovative technologies are being considered for their future use in livestock development and welfare. Precision livestock farming techniques, which include a wide span of technologies, are being applied, along with advanced technologies like microfluidics, sound analyzers, image-detection techniques, sweat and salivary sensing, serodiagnosis, and others. However, there is a need to integrate all the available sensors and create an efficient online monitoring system so that animal health status can be monitored in real time, without delay. This review paper discusses the scope of different wearable technologies for animals, nano biosensors and advanced molecular biology diagnostic techniques for the detection of various infectious diseases of cattle, along with the efforts to enlist and compare these technologies with respect to their drawbacks and advantages in the domain of animal health management. The paper considers all recent developments in the field of biosensors and their applications for animal health to provide insight regarding the appropriate approach to be used in the future of enhanced animal welfare.
ER  - 
TY  - JOUR
T1  - Towards automated clinical coding
A1  - Catling, Finneas
A1  - Spithourakis, Georgios P
A1  - Riedel, Sebastian
Y1  - 2018///
KW  -  Hierarchical representation learning
KW  -  Knowledge representation
KW  -  Machine learning
KW  -  Natural language processing
KW  -  Recurrent neural networks
KW  - Clinical coding
JF  - International Journal of Medical Informatics
VL  - 120
SP  - 50
EP  - 61
DO  - https://doi.org/10.1016/j.ijmedinf.2018.09.021
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618304039
N2  - Background
Patients’ encounters with healthcare services must undergo clinical coding. These codes are typically derived from free-text notes. Manual clinical coding is expensive, time-consuming and prone to error. Automated clinical coding systems have great potential to save resources, and realtime availability of codes would improve oversight of patient care and accelerate research. Automated coding is made challenging by the idiosyncrasies of clinical text, the large number of disease codes and their unbalanced distribution.
Methods
We explore methods for representing clinical text and the labels in hierarchical clinical coding ontologies. Text is represented as term frequency-inverse document frequency counts and then as word embeddings, which we use as input to recurrent neural networks. Labels are represented atomically, and then by learning representations of each node in a coding ontology and composing a representation for each label from its respective node path. We consider different strategies for initialisation of the node representations. We evaluate our methods using the publicly-available Medical Information Mart for Intensive Care III dataset: we extract the history of presenting illness section from each discharge summary in the dataset, then predicting the International Classification of Diseases, ninth revision, Clinical Modification codes associated with these.
Results
Composing the label representations from the clinical-coding-ontology nodes increased weighted F1 for prediction of the 17,561 disease labels to 0.264–0.281 from 0.232–0.249 for atomic representations. Recurrent neural network text representation improved weighted F1 for prediction of the 19 disease-category labels to 0.682–0.701 from 0.662–0.682 using term frequency-inverse document frequency. However, term frequency-inverse document frequency outperformed recurrent neural networks for prediction of the 17,561 disease labels.
Conclusions
This study demonstrates that hierarchically-structured medical knowledge can be incorporated into statistical models, and produces improved performance during automated clinical coding. This performance improvement results primarily from improved representation of rarer diseases. We also show that recurrent neural networks improve representation of medical text in some settings. Learning good representations of the very rare diseases in clinical coding ontologies from data alone remains challenging, and alternative means of representing these diseases will form a major focus of future work on automated clinical coding.
ER  - 
TY  - JOUR
T1  - Classifying Alzheimer's disease, Lewy body dementia, and normal controls using 3D texture analysis in magnetic resonance images
A1  - Oppedal, Ketil
A1  - Engan, Kjersti
A1  - Eftestøl, Trygve
A1  - Beyer, Mona
A1  - Aarsland, Dag
Y1  - 2017///
KW  -  Alzheimer's disease
KW  -  Classification
KW  -  Computer aided diagnosis
KW  -  Lewy body dementia
KW  -  Local binary pattern
KW  -  Normal appearing white matter
KW  -  Texture analysis
KW  -  White matter hyperintensities
KW  - Magnetic resonance imaging
JF  - Biomedical Signal Processing and Control
VL  - 33
SP  - 19
EP  - 29
DO  - https://doi.org/10.1016/j.bspc.2016.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S1746809416301690
N2  - Dementia is an evolving challenge in society and early intervention is important. The ability to distinguish between different dementia and non-dementia early in course may be essential for successful patient care. Magnetic resonance (MR) imaging may aid as a noninvasive method to increase prediction accuracy. In this work we explored the use of two different 3D local binary pattern (LBP) texture features extracted from T1 MR images of the brain combined with a random forest classifier in an attempt to discern patients with Alzheimer's disease (AD), Lewy body dementia (LBD), and normal controls (NC). Analysis were conducted in areas with white matter lesions (WML) and normal appearing white matter (NAWM). We also calculated correlations between texture features and cognition measured by mini mental state examination (MMSE) controlling for age. Additionally, two different methods for handling the imbalanced data problem were tested, namely cost-sensitive classification and resampling of the data using the synthetic minority oversampling technique (SMOTE). Four different classification tasks were extensively tested, a three-class problem: AD vs. LBD vs. NC, a two-class problem: NC vs. AD, a two-class problem NC vs. LBD, and a two-class problem: AD vs. LBD. Results from 10 folds nested cross validation are reported as mean accuracy, precision, and recall with standard deviation in brackets. The two-class problems NC vs. AD and NC vs. LBD, show encouraging results with total accuracy of 0.97 (0.07) and 0.97 (0.06) respectively. The three-class problem and the two-class problem AD vs. LBD are not equally encouraging but shows higher accuracy than clinical diagnosis with a total accuracy of 0.79 (0.07) and 0.79 (0.15) respectively. Possible explanations may be that the AD- and LBD group are too similar concerning LBP texture analysis and that the LBD group is too small. Most of the texture features calculated for the AD subjects in the NAWM region were significantly correlated with cognition. Together with the positive classification results from the NAWM region this may suggest that the NAWM region is an important area for studying AD. Both cost-sensitive classification and resampling using SMOTE proved useful and improved the results considerably in many of the tests.
ER  - 
TY  - JOUR
T1  - Combining algorithms for automatic detection of optic disc and macula in fundus images
A1  - Qureshi, Rashid Jalal
A1  - Kovacs, Laszlo
A1  - Harangi, Balazs
A1  - Nagy, Brigitta
A1  - Peto, Tunde
A1  - Hajdu, Andras
Y1  - 2012///
KW  -  Macula detection
KW  -  Optic disc detection
KW  -  Retinal imaging
KW  - Diabetic retinopathy
JF  - Computer Vision and Image Understanding
VL  - 116
IS  - 1
SP  - 138
EP  - 145
DO  - https://doi.org/10.1016/j.cviu.2011.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S1077314211001883
N1  - Virtual Representations and Modeling of Large-scale Environments (VRML)
N2  - This paper proposes an efficient combination of algorithms for the automated localization of the optic disc and macula in retinal fundus images. There is in fact no reason to assume that a single algorithm would be optimal. An ensemble of algorithms based on different principles can be more accurate than any of its individual members if the individual algorithms are doing better than random guessing. We aim to obtain an improved optic disc and macula detector by combining the prediction of multiple algorithms, benefiting from their strength and compensating their weaknesses. The location with maximum number of detectors’ outputs is formally the hotspot and is used to find the optic disc or macula center. An assessment of the performance of integrated system and detectors working separately is also presented. Our proposed combination of detectors achieved overall highest performance in detecting optic disc and fovea closest to the manually center chosen by the retinal specialist.
ER  - 
TY  - JOUR
T1  - On the construction of extreme learning machine for online and offline one-class classification—An expanded toolbox
A1  - Gautam, Chandan
A1  - Tiwari, Aruna
A1  - Leng, Qian
Y1  - 2017///
KW  -  Autoassociative ELM (AAELM)
KW  -  Extreme learning machine (ELM)
KW  -  One-class ELM (OCELM)
KW  -  Online sequential ELM (OSELM)
KW  - One-class classification (OCC)
JF  - Neurocomputing
VL  - 261
SP  - 126
EP  - 143
DO  - https://doi.org/10.1016/j.neucom.2016.04.070
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217302096
N1  - Advances in Extreme Learning Machines (ELM 2015)
N2  - One-class classification (OCC) has been prime concern for researchers and effectively employed in various disciplines. But, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. In this paper, we present six OCC methods and their thirteen variants based on extreme learning machine (ELM) and online sequential ELM (OSELM). Our proposed classifiers mainly lie in two categories: reconstruction based and boundary based, where three proposed classifiers belong to reconstruction based and three belong to boundary based. We are presenting both types of learning viz., online and offline learning for OCC. Out of six methods, four are offline and remaining two are online methods. Out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. We present a comprehensive discussion on these methods and their comparison to each other. Kernel feature mapping based approaches have been tested with RBF kernel and online version of one-class classifiers is tested with both types of nodes viz., additive and RBF. It is well known fact that threshold decision is a crucial factor in case of OCC, so, three different threshold deciding criteria have been employed so far and analyze the effectiveness of one threshold deciding criteria over another. Further, these methods are tested on two artificial datasets to check their boundary construction capability and on eight benchmark datasets from different discipline to evaluate the performance of the classifiers. Our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and ELM based two one-class classifiers. Through proposed one-class classifiers, we intend to expand the functionality of the most used toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with all the present features of the toolbox.
ER  - 
TY  - JOUR
T1  - Middleware for the Internet of Things: A survey on requirements, enabling technologies, and solutions
A1  - Zhang, Jingbin
A1  - Ma, Meng
A1  - Wang, Ping
A1  - Sun, Xiao-dong
Y1  - 2021///
KW  -  Context-aware computing
KW  -  Knowledge discovery
KW  -  Middleware
KW  -  Self-adaptation
KW  - Internet of Things
JF  - Journal of Systems Architecture
VL  - 117
SP  - 102098
EP  - 102098
DO  - https://doi.org/10.1016/j.sysarc.2021.102098
UR  - https://www.sciencedirect.com/science/article/pii/S1383762121000795
N2  - As the core layer of the Internet of Things (IoT), middleware bridges the gap between applications and devices to resolve many common IoT issues and enhancing application development. Consequently, developing suitable middleware is the main challenge that covers functionality and required quality to combine heterogeneous hardware and software as the integrated system in the IoT. This survey discusses IoT middleware requirements and challenges, and presents the current state of research in this domain. A technical taxonomy is presented for the IoT middleware according to the abstract and processing approach of data. We focus on discovering similarities and differences by making comparisons and appropriateness studies. Besides, this survey discusses three enabling techniques in detail to present analytically the current research trends on the IoT middleware. In the end, we summarize open issues in IoT middleware. This survey aims to provide guidance for the development and research of middleware in the IoT paradigm.
ER  - 
TY  - JOUR
T1  - InDetector – Automatic detection of infected driveline regions
A1  - Aras, Shravan
A1  - Johnson, Thienne
A1  - Gniady, Chris
A1  - Skaria, Rinku
A1  - Khalpey, Zain
Y1  - 2018///
JF  - Smart Health
VL  - 9-10
SP  - 170
EP  - 178
DO  - https://doi.org/10.1016/j.smhl.2018.07.016
UR  - https://www.sciencedirect.com/science/article/pii/S2352648318300515
N1  - CHASE 2018 Special Issue
N2  - Although there have been significant advancements in Left Ventricular Assist Device(LVAD) technology and improvements in mortality rates, infection remains one of the major complications associated with LVAD therapy with an incidence of 25-80% cases annually. Amongst such infections driveline infections are the most common and account for 14-28% according to the RE-MATCH trial of the total of LVAD related infections. If a patient is diagnosed with LVAD infection, it is important to initiate antibiotic therapy as early as possible. If left untreated it can lead to sepsis with multi-organ failure, longer hospital stay, delay heart transplant or early mortality. To improve infection detection and monitoring we propose InDetector, a driveline infection detection system that allows at-home patients to use a smartphone to capture images of their driveline regions to check for infections. InDetector uses a Convolutional Neural Network along with image augmentation techniques for inferring infected images and achieves an overall classification accuracy of 93.75% on our validation dataset.
ER  - 
TY  - JOUR
T1  - Limestone: High-throughput candidate phenotype generation via tensor factorization
A1  - Ho, Joyce C
A1  - Ghosh, Joydeep
A1  - Steinhubl, Steve R
A1  - Stewart, Walter F
A1  - Denny, Joshua C
A1  - Malin, Bradley A
A1  - Sun, Jimeng
Y1  - 2014///
KW  -  EHR phenotyping
KW  -  Nonnegative tensor factorization
KW  - Dimensionality reduction
JF  - Journal of Biomedical Informatics
VL  - 52
SP  - 199
EP  - 211
DO  - https://doi.org/10.1016/j.jbi.2014.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414001488
N1  - Special Section: Methods in Clinical Research Informatics
N2  - The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately, EHR data do not always directly and reliably map to medical concepts that clinical researchers need or use. Some recent studies have focused on EHR-derived phenotyping, which aims at mapping the EHR data to specific medical concepts; however, most of these approaches require labor intensive supervision from experienced clinical professionals. Furthermore, existing approaches are often disease-centric and specialized to the idiosyncrasies of the information technology and/or business practices of a single healthcare organization. In this paper, we propose Limestone, a nonnegative tensor factorization method to derive phenotype candidates with virtually no human supervision. Limestone represents the data source interactions naturally using tensors (a generalization of matrices). In particular, we investigate the interaction of diagnoses and medications among patients. The resulting tensor factors are reported as phenotype candidates that automatically reveal patient clusters on specific diagnoses and medications. Using the proposed method, multiple phenotypes can be identified simultaneously from data. We demonstrate the capability of Limestone on a cohort of 31,815 patient records from the Geisinger Health System. The dataset spans 7years of longitudinal patient records and was initially constructed for a heart failure onset prediction study. Our experiments demonstrate the robustness, stability, and the conciseness of Limestone-derived phenotypes. Our results show that using only 40 phenotypes, we can outperform the original 640 features (169 diagnosis categories and 471 medication types) to achieve an area under the receiver operator characteristic curve (AUC) of 0.720 (95% CI 0.715 to 0.725). Moreover, in consultation with a medical expert, we confirmed 82% of the top 50 candidates automatically extracted by Limestone are clinically meaningful.
ER  - 
TY  - JOUR
T1  - A support vector machine with integer parameters
A1  - Anguita, Davide
A1  - Ghio, Alessandro
A1  - Pischiutta, Stefano
A1  - Ridella, Sandro
Y1  - 2008///
KW  -  Branch-and-bound
KW  -  Mixed integer quadratic programming (MIQP)
KW  -  Resource-limited hardware
KW  -  Sensor networks
KW  -  Sequential minimal optimization (SMO)
KW  - Support vector machine (SVM)
JF  - Neurocomputing
VL  - 72
IS  - 1
SP  - 480
EP  - 489
DO  - https://doi.org/10.1016/j.neucom.2007.12.006
UR  - https://www.sciencedirect.com/science/article/pii/S092523120700389X
N1  - Machine Learning for Signal Processing (MLSP 2006) / Life System Modelling, Simulation, and Bio-inspired Computing (LSMS 2007)
N2  - We describe here a method for building a support vector machine (SVM) with integer parameters. Our method is based on a branch-and-bound procedure, derived from modern mixed integer quadratic programming solvers, and is useful for implementing the feed-forward phase of the SVM in fixed–point arithmetic. This allows the implementation of the SVM algorithm on resource–limited hardware like, for example, computing devices used for building sensor networks, where floating–point units are rarely available. The experimental results on well–known benchmarking data sets and a real–world people-detection application show the effectiveness of our approach.
ER  - 
TY  - JOUR
T1  - The fusion of Internet of Intelligent Things (IoIT) in remote diagnosis of obstructive Sleep Apnea: A survey and a new model
A1  - Abdel-Basset, Mohamed
A1  - Ding, Weiping
A1  - Abdel-Fatah, Laila
Y1  - 2020///
KW  -  Artificial Intelligence
KW  -  Internet of Intelligent Things
KW  -  Internet of Things
KW  -  Optimization
KW  -  Remote diagnosis
KW  - Obstructive Sleep Apnea
JF  - Information Fusion
VL  - 61
SP  - 84
EP  - 100
DO  - https://doi.org/10.1016/j.inffus.2020.03.010
UR  - https://www.sciencedirect.com/science/article/pii/S1566253519307043
N2  - Obstructive Sleep Apnea (OSA) syndrome is one of the most widespread diseases that difficult to be detected and remedied. In particular, the examination of OSA by using the traditional Polysomnography (PSG) is one of formidable complexity as it requires full observation in a laboratory overnight. Meanwhile, the number of available laboratories and beds is minimal comparing to the number of OSA patients. What's more, the unusual environment and restricted mobility of patients may result in deficient diagnosis results. The Internet of Things (IoT) is the most appropriate solution for the previous diagnosis obstacles by allowing doctors to synchronize patient status. Besides, several studies have been introduced to consolidate the performance of IoT interoperability via the fusion with Artificial Intelligence (AI) resulting in the Internet of Intelligent Things (IoIT). This paper presents a literature survey about the intensification of IoT technologies for smart monitoring of sleep quality and OSA diagnosis. Mainly, the most recent enabling IoT and support technologies such as (smart devices, fog computing, cloud, big data, and machine learning) are covered via the discussion of more recent works of literature published from 2016 to 2019. Also, the roles of AI in optimizing the efficiency of OSA smart diagnosis are presented. Besides, a new comprehensive IoIT optimization framework is presented which employing AI for optimizing the performance of intelligent diagnosis of OSA. Finally, the open issues and challenges in this field are argued. This paper is, therefore, a major contributor to the compilation of all IoT innovative and efficient AI methods that improving the quality of OSA diagnosis.
ER  - 
TY  - JOUR
T1  - An exhaustive survey on security and privacy issues in Healthcare 4.0
A1  - Hathaliya, Jigna J
A1  - Tanwar, Sudeep
Y1  - 2020///
KW  -  Biometric
KW  -  Blockchain technology
KW  -  Healthcare system
KW  -  Privacy
KW  -  Wearable device
KW  - Security
JF  - Computer Communications
VL  - 153
SP  - 311
EP  - 335
DO  - https://doi.org/10.1016/j.comcom.2020.02.018
UR  - https://www.sciencedirect.com/science/article/pii/S0140366419311880
N2  - The healthcare industry has revolutionized from 1.0 to 4.0 where Healthcare 1.0 was more doctor centric and Healthcare 2.0 replaced manual records with electronic healthcare records (EHRs). Healthcare 3.0 was patient-centric and Healthcare 4.0 uses cloud computing (CC), fog computing (FC), Internet of things (IoT), and telehealthcare technologies to share data between various stakeholders. However, framing a secure technique for Healthcare 4.0 has always been a challenging task. An in-secure technique for Healthcare 4.0 may lead to the healthcare data breach where hackers can gain full access to patients’ email accounts, messages, and reports. On the contrary, a secured technique for Healthcare 4.0 can provide satisfaction to all stakeholders, including patients and caregivers. Motivated from these facts, this paper presents an extensive literature review and analysis of state-of-the-art proposals to maintain security and privacy in Healthcare 4.0. We also explored the blockchain-based solution to give insights to both researchers and practitioners communities. Different taxonomies used for exploring various security and privacy issues in Healthcare 4.0 are also presented in a structured manner. Then, the advantages and limitations of various security and privacy techniques are explored and discussed in the paper. Finally, existing challenges and future research directions of security and privacy in Healthcare 4.0 are presented.
ER  - 
TY  - JOUR
T1  - Predictive modeling of colorectal cancer using a dedicated pre-processing pipeline on routine electronic medical records
A1  - Kop, Reinier
A1  - Hoogendoorn, Mark
A1  - ten Teije, Annette
A1  - Büchner, Frederike L
A1  - Slottje, Pauline
A1  - Moons, Leon M G
A1  - Numans, Mattijs E
Y1  - 2016///
KW  -  Data mining
KW  -  Data processing
KW  -  Electronic medical records
KW  -  Machine learning
KW  - Colorectal cancer
JF  - Computers in Biology and Medicine
VL  - 76
SP  - 30
EP  - 38
DO  - https://doi.org/10.1016/j.compbiomed.2016.06.019
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516301573
N2  - Over the past years, research utilizing routine care data extracted from Electronic Medical Records (EMRs) has increased tremendously. Yet there are no straightforward, standardized strategies for pre-processing these data. We propose a dedicated medical pre-processing pipeline aimed at taking on many problems and opportunities contained within EMR data, such as their temporal, inaccurate and incomplete nature. The pipeline is demonstrated on a dataset of routinely recorded data in general practice EMRs of over 260,000 patients, in which the occurrence of colorectal cancer (CRC) is predicted using various machine learning techniques (i.e., CART, LR, RF) and subsets of the data. CRC is a common type of cancer, of which early detection has proven to be important yet challenging. The results are threefold. First, the predictive models generated using our pipeline reconfirmed known predictors and identified new, medically plausible, predictors derived from the cardiovascular and metabolic disease domain, validating the pipeline's effectiveness. Second, the difference between the best model generated by the data-driven subset (AUC 0.891) and the best model generated by the current state of the art hypothesis-driven subset (AUC 0.864) is statistically significant at the 95% confidence interval level. Third, the pipeline itself is highly generic and independent of the specific disease targeted and the EMR used. In conclusion, the application of established machine learning techniques in combination with the proposed pipeline on EMRs has great potential to enhance disease prediction, and hence early detection and intervention in medical practice.
ER  - 
TY  - JOUR
T1  - Tracking word semantic change in biomedical literature
A1  - Yan, Erjia
A1  - Zhu, Yongjun
Y1  - 2018///
KW  -  Semantic change
KW  -  Skip-gram
KW  -  Topic modeling
KW  -  Word2vec
KW  - PubMed
JF  - International Journal of Medical Informatics
VL  - 109
SP  - 76
EP  - 86
DO  - https://doi.org/10.1016/j.ijmedinf.2017.11.006
UR  - https://www.sciencedirect.com/science/article/pii/S1386505617304185
N2  - Up to this point, research on written scholarly communication has focused primarily on syntactic, rather than semantic, analyses. Consequently, we have yet to understand semantic change as it applies to disciplinary discourse. The objective of this study is to illustrate word semantic change in biomedical literature. To that end, we identify a set of representative words in biomedical literature based on word frequency and word-topic probability distributions. A word2vec language model is then applied to the identified words in order to measure word- and topic-level semantic changes. We find that for the selected words in PubMed, overall, meanings are becoming more stable in the 2000s than they were in the 1980s and 1990s. At the topic level, the global distance of most topics (19 out of 20 tested) is declining, suggesting that the words used to discuss these topics are stabilizing semantically. Similarly, the local distance of most topics (19 out of 20) is also declining, showing that the meanings of words from these topics are becoming more consistent with those of their semantic neighbors. At the word level, this paper identifies two different trends in word semantics, as measured by the aforementioned distance metrics: on the one hand, words can form clusters with their semantic neighbors, and these words, as a cluster, coevolve semantically; on the other hand, words can drift apart from their semantic neighbors while nonetheless stabilizing in the global context. In relating our work to language laws on semantic change, we find no overwhelming evidence to support either the law of parallel change or the law of conformity.
ER  - 
TY  - JOUR
T1  - Sparse feature selection: Relevance, redundancy and locality structure preserving guided by pairwise constraints
A1  - Noorie, Zahir
A1  - Afsari, Fatemeh
Y1  - 2020///
KW  -  -norm
KW  -  Graph Laplacian
KW  -  Locality structure preserving
KW  -  Pairwise constraints
KW  -  Pairwise redundancy
KW  - Sparse feature selection
JF  - Applied Soft Computing
VL  - 87
SP  - 105956
EP  - 105956
DO  - https://doi.org/10.1016/j.asoc.2019.105956
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619307379
N2  - Selection of features as a pre-processing stage is an essential issue in many machine learning tasks (such as classification) to reduce data dimensionality as there are many irrelevant and redundant features that can mislead the learning process. Graph-based sparse feature selection is developed to overcome this issue. In this paper, a novel graph-based sparse feature selection method is proposed that take into account both issues: relevancy and redundancy analysis. An empirical loss function joining with ℓ1-norm regularization term is proposed to overcome the relevancy issue and the redundancy issue is overcome by introducing a regularization term that prefers uncorrelated features. Furthermore, the proposed learning procedure is guided by two different sets of supervision information as pairs of must-linked (positive) and cannot-linked (negative) constraint sets to select a discriminative feature subset. These guiding information besides the whole data points are encoded in the graph Laplacian matrix that preserves the locality structure of the original data. The graph Laplacian matrix is constructed by two different approaches. Our first approach tries to preserve the structure of the original data guided just by the positive data points (unique samples in the must-linked constraints), and our second approach applies a normalized adapted affinity matrix to embed the pairwise must-linked and cannot-linked constraints as well as the neighborhood relationships information, all together. The experimental results on a number of several datasets from the University of California-Irvine machine learning repository, in addition to several high dimensional gene expression datasets show the efficacy of the proposed methods in the classification tasks compared to several powerful feature selection methods.
ER  - 
TY  - JOUR
T1  - Two methods of selecting Gaussian kernel parameters for one-class SVM and their application to fault detection
A1  - Xiao, Yingchao
A1  - Wang, Huangang
A1  - Zhang, Lin
A1  - Xu, Wenli
Y1  - 2014///
KW  -  Fault detection
KW  -  Gaussian kernel
KW  -  OCSVM
KW  -  Parameter selection
KW  - One-class classification
JF  - Knowledge-Based Systems
VL  - 59
SP  - 75
EP  - 84
DO  - https://doi.org/10.1016/j.knosys.2014.01.020
UR  - https://www.sciencedirect.com/science/article/pii/S0950705114000379
N2  - As one of the methods to solve one-class classification problems (OCC), one-class support vector machines (OCSVM) have been applied to fault detection in recent years. Among all the kernels available for OCSVM, the Gaussian kernel is the most commonly used one. The selection of Gaussian kernel parameters influences greatly the performances of classifiers, which remains as an open problem. In this paper two methods are proposed to select Gaussian kernel parameters in OCSVM: according to the first one, the parameters are selected using the information of the farthest and the nearest neighbors of each sample; using the second one, the parameters are determined via detecting the “tightness” of the decision boundaries. The two proposed methods are tested on UCI data sets and Tennessee Eastman Process benchmark data sets. The results show that, the two proposed methods can be used to select suitable parameters for the Gaussian kernel, enabling the resulting OCSVM models to perform well on fault detection.
ER  - 
TY  - JOUR
T1  - Concurrent optimization of multiple base learners in neural network ensembles: An adaptive niching differential evolution approach
A1  - Huang, Ting
A1  - Duan, Dan-Ting
A1  - Gong, Yue-Jiao
A1  - Ye, Long
A1  - Ng, Wing W Y
A1  - Zhang, Jun
Y1  - 2020///
KW  -  Multimodal optimization
KW  -  Neural network ensemble
KW  -  Population size adaptation
KW  - Niching differential evolution
JF  - Neurocomputing
VL  - 396
SP  - 24
EP  - 38
DO  - https://doi.org/10.1016/j.neucom.2020.02.020
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220301958
N2  - Neural network ensemble (NNE) exhibits improved performance when compared with a single neural network (NN) in most cases. Traditionally, each base network in an NNE is trained individually, which may result in network redundancy and expensive training overhead. This paper proposes a new adaptive niching evolutionary algorithm, which possesses promising performance in finding multiple optima in terms of good accuracy and diversity. By means of this algorithm, all NNs in an NNE can be trained simultaneously. In particular, the proposed algorithm is named adaptive niching differential evolution (ANDE), which is characterized by a heuristic clustering method to enable iteratively cluster subpopulations that track and locate multiple optima, a parameter adaptation strategy to adaptively adjust parameters according to the subpopulation states, and an auxiliary movement scheme to promote the equilibrium between exploration and exploitation. Experimental results validate the efficiency and effectiveness of the proposed ANDE on the benchmark test suite of multimodal optimization. Furthermore, ANDE is extended to concurrently train multiple base NNs for ensemble and the experiments show a promising performance of ANDE-NNE.
ER  - 
TY  - JOUR
T1  - A methodology based on multiple criteria decision analysis for combining antibiotics in empirical therapy
A1  - Campos, Manuel
A1  - Jimenez, Fernando
A1  - Sanchez, Gracia
A1  - Juarez, Jose M
A1  - Morales, Antonio
A1  - Canovas-Segura, Bernardo
A1  - Palacios, Francisco
Y1  - 2020///
KW  -  Clinical decision support
KW  -  Combination therapy
KW  -  Empiric antimicrobial therapy
KW  -  Multiobjective-optimization
KW  - Multiple criteria decision analysis
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101751
EP  - 101751
DO  - https://doi.org/10.1016/j.artmed.2019.101751
UR  - https://www.sciencedirect.com/science/article/pii/S0933365718305918
N2  - Background
The current situation of critical progression in resistance to more effective antibiotics has forced the reuse of old highly toxic antibiotics and, for several reasons, the extension of the indications of combined antibiotic therapy as alternative options to broad spectrum empirical mono-therapy. A key aspect for selecting an appropriate and adequate antimicrobial therapy is that prescription must be based on local epidemiology and knowledge since many aspects, such as prevalence of microorganisms and effectiveness of antimicrobials, change from hospitals, or even areas and services within a single hospital. Therefore, the selection of combinations of antibiotics requires the application of a methodology that provides objectivity, completeness and reproducibility to the analysis of the detailed microbiological, epidemiological, pharmacological information on which to base a rational and reasoned choice.
Methods
We proposed a methodology for decision making that uses a multiple criteria decision analysis (MCDA) to support the clinician in the selection of an efficient combined empiric therapy. The MCDA includes a multi-objective constrained optimization model whose criteria are the maximum efficacy of therapy, maximum activity, the minimum activity overlapping, the minimum use of restricted antibiotics, the minimum toxicity of antibiotics and the activity against the most prevalent and virulent bacteria. The decision process can be defined in 4 steps: (1) selection of clinical situation of interest, (2) definition of local optimization criteria, (3) definition of constraints for reducing combinations, (4) manual sorting of solutions according to patient's clinical conditions, and (5) selection of a combination.
Experiments and results
In order to show the application of the methodology to a clinical case, we carried out experiments with antibiotic susceptibility tests in blood samples taken during a five years period at a university hospital. The validation of the results consists of a manual review of the combinations and experiments carried out by an expert physician that has explained the most relevant solutions proposed according to current clinical knowledge and their use.
Conclusion
We show that with the decision process proposed, the physician is able to select the best combined therapy according to different criteria such as maximum efficacy, activity and minimum toxicity. A method for the recommendation of combined antibiotic therapy developed on the basis of a multi-objective optimization model may assist the physicians in the search for alternatives to the use of broad-spectrum antibiotics or restricted antibiotics for empirical therapy. The decision proposed can be easily reproduced for any local epidemiology and any different clinical settings.
ER  - 
TY  - JOUR
T1  - An ensemble heterogeneous classification methodology for discovering health-related knowledge in social media messages
A1  - Tuarob, Suppawong
A1  - Tucker, Conrad S
A1  - Salathe, Marcel
A1  - Ram, Nilam
Y1  - 2014///
KW  -  Classification
KW  -  Machine learning
KW  - Social media
JF  - Journal of Biomedical Informatics
VL  - 49
SP  - 255
EP  - 268
DO  - https://doi.org/10.1016/j.jbi.2014.03.005
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414000628
N2  - Objectives
The role of social media as a source of timely and massive information has become more apparent since the era of Web 2.0.Multiple studies illustrated the use of information in social media to discover biomedical and health-related knowledge.Most methods proposed in the literature employ traditional document classification techniques that represent a document as a bag of words.These techniques work well when documents are rich in text and conform to standard English; however, they are not optimal for social media data where sparsity and noise are norms.This paper aims to address the limitations posed by the traditional bag-of-word based methods and propose to use heterogeneous features in combination with ensemble machine learning techniques to discover health-related information, which could prove to be useful to multiple biomedical applications, especially those needing to discover health-related knowledge in large scale social media data.Furthermore, the proposed methodology could be generalized to discover different types of information in various kinds of textual data.
Methodology
Social media data is characterized by an abundance of short social-oriented messages that do not conform to standard languages, both grammatically and syntactically.The problem of discovering health-related knowledge in social media data streams is then transformed into a text classification problem, where a text is identified as positive if it is health-related and negative otherwise.We first identify the limitations of the traditional methods which train machines with N-gram word features, then propose to overcome such limitations by utilizing the collaboration of machine learning based classifiers, each of which is trained to learn a semantically different aspect of the data.The parameter analysis for tuning each classifier is also reported.
Data sets
Three data sets are used in this research.The first data set comprises of approximately 5000 hand-labeled tweets, and is used for cross validation of the classification models in the small scale experiment, and for training the classifiers in the real-world large scale experiment.The second data set is a random sample of real-world Twitter data in the US.The third data set is a random sample of real-world Facebook Timeline posts.
Evaluations
Two sets of evaluations are conducted to investigate the proposed model’s ability to discover health-related information in the social media domain: small scale and large scale evaluations.The small scale evaluation employs 10-fold cross validation on the labeled data, and aims to tune parameters of the proposed models, and to compare with the stage-of-the-art method.The large scale evaluation tests the trained classification models on the native, real-world data sets, and is needed to verify the ability of the proposed model to handle the massive heterogeneity in real-world social media.
Findings
The small scale experiment reveals that the proposed method is able to mitigate the limitations in the well established techniques existing in the literature, resulting in performance improvement of 18.61% (F-measure).The large scale experiment further reveals that the baseline fails to perform well on larger data with higher degrees of heterogeneity, while the proposed method is able to yield reasonably good performance and outperform the baseline by 46.62% (F-Measure) on average.
ER  - 
TY  - JOUR
T1  - MetFlexo: An Automated Simulation of Realistic H1-NMR Spectra
A1  - Atieh, Zeinab
A1  - Suhre, Karsten
A1  - Bensmail, Halima
Y1  - 2013///
KW  -  chemical shifts
KW  -  eigenvectors
KW  -  metabolites
KW  -  spin-spin couplings
KW  - H1-NMR
JF  - Procedia Computer Science
VL  - 18
SP  - 1382
EP  - 1391
DO  - https://doi.org/10.1016/j.procs.2013.05.305
UR  - https://www.sciencedirect.com/science/article/pii/S1877050913004481
N1  - 2013 International Conference on Computational Science
N2  - The development of the ‘omics’ technologies such as transcriptomics, proteomics and metabolomics has made it possible to realize some of the goals of systems biology, where biological systems are interrogated at different levels of biochemical activity (such as gene expression, protein activity and/or metabolite concentration). Metabolomics deals with the metabolome that represents the complete set of small-molecule metabolites. Even though metabolomics can be thought of as a relatively young method, it is nevertheless a rapidly growing one that has the potential to reveal the molecular mechanism of certain diseases. H1 nuclear magnetic resonance (NMR) spectroscopy is commonly used in the metabolic profiling of biofluids as it has the potential to detect all proton-containing metabolites. Metabolites in biofluids are in dynamic equilibrium with those in cells and tissues, so their metabolic profile reflects changes in the state of an organism due to disease or environmental effects.
Results
MetFlexo is as an easy-to-use C package that allows the simulation of datasets of 1H-NMR spectra in order to test data analysis techniques, hypotheses and experimental designs. The idea is based on transforming statistical parameters of metabolites (shifts, couplings, concentrations and magnetic field) to an NMR spectrum using chemical-physics theory. Our method helps in the deconvolution of NMR spectra and in a better determination of metabolite concentrations, as these concentrations are key in detecting diseases and abnormalities. Unlike others, this program generates NMR spectrum of biofluids with no limit on magnetic field or pH. Thus, our approach is able to produce complex NMR profiles with flexible conditions. It is also simple to implement in C, requires small storage, is easy to compute and uses an independent platform. It will be available in R and MATLAB soon. The algorithm is freely available upon request to the corresponding author.
ER  - 
TY  - JOUR
T1  - Gabor frames for classification of paroxysmal and persistent atrial fibrillation episodes
A1  - Ortigosa, Nuria
A1  - Galbis, Antonio
A1  - Fernández, Carmen
A1  - Cano, Óscar
Y1  - 2017///
KW  - Atrial fibrillation
KW  - Electrocardiogram
KW  - Gabor frames
JF  - Medical Engineering & Physics
VL  - 39
SP  - 31
EP  - 37
DO  - https://doi.org/10.1016/j.medengphy.2016.10.013
UR  - https://www.sciencedirect.com/science/article/pii/S1350453316302594
N2  - In this study, we propose a new classification method for early differentiation of paroxysmal and persistent atrial fibrillation episodes, i.e. those which spontaneously or with external intervention will return to sinus rhythm within 7 days of onset from the ones where the arrhythmia is sustained for more than 7 days. Today, clinicians provide patients classification once the course of the arrhythmia has been disclosed. This classification problem is dealt with in this study. We study a sparse representation of surface electrocardiogram signals by means of Gabor frames and afterwards we apply a linear discriminant analysis. Thus, we provide an early discrimination, obtaining promising performances on a heterogeneous cohort of patients in terms of pharmacological treatment and state of progression of the arrhythmia: 95% sensitivity, 82% specificity, 89% accuracy. In this manner, the proposed method can help clinicians to choose the most appropriate treatment using the electrocardiogram, which is a widely available and non-invasive technique. This early differentiation is clinically highly significant in order to choose optimal patients who may undergo catheter ablation with higher success rates.
ER  - 
TY  - JOUR
T1  - Indicators of hypertriglyceridemia from anthropometric measures based on data mining
A1  - Ju Lee, Bum
A1  - Yeol Kim, Jong
Y1  - 2015///
KW  -  Anthropometry
KW  -  Data mining
KW  -  Machine learning
KW  -  Prediction
KW  -  Statistical data analysis
KW  -  Triglycerides
KW  - Hypertriglyceridemia
JF  - Computers in Biology and Medicine
VL  - 57
SP  - 201
EP  - 211
DO  - https://doi.org/10.1016/j.compbiomed.2014.12.005
UR  - https://www.sciencedirect.com/science/article/pii/S0010482514003515
N2  - Background
The best indicator for the prediction of hypertriglyceridemia derived from anthropometric measures of body shape remains a matter of debate. The objectives are to determine the strongest predictor of hypertriglyceridemia from anthropometric measures and to investigate whether a combination of measures can improve the prediction accuracy compared with individual measures.
Methods
A total of 5517 subjects aged 20–90 years participated in this study. The numbers of normal and hypertriglyceridemia subjects were 3022 and 653 females, respectively, and 1306 and 536 males, respectively. We evaluated 33 anthropometric measures for the prediction of hypertriglyceridemia using statistical analysis and data mining.
Results
In the 20–90-year-old groups, age in women was the variable that exhibited the highest predictive power; however, this was not the case in men in all age groups. Of the anthropometric measures, the waist-to-height ratio (WHtR) was the best predictor of hypertriglyceridemia in women. In men, the rib-to-forehead circumference ratio (RFcR) was the strongest indicator. The use of a combination of measures provides better predictive power compared with individual measures in both women and men. However, in the subgroups of ages 20–50 and 51–90 years, the strongest indicators for hypertriglyceridemia were rib circumference in the 20–50-year-old group and WHtR in the 51–90-year-old group in women and RFcR in the 20–50-year-old group and BMI in the 51–90-year-old group in men.
Conclusions
Our results demonstrated that the best predictor of hypertriglyceridemia may differ according to gender and age.
ER  - 
TY  - JOUR
T1  - A two-step approach for mining patient treatment pathways in administrative healthcare databases
A1  - Najjar, Ahmed
A1  - Reinharz, Daniel
A1  - Girouard, Catherine
A1  - Gagné, Christian
Y1  - 2018///
KW  -  HMM
KW  -  Healthcare databases
KW  -  Medical treatment process
KW  -  Mixed variables
KW  -  Process mining
KW  -  k-Prototypes
KW  - Process clustering
JF  - Artificial Intelligence in Medicine
VL  - 87
SP  - 34
EP  - 48
DO  - https://doi.org/10.1016/j.artmed.2018.03.004
UR  - https://www.sciencedirect.com/science/article/pii/S0933365716304857
N2  - Clustering electronic medical records allows the discovery of information on healthcare practices. Entries in such medical records are usually composed of a succession of diagnostics or therapeutic steps. The corresponding processes are complex and heterogeneous since they depend on medical knowledge integrating clinical guidelines, the physician's individual experience, and patient data and conditions. To analyze such data, we are first proposing to cluster medical visits, consultations, and hospital stays into homogeneous groups, and then to construct higher-level patient treatment pathways over these different groups. These pathways are then also clustered to distill typical pathways, enabling interpretation of clusters by experts. This approach is evaluated on a real-world administrative database of elderly people in Québec suffering from heart failures.
ER  - 
TY  - JOUR
T1  - Dual-Stage Attention Based Spatio-Temporal Sequence Learning for Multi-Step Traffic Prediction
A1  - Cui, Ziqiang
A1  - Zhao, Chunhui
Y1  - 2020///
KW  -  attention mechanism
KW  -  sequence learning
KW  -  spatial correlation
KW  -  temporal correlation
KW  - multi-step traffic prediction
JF  - IFAC-PapersOnLine
VL  - 53
IS  - 2
SP  - 17035
EP  - 17040
DO  - https://doi.org/10.1016/j.ifacol.2020.12.1518
UR  - https://www.sciencedirect.com/science/article/pii/S2405896320321133
N1  - 21st IFAC World Congress
N2  - Traffic prediction has great significance including but not limited to mitigating traffic congestion, reducing traffic accidents, and reducing waiting time. At the same time, traffic prediction, especially multi-step prediction, faces many difficulties including temporal correlations and spatial correlations. We propose a dual-stage attention based spatio-temporal sequence learning for multi-step traffic prediction which can not only express temporal correlation and spatial correlation, but also can adaptively learn the contribution weights of different related roads and historical moments. More specifically, for spatial dependencies, we first generate the input vector for each historical moment considering the information of relevant road segments by the method of spatial region of support and further add the first-stage attention termed spatial attention to automatically determine the weight of each relevant road segment for each historical moment. For temporal dependencies, we use LSTM based encoder-decoder networks to fully learn the temporal characteristic and make multi-step prediction considering temporal correlation between multi steps. We further add the second-stage attention termed temporal attention in the decoder part to automatically learn the contribution of different historical moments to each prediction moment. In addition, we consider external factors including weather and holidays and characterize their impacts using fully connected networks. Finally, the effectiveness of the proposed method is evaluated using traffic data in Hangzhou, China.
ER  - 
TY  - JOUR
T1  - Blood vessel segmentation methodologies in retinal images – A survey
A1  - Fraz, M M
A1  - Remagnino, P
A1  - Hoppe, A
A1  - Uyyanonvara, B
A1  - Rudnicka, A R
A1  - Owen, C G
A1  - Barman, S A
Y1  - 2012///
KW  -  Blood vessel segmentation
KW  -  Image segmentation
KW  -  Retinal images
KW  -  Retinopathy
KW  -  Survey
KW  - Medical imaging
JF  - Computer Methods and Programs in Biomedicine
VL  - 108
IS  - 1
SP  - 407
EP  - 433
DO  - https://doi.org/10.1016/j.cmpb.2012.03.009
UR  - https://www.sciencedirect.com/science/article/pii/S0169260712000843
N2  - Retinal vessel segmentation algorithms are a fundamental component of automatic retinal disease screening systems. This work examines the blood vessel segmentation methodologies in two dimensional retinal images acquired from a fundus camera and a survey of techniques is presented. The aim of this paper is to review, analyze and categorize the retinal vessel extraction algorithms, techniques and methodologies, giving a brief description, highlighting the key points and the performance measures. We intend to give the reader a framework for the existing research; to introduce the range of retinal vessel segmentation algorithms; to discuss the current trends and future directions and summarize the open problems. The performance of algorithms is compared and analyzed on two publicly available databases (DRIVE and STARE) of retinal images using a number of measures which include accuracy, true positive rate, false positive rate, sensitivity, specificity and area under receiver operating characteristic (ROC) curve.
ER  - 
TY  - JOUR
T1  - Machine learning algorithms to predict early pregnancy loss after in vitro fertilization-embryo transfer with fetal heart rate as a strong predictor
A1  - Liu, Lijue
A1  - Jiao, Yongxia
A1  - Li, Xihong
A1  - Ouyang, Yan
A1  - Shi, Danni
Y1  - 2020///
KW  - Fetal heart rate
KW  - In vitro fertilization-embryo transfer
KW  - Machine learning
KW  - Random forest
JF  - Computer Methods and Programs in Biomedicine
VL  - 196
SP  - 105624
EP  - 105624
DO  - https://doi.org/10.1016/j.cmpb.2020.105624
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720314577
N2  - Background and objective
According to previous studies, after in vitro fertilization-embryo transfer (IVF-ET) there exist a high early pregnancy loss (EPL) rate. The objectives of this study were to construct a prediction model of embryonic development by using machine learning algorithms based on historical case data, in this way doctors can make more accurate suggestions on the number of patient follow-ups, and provide decision support for doctors who are relatively inexperienced in clinical practice.
Methods
We analyzed the significance of the same type of features between ongoing pregnancy samples and EPL samples. At the same time, by analyzing the correlation between days after embryo transfer (ETD) and fetal heart rate (FHR) of those normal embryo samples, a regression model between the two was established to obtain FHR model of normal development, and the residual analysis was used to further clarify the importance of FHR in predicting pregnancy outcome. Finally we applied six representative machine learning algorithms including Logistic Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), Back Propagation Neural Network (BNN), XGBoost and Random Forest (RF) to build prediction models. Sensitivity was selected to evaluate prediction results, and accuracy of what each algorithm above predicted under both the conditions with and without FHR was compared as well.
Results
There were statically significant differences in the same type of features between ongoing pregnancy samples and EPL samples, which could serve as predictors. FHR, of which the normal development showed a strong correlation with ETD, had great predictive value for embryonic development. Among the six predictive models the one predicted with the highest accuracy was Random Forest, of which recall ratio and F1 could reach 97%, and AUC could reach 0.97, FHR taken into account as a feature. In addition, Random Forest had a higher prediction accuracy rate for samples with longer ETD—its accuracy rate could reach 99% when predicting those at 10 weeks after embryo transfer.
Conclusion
In this study, we established and compared six classification models to accurately predict EPL after the appearance of embryonic cardiac activity undergoing IVF-ET. Finally, Random Forest model outperformed the others. The implementation of Random Forest model in clinical environment can assist doctors to make clinical decisions.
ER  - 
TY  - JOUR
T1  - Abnormal respiratory event detection in sleep: A prescreening system with smart wearables
A1  - Camcı, Burçin
A1  - Ersoy, Cem
A1  - Kaynak, Hakan
Y1  - 2019///
KW  -  Machine learning
KW  -  Pervasive health
KW  -  Prescreening system
KW  -  Smart phone
KW  -  Smart watch
KW  - Respiratory event in sleep
JF  - Journal of Biomedical Informatics
VL  - 95
SP  - 103218
EP  - 103218
DO  - https://doi.org/10.1016/j.jbi.2019.103218
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419301376
N2  - Sleeping is an important activity to monitor since it has a crucial role in the overall health and well-being of the people and society. In order to diagnose the problems in sleep, different monitoring systems are developed in the literature. The unobtrusiveness, reduced cost, objectiveness, protection of privacy and user-friendliness are the main design considerations and the proposed system design achieves those objectives by utilizing smart wearables, smart watch and smart phone. The accelerometer and heart rate monitor sensors on smart watch and the sound level sensor on the smart phone are activated. The experiments with this system are performed with 17 subjects in a sleep clinic. The data collected from these subjects is used to generate various combinations by employing varied feature extraction, feature selection and sampling approaches. Five different machine learning algorithms are implemented and the classification results are generated using the various combinations of data, training and scoring strategies. The system performance is measured in two ways, the accuracy rate of distinguishing abnormal respiratory events is 85.95% and the classification success of subjects according to the problems in their respiration is one misclassification among 17 subjects. With all the methodology utilized in this study, the proposed system is a novel prescreening tool which recognizes the severity of problems in respiration during sleep.
ER  - 
TY  - JOUR
T1  - An improved I-FAST system for the diagnosis of Alzheimer's disease from unprocessed electroencephalograms by using robust invariant features
A1  - Buscema, Massimo
A1  - Vernieri, Fabrizio
A1  - Massini, Giulia
A1  - Scrascia, Federica
A1  - Breda, Marco
A1  - Rossini, Paolo Maria
A1  - Grossi, Enzo
Y1  - 2015///
KW  -  Alzheimer's disease
KW  -  Electroencephalogram
KW  -  Mild cognitive impairment
KW  -  Multi scale ranked organizing maps
KW  -  Training with input selection and testing
KW  - Implicit function as squashing time
JF  - Artificial Intelligence in Medicine
VL  - 64
IS  - 1
SP  - 59
EP  - 74
DO  - https://doi.org/10.1016/j.artmed.2015.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S0933365715000263
N2  - Objective
This paper proposes a new, complex algorithm for the blind classification of the original electroencephalogram (EEG) tracing of each subject, without any preliminary pre-processing. The medical need in this field is to reach an early differential diagnosis between subjects affected by mild cognitive impairment (MCI), early Alzheimer's disease (AD) and the healthy elderly (CTR) using only the recording and the analysis of few minutes of their EEG.
Methods and material
This study analyzed the EEGs of 272 subjects, recorded at Rome's Neurology Unit of the Policlinico Campus Bio-Medico. The EEG recordings were performed using 19 electrodes, in a 0.3–70Hz bandpass, positioned according to the International 10–20 System. Many powerful learning machines and algorithms have been proposed during the last 20 years to effectively resolve this complex problem, resulting in different and interesting outcomes. Among these algorithms, a new artificial adaptive system, named implicit function as squashing time (I-FAST), is able to diagnose, with high accuracy, a few minutes of the subject's EEG track; whether it manifests an AD, MCI or CTR condition. An updating of this system, carried out by adding a new algorithm, named multi scale ranked organizing maps (MS-ROM), to the I-FAST system, is presented, in order to classify with greater accuracy the unprocessed EEG's of AD, MCI and control subjects.
Results
The proposed system has been measured on three independent pattern recognition tasks from unprocessed EEG tracks of a sample of AD subjects, MCI subjects and CTR: (a) AD compared with CTR; (b) AD compared with MCI; (c) CTR compared with MCI. While the values of accuracy of the previous system in distinguishing between AD and MCI were around 92%, the new proposed system reaches values between 94% and 98%. Similarly, the overall accuracy with best artificial neural networks (ANNs) is 98.25% for the distinguishing between CTR and MCI.
Conclusions
This new version of I-FAST makes different steps forward: (a) avoidance of pre-processing phase and filtering procedure of EEG data, being the algorithm able to directly process an unprocessed EEG; (b) noise elimination, through the use of a training variant with input selection and testing system, based on naïve Bayes classifier; (c) a more robust classification phase, showing the stability of results on nine well known learning machine algorithms; (d) extraction of spatial invariants of an EEG signal using, in addition to the unsupervised ANN, the principal component analysis and the multi scale entropy, together with the MS-ROM; a more accurate performance in this specific task.
ER  - 
TY  - JOUR
T1  - Relating ensemble diversity and performance: A study in class noise detection
A1  - Sluban, Borut
A1  - Lavrač, Nada
Y1  - 2015///
KW  -  Diversity measures
KW  -  Ensemble methods
KW  -  Label noise
KW  -  Noise detection
KW  - Class noise
JF  - Neurocomputing
VL  - 160
SP  - 120
EP  - 131
DO  - https://doi.org/10.1016/j.neucom.2014.10.086
UR  - https://www.sciencedirect.com/science/article/pii/S0925231215001265
N2  - The advantage of ensemble methods over single methods is their ability to correct the errors of individual ensemble members and thereby improve the overall ensemble performance. This paper explores the relation between ensemble diversity and noise detection performance in the context of ensemble-based class noise detection by studying different diversity measures on a range of heterogeneous noise detection ensembles. In the empirical analysis the majority and the consensus ensemble voting schemes are studied. It is shown that increased diversity of ensembles using the majority voting scheme does not lead to better noise detection performance and may even degrade the performance of heterogeneous noise detection ensembles. On the other hand, for consensus-based noise detection ensembles the results show that more diverse ensembles achieve higher precision of class noise detection, whereas less diverse ensembles lead to higher recall of noise detection and higher F-scores.
ER  - 
TY  - JOUR
T1  - Quantitative 3D Analysis of Coronary Wall Morphology in Heart Transplant Patients: OCT-Assessed Cardiac Allograft Vasculopathy Progression
A1  - Chen, Zhi
A1  - Pazdernik, Michal
A1  - Zhang, Honghai
A1  - Wahle, Andreas
A1  - Guo, Zhihui
A1  - Bedanova, Helena
A1  - Kautzner, Josef
A1  - Melenovsky, Vojtech
A1  - Kovarnik, Tomas
A1  - Sonka, Milan
Y1  - 2018///
KW  -  CAV prediction
KW  -  CAV progression
KW  -  LOGISMOS
KW  -  optical coherence tomography (OCT)
KW  - Cardiac allograft vasculopathy (CAV)
JF  - Medical Image Analysis
VL  - 50
SP  - 95
EP  - 105
DO  - https://doi.org/10.1016/j.media.2018.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S1361841518306984
N2  - Cardiac allograft vasculopathy (CAV) accounts for about 30% of all heart-transplant (HTx) patient deaths. For patients at high risk for CAV complications after HTx, therapy must be initiated early to be effective. Therefore, new phenotyping approaches are needed to identify such HTx patients at the earliest possible time. Coronary optical coherence tomography (OCT) images were acquired from 50 HTx patients 1 and 12 months after HTx. Quantitative analysis of coronary wall morphology used LOGISMOS segmentation strategy to simultaneously identify three wall-layer surfaces for the entire pullback length in 3D: luminal, outer intimal, and outer medial surfaces. To quantify changes of coronary wall morphology between 1 and 12 months after HTx, the two pullbacks were mutually co-registered. Validation of layer thickness measurements showed high accuracy of performed layer analyses with layer thickness measures correlating well with manually-defined independent standard (Rautomated2 = 0.93, y=1.0x−6.2μm), average intimal+medial thickness errors were 4.98 ± 31.24 µm, comparable with inter-observer variability. Quantitative indices of coronary wall morphology 1 month and 12 months after HTx showed significant local as well as regional changes associated with CAV progression. Some of the newly available fully-3D baseline indices (intimal layer brightness, medial layer brightness, medial thickness, and intimal+medial thickness) were associated with CAV-related progression of intimal thickness showing promise of identifying patients subjected to rapid intimal thickening at 12 months after HTx from OCT-image data obtained just 1 month after HTx. Our approach allows quantification of location-specific alterations of coronary wall morphology over time and is sensitive even to very small changes of wall layer thicknesses that occur in patients following heart transplant.
ER  - 
TY  - JOUR
T1  - Disease Specific Ontology of Adverse Events: Ontology extension and adaptation for Chronic Kidney Disease
A1  - Kang, Yin
A1  - Fink, Jeffrey C
A1  - Doerfler, Rebecca
A1  - Zhou, Lina
Y1  - 2018///
KW  -  Chronic kidney disease
KW  -  Disease specific adverse event
KW  -  Ontology extension and adaptation
KW  - Adverse event ontology
JF  - Computers in Biology and Medicine
VL  - 101
SP  - 210
EP  - 217
DO  - https://doi.org/10.1016/j.compbiomed.2018.08.024
UR  - https://www.sciencedirect.com/science/article/pii/S0010482518302506
N2  - Background
Adverse Event (AE) ontology can be used to support interoperability and computer-assisted reasoning of AEs. Despite significant progress in developing biomedical ontologies, they are facing the obstacle of adoption partly because those ontologies are too general to meet the requirements of a specific domain. Understanding and representing of AEs for a specific domain such as Chronic Kidney Disease (CKD) has both theoretical and clinical significance. CKD patients are at a high risk for an array of disease-intervention specific AEs, and these in turn can contribute to disease progression unlike other diseases. This study proposes Disease Specific Ontology of Adverse Events (DSOAE) to address specific requirements of CKD, and applies it to different usage scenarios with real data.
Methods
We introduce a method for developing DSOAE through the extension and adaption of general ontologies by incorporating domain-specific information and usage requirements. It starts with specifying the goal and scope of a target domain (i.e. selecting seed ontologies), followed by identifying main AE classes and relations, extracting and creating classes and relations, aligning and identifying upper-level classes and lower-level classes, and finally populating the ontology with instances. Any of these steps may be repeated to refine the ontology.
Results
DSOAE contains 22 CKD-specific AE classes, which are grouped into two general categories: patient-reported AEs and biochemical/laboratory-related AEs. In addition, disease history and comorbidity classes as introduced in this study help model patient-related risk factors for AEs. With the support of DSOAE, we build a knowledge base of CKD-specific AEs using data from different sources (e.g. patient cohort data and social media), and apply the knowledge base to data analysis and data integration.
Conclusions
DSOAE enables the interoperability of AEs across different sources and supports the development of a knowledge base of domain-specific AEs. DSOAE can also meet the needs of different usage scenarios. The approach to constructing DSOAE is generalizable and can be used to develop AE ontology in other domains.
ER  - 
TY  - JOUR
T1  - Simulated microgravity led to increased brown adipose tissue activity in rats
A1  - Chen, Yongjie
A1  - Zhang, Hongyu
A1  - Xu, Ji
A1  - Yang, Chao
A1  - Wu, Feng
A1  - Lu, Xin
A1  - Chen, Jian
A1  - Li, Kai
A1  - Wang, Hailong
A1  - Zhong, Yue
A1  - Nie, Huan
A1  - Li, Yu
A1  - Li, Yinghui
A1  - Dai, Zhongquan
Y1  - 2019///
KW  -  Brown adipose tissue
KW  -  Browning of white adipose tissue
KW  -  Fatty acid metabolism
KW  -  Metabolomics
KW  - Tail suspension
JF  - Acta Astronautica
VL  - 160
SP  - 538
EP  - 551
DO  - https://doi.org/10.1016/j.actaastro.2018.11.032
UR  - https://www.sciencedirect.com/science/article/pii/S0094576518310130
N2  - Long-term spaceflight has been proven to induce metabolic dysfunction. Brown adipose tissue (BAT) plays an important role in whole-body energy metabolism. We speculated that BAT function would change under microgravity. Here, we employed a tail suspension (TS) rat model to simulate the effects of microgravity and found that TS increased BAT activity including 1.79, 2.74, 2.69 folds upregulation of UCP1 mRNA at different TS time and induced a phenotypic switch from white adipose tissue (WAT) to brown fat. Furthermore, serum metabolomics revealed abnormal fatty acid metabolism pathway in TS rats. Additionally, we observed that TS led to higher levels of circulating epinephrine (1.28, 1.2, 1.26 folds), norepinephrine (1.12, 1.08, 1.26 folds), adiponectin (1.52, 1.36, 1.64 folds), and FGF21 (2.2, 1.9, 2.7 folds) in different TS rats groups respectively, which contributed to the metabolic disorder in TS rats. In summary, our results indicated that microgravity increased the activity of BAT which might contribute to metabolic dysfunction during long-term exposure to the space environment.
ER  - 
TY  - JOUR
T1  - Development of ontology-based multiplatform adaptive scientific visualization system
A1  - Ryabinin, Konstantin
A1  - Chuprina, Svetlana
Y1  - 2015///
KW  -  Adaptivity
KW  -  Mobile devices
KW  -  Multiplatform portability
KW  -  Ontology
KW  - Scientific visualization
JF  - Journal of Computational Science
VL  - 10
SP  - 370
EP  - 381
DO  - https://doi.org/10.1016/j.jocs.2015.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S1877750315000307
N2  - In this paper, we propose methods and tools for multi-platform adaptive visualization systems’ development that meets the specific visualization requirements of the computational experiments in the different fields of science. The proposed approach was implemented within the client-server rendering system SciVi (Scientific Visualizer) presented in this paper. This system provides multi-platform portability and automated integration with different solvers based on ontology engineering methods. SciVi is used in Perm State University to help scientists and researchers acquire the necessary multidisciplinary skills and to solve real scientific problems by means of adaptive visualization tools.
ER  - 
TY  - JOUR
T1  - Diagnosis of heart diseases by a secure Internet of Health Things system based on Autoencoder Deep Neural Network
A1  - Deperlioglu, Omer
A1  - Kose, Utku
A1  - Gupta, Deepak
A1  - Khanna, Ashish
A1  - Sangaiah, Arun Kumar
Y1  - 2020///
KW  -  Autoencoder neural networks
KW  -  Deep learning
KW  -  Heart sounds classification
KW  -  Secure internet of health things
KW  - Heart diseases
JF  - Computer Communications
VL  - 162
SP  - 31
EP  - 50
DO  - https://doi.org/10.1016/j.comcom.2020.08.011
UR  - https://www.sciencedirect.com/science/article/pii/S0140366420318909
N2  - Objective of this study is to introduce a secure IoHT system, which acts as a clinical decision support system with the diagnosis of cardiovascular diseases. In this sense, it was emphasized that the accuracy rate of diagnosis (classification) can be improved via deep learning algorithms, by needing no hybrid-complex models, and a secure data processing can be achieved with a multi-authentication and Tangle based approach. In detail, heart sounds were classified with Autoencoder Neural Networks (AEN) and the IoHT system was built for supporting doctors in real-time. For developing the diagnosis infrastructure by the AEN, PASCAL B-Training and Physiobank-PhysioNet A-Training heart sound datasets were used accordingly. For the PASCAL dataset, the AEN provided a diagnosis-classification performance with the accuracy of 100%, sensitivity of 100%, and the specificity of 100% whereas the rates were respectively 99.8%, 99.65%, and 99.13% for the PhysioNet dataset. It was seen that the findings by the developed AEN based solution were better than the alternative solutions from the literature. Additionally, usability of the whole IoHT system was found positive by the doctors, and according to the 479 real-case applications, the system was able to achieve accuracy rates of 96.03% for normal heart sounds, 91.91% for extrasystole, and 90.11% for murmur. In terms of security approach, the system was also robust against several attacking methods including synthetic data impute as well as trying to penetrating to the system via central system or mobile devices.
ER  - 
TY  - JOUR
T1  - Retinal vessel segmentation of color fundus images using multiscale convolutional neural network with an improved cross-entropy loss function
A1  - Hu, Kai
A1  - Zhang, Zhenzhen
A1  - Niu, Xiaorui
A1  - Zhang, Yuan
A1  - Cao, Chunhong
A1  - Xiao, Fen
A1  - Gao, Xieping
Y1  - 2018///
KW  - Conditional random field
KW  - Convolutional neural network
KW  - Cross-entropy loss function
KW  - Multiscale
KW  - Retinal vessel segmentation
JF  - Neurocomputing
VL  - 309
SP  - 179
EP  - 191
DO  - https://doi.org/10.1016/j.neucom.2018.05.011
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218305423
N2  - Retinal vessel analysis of fundus images is an indispensable method for the screening and diagnosis of related diseases. In this paper, we propose a novel retinal vessel segmentation method of the fundus images based on convolutional neural network (CNN) and fully connected conditional random fields (CRFs). The segmentation process is mainly divided into two steps. Firstly, a multiscale CNN architecture with an improved cross-entropy loss function is proposed to produce the probability map from image to image. We construct the multiscale network by combining the feature map of each middle layer to learn more detail information of the retinal vessels. Meanwhile, our proposed cross-entropy loss function ignores the slightest loss of relatively easy samples in order to take more attention to learn the hard examples. Secondly, CRFs is applied to get the final binary segmentation result which makes use of more spatial context information by taking into account the interactions among all of the pixels in the fundus images. The effectiveness of the proposed method has been evaluated on two public datasets, i.g., DRIVE and STARE with comparisons against eleven state-of-the-art approaches including five deep learning based methods. Results show that our method allows for detection of more tiny blood vessels and more precise locating of the edges.
ER  - 
TY  - JOUR
T1  - Fuzzy feature selection based on min–max learning rule and extension matrix
A1  - Li, Yun
A1  - Wu, Zhong-Fu
Y1  - 2008///
KW  -  Extension matrix
KW  -  Feature selection
KW  -  Min–max rule
KW  - Fuzzy set theory
JF  - Pattern Recognition
VL  - 41
IS  - 1
SP  - 217
EP  - 226
DO  - https://doi.org/10.1016/j.patcog.2007.06.007
UR  - https://www.sciencedirect.com/science/article/pii/S0031320307002725
N2  - In many systems, such as fuzzy neural network, we often adopt the language labels (such as large, medium, small, etc.) to split the original feature into several fuzzy features. In order to reduce the computation complexity of the system after the fuzzification of features, the optimal fuzzy feature subset should be selected. In this paper, we propose a new heuristic algorithm, where the criterion is based on min–max learning rule and fuzzy extension matrix is designed as the search strategy. The algorithm is proved in theory and has shown its high performance over several real-world benchmark data sets.
ER  - 
TY  - JOUR
T1  - An approach to integrating shape and biomedical attributes in vascular models
A1  - Li, Jie
A1  - Regli, William C
A1  - Sun, Wei
Y1  - 2007///
KW  -  Biomedical CAD
KW  -  Geometric representation
KW  -  Swept volumes
KW  -  Tissue engineering
KW  - Vascular modeling
JF  - Computer-Aided Design
VL  - 39
IS  - 7
SP  - 598
EP  - 609
DO  - https://doi.org/10.1016/j.cad.2007.03.002
UR  - https://www.sciencedirect.com/science/article/pii/S0010448507000747
N1  - Human Modeling and Applications
N2  - Computational models have been used widely in tissue engineering research and have proven to be powerful tools for bio-mechanical analysis (i.e., blood flow, growth models, drug delivery, etc). This paper focuses on developing higher-fidelity models for vascular structures and blood vessels that integrate computational shape representations with biomedical properties and features. Previous work in computer-aided vascular modeling comes from two communities. For those in biomedical imaging, the goal of past research has been to develop image understanding techniques for the interpretation of x-ray, magnetic resonance imaging (MRI), or other radiological data. These representations are predominantly discrete shape models that are not tied to physiological properties. The other corpus of existing work comes from those interested in developing physiological models for vascular growth and behavior based on bio-medical attributes. These models usually either have a highly simplified shape representation, or lack one entirely. Further, neither of these representations are suitable for the kind of interactive modeling required by tissue engineering applications. This paper aims to bridge these two approaches and develop a set of mathematical tools and algorithms for feature-based representation and computer-aided modeling of vascular trees for use in computer-aided tissue engineering applications. The paper offers a multi-scale representation based on swept volumes and a feature-based representation that can attribute the geometric representation with information about blood flow, pressure, and other biomedical properties. The paper shows how the resulting representation can be used as part of an overall approach for designing and visualizing vascular scaffolds. As a real-world example, we show how this computational model can be used to develop a tissue scaffold for liver tissue engineering. Such scaffolds may prove useful in a number of biomedical applications, including the growth of replacement tissue grafts and in vitro study of the pharmacological affects of new drugs on tissue cultures.
ER  - 
TY  - JOUR
T1  - Retrieval and classification of food images
A1  - Farinella, Giovanni Maria
A1  - Allegra, Dario
A1  - Moltisanti, Marco
A1  - Stanco, Filippo
A1  - Battiato, Sebastiano
Y1  - 2016///
KW  -  Anti-Textons
KW  -  Food classification
KW  -  Food representation
KW  -  Textons
KW  - Food retrieval
JF  - Computers in Biology and Medicine
VL  - 77
SP  - 23
EP  - 39
DO  - https://doi.org/10.1016/j.compbiomed.2016.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0010482516301822
N2  - Automatic food understanding from images is an interesting challenge with applications in different domains. In particular, food intake monitoring is becoming more and more important because of the key role that it plays in health and market economies. In this paper, we address the study of food image processing from the perspective of Computer Vision. As first contribution we present a survey of the studies in the context of food image processing from the early attempts to the current state-of-the-art methods. Since retrieval and classification engines able to work on food images are required to build automatic systems for diet monitoring (e.g., to be embedded in wearable cameras), we focus our attention on the aspect of the representation of the food images because it plays a fundamental role in the understanding engines. The food retrieval and classification is a challenging task since the food presents high variableness and an intrinsic deformability. To properly study the peculiarities of different image representations we propose the UNICT-FD1200 dataset. It was composed of 4754 food images of 1200 distinct dishes acquired during real meals. Each food plate is acquired multiple times and the overall dataset presents both geometric and photometric variabilities. The images of the dataset have been manually labeled considering 8 categories: Appetizer, Main Course, Second Course, Single Course, Side Dish, Dessert, Breakfast, Fruit. We have performed tests employing different representations of the state-of-the-art to assess the related performances on the UNICT-FD1200 dataset. Finally, we propose a new representation based on the perceptual concept of Anti-Textons which is able to encode spatial information between Textons outperforming other representations in the context of food retrieval and Classification.
ER  - 
TY  - JOUR
T1  - Multi-sensor fusion in body sensor networks: State-of-the-art and research challenges
A1  - Gravina, Raffaele
A1  - Alinia, Parastoo
A1  - Ghasemzadeh, Hassan
A1  - Fortino, Giancarlo
Y1  - 2017///
KW  -  Data-level fusion
KW  -  Decision-level fusion
KW  -  Feature-level fusion
KW  -  Human activity recognition
KW  - Multi-sensor data fusion
JF  - Information Fusion
VL  - 35
SP  - 68
EP  - 80
DO  - https://doi.org/10.1016/j.inffus.2016.09.005
UR  - https://www.sciencedirect.com/science/article/pii/S156625351630077X
N2  - Body Sensor Networks (BSNs) have emerged as a revolutionary technology in many application domains in health-care, fitness, smart cities, and many other compelling Internet of Things (IoT) applications. Most commercially available systems assume that a single device monitors a plethora of user information. In reality, BSN technology is transitioning to multi-device synchronous measurement environments; fusion of the data from multiple, potentially heterogeneous, sensor sources is therefore becoming a fundamental yet non-trivial task that directly impacts application performance. Nevertheless, only recently researchers have started developing technical solutions for effective fusion of BSN data. To the best of our knowledge, the community is currently lacking a comprehensive review of the state-of-the-art techniques on multi-sensor fusion in the area of BSN. This survey discusses clear motivations and advantages of multi-sensor data fusion and particularly focuses on physical activity recognition, aiming at providing a systematic categorization and common comparison framework of the literature, by identifying distinctive properties and parameters affecting data fusion design choices at different levels (data, feature, and decision). The survey also covers data fusion in the domains of emotion recognition and general-health and introduce relevant directions and challenges of future research on multi-sensor fusion in the BSN domain.
ER  - 
TY  - JOUR
T1  - An empirical study on improving dissimilarity-based classifications using one-shot similarity measure
A1  - Kim, Sang-Woon
Y1  - 2014///
KW  -  Dissimilarity-based classification (DBC)
KW  -  One-shot similarity (OSS) measure
KW  -  Semi-supervised learning (SSL)
KW  - Statistical pattern recognition
JF  - Digital Signal Processing
VL  - 27
SP  - 69
EP  - 78
DO  - https://doi.org/10.1016/j.dsp.2013.12.013
UR  - https://www.sciencedirect.com/science/article/pii/S1051200414000025
N2  - This paper reports an experimental result obtained by additionally using unlabeled data together with labeled ones to improve the classification accuracy of dissimilarity-based methods, namely, dissimilarity-based classifications (DBC) [25]. In DBC, classifiers among classes are not based on the feature measurements of individual objects, but on a suitable dissimilarity measure among the objects instead. In order to measure the dissimilarity distance between pairwise objects, an approach using the one-shot similarity (OSS) [30] measuring technique instead of the Euclidean distance is investigated in this paper. In DBC using OSS, the unlabeled set can be used to extend the set of prototypes as well as to compute the OSS distance. The experimental results, obtained with artificial and real-life benchmark datasets, demonstrate that designing the classifiers in the OSS dissimilarity matrices instead of expanding the set of prototypes can further improve the classification accuracy in comparison with the traditional Euclidean approach. Moreover, the results demonstrate that the proposed setting does not work with non-Euclidean data.
ER  - 
TY  - JOUR
T1  - Exudates segmentation using inverse surface adaptive thresholding
A1  - Yazid, Haniza
A1  - Arof, Hamzah
A1  - Mohd Isa, Hazlita
Y1  - 2012///
KW  -  Biomedical applications
KW  -  Diabetic retinopathy
KW  -  Image segmentation
KW  -  Inverse surface thresholding
KW  -  Medical images
KW  - Thresholding
JF  - Measurement
VL  - 45
IS  - 6
SP  - 1599
EP  - 1608
DO  - https://doi.org/10.1016/j.measurement.2012.02.016
UR  - https://www.sciencedirect.com/science/article/pii/S0263224112000954
N2  - This paper presents a new approach to detect exudates and optic disc from color fundus images based on inverse surface thresholding. The strategy involves the applications of fuzzy c-means clustering, edge detection, otsu thresholding and inverse surface thresholding. The main advantage of the proposed approach is that it does not depend on manually selected parameters that are normally chosen to suit the tested databases. When applied to two sets of databases the proposed method outperforms methods based on watershed segmentation and morphological reconstruction. The proposed method obtained 98.2 and 90.4 in terms of sensitivity for Standard Diabetic Retinopathy Database – Calibration Level 1 (DIARETDB1) and a local dataset provided by National University Hospital of Malaysia (NUHM), respectively.
ER  - 
TY  - JOUR
T1  - Affine feature extraction: A generalization of the Fukunaga–Koontz transformation
A1  - Cao, Wenbo
A1  - Haralick, Robert
Y1  - 2009///
KW  -  Affine transformation
KW  -  Fisher's discriminant analysis
KW  -  Fukunaga–Koontz transformation
KW  -  Kullback–Leibler divergence
KW  - Feature extraction
JF  - Engineering Applications of Artificial Intelligence
VL  - 22
IS  - 1
SP  - 40
EP  - 47
DO  - https://doi.org/10.1016/j.engappai.2008.04.008
UR  - https://www.sciencedirect.com/science/article/pii/S0952197608000833
N2  - Dimension reduction methods are often applied in machine learning and data mining problems. Linear subspace methods are the commonly used ones, such as principal component analysis (PCA), Fisher's linear discriminant analysis (FDA), common spatial pattern (CSP), et al. In this paper, we describe a novel feature extraction method for binary classification problems. Instead of finding linear subspaces, our method finds lower-dimensional affine subspaces satisfying a generalization of the Fukunaga–Koontz transformation (FKT). The proposed method has a closed-form solution and thus can be solved very efficiently. Under normality assumption, our method can be seen as finding an optimal truncated spectrum of the Kullback–Leibler divergence. Also we show that FDA and CSP are special cases of our proposed method under normality assumption. Experiments on simulated data show that our method performs better than PCA and FDA on data that is distributed on two cylinders, even one within the other. We also show that, on several real data sets, our method provides statistically significant improvement on test set accuracy over FDA, CSP and FKT. Therefore the proposed method can be used as another preliminary data-exploring tool to help solve machine learning and data mining problems.
ER  - 
TY  - JOUR
T1  - Nonlinear heart rate dynamics: Circadian profile and influence of age and gender
A1  - Vandeput, S
A1  - Verheyden, B
A1  - Aubert, A E
A1  - Van Huffel, S
Y1  - 2012///
KW  -  Aging
KW  -  Circadian variations
KW  -  Heart rate variability
KW  -  Nonlinear dynamics
KW  - Numerical noise titration
JF  - Medical Engineering & Physics
VL  - 34
IS  - 1
SP  - 108
EP  - 117
DO  - https://doi.org/10.1016/j.medengphy.2011.07.004
UR  - https://www.sciencedirect.com/science/article/pii/S1350453311001615
N2  - Heart rate variability (HRV) is used as a marker of autonomic modulation of heart rate. Nonlinear HRV parameters providing information about the scaling behaviour or the complexity of the cardiac system were included. In addition, the chaotic behaviour was quantified by means of the recently developed numerical noise titration technique. 24h Holter recordings of a large healthy population (N=276, 141 males, 18–71 years of age) were available. The goal was to investigate the influence of gender, age and day–night variation on these nonlinear HRV parameters. Numerical titration yielded similar information as other nonlinear HRV parameters do. However, it does not require long and cleaned data and therefore applicable on short (5min) noisy time series. A higher nonlinear behaviour was observed during the night (NLdr; day: 50.8±19.6%, night: 59.1±19.5%; P<0.001) while nonlinear heart rate fluctuations decline with increasing age (NLdr; Pearson correlation coefficient r between −0.260 and −0.319 dependent on gender and day or night, all P<0.01). A clear circadian profile could be found for almost every parameter, showing in particular which changes occur during the transition phases of waking up and going to sleep. Our results support the involvement of the autonomic nervous system in the generation of nonlinear and complex heart rate dynamics.
ER  - 
TY  - JOUR
T1  - SENET: A novel architecture for IoT-based body sensor networks
A1  - Arabi Bulaghi, Zohre
A1  - Habibi Zad Navin, Ahmad
A1  - Hosseinzadeh, Mehdi
A1  - Rezaee, Ali
Y1  - 2020///
KW  -  Energy-consumption
KW  -  Wireless body sensor networks
KW  -  World competitive contest algorithm
KW  - Architecture
JF  - Informatics in Medicine Unlocked
VL  - 20
SP  - 100365
EP  - 100365
DO  - https://doi.org/10.1016/j.imu.2020.100365
UR  - https://www.sciencedirect.com/science/article/pii/S2352914819304095
N2  - Wireless sensor networks (WSNs) have been applied to various fields of study including medicine, agriculture, and engineering. Although recently, many architecture styles have been proposed to manage WSNs, most of them have ignored the application of artificial intelligence (AI) in wireless body sensor networks (WBSN). To this end, the present study aims to introduce a novel architecture (SENET), which is based on AI techniques and consists of three main layers. After describing the proposed architecture, the performance of four efficient and popular algorithms, i.e., world competitive contests (WCC), particle swarm optimization (PSO), ant colony optimization (ACO), and genetic algorithm (GA) is investigated for covering WBSNs using k head clusters (the k-coverage problem). The results show not only that the proposed architecture saves energy consumed by the wireless sensors, but also that the WCC algorithm is a suitable option for determining the positions of sensors in the proposed architecture in terms of WSN energy-consumption, the total number of required sensors, and reliability. The results also show that the proposed WCC algorithm, with an average 38.44 value of score on nine scenarios, outperforms other techniques.
ER  - 
TY  - JOUR
T1  - A novel and robust Bayesian approach for segmentation of psoriasis lesions and its risk stratification
A1  - Shrivastava, Vimal K
A1  - Londhe, Narendra D
A1  - Sonawane, Rajendra S
A1  - Suri, Jasjit S
Y1  - 2017///
KW  -  Bayesian segmentation
KW  -  Color features
KW  -  Machine learning
KW  -  Performance evaluation
KW  -  Texture features
KW  - Psoriasis
JF  - Computer Methods and Programs in Biomedicine
VL  - 150
SP  - 9
EP  - 22
DO  - https://doi.org/10.1016/j.cmpb.2017.07.011
UR  - https://www.sciencedirect.com/science/article/pii/S016926071730010X
N2  - Background and Objective
The need for characterization of psoriasis lesion severity is clinically valuable and vital for dermatologists since it provides a reliable and precise decision on risk assessment. The automated delineation of lesion is a prerequisite prior to characterization, which is challenging itself. Thus, this paper has two major objectives: (a) design of a segmentation system which can model by learning the lesion characteristics and this is posed as a Bayesian model; (b) develop a psoriasis risk assessment system (pRAS) by crisscrossing the blocks which drives the fundamental machine learning paradigm.
Methods
The segmentation system uses the knowledge derived by the experts along with the features reflected by the lesions to build a Bayesian framework that helps to classify each pixel of the image into lesion vs. background. Since this lesion has several stages and grades, hence the system undergoes the risk assessment to classify into five levels of severity: healthy, mild, moderate, severe and very severe. We build nine kinds of pRAS utilizing different combinations of the key blocks. These nine pRAS systems use three classifiers (Support Vector Machine (SVM), Decision Tree (DT) and Neural Network (NN)) and three feature selection techniques (Principal Component Analysis (PCA), Fisher Discriminant Ratio (FDR) and Mutual Information (MI)). The two major experiments conducted using these nine systems were: (i) selection of best system combination based on classification accuracy and (ii) understanding the reliability of the system. This leads us to computation of key system performance parameters such as: feature retaining power, aggregated feature effect and reliability index besides conventional attributes like accuracy, sensitivity, specificity.
Results
Using the database used in this study consisted of 670 psoriasis images, the combination of SVM and FDR was revealed as the optimal pRAS system and yielded a classification accuracy of 99.84% using cross-validation protocol. Further, SVM-FDR system provides the reliability of 99.99% using cross-validation protocol.
Conclusions
The study demonstrates a fully novel model of segmentation embedded with risk assessment. Among all nine systems, SVM-FDR produced best results. Further, we validated our pRAS system with automatic segmented lesions against manually segmented lesions showing comparable performance.
ER  - 
TY  - JOUR
T1  - HPIminer: A text mining system for building and visualizing human protein interaction networks and pathways
A1  - Subramani, Suresh
A1  - Kalpana, Raja
A1  - Monickaraj, Pankaj Moses
A1  - Natarajan, Jeyakumar
Y1  - 2015///
KW  - Biomedical informatics
KW  - Information extraction
KW  - Knowledge discovery
KW  - Network visualization
KW  - Pathway visualization
KW  - Protein–protein interactions
KW  - Text mining
JF  - Journal of Biomedical Informatics
VL  - 54
SP  - 121
EP  - 131
DO  - https://doi.org/10.1016/j.jbi.2015.01.006
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415000088
N2  - The knowledge on protein–protein interactions (PPI) and their related pathways are equally important to understand the biological functions of the living cell. Such information on human proteins is highly desirable to understand the mechanism of several diseases such as cancer, diabetes, and Alzheimer’s disease. Because much of that information is buried in biomedical literature, an automated text mining system for visualizing human PPI and pathways is highly desirable. In this paper, we present HPIminer, a text mining system for visualizing human protein interactions and pathways from biomedical literature. HPIminer extracts human PPI information and PPI pairs from biomedical literature, and visualize their associated interactions, networks and pathways using two curated databases HPRD and KEGG. To our knowledge, HPIminer is the first system to build interaction networks from literature as well as curated databases. Further, the new interactions mined only from literature and not reported earlier in databases are highlighted as new. A comparative study with other similar tools shows that the resultant network is more informative and provides additional information on interacting proteins and their associated networks.
ER  - 
TY  - JOUR
T1  - PSOLDA: A particle swarm optimization approach for enhancing classification accuracy rate of linear discriminant analysis
A1  - Lin, Shih-Wei
A1  - Chen, Shih-Chieh
Y1  - 2009///
KW  -  Feature selection
KW  -  Particle swarm optimization
KW  - Linear discriminant analysis
JF  - Applied Soft Computing
VL  - 9
IS  - 3
SP  - 1008
EP  - 1015
DO  - https://doi.org/10.1016/j.asoc.2009.01.001
UR  - https://www.sciencedirect.com/science/article/pii/S1568494609000180
N2  - Linear discriminant analysis (LDA) is a commonly used classification method. It can provide important weight information for constructing a classification model. However, real-world data sets generally have many features, not all of which benefit the classification results. If a feature selection algorithm is not employed, unsatisfactory classification will result, due to the high correlation between features and noise. This study points out that the feature selection has influence on the LDA by showing an example. The methods traditionally used for LDA to determine the beneficial feature subset are not easy or cannot guarantee the best results when problems have larger number of features. The particle swarm optimization (PSO) is a powerful meta-heuristic technique in the artificial intelligence field; therefore, this study proposed a PSO-based approach, called PSOLDA, to specify the beneficial features and to enhance the classification accuracy rate of LDA. To measure the performance of PSOLDA, many public datasets are employed to measure the classification accuracy rate. Comparing the optimal result obtained by the exhaustive enumeration, the PSOLDA approach can obtain the same optimal result. Due to much time required for exhaustive enumeration when problems have larger number of features, exhaustive enumeration cannot be applied. Therefore, many heuristic approaches, such as forward feature selection, backward feature selection, and PCA-based feature selection are used. This study showed that the classification accuracy rates of the PSOLDA were higher than those of these approaches in many public data sets.
ER  - 
TY  - JOUR
T1  - Using neural attention networks to detect adverse medical events from electronic health records
A1  - Chu, Jiebin
A1  - Dong, Wei
A1  - He, Kunlun
A1  - Duan, Huilong
A1  - Huang, Zhengxing
Y1  - 2018///
KW  -  Cardiovascular disease
KW  -  Deep learning
KW  -  Electronic health record
KW  -  Neural attention network
KW  - Adverse medical event
JF  - Journal of Biomedical Informatics
VL  - 87
SP  - 118
EP  - 130
DO  - https://doi.org/10.1016/j.jbi.2018.10.002
UR  - https://www.sciencedirect.com/science/article/pii/S1532046418301989
N2  - The detection of Adverse Medical Events (AMEs) plays an important role in disease management in ensuring efficient treatment delivery and quality improvement of health services. Recently, with the rapid development of hospital information systems, a large volume of Electronic Health Records (EHRs) have been produced, in which AMEs are regularly documented in a free-text manner. In this study, we are concerned with the problem of AME detection by utilizing a large volume of unstructured EHR data. To address this challenge, we propose a neural attention network-based model to incorporate the contextual information of words into AME detection. Specifically, we develop a context-aware attention mechanism to locate salient words with respect to the target AMEs in patient medical records. And then we combine the proposed context attention mechanism with the deep learning tactic to boost the performance of AME detection. We validate our proposed model on a real clinical dataset that consists of 8845 medical records of patients with cardiovascular diseases. The experimental results show that our proposed model advances state-of-the-art models and achieves competitive performance in terms of AME detection.
ER  - 
TY  - JOUR
T1  - A H-QoS-demand personalized home physiological monitoring system over a wireless multi-hop relay network for mobile home healthcare applications
A1  - Lai, Chien-Chih
A1  - Lee, Ren-Guey
A1  - Hsiao, Chun-Chieh
A1  - Liu, Hsin-Sheng
A1  - Chen, Chun-Chang
Y1  - 2009///
KW  -  Electrocardiogram (ECG)
KW  -  Health monitoring
KW  -  Healthcare informatics
KW  -  Healthcare quality of service (H-QoS)
KW  -  Long-term
KW  -  Mobile care
KW  -  Multi-hop relay network
KW  -  Physiological monitoring
KW  -  Telehomecare
KW  -  Wireless
KW  - Home healthcare
JF  - Journal of Network and Computer Applications
VL  - 32
IS  - 6
SP  - 1229
EP  - 1241
DO  - https://doi.org/10.1016/j.jnca.2009.05.007
UR  - https://www.sciencedirect.com/science/article/pii/S1084804509000708
N2  - For the elderly and chronic patients with cardiovascular disease who live alone, it is necessary to constantly monitor their physiological parameters, especially the electrocardiogram (ECG), to effectively prevent and control their health condition and even to provide urgent treatment or care while an emergency such as the abnormal variation of heart rate (HR) occurs. In this paper, a wireless in-home physiological monitoring system, based on multi-hop relay communications, which can ubiquitously and continuously monitor the patient's ECG at any time or any place at home without space limit and the “dead spot” due to the extended communication coverage by multi-hop wireless connectivity, is proposed. The system consists of a mobile-care device, which is responsible for capturing and wirelessly sending the patient's ECG data, a wireless multi-hop relay network (WMHRN) that is in charge of relaying the data sent by the former, and a residential gateway (RG), which is responsible for gathering and uploading the received ECG data to the remote care server through the Internet to carry out the patient's health condition monitoring and the management of pathological data. However, in order to assure that the ECG data can be effectively and timely forwarded, from the mobile-care device to the RG through the WMHRN, to meet the healthcare quality of service (H-QoS) demand for reliable and real-time end-to-end ECG transmission, the analysis of WMHRN latency in data-forwarding stage and the deployment consideration of wireless relay nodes are investigated in detail in this work. Moreover, an emergency alert service using short message service (SMS), based on the detection of abnormal variation of HR, is also used in the RG to further enhance the healthcare service quality. A prototype of this system has been developed and implemented. Finally, the experimental results are presented to verify the feasibility of the proposed system.
ER  - 
TY  - JOUR
T1  - Fuzzy algorithms: Application to adipose tissue quantification on MR images
A1  - Roullier, Vincent
A1  - Cavaro-Ménard, Christine
A1  - Calmon, Guillaume
A1  - Aubé, Christophe
Y1  - 2007///
KW  -  FGcM
KW  -  MRI
KW  -  Medical image analysis
KW  - Fuzzy methods
JF  - Biomedical Signal Processing and Control
VL  - 2
IS  - 3
SP  - 239
EP  - 247
DO  - https://doi.org/10.1016/j.bspc.2007.07.005
UR  - https://www.sciencedirect.com/science/article/pii/S1746809407000481
N1  - IFAC Symposia on Biomedical Systems Modelling &amp; Control
N2  - Metabolic syndrome, which is related to abdominal obesity, is a fast growing disease in our western countries. Its presence greatly increases the risk of developing cardiovascular diseases. The accumulation of visceral adipose tissue plays a key role in the development of the metabolic syndrome. The increase of waist circumference is one of the five criteria of the metabolic syndrome diagnosis. But this increase can be due to visceral or subcutaneous adipose tissues. And these adipose tissues do not play the same rule in metabolic syndrome. The purpose of this study was to develop software for automatic and reliable quantification of visceral and subcutaneous adipose tissues, to detect patient with high risk to develop metabolic syndrome and to follow the evolution of adipose tissue repartition after treatment. A gradient echo magnetic resonance (MR) technique is used, with a TE such that fat and water are opposed in phase. The developed process is based on two fuzzy algorithms. First, we fuzzy generalized clustering algorithms allow to merge pixels according to their intensities. Then, fuzzy connectedness algorithm allows to merge pixels according to cost function related to distance, gradient distance and intensities. A validation is performed with a comparison between expert results made by manual drawing and purpose-made software results. Our software provides an automatic and reliable method to segment visceral and subcutaneous adipose tissue and additionally avoids in some case the problem of inhomogeneity of signal intensity.
ER  - 
TY  - JOUR
T1  - State estimation-based control of COVID-19 epidemic before and after vaccine development
A1  - Rajaei, Arman
A1  - Raeiszadeh, Mahsa
A1  - Azimi, Vahid
A1  - Sharifi, Mojtaba
Y1  - 2021///
KW  -  COVID-19 epidemic
KW  -  Extended Kalman filter (EKF)
KW  -  Lyapunov stability
KW  -  Social distancing
KW  -  hospitalization and vaccination
KW  - Nonlinear robust control
JF  - Journal of Process Control
VL  - 102
SP  - 1
EP  - 14
DO  - https://doi.org/10.1016/j.jprocont.2021.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S0959152421000494
N2  - In this study, a nonlinear robust control policy is designed together with a state observer in order to manage the novel coronavirus disease (COVID-19) outbreak having an uncertain epidemiological model with unmeasurable variables. This nonlinear model for the COVID-19 epidemic includes eight state variables (susceptible, exposed, infected, quarantined, hospitalized, recovered, deceased, and insusceptible populations). Two plausible scenarios are put forward in this article to control this epidemic before and after its vaccine invention. In the first scenario, the social distancing and hospitalization rates are employed as two applicable control inputs to diminish the exposed and infected groups. However, in the second scenario after the vaccine development, the vaccination rate is taken into account as the third control input to reduce the susceptible populations, in addition to the two objectives of the first scenario. The proposed feedback control measures are defined in terms of the hospitalized and deceased populations due to the available statistical data, while other unmeasurable compartmental variables are estimated by an extended Kalman filter (EKF). In other words, the susceptible, exposed, infected, quarantined, recovered, and insusceptible individuals cannot be identified precisely because of the asymptomatic infection of COVID-19 in some cases, its incubation period, and the lack of an adequate community screening. Utilizing the Lyapunov theorem, the stability and bounded tracking convergence of the closed-loop epidemiological system are investigated in the presence of modeling uncertainties. Finally, a comprehensive simulation study is conducted based on Canada’s reported cases for two defined timing plans (with different treatment rates). Obtained results demonstrate that the developed EKF-based control scheme can achieve desired epidemic goals (exponential decrease of infected, exposed, and susceptible people).
ER  - 
TY  - JOUR
T1  - A semisupervised learning model based on fuzzy min–max neural networks for data classification
A1  - Pourpanah, Farhad
A1  - Wang, Di
A1  - Wang, Ran
A1  - Lim, Chee Peng
Y1  - 2021///
KW  -  Data classification
KW  -  Fuzzy min–max neural networks
KW  -  Human motion recognition
KW  -  Incremental learning
KW  - Semisupervised learning
JF  - Applied Soft Computing
VL  - 112
SP  - 107856
EP  - 107856
DO  - https://doi.org/10.1016/j.asoc.2021.107856
UR  - https://www.sciencedirect.com/science/article/pii/S156849462100778X
N2  - Semisupervised learning (SSL) models are useful for undertaking classification problems with a small set of labeled samples and a large number of unlabeled samples. In this regard, the family of fuzzy min–max (FMM) neural networks offers the capability of online learning for addressing both unsupervised and supervised problems. As such, this paper proposes a novel two-stage SSL model based on FMM networks, denoted SSL–FMM. The first stage employs the unlabeled samples to generate a number of hyperboxes using the unsupervised FMM algorithm, while the second stage uses the labeled samples to associate the generated hyperboxes with their target classes using the supervised FMM algorithm. In addition, a neighborhood-labeling mechanism based on the Euclidean distance and hyperbox centroids is formulated to associate the unlabeled hyperboxes with the most likely target classes. A number of benchmark problems and a real-world case study are employed to evaluate the effectiveness of the proposed SSL–FMM model. The outcome indicates that SSL–FMM is able to use unlabeled samples effectively and improve the FMM performance, producing promising results compared with other SSL methods in the literature.
ER  - 
TY  - JOUR
T1  - Securing private information by data perturbation using statistical transformation with three dimensional shearing
A1  - Kumar, G Sathish
A1  - Premalatha, K
Y1  - 2021///
KW  -  3-Dimensional Shearing
KW  -  Information Value
KW  -  Min–Max normalization
KW  -  Variance
KW  -  Weight of Evidence
KW  - Privacy Preserving Data Mining
JF  - Applied Soft Computing
VL  - 112
SP  - 107819
EP  - 107819
DO  - https://doi.org/10.1016/j.asoc.2021.107819
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621007407
N2  - Privacy is very important in shared data for the knowledge based applications. However it causes serious privacy concerns, when the sensitive data is stored and moved to other applications. It is vital to incorporate privacy in the sensitive data for the data mining process. While preserving privacy, certain protocols allow the knowledge extraction from the modified data without revealing the original information. In this work, a series of steps like, Weight of Evidence, Information Value, Min–Max normalization and 3D shearing are applied to perturb the quasi-identifiers in the data. The classification techniques such as Decision Tree, Random Forest, Extreme Gradient Boost and Support Vector Machines are employed in adult income, bank marketing and lung cancer datasets to analyze the performance of the original and perturbed data. Accuracy, variance and sensitivity-specificity are being considered as performance measures of the classifiers. This research work is compared with 2D rotation and 3D rotation algorithms. The experimental results clearly show that the proposed work preserves the data utility with higher data transformation capacity and privacy preserving capacity than the existing geometric transformation techniques.
ER  - 
TY  - JOUR
T1  - System identification to analyse changed kinetics of SERCA in intact rat heart
A1  - van Riel, Natal A W
A1  - Ivanics, Tamas
A1  - Ligeti, Laszlo
A1  - van der Vusse, Ger J
Y1  - 2003///
KW  -  identifiability
KW  -  least-squares identification
KW  -  model reduction
KW  -  parameter estimation
KW  -  physiological models
KW  - dynamic models
JF  - IFAC Proceedings Volumes
VL  - 36
IS  - 15
SP  - 123
EP  - 128
DO  - https://doi.org/10.1016/S1474-6670(17)33486-9
UR  - https://www.sciencedirect.com/science/article/pii/S1474667017334869
N1  - 5th IFAC Symposium on Modelling and Control in Biomedical Systems 2003, Melbourne, Australia, 21-23 August 2003
N2  - A mechanistic-based model has been derived of calcium handling in the intact heart. This model incorporates the quantitatively most important processes involved in beat-to-beat calcium homeostasis. Based on a priori physiological information the model has been reduced to yield (kinetic) parameters that could be estimated using time-series data of the free calcium concentration in the sarcoplasma. Observations of the dynamics of the overall system were translated into the underlying mechanisms. Experiments in which the most important calcium extrusion pump (Sarcoplasmic Reticulum Ca2+- ATPase, SERCA) was disturbed have been successfully analysed and interpreted using model and identification.
ER  - 
TY  - JOUR
T1  - Performance analysis of descriptive statistical features in retinal vessel segmentation via fuzzy logic, ANN, SVM, and classifier fusion
A1  - Barkana, Buket D
A1  - Saricicek, Inci
A1  - Yildirim, Burak
Y1  - 2017///
KW  -  ANN
KW  -  Classification
KW  -  Classifier fusion
KW  -  Fuzzy logic
KW  -  SVM
KW  -  Statistical features
KW  - Retinal vessel segmentation
JF  - Knowledge-Based Systems
VL  - 118
SP  - 165
EP  - 176
DO  - https://doi.org/10.1016/j.knosys.2016.11.022
UR  - https://www.sciencedirect.com/science/article/pii/S0950705116304786
N2  - Diabetic retinopathy is the most common diabetic eye disease and a leading cause of blindness in the world. Diagnosis of diabetic retinopathy at an early stage can be done through the segmentation of blood vessels of the retina. In this work, the performance of descriptive statistical features in retinal vessel segmentation is evaluated by using fuzzy logic, an artificial neural network classifier (ANN), a support vector machine (SVM), and classifier fusion. Newly constructed eight features are formed by statistical moments. Mean and median measurements of image pixels’ intensity values in four directions, horizontal, vertical, up-diagonal, and down-diagonal, are calculated. Features, F1, F2, F3, and F4 are calculated as the mean values and F5, F6, F7, and F8 are calculated as the median values of a processed pixel in each direction. A fuzzy rule-based classifier, an ANN, a SVM, and a classifier fusion are designed. The publicly available DRIVE and STARE databases are used for evaluation. The fuzzy classifier achieved 93.82% of an overall accuracy, 72.28% of sensitivity, and 97.04% of specificity. For the ANN classifier, 94.2% of overall accuracy, 67.7% of sensitivity, and 98.1% of specificity are achieved on the DRIVE database. For the STARE database, the fuzzy classifier achieved 92.4% of overall accuracy, 75% of sensitivity, and 94.3% of specificity. The ANN classifier achieved the overall accuracy, sensitivity, and specificity as 94.2%, 56.9%, and 98.4%, respectively. Although the overall accuracy of the SVM is calculated lower than the fuzzy and the ANN classifiers, it achieved higher sensitivity rates. Designed classifier fusion achieved the best performance among all by using the proposed statistical features. Its overall accuracy, sensitivity, and specificity are calculated as 95.10%, 74.09%, 98.35% for the DRIVE and 95.53%, 70.14%, 98.46 for the STARE database, respectively. The experimental results validate that the descriptive statistical features can be employed in retinal vessel segmentation and can be used in rule-based and supervised classifiers.
ER  - 
TY  - JOUR
T1  - Non-invasive estimate of blood glucose and blood pressure from a photoplethysmograph by means of machine learning techniques
A1  - Monte-Moreno, Enric
Y1  - 2011///
KW  - Blood glucose estimate
KW  - Blood pressure estimate
KW  - Machine learning
KW  - Noninvasive measurement
KW  - Photoplethysmography
JF  - Artificial Intelligence in Medicine
VL  - 53
IS  - 2
SP  - 127
EP  - 138
DO  - https://doi.org/10.1016/j.artmed.2011.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S093336571100056X
N2  - Objective
This work presents a system for a simultaneous non-invasive estimate of the blood glucose level (BGL) and the systolic (SBP) and diastolic (DBP) blood pressure, using a photoplethysmograph (PPG) and machine learning techniques. The method is independent of the person whose values are being measured and does not need calibration over time or subjects.
Methodology
The architecture of the system consists of a photoplethysmograph sensor, an activity detection module, a signal processing module that extracts features from the PPG waveform, and a machine learning algorithm that estimates the SBP, DBP and BGL values. The idea that underlies the system is that there is functional relationship between the shape of the PPG waveform and the blood pressure and glucose levels.
Results
As described in this paper we tested this method on 410 individuals without performing any personalized calibration. The results were computed after cross validation. The machine learning techniques tested were: ridge linear regression, a multilayer perceptron neural network, support vector machines and random forests. The best results were obtained with the random forest technique. In the case of blood pressure, the resulting coefficients of determination for reference vs. prediction were RSBP2=0.91, RDBP2=0.89, and RBGL2=0.90. For the glucose estimation, distribution of the points on a Clarke error grid placed 87.7% of points in zone A, 10.3% in zone B, and 1.9% in zone D. Blood pressure values complied with the grade B protocol of the British Hypertension society.
Conclusion
An effective system for estimate of blood glucose and blood pressure from a photoplethysmograph is presented. The main advantage of the system is that for clinical use it complies with the grade B protocol of the British Hypertension society for the blood pressure and only in 1.9% of the cases did not detect hypoglycemia or hyperglycemia.
ER  - 
TY  - JOUR
T1  - LODE: A distance-based classifier built on ensembles of positive and negative observations
A1  - Meo, Rosa
A1  - Bachar, Dipankar
A1  - Ienco, Dino
Y1  - 2012///
KW  -  Associative classifier
KW  -  Concept learning
KW  -  Supervised learning
KW  - Data mining
JF  - Pattern Recognition
VL  - 45
IS  - 4
SP  - 1409
EP  - 1425
DO  - https://doi.org/10.1016/j.patcog.2011.10.015
UR  - https://www.sciencedirect.com/science/article/pii/S0031320311004390
N2  - Current work on assembling a set of local patterns such as rules and class association rules into a global model for the prediction of a target usually focuses on the identification of the minimal set of patterns that cover the training data. In this paper we present a different point of view: the model of a class has been built with the purpose to emphasize the typical features of the examples of the class. Typical features are modeled by frequent itemsets extracted from the examples and constitute a new representation space of the examples of the class. Prediction of the target class of test examples occurs by computation of the distance between the vector representing the example in the space of the itemsets of each class and the vectors representing the classes. It is interesting to observe that in the distance computation the critical contribution to the discrimination between classes is given not only by the itemsets of the class model that match the example but also by itemsets that do not match the example. These absent features constitute some pieces of information on the examples that can be considered for the prediction and should not be disregarded. Second, absent features are more abundant in the wrong classes than in the correct ones and their number increases the distance between the example vector and the negative class vectors. Furthermore, since absent features are frequent features in their respective classes, they make the prediction more robust against over-fitting and noise. The usage of features absent in the test example is a novel issue in classification: existing learners usually tend to select the best local pattern that matches the example and do not consider the abundance of other patterns that do not match it. We demonstrate the validity of our observations and the effectiveness of LODE, our learner, by means of extensive empirical experiments in which we compare the prediction accuracy of LODE with a consistent set of classifiers of the state of the art. In this paper we also report the methodology that we adopted in order to determine automatically the setting of the learner and of its parameters.
ER  - 
TY  - JOUR
T1  - A multiobjective optimization-based sparse extreme learning machine algorithm
A1  - Wu, Yu
A1  - Zhang, Yongshan
A1  - Liu, Xiaobo
A1  - Cai, Zhihua
A1  - Cai, Yaoming
Y1  - 2018///
KW  -  Multiobjective optimization
KW  -  Parameter optimization
KW  -  Sparse connecting structure
KW  -  Structure learning
KW  - Extreme learning machine
JF  - Neurocomputing
VL  - 317
SP  - 88
EP  - 100
DO  - https://doi.org/10.1016/j.neucom.2018.07.060
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218308956
N2  - Extreme Learning Machine (ELM) is a popular machine learning method and has been widely applied to real-world problems due to its fast training speed and good generalization performance. However, in ELM, the randomly assigned input weights and hidden biases usually degrade the generalization performance. Furthermore, ELM is considered as an empirical risk minimization model and easily leads to overfitting when dataset exists some outliers. In this paper, we proposed a novel algorithm named Multiobjective Optimization-based Sparse Extreme Learning Machine (MO-SELM), where parameter optimization and structure learning are integrated into the learning process to simultaneously enhance the generalization performance and alleviate the overfitting problem. In MO-SELM, the training error and the connecting sparsity are taken as two conflicting objectives of the multiobjective model, which aims to find sparse connecting structures with optimal weights and biases. Then, a hybrid encoding-based MOEA/D is used to optimize the multiobjective model. In addition, ensemble learning is embedded into this algorithm to make decisions after multiobjective optimization. Experimental results of several classification and regression applications demonstrate the effectiveness of the proposed MO-SELM.
ER  - 
TY  - JOUR
T1  - An iterative algorithm learning the maximal margin classifier
A1  - Franc, Vojtěch
A1  - Hlaváč, Václav
Y1  - 2003///
KW  -  Kernel functions
KW  -  Linear classifier
KW  -  Supervised learning
KW  -  Support vector machines
KW  - Pattern recognition
JF  - Pattern Recognition
VL  - 36
IS  - 9
SP  - 1985
EP  - 1996
DO  - https://doi.org/10.1016/S0031-3203(03)00060-8
UR  - https://www.sciencedirect.com/science/article/pii/S0031320303000608
N1  - Kernel and Subspace Methods for Computer Vision
N2  - A simple learning algorithm for maximal margin classifiers (also support vector machines with quadratic cost function) is proposed. We build our iterative algorithm on top of the Schlesinger–Kozinec algorithm (S–K-algorithm) from 1981 which finds a maximal margin hyperplane with a given precision for separable data. We suggest a generalization of the S–K-algorithm (i) to the non-linear case using kernel functions and (ii) for non-separable data. The requirement in memory storage is linear to the data. This property allows the proposed algorithm to be used for large training problems. The resulting algorithm is simple to implement and as the experiments showed competitive to the state-of-the-art algorithms. The implementation of the algorithm in Matlab is available. We tested the algorithm on the problem aiming at recognition poor quality numerals.
ER  - 
TY  - JOUR
T1  - RIB: A Robust Itemset-based Bayesian approach to classification
A1  - Baralis, Elena
A1  - Cagliero, Luca
Y1  - 2014///
KW  -  Bayesian modeling
KW  -  Classification
KW  -  Frequent itemset mining
KW  -  Noisy data
KW  - Data mining
JF  - Knowledge-Based Systems
VL  - 71
SP  - 366
EP  - 375
DO  - https://doi.org/10.1016/j.knosys.2014.08.015
UR  - https://www.sciencedirect.com/science/article/pii/S0950705114003037
N2  - Real-life data is often affected by noise. To cope with this issue, classification techniques robust to noisy data are needed. Bayesian approaches are known to be fairly robust to noise. However, to compute probability estimates state-of-the-art Bayesian approaches adopt a lazy pattern-based strategy, which shows some limitations when coping data affected by a notable amount of noise. This paper proposes RIB (Robust Itemset-based Bayesian classifier), a novel eager and pattern-based Bayesian classifier which discovers frequent itemsets from training data and exploits them to build accurate probability estimates. Enforcing a minimum frequency of occurrence on the considered itemsets reduces the sensitivity of the probability estimates to noise. Furthermore, learning a Bayesian Network that also considers high-order dependences among data usually neglected by traditional Bayesian approaches appears to be more robust to noise and data overfitting than selecting a small subset of patterns tailored to each test instance. The experiments demonstrate that RIB is, on average, more accurate than most state-of-the-art classifiers, Bayesian and not, on benchmark datasets in which different kinds and levels of noise are injected. Furthermore, its performance on the same datasets prior to noise injection is competitive with that of state-of-the-art classifiers.
ER  - 
TY  - JOUR
T1  - Minimal Learning Machine: A novel supervised distance-based approach for regression and classification
A1  - de Souza, Amauri Holanda
A1  - Corona, Francesco
A1  - Barreto, Guilherme A
A1  - Miche, Yoan
A1  - Lendasse, Amaury
Y1  - 2015///
KW  -  Pattern classification
KW  -  Regression
KW  -  Supervised learning
KW  - Learning machines
JF  - Neurocomputing
VL  - 164
SP  - 34
EP  - 44
DO  - https://doi.org/10.1016/j.neucom.2014.11.073
UR  - https://www.sciencedirect.com/science/article/pii/S0925231215003021
N2  - In this work, a novel supervised learning method, the Minimal Learning Machine (MLM), is proposed. Learning in MLM consists in building a linear mapping between input and output distance matrices. In the generalization phase, the learned distance map is used to provide an estimate of the distance from K output reference points to the unknown target output value. Then, the output estimation is formulated as multilateration problem based on the predicted output distance and the locations of the reference points. Given its general formulation, the Minimal Learning Machine is inherently capable of operating on nonlinear regression problems as well as on multidimensional response spaces. In addition, an intuitive extension of the MLM is proposed to deal with classification problems. A comprehensive set of computer experiments illustrates that the proposed method achieves accuracies that are comparable to more traditional machine learning methods for regression and classification thus offering a computationally valid alternative to such approaches.
ER  - 
TY  - JOUR
T1  - A BA-based algorithm for parameter optimization of Support Vector Machine
A1  - Tharwat, Alaa
A1  - Hassanien, Aboul Ella
A1  - Elnaghi, Basem E
Y1  - 2017///
KW  -  Bat algorithm (BA)
KW  -  Classification
KW  -  Parameter optimization
KW  -  Support Vector Machine (SVM)
KW  -  Swarm intelligent
KW  - Optimization algorithms
JF  - Pattern Recognition Letters
VL  - 93
SP  - 13
EP  - 22
DO  - https://doi.org/10.1016/j.patrec.2016.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S0167865516302720
N1  - Pattern Recognition Techniques in Data Mining
N2  - Support Vector Machine (SVM) parameters such as kernel parameter and penalty parameter (C) have a great impact on the complexity and accuracy of predicting model. In this paper, Bat algorithm (BA) has been proposed to optimize the parameters of SVM, so that the classification error can be reduced. To evaluate the proposed model (BA-SVM), the experiment adopted nine standard datasets which are obtained from UCI machine learning data repository. For verification, the results of the BA-SVM algorithm are compared with grid search, which is a conventional method of searching parameter values, and two well-known optimization algorithms: Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). The experimental results proved that the proposed model is capable to find the optimal values of the SVM parameters and avoids the local optima problem. The results also demonstrated lower classification error rates compared with PSO and GA algorithms.
ER  - 
TY  - JOUR
T1  - Natural Language Processing methods and systems for biomedical ontology learning
A1  - Liu, Kaihong
A1  - Hogan, William R
A1  - Crowley, Rebecca S
Y1  - 2011///
KW  -  Information extraction
KW  -  Natural Language Processing
KW  -  Ontology enrichment
KW  -  Ontology learning from text
KW  - Ontology
JF  - Journal of Biomedical Informatics
VL  - 44
IS  - 1
SP  - 163
EP  - 179
DO  - https://doi.org/10.1016/j.jbi.2010.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S153204641000105X
N1  - Ontologies for Clinical and Translational Research
N2  - While the biomedical informatics community widely acknowledges the utility of domain ontologies, there remain many barriers to their effective use. One important requirement of domain ontologies is that they must achieve a high degree of coverage of the domain concepts and concept relationships. However, the development of these ontologies is typically a manual, time-consuming, and often error-prone process. Limited resources result in missing concepts and relationships as well as difficulty in updating the ontology as knowledge changes. Methodologies developed in the fields of Natural Language Processing, information extraction, information retrieval and machine learning provide techniques for automating the enrichment of an ontology from free-text documents. In this article, we review existing methodologies and developed systems, and discuss how existing methods can benefit the development of biomedical ontologies.
ER  - 
TY  - JOUR
T1  - Quantum squirrel inspired algorithm for gene selection in methylation and expression data of prostate cancer
A1  - Ghosh, Manosij
A1  - Sen, Sagnik
A1  - Sarkar, Ram
A1  - Maulik, Ujjwal
Y1  - 2021///
KW  -  Feature Selection
KW  -  Methylation data
KW  -  Prostate cancer
KW  -  Quantum mechanics
KW  -  Squirrel Search Algorithm
KW  - Expression data
JF  - Applied Soft Computing
VL  - 105
SP  - 107221
EP  - 107221
DO  - https://doi.org/10.1016/j.asoc.2021.107221
UR  - https://www.sciencedirect.com/science/article/pii/S1568494621001447
N2  - Prostate cancer is the second most common type of cancer among men after skin cancer In this work, we present a comprehensive view on genomic and epigenomic changes following the incremental biological functionality. For gene selection, a new Feature Selection algorithm called Quantum Squirrel inspired Feature Selection is proposed here. While exploring the feature space, the proposed algorithm exploits the benefits of Squirrel Search Algorithm (a recently proposed swarm intelligence algorithm) along with Quantum mechanics. Moreover, a modified version of the end of winter concept is used to achieve effective dimension reduction capacity. Quantum Squirrel inspired Feature Selection is executed on both expression and methylation data of prostate cancer. The major challenge in gene selection is to bring down the number of selected features without compromising on accuracy. The proposed algorithm consistently achieves this goal and outperforms other state-of-the-art algorithms. The proposed algorithm has steadily attained 100% accuracy while selecting a much lower number of features (around 4), which is a major improvement over others. The top selected genes are biologically validated in terms of Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway and Gene Ontologies (GO), which further demonstrates the usefulness of the proposed method. The genes selected by Quantum Squirrel inspired Feature Selection show an association with prostate carcinoma and most are known biomarkers. A few novel biomarkers selected by proposed algorithm have also been detailed in this work. Source code of this work is available at: Quantum Squirrel inspired Feature Selection.
ER  - 
TY  - JOUR
T1  - Differential Evolution for learning the classification method PROAFTN
A1  - Al-Obeidat, Feras
A1  - Belacel, Nabil
A1  - Carretero, Juan A
A1  - Mahanti, Prabhat
Y1  - 2010///
KW  -  Differential Evolution
KW  -  Multiple criteria classification
KW  -  PROAFTN
KW  -  Supervised learning
KW  - Knowledge discovery
JF  - Knowledge-Based Systems
VL  - 23
IS  - 5
SP  - 418
EP  - 426
DO  - https://doi.org/10.1016/j.knosys.2010.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S0950705110000250
N2  - This paper introduces a new learning technique for the multicriteria classification method PROAFTN. This new technique, called DEPRO, utilizes a Differential Evolution (DE) algorithm for learning and optimizing the output of the classification method PROAFTN. The limitation of the PROAFTN method is largely due to the set of parameters (e.g., intervals and weights) required to be obtained to perform the classification procedure. Therefore, a learning method is needed to induce and extract these parameters from data. DE is an efficient metaheuristic optimization algorithm based on a simple mathematical structure to mimic a complex process of evolution. Some of the advantages of DE over other global optimization methods are that it often converges faster and with more certainty than many other methods and it uses fewer control parameters. In this work, the DE algorithm is proposed to inductively obtain PROAFTN’s parameters from data to achieve a high classification accuracy. Based on results generated from 12 public datasets, DEPRO provides excellent results, outperforming the most common classification algorithms.
ER  - 
TY  - JOUR
T1  - Community detection in node-attributed social networks: A survey
A1  - Chunaev, Petr
Y1  - 2020///
KW  -  clusterization
KW  -  complex network
KW  -  node-attributed graph
KW  -  social network
KW  - community detection
JF  - Computer Science Review
VL  - 37
SP  - 100286
EP  - 100286
DO  - https://doi.org/10.1016/j.cosrev.2020.100286
UR  - https://www.sciencedirect.com/science/article/pii/S1574013720303865
N2  - Community detection is a fundamental problem in social network analysis consisting, roughly speaking, in unsupervised dividing social actors (modeled as nodes in a social graph) with certain social connections (modeled as edges in the social graph) into densely knitted and highly related groups with each group well separated from the others. Classical approaches for community detection usually deal only with the structure of the network and ignore features of the nodes (traditionally called node attributes), although the majority of real-world social networks provide additional actors’ information such as age, gender, interests, etc. It is believed that the attributes may clarify and enrich the knowledge about the actors and give sense to the detected communities. This belief has motivated the progress in developing community detection methods that use both the structure and the attributes of the network (modeled already via a node-attributed graph) to yield more informative and qualitative community detection results. During the last decade many such methods based on different ideas and techniques have appeared. Although there exist partial overviews of them, a recent survey is a necessity as the growing number of the methods may cause repetitions in methodology and uncertainty in practice. In this paper we aim at describing and clarifying the overall situation in the field of community detection in node-attributed social networks. Namely, we perform an exhaustive search of known methods and propose a classification of them based on when and how the structure and the attributes are fused. We not only give a description of each class but also provide general technical ideas behind each method in the class. Furthermore, we pay attention to available information which methods outperform others and which datasets and quality measures are used for their performance evaluation. Basing on the information collected, we make conclusions on the current state of the field and disclose several problems that seem important to be resolved in future.
ER  - 
TY  - JOUR
T1  - Memetic Extreme Learning Machine
A1  - Zhang, Yongshan
A1  - Wu, Jia
A1  - Cai, Zhihua
A1  - Zhang, Peng
A1  - Chen, Ling
Y1  - 2016///
KW  -  Classification
KW  -  Evolutionary Machine Learning
KW  -  Memetic Algorithm
KW  -  Self-adaptive
KW  - Extreme Learning Machine
JF  - Pattern Recognition
VL  - 58
SP  - 135
EP  - 148
DO  - https://doi.org/10.1016/j.patcog.2016.04.003
UR  - https://www.sciencedirect.com/science/article/pii/S003132031630036X
N2  - Extreme Learning Machine (ELM) is a promising model for training single-hidden layer feedforward networks (SLFNs) and has been widely used for classification. However, ELM faces the challenge of arbitrarily selected parameters, e.g., the network weights and hidden biases. Therefore, many efforts have been made to enhance the performance of ELM, such as using evolutionary algorithms to explore promising areas of the solution space. Although evolutionary algorithms can explore promising areas of the solution space, they are not able to locate global optimum efficiently. In this paper, we present a new Memetic Algorithm (MA)-based Extreme Learning Machine (M-ELM for short). M-ELM embeds the local search strategy into the global optimization framework to obtain optimal network parameters. Experiments and comparisons on 46 UCI data sets validate the performance of M-ELM. The corresponding results demonstrate that M-ELM significantly outperforms state-of-the-art ELM algorithms.
ER  - 
TY  - JOUR
T1  - Towards evaluating the robustness of deep diagnostic models by adversarial attack
A1  - Xu, Mengting
A1  - Zhang, Tao
A1  - Li, Zhongnian
A1  - Liu, Mingxia
A1  - Zhang, Daoqiang
Y1  - 2021///
KW  -  Adversarial attack
KW  -  Defense
KW  -  Robustness
KW  - Deep diagnostic models
JF  - Medical Image Analysis
VL  - 69
SP  - 101977
EP  - 101977
DO  - https://doi.org/10.1016/j.media.2021.101977
UR  - https://www.sciencedirect.com/science/article/pii/S1361841521000232
N2  - Deep learning models (with neural networks) have been widely used in challenging tasks such as computer-aided disease diagnosis based on medical images. Recent studies have shown deep diagnostic models may not be robust in the inference process and may pose severe security concerns in clinical practice. Among all the factors that make the model not robust, the most serious one is adversarial examples. The so-called “adversarial example” is a well-designed perturbation that is not easily perceived by humans but results in a false output of deep diagnostic models with high confidence. In this paper, we evaluate the robustness of deep diagnostic models by adversarial attack. Specifically, we have performed two types of adversarial attacks to three deep diagnostic models in both single-label and multi-label classification tasks, and found that these models are not reliable when attacked by adversarial example. We have further explored how adversarial examples attack the models, by analyzing their quantitative classification results, intermediate features, discriminability of features and correlation of estimated labels for both original/clean images and those adversarial ones. We have also designed two new defense methods to handle adversarial examples in deep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT) and Misclassification-Aware Adversarial Training (MAAdvT). The experimental results have shown that the use of defense methods can significantly improve the robustness of deep diagnostic models against adversarial attacks.
ER  - 
TY  - JOUR
T1  - SNOMED CT module-driven clinical archetype management
A1  - Allones, J L
A1  - Taboada, M
A1  - Martinez, D
A1  - Lozano, R
A1  - Sobrido, M J
Y1  - 2013///
KW  -  Clinical archetypes
KW  -  Modularization
KW  -  SNOMED CT
KW  -  Semantic interoperability
KW  -  Semantic search
KW  - openEHR
JF  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 388
EP  - 400
DO  - https://doi.org/10.1016/j.jbi.2013.01.003
UR  - https://www.sciencedirect.com/science/article/pii/S1532046413000051
N2  - Objective
To explore semantic search to improve management and user navigation in clinical archetype repositories.
Methods
In order to support semantic searches across archetypes, an automated method based on SNOMED CT modularization is implemented to transform clinical archetypes into SNOMED CT extracts. Concurrently, query terms are converted into SNOMED CT concepts using the search engine Lucene. Retrieval is then carried out by matching query concepts with the corresponding SNOMED CT segments.
Results
A test collection of the 16 clinical archetypes, including over 250 terms, and a subset of 55 clinical terms from two medical dictionaries, MediLexicon and MedlinePlus, were used to test our method. The keyword-based service supported by the OpenEHR repository offered us a benchmark to evaluate the enhancement of performance. In total, our approach reached 97.4% precision and 69.1% recall, providing a substantial improvement of recall (more than 70%) compared to the benchmark.
Conclusions
Exploiting medical domain knowledge from ontologies such as SNOMED CT may overcome some limitations of the keyword-based systems and thus improve the search experience of repository users. An automated approach based on ontology segmentation is an efficient and feasible way for supporting modeling, management and user navigation in clinical archetype repositories.
ER  - 
TY  - JOUR
T1  - Rule-based OneClass-DS learning algorithm
A1  - Nguyen, Dat T
A1  - Cios, Krzysztof J
Y1  - 2015///
KW  -  Anomaly detection
KW  -  Novelty detection
KW  -  Outlier detection
KW  - One-class learning algorithm: OneClass-DS
JF  - Applied Soft Computing
VL  - 35
SP  - 267
EP  - 279
DO  - https://doi.org/10.1016/j.asoc.2015.05.043
UR  - https://www.sciencedirect.com/science/article/pii/S1568494615003889
N2  - One-class learning algorithms are used in situations when training data are available only for one class, called target class. Data for other class(es), called outliers, are not available. One-class learning algorithms are used for detecting outliers, or novelty, in the data. The common approach in one-class learning is to use density estimation techniques or adapt standard classification algorithms to define a decision boundary that encompasses only the target data. In this paper, we introduce OneClass-DS learning algorithm that combines rule-based classification with greedy search algorithm based on density of features. Its performance is tested on 25 data sets and compared with eight other one-class algorithms; the results show that it performs on par with those algorithms.
ER  - 
TY  - JOUR
T1  - SMOTE-NaN-DE: Addressing the noisy and borderline examples problem in imbalanced classification by natural neighbors and differential evolution
A1  - Li, Junnan
A1  - Zhu, Qingsheng
A1  - Wu, Quanwang
A1  - Zhang, Zhiyong
A1  - Gong, Yanlu
A1  - He, Ziqing
A1  - Zhu, Fan
Y1  - 2021///
KW  -  Class-imbalance classification
KW  -  Differential evolution
KW  -  Natural neighbors
KW  -  Oversampling
KW  - Class-imbalance learning
JF  - Knowledge-Based Systems
VL  - 223
SP  - 107056
EP  - 107056
DO  - https://doi.org/10.1016/j.knosys.2021.107056
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121003191
N2  - Learning a classifier from class-imbalance data is an important challenge. Among existing solutions, SMOTE is one of the most successful methods and has an extensive range of practical applications. The performance of SMOTE and its extensions usually degrades owing to noisy and borderline examples. Filtering-based methods have been developed to address this problem but still have the following technical defects: (a) Error detection techniques heavily rely on parameter settings; (b) Examples detected by error detection techniques are directly eliminated, leading to deviation of obtained decision boundary and class imbalance again. To advance the state of the art, a novel filtering-based oversampling method called SMOTE-NaN-DE is proposed in this paper. In SMOTE-NaN-DE, a SMOTE-based method is first used to generate synthetic samples and improve original class-imbalance data. Secondly, an error detection technique based on natural neighbors is used to detect noisy and borderline examples. Thirdly, the differential evolution (DE) is used to optimize and change iteratively the position (attributes) of found examples instead of eliminating them. The main advantages of SMOTE-NaN-DE are that (a) It can improve almost all of SMOTE-based methods in terms of the noise problem; (b) Error detection technique is parameter-free; (c) Examples found by error detection technique are optimized by the differential evolution rather than removed, which keeps imbalance ratio and improve the boundary; (d) It is more suitable for data sets with more noise (especially class noise). The effectiveness of the proposed SMOTE-NaN-DE is validated by intensive comparison experiments on artificial and real data sets.
ER  - 
TY  - JOUR
T1  - Classification of Heart Disease Using K- Nearest Neighbor and Genetic Algorithm
A1  - jabbar, M Akhil
A1  - Deekshatulu, B L
A1  - Chandra, Priti
Y1  - 2013///
KW  - Andhra pradesh
KW  - Data mining
KW  - Genetic algorithm
KW  - Heart disease
KW  - KNN algorithm
JF  - Procedia Technology
VL  - 10
SP  - 85
EP  - 94
DO  - https://doi.org/10.1016/j.protcy.2013.12.340
UR  - https://www.sciencedirect.com/science/article/pii/S2212017313004945
N1  - First International Conference on Computational Intelligence: Modeling Techniques and Applications (CIMTA) 2013
N2  - Data mining techniques have been widely used to mine knowledgeable information from medical data bases. In data mining classification is a supervised learning that can be used to design models describing important data classes, where class attribute is involved in the construction of the classifier. Nearest neighbor (KNN) is very simple, most popular, highly efficient and effective algorithm for pattern recognition.KNN is a straight forward classifier, where samples are classified based on the class of their nearest neighbor. Medical data bases are high volume in nature. If the data set contains redundant and irrelevant attributes, classification may produce less accurate result. Heart disease is the leading cause of death in INDIA. In Andhra Pradesh heart disease was the leading cause of mortality accounting for 32%of all deaths, a rate as high as Canada (35%) and USA.Hence there is a need to define a decision support system that helps clinicians decide to take precautionary steps. In this paper we propose a new algorithm which combines KNN with genetic algorithm for effective classification. Genetic algorithms perform global search in complex large and multimodal landscapes and provide optimal solution. Experimental results shows that our algorithm enhance the accuracy in diagnosis of heart disease.
ER  - 
TY  - JOUR
T1  - A self-calibrating approach for the segmentation of retinal vessels by template matching and contour reconstruction
A1  - Kovács, György
A1  - Hajdu, András
Y1  - 2016///
KW  -  Contour reconstruction
KW  -  Gabor filter
KW  -  Self-calibration
KW  -  Vessel segmentation
KW  - Retinal image analysis
JF  - Medical Image Analysis
VL  - 29
SP  - 24
EP  - 46
DO  - https://doi.org/10.1016/j.media.2015.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S136184151500184X
N2  - The automated processing of retinal images is a widely researched area in medical image analysis. Screening systems based on the automated and accurate recognition of retinopathies enable the earlier diagnosis of diseases like diabetic retinopathy, hypertension and their complications. The segmentation of the vascular system is a crucial task in the field: on the one hand, the accurate extraction of the vessel pixels aids the detection of other anatomical parts (like the optic disc Hoover and Goldbaum, 2003) and lesions (like microaneurysms Sopharak et al., 2013); on the other hand, the geometrical features of the vascular system and their temporal changes are shown to be related to diseases, like the vessel tortuosity to Fabry disease Sodi et al., 2013 and the arteriolar-to-venus (A/V) ratio to hypertension (Pakter et al., 2005). In this study, a novel technique based on template matching and contour reconstruction is proposed for the segmentation of the vasculature. In the template matching step generalized Gabor function based templates are used to extract the center lines of vessels. Then, the intensity characteristics of vessel contours measured in training databases are reconstructed. The method was trained and tested on two publicly available databases, DRIVE and STARE; and reached an average accuracy of 0.9494 and 0.9610, respectively. We have also carried out cross-database tests and found that the accuracy scores are higher than that of any previous technique trained and tested on the same database.
ER  - 
TY  - JOUR
T1  - Real-time activity monitoring with a wristband and a smartphone
A1  - Cvetković, Božidara
A1  - Szeklicki, Robert
A1  - Janko, Vito
A1  - Lutomski, Przemyslaw
A1  - Luštrek, Mitja
Y1  - 2018///
KW  -  Activity recognition
KW  -  Estimation of energy expenditure
KW  -  Machine learning
KW  -  Smartphone sensors
KW  - Wristband sensors
JF  - Information Fusion
VL  - 43
SP  - 77
EP  - 93
DO  - https://doi.org/10.1016/j.inffus.2017.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S1566253517303421
N2  - Activity monitoring is a very important task in lifestyle and health domains where physical activity of a person plays an important role in further reasoning or for providing personalized recommendations. To make such services available to a broader population, one should use devices that most users already have, such as smartphones. Since trends show an increasing popularity of wrist-worn wearables we also consider a sensor-rich wristband as an optional device in this research. We present a real-time activity monitoring algorithm which utilizes data from the smartphone sensors, wristband sensors or their fusion for activity recognition and estimation of energy expenditure of the user. The algorithm detects which devices are present and uses an interval of walking for gravity detection and normalization of the orientation of the devices. The normalized data is afterwards used for the detection of the location of the smartphone on the body, which serves as a context for the selection of location-specific classification model for activity recognition. The recognized activity is finally used for the selection of one or multiple regression models for the estimation of the user’s energy expenditure. To develop the machine-learning models, which can be deployed on the smartphone, we optimized the number and type of extracted features via automatic feature selection. We evaluated each step of the algorithm and each device configuration, and compared the human energy expenditure estimation results against the Bodymedia armband and Microsoft Band 2. We also evaluated the benefit of decision fusion where appropriate. The results show that we achieve a 87% ± 5% average accuracy for activity recognition and that we outperformed both competing devices in the estimation of human energy expenditure by achieving the mean absolute error of 0.6 ± 0.1 MET on average.
ER  - 
TY  - JOUR
T1  - Novel gridded descriptors of poincaré plot for analyzing heartbeat interval time-series
A1  - Yan, Chang
A1  - Li, Peng
A1  - Liu, Changchun
A1  - Wang, Xinpei
A1  - Yin, Chunyan
A1  - Yao, Lianke
Y1  - 2019///
KW  -  Coronary artery disease
KW  -  Gridded distribution entropy
KW  -  Gridded distribution rate
KW  -  Short-term time-series
KW  - Poincaré plot
JF  - Computers in Biology and Medicine
VL  - 109
SP  - 280
EP  - 289
DO  - https://doi.org/10.1016/j.compbiomed.2019.04.015
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519301246
N2  - A Poincaré plot is a return map that geometrically elucidates the progression of a time-series. It has frequently been used in heart rate variability analyses. However, algorithms for dedicatedly dissecting the shape of this geometrical plot are yet to be established. In this study, we proposed a gridded Poincaré plot by coarse-graining the original graph and using the newly proposed one, defined two novel measures, namely gridded distribution rate (GDR) and gridded distribution entropy (GDE). The GDR essentially represents the percentage of grids with points, while the GDE estimates the Shannon entropy of the grid weight; that is, the number of points in each grid. The performances of the two measures were examined using both theoretical data with known dynamics and experimental short-term RR interval time-series, and they were compared with several existing metrics. Simulation tests demonstrated that both the GDR and GDE could distinguish among different dynamics, while all the compared methods failed. The experimental results further indicated the ability of the GDR and GDE to differentiate healthy young people from healthy aged adults as well as distinguish healthy subjects from patients with coronary artery disease. Our results suggest that the proposed GDR and GDE may better characterize the Poincaré plot in terms of differentiating between varying dynamical regimes, and between human physiological or pathological conditions. Further studies are warranted to establish their feasibility in evaluating cardiovascular functions in clinical practice.
ER  - 
TY  - JOUR
T1  - Counting trees in Random Forests: Predicting symptom severity in psychiatric intake reports
A1  - Scheurwegs, Elyne
A1  - Sushil, Madhumita
A1  - Tulkens, Stéphan
A1  - Daelemans, Walter
A1  - Luyckx, Kim
Y1  - 2017///
KW  -  Concept detection
KW  -  Natural language processing
KW  -  Positive valence
KW  -  Psychiatry
KW  - Symptom severity identification
JF  - Journal of Biomedical Informatics
VL  - 75
SP  - S112
EP  - S119
DO  - https://doi.org/10.1016/j.jbi.2017.06.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417301302
N1  - Supplement: A Natural Language Processing Challenge for Clinical Records: Research Domains Criteria (RDoC) for Psychiatry
N2  - The CEGS N-GRID 2016 Shared Task (Filannino et al., 2017) in Clinical Natural Language Processing introduces the assignment of a severity score to a psychiatric symptom, based on a psychiatric intake report. We present a method that employs the inherent interview-like structure of the report to extract relevant information from the report and generate a representation. The representation consists of a restricted set of psychiatric concepts (and the context they occur in), identified using medical concepts defined in UMLS that are directly related to the psychiatric diagnoses present in the Diagnostic and Statistical Manual of Mental Disorders, 4th Edition (DSM-IV) ontology. Random Forests provides a generalization of the extracted, case-specific features in our representation. The best variant presented here scored an inverse mean absolute error (MAE) of 80.64%. A concise concept-based representation, paired with identification of concept certainty and scope (family, patient), shows a robust performance on the task.
ER  - 
TY  - JOUR
T1  - Learning simultaneous adaptive clustering and classification via MOEA
A1  - Luo, Juanjuan
A1  - Jiao, Licheng
A1  - Shang, Ronghua
A1  - Liu, Fang
Y1  - 2016///
KW  -  Classification learning
KW  -  Clustering learning
KW  -  Evolutionary algorithm
KW  -  Image segmentation
KW  -  Simultaneous learning
KW  - Multiobjective optimization
JF  - Pattern Recognition
VL  - 60
SP  - 37
EP  - 50
DO  - https://doi.org/10.1016/j.patcog.2016.05.004
UR  - https://www.sciencedirect.com/science/article/pii/S003132031630084X
N2  - Clustering learning and classification learning are two major tasks in pattern recognition. The traditional hybrid clustering and classification algorithms handle them in a sequential way rather than a simultaneous way. Fortunately, multiobjective optimization provides a way to solve this problem. In this paper, an algorithm that learns simultaneous clustering and classification adaptively via multiobjective evolutionary algorithm is proposed. The main idea of this paper is to optimize two objective functions which represent fuzzy cluster connectedness and classification error rate to achieve the goal of simultaneous learning. Firstly, we adopt a graph based representation scheme to encode so that it can generate a set of solutions with different number of clusters in a single run. Then the relationship between clustering and classification is built via the Bayesian theory during the optimization process. The quality of clustering and classification is measured by the objective functions and the feedback drawn from both aspects is used to guide the mutation. At last, a set of nondominated solutions are generated, from which the final Pareto optimal solution is selected by using Adjusted Rand Index. The results on synthetic datasets and real-life datasets demonstrate the rationality and effectiveness of the proposed algorithm. Furthermore, we apply the proposed algorithm to image segmentation including texture images and synthetic aperture radar images, the experimental results show the superiority of the proposed algorithm compared with other five algorithms.
ER  - 
TY  - JOUR
T1  - Continual learning classification method with constant-sized memory cells based on the artificial immune system
A1  - Li, Dong
A1  - Liu, Shulin
A1  - Gao, Furong
A1  - Sun, Xin
Y1  - 2021///
KW  -  Classification
KW  -  Clustering
KW  -  Continual learning
KW  -  Machine learning
KW  - Artificial immune system
JF  - Knowledge-Based Systems
VL  - 213
SP  - 106673
EP  - 106673
DO  - https://doi.org/10.1016/j.knosys.2020.106673
UR  - https://www.sciencedirect.com/science/article/pii/S0950705120308029
N2  - Most classification methods cannot further improve their classification performance by learning the testing data during the testing stage, for lacking continual learning ability. A new classification method, continual learning classification method with constant-sized memory cells based on the artificial immune system (C-CLCM), is proposed. It is inspired by the continual learning mechanism of the biological immune system. C-CLCM gradually enhances its classification performance by continually learning the testing data especially the new types of labeled data and new types of unlabeled data during the testing stage. At the same moment, it updates the existing memory cells and culture new types of memory cells. C-CLCM degenerates into a common supervised learning classification method under certain conditions. To assess its performance and possible advantages, the experiments on well-known datasets from the UCI repository were performed. Results show that C-CLCM has better classification performance when it degenerates into a common supervised learning classification method. It outperforms the other methods when the training data do not cover all types. The less type of training, the more advantages it has.
ER  - 
TY  - JOUR
T1  - Fourier-based quantification of renal glomeruli size using Hough transform and shape descriptors
A1  - Najafian, Sohrab
A1  - Beigzadeh, Borhan
A1  - Riahi, Mohammad
A1  - Khadir Chamazkoti, Fatemeh
A1  - Pouramir, Mahdi
Y1  - 2017///
KW  -  Cross validation
KW  -  Fourier transform
KW  -  Hough transform
KW  -  ROC curves
KW  -  Shape descriptors
KW  - Renal glomeruli
JF  - Computer Methods and Programs in Biomedicine
VL  - 151
SP  - 179
EP  - 192
DO  - https://doi.org/10.1016/j.cmpb.2017.08.011
UR  - https://www.sciencedirect.com/science/article/pii/S0169260717302766
N2  - Background and objective
Analysis of glomeruli geometry is important in histopathological evaluation of renal microscopic images. Due to the shape and size disparity of even glomeruli of same kidney, automatic detection of these renal objects is not an easy task. Although manual measurements are time consuming and at times are not very accurate, it is commonly used in medical centers. In this paper, a new method based on Fourier transform following usage of some shape descriptors is proposed to detect these objects and their geometrical parameters.
Methods
Reaching the goal, a database of 400 regions are selected randomly. 200 regions of which are part of glomeruli and the other 200 regions are not belong to renal corpuscles. ROC curve is used to decide which descriptor could classify two groups better. f_measure, which is a combination of both tpr (true positive rate) and fpr (false positive rate), is also proposed to select optimal threshold for descriptors. Combination of three parameters (solidity, eccentricity, and also mean squared error of fitted ellipse) provided better result in terms of f_measure to distinguish desired regions. Then, Fourier transform of outer edges is calculated to form a complete curve out of separated region(s).
Results
The generality of proposed model is verified by use of cross validation method, which resulted tpr of 94%, and fpr of 5%. Calculation of glomerulus’ and Bowman's space with use of the algorithm are also compared with a non-automatic measurement done by a renal pathologist, and errors of 5.9%, 5.4%, and 6.26% are resulted in calculation of Capsule area, Bowman space, and glomeruli area, respectively.
Conclusions
Having tested different glomeruli with various shapes, the experimental consequences show robustness and reliability of our method. Therefore, it could be used to illustrate renal diseases and glomerular disorders by measuring the morphological changes accurately and expeditiously.
ER  - 
TY  - JOUR
T1  - An efficient kernel matrix evaluation measure
A1  - Nguyen, Canh Hao
A1  - Ho, Tu Bao
Y1  - 2008///
KW  -  Class separability measure
KW  -  Kernel matrix quality measure
KW  -  Kernel methods
KW  -  Kernel target alignment
KW  - Classification
JF  - Pattern Recognition
VL  - 41
IS  - 11
SP  - 3366
EP  - 3372
DO  - https://doi.org/10.1016/j.patcog.2008.04.005
UR  - https://www.sciencedirect.com/science/article/pii/S0031320308001350
N2  - We study the problem of evaluating the goodness of a kernel matrix for a classification task. As kernel matrix evaluation is usually used in other expensive procedures like feature and model selections, the goodness measure must be calculated efficiently. Most previous approaches are not efficient except for kernel target alignment (KTA) that can be calculated in O(n2) time complexity. Although KTA is widely used, we show that it has some serious drawbacks. We propose an efficient surrogate measure to evaluate the goodness of a kernel matrix based on the data distributions of classes in the feature space. The measure not only overcomes the limitations of KTA but also possesses other properties like invariance, efficiency and an error bound guarantee. Comparative experiments show that the measure is a good indication of the goodness of a kernel matrix.
ER  - 
TY  - JOUR
T1  - Abnormal image detection in endoscopy videos using a filter bank and local binary patterns
A1  - Nawarathna, Ruwan
A1  - Oh, JungHwan
A1  - Muthukudage, Jayantha
A1  - Tavanapong, Wallapak
A1  - Wong, Johnny
A1  - de Groen, Piet C
A1  - Tang, Shou Jiang
Y1  - 2014///
KW  -  Colonoscopy
KW  -  Filter bank
KW  -  Local binary pattern
KW  -  Texton
KW  -  Texton dictionary
KW  - Wireless capsule endoscopy
JF  - Neurocomputing
VL  - 144
SP  - 70
EP  - 91
DO  - https://doi.org/10.1016/j.neucom.2014.02.064
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214007334
N2  - Finding mucosal abnormalities (e.g., erythema, blood, ulcer, erosion, and polyp) is one of the most essential tasks during endoscopy video review. Since these abnormalities typically appear in a small number of frames (around 5% of the total frame number), automated detection of frames with an abnormality can save physician׳s time significantly. In this paper, we propose a new multi-texture analysis method that effectively discerns images showing mucosal abnormalities from the ones without any abnormality since most abnormalities in endoscopy images have textures that are clearly distinguishable from normal textures using an advanced image texture analysis method. The method uses a “texton histogram” of an image block as features. The histogram captures the distribution of different “textons” representing various textures in an endoscopy image. The textons are representative response vectors of an application of a combination of Leung and Malik (LM) filter bank (i.e., a set of image filters) and a set of Local Binary Patterns on the image. Our experimental results indicate that the proposed method achieves 92% recall and 91.8% specificity on wireless capsule endoscopy (WCE) images and 91% recall and 90.8% specificity on colonoscopy images.
ER  - 
TY  - JOUR
T1  - Multi-label classification methods for improving comorbidities identification
A1  - Wosiak, A
A1  - Glinka, K
A1  - Zakrzewska, D
Y1  - 2018///
KW  -  Comorbidity identification
KW  -  Improving medical diagnosis
KW  -  Label dependence
KW  -  Multi-label classification
KW  - Multi-perspective recognition
JF  - Computers in Biology and Medicine
VL  - 100
SP  - 279
EP  - 288
DO  - https://doi.org/10.1016/j.compbiomed.2017.07.006
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517302305
N2  - The medical diagnostic process may be supported by computational classification techniques. In many cases, patients are affected by multiple illnesses, and more than one classification label is required to improve medical decision-making. In this paper, we consider a multi-perspective classification problem for medical diagnostics, where cases are described by labels from separate sets. We attempt to improve the identification of comorbidities using multi-label classification techniques. Several investigated methods, which provide label dependencies, are analysed and evaluated. The methods' performances are verified by experiments conducted on four sets of medical data from subject patients. The results were evaluated using several metrics and were statistically verified. We compare the effects of the techniques that do and do not consider label correlations. We demonstrate that multi-label classification methods from the first group outperform the techniques from the second one.
ER  - 
TY  - JOUR
T1  - Heart rate variability feature selection in the presence of sleep apnea: An expert system for the characterization and detection of the disorder
A1  - Martín-González, Sofía
A1  - Navarro-Mesa, Juan L
A1  - Juliá-Serdá, Gabriel
A1  - Kraemer, Jan F
A1  - Wessel, Niels
A1  - Ravelo-García, Antonio G
Y1  - 2017///
KW  -  Cepstrum
KW  -  Detrended Fluctuation Analysis
KW  -  Feature selection
KW  -  Filter bank
KW  -  Heart rate variability
KW  -  Single-lead ECG
KW  - Sleep apnea
JF  - Computers in Biology and Medicine
VL  - 91
SP  - 47
EP  - 58
DO  - https://doi.org/10.1016/j.compbiomed.2017.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517303244
N2  - We introduce a sleep apnea characterization and classification approach based on a Heart Rate Variability (HRV) feature selection process, thus focusing on the characterization of the underlying process from a cardiac rate point of view. Therefore, we introduce linear and nonlinear variables, namely Cepstrum Coefficients (CC), Filterbanks (Fbank) and Detrended Fluctuation Analysis (DFA). Logistic Regression, Linear Discriminant Analysis and Quadratic Discriminant Analysis were used for classification purposes. The experiments were carried out using two databases. We achieved a per-segment accuracy of 84.76% (sensitivity = 81.45%, specificity = 86.82%, AUC = 0.92) in the Apnea-ECG Physionet database, whereas in the HuGCDN2014 database, provided by the Dr. Negrín University Hospital (Las Palmas de Gran Canaria, Spain), the best results were: accuracy = 81.96%, sensitivity = 70.95%, specificity = 85.47%, AUC = 0.87. The former results were comparable or better than those obtained by other methods for the same database in the recent literature. We have concluded that the selected features that best characterize the underlying process are common to both databases. This supports the fact that the conclusions reached are potentially generalizable. The best results were obtained when the three kinds of features were jointly used. Another notable fact is the small number of features needed to describe the phenomenon. Results suggest that the two first Fbanks, the first CC and the first DFA coefficient are the variables that best describe the RR pattern in OSA and, therefore, are especially relevant to extract discriminative information for apnea screening purposes.
ER  - 
TY  - JOUR
T1  - A competency question-oriented approach for the transformation of semi-structured bioinformatics data into linked open data
A1  - de Paula, Gabriel C S G
A1  - de Farias, Cléver R G
Y1  - 2020///
KW  -  Competency questions
KW  -  Linked open data
KW  -  Stepwise transformation approach
KW  - Semi-structured bioinformatics data
JF  - Engineering Applications of Artificial Intelligence
VL  - 90
SP  - 103495
EP  - 103495
DO  - https://doi.org/10.1016/j.engappai.2020.103495
UR  - https://www.sciencedirect.com/science/article/pii/S0952197620300130
N2  - Bioinformatics data obtained using different molecular biology techniques must be processed through different analysis tools to discover new biological knowledge. Since plain processed data have no explicit semantic value, the extraction of additional knowledge through data exploration would benefit from the transformation of bioinformatics data into Linked Open Data (LOD). Different approaches have been proposed to support the transformation of different types of biomedical data into LOD. However, these approaches are not flexible enough so they can be easily adapted for the transformation of semi-structured bioinformatics data into LOD. Thus, this paper proposes a novel approach to support such transformation. According to this approach, a set of competency questions drive not only the definition of transformation rules, but also the data transformation and exploration afterwards. The paper also presents a support toolset and describes the successful application of the proposed approach in the functional genomics domain.
ER  - 
TY  - JOUR
T1  - Fusion of evolvable genome structure and multi-objective optimization for subspace clustering
A1  - Paul, Dipanjyoti
A1  - Saha, Sriparna
A1  - Mathew, Jimson
Y1  - 2019///
KW  - Biclustering
KW  - Cluster validity indices
KW  - Evolvable genome structure
KW  - Multi-objective optimization
KW  - Subspace clustering
JF  - Pattern Recognition
VL  - 95
SP  - 58
EP  - 71
DO  - https://doi.org/10.1016/j.patcog.2019.05.033
UR  - https://www.sciencedirect.com/science/article/pii/S0031320319302183
N2  - Subspace clustering techniques become paramount in pattern recognition for detecting local variations from high dimensional data. Several techniques exist in the recent literature for subspace clustering, majority of which optimize implicitly or explicitly a single cluster quality measure. Inspired by the success of multi-objective optimization in solving clustering problem, we developed a multi-objective based subspace clustering technique in this paper. The proposed technique simultaneously optimizes two subspace cluster quality measures, capable of capturing different cluster shapes/properties. Two existing cluster quality measures, XB-index and PBM-index, are modified to develop subspace cluster validity indices, and then those are used as optimization criteria. These cluster validity indices measure the appropriateness of generated subspace clusters in terms of intra-subspace cluster similarity and separation between subspace clusters. The proposed approach utilizes a new evolvable genome structure which stores the information about subspaces in its phenotype and genotype and evolves this genome structure with the help of different genetic operators. The developed algorithm is applied on ten standard real-life data sets and sixteen synthetic datasets for identifying different subspace clusters. The results obtained by this algorithm are compared against some state-of-the-art techniques with respect to different performance metrics. Experimentation reveals that the proposed algorithm is able to take advantages of its evolvable genomic structure and multi-objective based framework and it can be applied to any data set. In a part of the paper, the efficacy of the proposed technique is also shown for bi-clustering of gene-expression data sets.
ER  - 
TY  - JOUR
T1  - Taguchi-TOPSIS based HOG parameter selection for complex background sign language recognition
A1  - Joshi, Garima
A1  - Singh, Sukhwinder
A1  - Vig, Renu
Y1  - 2020///
KW  -  Complex background
KW  -  Indian Sign Language (ISL)
KW  -  Sign Language Recognition System (SLRS)
KW  -  TOPSIS
KW  -  Taguchi
KW  - Histogram of Oriented Gradients (HOG)
JF  - Journal of Visual Communication and Image Representation
VL  - 71
SP  - 102834
EP  - 102834
DO  - https://doi.org/10.1016/j.jvcir.2020.102834
UR  - https://www.sciencedirect.com/science/article/pii/S1047320320300845
N2  - This paper presents an approach to design Indian Sign Language (ISL) recognition system for complex background. In many applications, Histogram of Oriented Gradients (HOG) have been proved to be effective. However, it is observed that the choice of HOG parameters affects the feature vector size and its classification capability. The objective is to select the parameter values in order to have maximal accuracy at a minimal computational time and reduced feature vector size. A combined Taguchi and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) based decision-making technique is applied to determine the values of these parameters. Results show that the combined TOPSIS-Taguchi based technique is effective in selecting the parameter combination to get high overall performance. For the acquired ISL complex background dataset, the selected values of parameters are further used to obtain multi-level HOG resulting in the overall accuracy of 92% for 280 features.
ER  - 
TY  - JOUR
T1  - Ensemble-based hybrid probabilistic sampling for imbalanced data learning in lung nodule CAD
A1  - Cao, Peng
A1  - Yang, Jinzhu
A1  - Li, Wei
A1  - Zhao, Dazhe
A1  - Zaiane, Osmar
Y1  - 2014///
KW  -  Ensemble classifier
KW  -  False positive reduction
KW  -  Imbalanced data learning
KW  -  Random subspace method
KW  -  Re-sampling
KW  - Lung nodule detection
JF  - Computerized Medical Imaging and Graphics
VL  - 38
IS  - 3
SP  - 137
EP  - 150
DO  - https://doi.org/10.1016/j.compmedimag.2013.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S0895611113002000
N2  - Classification plays a critical role in false positive reduction (FPR) in lung nodule computer aided detection (CAD). The difficulty of FPR lies in the variation of the appearances of the nodules, and the imbalance distribution between the nodule and non-nodule class. Moreover, the presence of inherent complex structures in data distribution, such as within-class imbalance and high-dimensionality are other critical factors of decreasing classification performance. To solve these challenges, we proposed a hybrid probabilistic sampling combined with diverse random subspace ensemble. Experimental results demonstrate the effectiveness of the proposed method in terms of geometric mean (G-mean) and area under the ROC curve (AUC) compared with commonly used methods.
ER  - 
TY  - JOUR
T1  - Studies in the extensively automatic construction of large odds-based inference networks from structured data. Examples from medical, bioinformatics, and health insurance claims data
A1  - Robson, B
A1  - Boray, S
Y1  - 2018///
KW  -  Anomaly detection
KW  -  Bayes Net
KW  -  Big Data
KW  -  Bioinformatics
KW  -  Clinical decision support
KW  -  Data mining
KW  -  Fraud detection
KW  -  Hyperbolic Dirac Net
KW  -  Machine learning
KW  - Inference net
JF  - Computers in Biology and Medicine
VL  - 95
SP  - 147
EP  - 166
DO  - https://doi.org/10.1016/j.compbiomed.2018.02.013
UR  - https://www.sciencedirect.com/science/article/pii/S001048251830043X
N2  - Theoretical and methodological principles are presented for the construction of very large inference nets for odds calculations, composed of hundreds or many thousands or more of elements, in this paper generated by structured data mining. It is argued that the usual small inference nets can sometimes represent rather simple, arbitrary estimates. Examples of applications in clinical and public health data analysis, medical claims data and detection of irregular entries, and bioinformatics data, are presented. Construction of large nets benefits from application of a theory of expected information for sparse data and the Dirac notation and algebra. The extent to which these are important here is briefly discussed. Purposes of the study include (a) exploration of the properties of large inference nets and a perturbation and tacit conditionality models, (b) using these to propose simpler models including one that a physician could use routinely, analogous to a “risk score”, (c) examination of the merit of describing optimal performance in a single measure that combines accuracy, specificity, and sensitivity in place of a ROC curve, and (d) relationship to methods for detecting anomalous and potentially fraudulent data.
ER  - 
TY  - JOUR
T1  - Topological data analysis to model the shape of immune responses during co-infections
A1  - Sasaki, Karin
A1  - Bruder, Dunja
A1  - Hernandez-Vargas, Esteban A
Y1  - 2020///
KW  -  Complex data analysis
KW  -  Immune system dynamics
KW  -  Influenza infections
KW  - Topological data analysis
JF  - Communications in Nonlinear Science and Numerical Simulation
VL  - 85
SP  - 105228
EP  - 105228
DO  - https://doi.org/10.1016/j.cnsns.2020.105228
UR  - https://www.sciencedirect.com/science/article/pii/S1007570420300629
N2  - Co-infections by multiple pathogens have important implications in many aspects of health, epidemiology and evolution. However, how to disentangle the non-linear dynamics of the immune response when two infections take place at the same time is largely unexplored. Using data sets of the immune response during influenza-pneumococcal co-infection in mice, we employ here topological data analysis to simplify and visualise high dimensional data sets. We identified persistent shapes of the simplicial complexes of the data in the three infection scenarios: single viral infection, single bacterial infection, and co-infection. The immune response was found to be distinct for each of the infection scenarios and we uncovered that the immune response during the co-infection has three phases and two transition points. During the first phase, its dynamics is inherited from its response to the primary (viral) infection. The immune response has an early shift (few hours post co-infection) and then modulates its response to react against the secondary (bacterial) infection. Between 18 and 26 h post co-infection the nature of the immune response changes again and does no longer resembles either of the single infection scenarios.
ER  - 
TY  - JOUR
T1  - Automatic diabetic retinopathy diagnosis using adjustable ophthalmoscope and multi-scale line operator
A1  - Qu, Meng
A1  - Ni, Chun
A1  - Chen, Mufan
A1  - Zheng, Linghan
A1  - Dai, Ling
A1  - Sheng, Bin
A1  - Li, Ping
A1  - Wu, Qiang
Y1  - 2017///
KW  -  DR grading
KW  -  Multi-scale line operator
KW  -  Ophthalmoscope
KW  - Diabetic retinopathy screening
JF  - Pervasive and Mobile Computing
VL  - 41
SP  - 490
EP  - 503
DO  - https://doi.org/10.1016/j.pmcj.2017.04.003
UR  - https://www.sciencedirect.com/science/article/pii/S1574119217301852
N2  - Diabetic Retinopathy (DR), the most common one of diabetic eye diseases that cause loss of vision and blindness, has become one of major health problems today. However, DR can be eased through timely treatment and periodical screening. In this paper, we proposes an automatic diabetic retinopathy diagnostic system to help patients know about their retinal conditions. We design a portable ophthalmoscope, which is composed of a retinal lens, a smartphone and a frame between them to help patients take fundus images anywhere and anytime. Then the images are transmitted to be analyzed, including localization of optic disk and macular, vessel segmentation, detection of lesions, and grading of DR. We use a multi-scale line operator to improve accuracy in segmenting small-scale vessels, a binary mask and image restoration to reduce the effect of the existence of the vessels on optic disk localization. After the analysis, the fundus image are then graded as normal, mild Non-Proliferative Diabetic Retinopathy (NPDR), moderate NPDR or severe NPDR. The grading process uses region segmentation to improve the efficiency. The final grading results are tested based on the fundus images provided by the hospitals. We evaluate our system through comparing our grading result with those graded by experts, which comes out with an overall accuracy of up to 85%.
ER  - 
TY  - JOUR
T1  - Real-time feature selection technique with concept drift detection using adaptive micro-clusters for data stream mining
A1  - Hammoodi, Mahmood Shakir
A1  - Stahl, Frederic
A1  - Badii, Atta
Y1  - 2018///
KW  -  Concept drift detection
KW  -  Real-time feature selection
KW  - Data Stream Mining
JF  - Knowledge-Based Systems
VL  - 161
SP  - 205
EP  - 239
DO  - https://doi.org/10.1016/j.knosys.2018.08.007
UR  - https://www.sciencedirect.com/science/article/pii/S0950705118304039
N2  - Data streams are unbounded, sequential data instances that are generated with high Velocity. Classifying sequential data instances is a very challenging problem in machine learning with applications in network intrusion detection, financial markets and applications requiring real-time sensor-networks-based situation assessment. Data stream classification is concerned with the automatic labelling of unseen instances from the stream in real-time. For this the classifier needs to adapt to concept drifts and can only have a single pass through the data if the stream is fast moving. This research paper presents work on a real-time pre-processing technique, in particular feature tracking. The feature tracking technique is designed to improve Data Stream Mining (DSM) classification algorithms by enabling and optimising real-time feature selection. The technique is based on tracking adaptive statistical summaries of the data and class label distributions, known as Micro-Clusters. Currently the technique is able to detect concept drifts and identify which features have been influential in the drift.
ER  - 
TY  - JOUR
T1  - The novel approach of temporal dependency complexity analysis of heart rate variability in obstructive sleep apnea
A1  - Tang, Lan
A1  - Liu, Guanzheng
Y1  - 2021///
KW  -  Gramian angular summation field (GASF)
KW  -  Obstructive sleep apnea (OSA)
KW  -  Temporal-dependency complexity analysis
KW  -  Two-dimensional sample entropy of coarse-grained gramian angular summation field image (CgSampEn)
KW  - Heart rate variability (HRV)
JF  - Computers in Biology and Medicine
VL  - 135
SP  - 104632
EP  - 104632
DO  - https://doi.org/10.1016/j.compbiomed.2021.104632
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521004261
N2  - Obstructive sleep apnea (OSA) is a serious sleep disorder, which leads to changes in autonomic nerve function and increases the risk of cardiovascular disease. Heart rate variability (HRV) has been widely used as a non-invasive method for assessing the autonomic nervous system (ANS). We proposed the two-dimensional sample entropy of the coarse-grained Gramian angular summation field image (CgSampEn2D) index. It is a new index for HRV analysis based on the temporal dependency complexity. In this study, we used 60 electrocardiogram (ECG) records from the Apnea-ECG database of PhysioNet (20 healthy records and 40 OSA records). These records were divided into 5-min segments. Compared with the classical indices low-to-high frequency power ratio (LF/HF) and sample entropy (SampEn), CgSampEn2D utilizes the correlation information between different time intervals in the RR sequences and preserves the temporal dependency of the RR sequences, which improves the OSA detection performance significantly. The OSA screening accuracy of CgSampEn2D (93.3%) is higher than that of LF/HF (80.0%) and SampEn (73.3%). Additionally, CgSampEn2D has a significant association with the apnea-hypopnea index (AHI) (R = −0.740, p = 0). CgSampEn2D reflects the complexity of the OSA autonomic nerve more comprehensively and provides a novel idea for the screening of OSA disease.
ER  - 
TY  - JOUR
T1  - Discriminatively regularized least-squares classification
A1  - Xue, Hui
A1  - Chen, Songcan
A1  - Yang, Qiang
Y1  - 2009///
KW  -  Discriminative information
KW  -  Manifold learning
KW  -  Pattern recognition
KW  - Classifier design
JF  - Pattern Recognition
VL  - 42
IS  - 1
SP  - 93
EP  - 104
DO  - https://doi.org/10.1016/j.patcog.2008.07.010
UR  - https://www.sciencedirect.com/science/article/pii/S0031320308002872
N2  - Over the past decades, regularization theory is widely applied in various areas of machine learning to derive a large family of novel algorithms. Traditionally, regularization focuses on smoothing only, and does not fully utilize the underlying discriminative knowledge which is vital for classification. In this paper, we propose a novel regularization algorithm in the least-squares sense, called discriminatively regularized least-squares classification (DRLSC) method, which is specifically designed for classification. Inspired by several new geometrically motivated methods, DRLSC directly embeds the discriminative information as well as the local geometry of the samples into the regularization term so that it can explore as much underlying knowledge inside the samples as possible and aim to maximize the margins between the samples of different classes in each local area. Furthermore, by embedding equality type constraints in the formulation, the solutions of DRLSC can follow from solving a set of linear equations and the framework naturally contains multi-class problems. Experiments on both toy and real world problems demonstrate that DRLSC is often superior in classification performance to the classical regularization algorithms, including regularization networks, support vector machines and some of the recent studied manifold regularization techniques.
ER  - 
TY  - JOUR
T1  - Integer Linear Programming for the Bayesian network structure learning problem
A1  - Bartlett, Mark
A1  - Cussens, James
Y1  - 2017///
KW  -  Constrained optimisation
KW  -  Cutting planes
KW  -  Integer Linear Programming
KW  -  Separation
KW  - Bayesian networks
JF  - Artificial Intelligence
VL  - 244
SP  - 258
EP  - 271
DO  - https://doi.org/10.1016/j.artint.2015.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S0004370215000417
N1  - Combining Constraint Solving with Mining and Learning
N2  - Bayesian networks are a commonly used method of representing conditional probability relationships between a set of variables in the form of a directed acyclic graph (DAG). Determination of the DAG which best explains observed data is an NP-hard problem [1]. This problem can be stated as a constrained optimisation problem using Integer Linear Programming (ILP). This paper explores how the performance of ILP-based Bayesian network learning can be improved through ILP techniques and in particular through the addition of non-essential, implied constraints. There are exponentially many such constraints that can be added to the problem. This paper explores how these constraints may best be generated and added as needed. The results show that using these constraints in the best discovered configuration can lead to a significant improvement in performance and show significant improvement in speed using a state-of-the-art Bayesian network structure learner.
ER  - 
TY  - JOUR
T1  - A systematic literature review and classification of knowledge discovery in traditional medicine
A1  - Arji, Goli
A1  - Safdari, Reza
A1  - Rezaeizadeh, Hossein
A1  - Abbassian, Alireza
A1  - Mokhtaran, Mehrshad
A1  - Hossein Ayati, Mohammad
Y1  - 2019///
KW  -  Knowledge discovery
KW  -  Machine learning
KW  -  Traditional medicine
KW  - Data mining
JF  - Computer Methods and Programs in Biomedicine
VL  - 168
SP  - 39
EP  - 57
DO  - https://doi.org/10.1016/j.cmpb.2018.10.017
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718312859
N2  - Introduction and Objective
Despite the importance of machine learning methods application in traditional medicine there is a no systematic literature review and a classification for this field. This is the first comprehensive literature review of the application of data mining methods in traditional medicine.
Method
We reviewed 5 database between 2000 to 2017 based on the Kitchenham systematic review methodology. 502 articles were identified and reviewed for their relevance to application of machine learning methods in traditional medicine, 42 selected papers were classified and categorized on four dimension; 1) application domain of data mining techniques in traditional medicine; 2) the data mining methods most frequently used in traditional medicine; 3) main strength and limitation of data mining techniques in traditional medicine; 4) the performance evaluation methods in data mining methods in traditional medicine.
Result
The result obtained showed that main application domain of data mining techniques in traditional medicine was related to syndrome differentiation. Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Support Vector Machines (SVMs) were recognized as being the methods most frequently applied in traditional medicine. Furthermore, each data mining techniques has its own strength and limitations when applied in traditional medicine. Single scaler methods were frequently used for performance evaluation of data mining methods.
Conclusion
Machine learning methods have become an important research field in traditional medicine. Our research provides information about this methods by examining the related articles.
ER  - 
TY  - JOUR
T1  - Integrated machine learning approaches for complementing statistical process control procedures
A1  - Kang, Boo-Sik
A1  - Park, Sang-Chan
Y1  - 2000///
KW  -  Machine learning
KW  -  Multivariate process control
KW  -  Pattern detection
KW  - Feature selection
JF  - Decision Support Systems
VL  - 29
IS  - 1
SP  - 59
EP  - 72
DO  - https://doi.org/10.1016/S0167-9236(00)00063-4
UR  - https://www.sciencedirect.com/science/article/pii/S0167923600000634
N2  - Although statistical process control (SPC) procedures have played a central role in solving quality problems, their effectiveness is yet to be fully realized in the process industry with large volume data, a lot of dimensions of variables, and complex relationships among processes and variables. To complement SPC procedures, we suggest three integrated methods of inductive learning and neural networks for solving the quality problems. First, a feature subset selection method is proposed for reducing variables required for quality control. Second, a clustering inductive learning method is presented for improving the correct prediction rate (CPR) of inductive learning. Third, a pattern detection method is suggested for detection of different patterns comparing reference patterns. Three methods are experimented in two datasets. The results show that the three methods are effective for the multivariate process control.
ER  - 
TY  - JOUR
T1  - Predicting age by mining electronic medical records with deep learning characterizes differences between chronological and physiological age
A1  - Wang, Zichen
A1  - Li, Li
A1  - Glicksberg, Benjamin S
A1  - Israel, Ariel
A1  - Dudley, Joel T
A1  - Ma'ayan, Avi
Y1  - 2017///
KW  -  Age prediction
KW  -  Aging
KW  -  Machine learning
KW  -  Medical records
KW  - Deep learning
JF  - Journal of Biomedical Informatics
VL  - 76
SP  - 59
EP  - 68
DO  - https://doi.org/10.1016/j.jbi.2017.11.003
UR  - https://www.sciencedirect.com/science/article/pii/S153204641730240X
N2  - Determining the discrepancy between chronological and physiological age of patients is central to preventative and personalized care. Electronic medical records (EMR) provide rich information about the patient physiological state, but it is unclear whether such information can be predictive of chronological age. Here we present a deep learning model that uses vital signs and lab tests contained within the EMR of Mount Sinai Health System (MSHS) to predict chronological age. The model is trained on 377,686 EMR from patients of ages 18–85 years old. The discrepancy between the predicted and real chronological age is then used as a proxy to estimate physiological age. Overall, the model can predict the chronological age of patients with a standard deviation error of ∼7 years. The ages of the youngest and oldest patients were more accurately predicted, while patients of ages ranging between 40 and 60 years were the least accurately predicted. Patients with the largest discrepancy between their physiological and chronological age were further inspected. The patients predicted to be significantly older than their chronological age have higher systolic blood pressure, higher cholesterol, damaged liver, and anemia. In contrast, patients predicted to be younger than their chronological age have lower blood pressure and shorter stature among other indicators; both groups display lower weight than the population average. Using information from ∼10,000 patients from the entire cohort who have been also profiled with SNP arrays, genome-wide association study (GWAS) uncovers several novel genetic variants associated with aging. In particular, significant variants were mapped to genes known to be associated with inflammation, hypertension, lipid metabolism, height, and increased lifespan in mice. Several genes with missense mutations were identified as novel candidate aging genes. In conclusion, we demonstrate how EMR data can be used to assess overall health via a scale that is based on deviation from the patient’s predicted chronological age.
ER  - 
TY  - JOUR
T1  - Application of interpretable machine learning models for the intelligent decision
A1  - Li, Yawen
A1  - Yang, Liu
A1  - Yang, Bohan
A1  - Wang, Ning
A1  - Wu, Tian
Y1  - 2019///
KW  -  Firm size
KW  -  Innovation performance
KW  -  R&D investments
KW  -  XGBoost
KW  - Machine learning
JF  - Neurocomputing
VL  - 333
SP  - 273
EP  - 283
DO  - https://doi.org/10.1016/j.neucom.2018.12.012
UR  - https://www.sciencedirect.com/science/article/pii/S0925231218314668
N2  - In this study, an interpretable machine learning algorithm is proposed for the issues of intelligent decision through predicting the firms’ efficiency of innovation. Based on the unbalanced panel data collected in Zhongguancun Science Parks from year 2005 to 2015, the efficiency of over 10,000 firms have been analysed in this study, and the change and growth of these firms have been captured over time. The linear regression, decision tree, random forests, neural network and XGBoost models are applied to figure out the impact factors of innovation. After comparing the results of different models, it has been found that the accuracy of XGBoost for R&D efficiency labelled, commercial efficiency labelled and overall efficiency labelled classification problems are 73.65%, 70.02% and 70.09%, which outperform the other four models. Moreover, the interpretability of XGBoost is also better than other models. Thus, the XGBoost model makes it possible for managers to predict the firm's future innovation performance derived from their innovation strategies in the current stage. Furthermore, it helps firms to build an intelligent decision support system, which is of great importance for them to deal with complex decision environments, and to increase their efficiency of innovation in the long-term dynamic competition with other firms.
ER  - 
TY  - JOUR
T1  - Information Needs Mining of COVID-19 in Chinese Online Health Communities
A1  - Wang, Jie
A1  - Wang, Lei
A1  - Xu, Jing
A1  - Peng, Yan
Y1  - 2021///
KW  -  COVID-19
KW  -  Lexical meaning co-occurrence
KW  -  Online health communities
KW  -  Topic mining
KW  - Information needs
JF  - Big Data Research
VL  - 24
SP  - 100193
EP  - 100193
DO  - https://doi.org/10.1016/j.bdr.2021.100193
UR  - https://www.sciencedirect.com/science/article/pii/S2214579621000101
N2  - This study explores the information needs for the novel coronavirus pneumonia (COVID-19) in Chinese online health communities (OHCs). Based on the question and answer data about COVID-19 in six Chinese OHCs, topic mining and data analysis were conducted. We propose a CL-LDA topic model (Latent Dirichlet Allocation Model with co-occurrence of lexical meaning) based on lexical meaning co-occurrence analysis and LDA topic model. Four main information need topics and their proportion are found in this study, including symptom (45.50%), prevention (36.11%), inspection (10.97%), and treatment (7.42%). We also discover that men are most concerned about symptom information while women are most concerned about prevention information; young users have the largest proportion of information needs, and they are most concerned about prevention information. Experiment results show that the CL-LDA model can well adapt to the topic mining task of short text which is semantic sparse and lacking co-occurrence information in OHCs. The research results are helpful for OHCs to provide accurate information assistance and improve service quality.
ER  - 
TY  - JOUR
T1  - Neural random subspace
A1  - Cao, Yun-Hao
A1  - Wu, Jianxin
A1  - Wang, Hanchen
A1  - Lasenby, Joan
Y1  - 2021///
KW  -  Deep neural networks
KW  -  Ensemble learning
KW  - Random subspace
JF  - Pattern Recognition
VL  - 112
SP  - 107801
EP  - 107801
DO  - https://doi.org/10.1016/j.patcog.2020.107801
UR  - https://www.sciencedirect.com/science/article/pii/S003132032030604X
N2  - The random subspace method, also known as the pillar of random forests, is good at making precise and robust predictions. However, there is as yet no straightforward way to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than contemporary, higher-order pooling methods, producing excellent results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS demonstrates superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with only incremental cost.
ER  - 
TY  - JOUR
T1  - SSEL-ADE: A semi-supervised ensemble learning framework for extracting adverse drug events from social media
A1  - Liu, Jing
A1  - Zhao, Songzheng
A1  - Wang, Gang
Y1  - 2018///
KW  -  Adverse drug event extraction
KW  -  Semi-supervised learning
KW  -  Social media
KW  - Ensemble learning
JF  - Artificial Intelligence in Medicine
VL  - 84
SP  - 34
EP  - 49
DO  - https://doi.org/10.1016/j.artmed.2017.10.003
UR  - https://www.sciencedirect.com/science/article/pii/S0933365717301847
N2  - With the development of Web 2.0 technology, social media websites have become lucrative but under-explored data sources for extracting adverse drug events (ADEs), which is a serious health problem. Besides ADE, other semantic relation types (e.g., drug indication and beneficial effect) could hold between the drug and adverse event mentions, making ADE relation extraction – distinguishing ADE relationship from other relation types – necessary. However, conducting ADE relation extraction in social media environment is not a trivial task because of the expertise-dependent, time-consuming and costly annotation process, and the feature space’s high-dimensionality attributed to intrinsic characteristics of social media data. This study aims to develop a framework for ADE relation extraction using patient-generated content in social media with better performance than that delivered by previous efforts. To achieve the objective, a general semi-supervised ensemble learning framework, SSEL-ADE, was developed. The framework exploited various lexical, semantic, and syntactic features, and integrated ensemble learning and semi-supervised learning. A series of experiments were conducted to verify the effectiveness of the proposed framework. Empirical results demonstrate the effectiveness of each component of SSEL-ADE and reveal that our proposed framework outperforms most of existing ADE relation extraction methods The SSEL-ADE can facilitate enhanced ADE relation extraction performance, thereby providing more reliable support for pharmacovigilance. Moreover, the proposed semi-supervised ensemble methods have the potential of being applied to effectively deal with other social media-based problems.
ER  - 
TY  - JOUR
T1  - Life Model: A novel representation of life-long temporal sequences in health predictive analytics
A1  - Manashty, Alireza
A1  - Light, Janet
Y1  - 2019///
KW  -  Analytics
KW  -  Artificial intelligence
KW  -  Deep learning
KW  -  Health monitoring
KW  - Predictive
JF  - Future Generation Computer Systems
VL  - 92
SP  - 141
EP  - 156
DO  - https://doi.org/10.1016/j.future.2018.09.033
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X17326523
N2  - Predictive analytics in healthcare can prevent patients’ emergency health conditions, and reduce costs in the long term. Moreover, accurate and timely anomaly predictions by focusing on recent events can save lives. In real-time IoT predictive analytics, modeling historical temporal health records with missing values in diagnosis prediction is a major challenge. Recent studies have started using deep learning and data abstraction techniques to model health data. However, it is difficult to train a model to predict anomalies based on temporal sparse data, especially to classify all disease diagnosis classes. Modeling a lifetime of an individual’s medical history in a short, concise sequence is a challenge. Moreover, the model should be robust and preserve the concept of time for variety of examples despite the missing values; especially in an IoT system, in which real-time prediction depends on both recent data and historical records. The proposed solution in this research for modelingtemporal pattern sequences is called as Life Model (LM). LM provides a concise sequence to represent the history or future, using the novel intensity temporal sequence (ITS) tensors. LM algorithms and properties enable ITS tensors to train long short-term memory (LSTM) recurrent neural networks (RNN) efficiently in order to predict anomalies and diagnosis in real-time, even in the absence of some values. LM is used to predict mortality of 10,000 patients from MIMIC III dataset based on their diagnosis and procedures codes. The results show improvement in the model trained by LM-mapped data compared to fixed-sized intervals which achieved an accuracy of 99.6% with AUROC and brier score of 99.5% and of 0.00 respectively. In addition, the LM model can predict the approximate time of activities, with different granularity of seconds and up to years; tested on an activity dataset. Furthermore, a new LM-powered predictive health analytics and real-time monitoring schema (PHARMS) is proposed to enable design and implementation of predictive health analytic systems. PHARMS uses deep learning for real-time minimally-invasive intelligent activity monitoring and predictive analysis in a medical IoT scheme.
ER  - 
TY  - JOUR
T1  - AHP based Classification Algorithm Selection for Clinical Decision Support System Development
A1  - Khanmohammadi, Sina
A1  - Rezaeiahari, Mandana
Y1  - 2014///
KW  -  AHP
KW  -  Algorithm Selection
KW  -  Machine Learning
KW  -  Medical Informatics
KW  -  Meta-Learning
KW  - Clinical Decision Support System (CDSS)
JF  - Procedia Computer Science
VL  - 36
SP  - 328
EP  - 334
DO  - https://doi.org/10.1016/j.procs.2014.09.101
UR  - https://www.sciencedirect.com/science/article/pii/S1877050914013507
N1  - Complex Adaptive Systems Philadelphia, PA November 3-5, 2014
N2  - Supervised classification algorithms have become very popular because of their potential application in developing intelligent data analytic software. These algorithms are known to be sensitive to the characteristic and structure of input datasets, therefore, researchers use different algorithm selection methods to select the most suitable classification algorithm for specific dataset. These methods do not consider the uncertainty about input dataset, and relative importance of different performance measurements (such as speed, accuracy, and memory usage) in the target application domain. Therefore, these methods are not appropriate for software development. This is especially true in medical field where various high dimensional noisy data might be used with the software. Hence, software developers need to select one supervised classification algorithm that has the highest potential to provide good performance in wide variety of datasets. In this regard, an Analytic Hierarchy Process (AHP) based meta-learning algorithm is proposed to identify the most suitable supervised classification algorithm for developing clinical decision support system (CDSS). The results from ten publicly available medical datasets indicate that Support Vector Machine (SVM) has the highest potential to perform well on variety of medical datasets.
ER  - 
TY  - JOUR
T1  - Subclass Discriminant Analysis of morphological and textural features for HEp-2 staining pattern classification
A1  - Di Cataldo, Santa
A1  - Bottino, Andrea
A1  - Ul Islam, Ihtesham
A1  - Figueiredo Vieira, Tiago
A1  - Ficarra, Elisa
Y1  - 2014///
KW  -  Fluorescence pattern classification
KW  -  Morphological analysis
KW  -  Subclass Discriminant Analysis
KW  -  Textural analysis
KW  - Indirect Immunofluorescence
JF  - Pattern Recognition
VL  - 47
IS  - 7
SP  - 2389
EP  - 2399
DO  - https://doi.org/10.1016/j.patcog.2013.09.024
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313003968
N2  - Classifying HEp-2 fluorescence patterns in Indirect Immunofluorescence (IIF) HEp-2 cell imaging is important for the differential diagnosis of autoimmune diseases. The current technique, based on human visual inspection, is time-consuming, subjective and dependent on the operator's experience. Automating this process may be a solution to these limitations, making IIF faster and more reliable. This work proposes a classification approach based on Subclass Discriminant Analysis (SDA), a dimensionality reduction technique that provides an effective representation of the cells in the feature space, suitably coping with the high within-class variance typical of HEp-2 cell patterns. In order to generate an adequate characterization of the fluorescence patterns, we investigate the individual and combined contributions of several image attributes, showing that the integration of morphological, global and local textural features is the most suited for this purpose. The proposed approach provides an accuracy of the staining pattern classification of about 90%.
ER  - 
TY  - JOUR
T1  - A Bayesian stochastic search method for discovering Markov boundaries
A1  - Masegosa, Andrés R
A1  - Moral, Serafín
Y1  - 2012///
KW  -  Bayesian methods
KW  -  Feature selection
KW  -  Markov boundaries
KW  -  Stochastic search
KW  - Probabilistic graphical models
JF  - Knowledge-Based Systems
VL  - 35
SP  - 211
EP  - 223
DO  - https://doi.org/10.1016/j.knosys.2012.04.028
UR  - https://www.sciencedirect.com/science/article/pii/S0950705112001244
N2  - The discovery of the Markov Boundary (MB) of a target variable using observational data plays a central role in feature selection and local causal structure inference. Most existing methods previously employed for this task rely on statistical independence tests and, in consequence, do not take into account the partial evidence that a finite data set gives about the existence of this kind of probabilistic relationships among random variables. In this work, we employ a novel stochastic search method which explicitly deals with this problem by eliciting multiple alternative Markov boundaries. This technique is based on a Bayesian approach for statistical tests and on a method to score the different alternative solutions. We have also evaluated an interactive procedure for integrating domain or expert knowledge a posteriori (after the learning process), in order to simplify and enrich the set of alternative inferred MBs. In an extensive experimental evaluation we show that this method is able to find a rich and accurate set of alternative MBs which, if properly combined, provide better inferences than other state-of-the-art approaches for this task. Moreover, we think that this new kind of methods, capable of capturing the inherent uncertainty of any real data set and which allows human interventions, can make practitioners feel more confident about the extracted knowledge than fully automatic approaches.
ER  - 
TY  - JOUR
T1  - Adopting model checking techniques for clinical guidelines verification
A1  - Bottrighi, Alessio
A1  - Giordano, Laura
A1  - Molino, Gianpaolo
A1  - Montani, Stefania
A1  - Terenziani, Paolo
A1  - Torchio, Mauro
Y1  - 2010///
KW  -  Model checking
KW  -  Verification
KW  - Clinical guidelines
JF  - Artificial Intelligence in Medicine
VL  - 48
IS  - 1
SP  - 1
EP  - 19
DO  - https://doi.org/10.1016/j.artmed.2009.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S0933365709001365
N2  - Objectives
Clinical guidelines (GLs) are assuming a major role in the medical area, in order to grant the quality of the medical assistance and to optimize medical treatments within healthcare organizations. The verification of properties of the GL (e.g., the verification of GL correctness with respect to several criteria) is a demanding task, which may be enhanced through the adoption of advanced Artificial Intelligence techniques. In this paper, we propose a general and flexible approach to address such a task.
Methods and materials
Our approach to GL verification is based on the integration of a computerized GL management system with a model-checker. We propose a general methodology, and we instantiate it by loosely coupling GLARE, our system for acquiring, representing and executing GLs, with the model-checker SPIN.
Results
We have carried out an in-depth analysis of the types of properties that can be effectively verified using our approach, and we have completed an overview of the usefulness of the verification task at the different stages of the GL life-cycle. In particular, experimentation on a GL for ischemic stroke has shown that the automatic verification of properties in the model checking approach is able to discover inconsistencies in the GL that cannot be detected in advance by hand.
Conclusion
Our approach thus represents a further step in the direction of general and flexible automated GL verification, which also meets usability requirements.
ER  - 
TY  - JOUR
T1  - Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases
A1  - Pagán, Josué
A1  - Risco-Martín, José L
A1  - Moya, José M
A1  - Ayala, José L
Y1  - 2016///
KW  -  Feature
KW  -  Identification
KW  -  Modeling
KW  -  Prediction
KW  -  State-space
KW  -  WBSN
KW  - Migraine
JF  - Journal of Biomedical Informatics
VL  - 62
SP  - 136
EP  - 147
DO  - https://doi.org/10.1016/j.jbi.2016.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416300429
N2  - Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a prediction horizon close to 40min, which is in the time range of the drug pharmacokinetics. Experiments have been performed in a realistic scenario where input data have been acquired in an ambulatory clinical study by the deployment of a non-intrusive Wireless Body Sensor Network. Our results provide an effective methodology for the selection of the future horizon in the development of prediction algorithms for diseases experiencing symptomatic crises.
ER  - 
TY  - JOUR
T1  - Detection of peripheral arterial disease using Doppler spectrogram based expert system for Point-of-Care applications
A1  - Jana, Biswabandhu
A1  - Oswal, Kamal
A1  - Mitra, Sankar
A1  - Saha, Goutam
A1  - Banerjee, Swapna
Y1  - 2019///
KW  -  Android
KW  -  Features extraction
KW  -  Machine learning
KW  -  Peripheral artery disease
KW  - Ultrasonography
JF  - Biomedical Signal Processing and Control
VL  - 54
SP  - 101599
EP  - 101599
DO  - https://doi.org/10.1016/j.bspc.2019.101599
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419301806
N2  - Peripheral arterial disease (PAD) is a common manifestation of cardiovascular diseases and more prevalent in underdeveloped countries. Ultrasound (US) is one of the preferred non-invasive diagnostic techniques for the evaluation of PAD. This work aims at achieving a low-cost PAD detection technique for mass screening. A computer aided diagnosis (CAD) method has been proposed based on the Doppler blood flow spectrograms of lower limb arteries. The proposed scheme initially removes noise from the spectrogram (350 × 175 pixels) and extracts the hemodynamic features which are generally independent of the Doppler angle. From these, best feature subsets are selected using the wrapper algorithm and supervised classifiers are developed in a machine learning framework to perform using 10-fold cross-validation technique. Overall, 334 arterial segments of 60 subjects are investigated where reference measurement is taken from the triplex mode US scanning. The quantitative assessment using random forest based classifier provides an accuracy of 84.37% and 87.93% for detecting the blood flow irregularities in above-knee and below-knee arterial segments, respectively. To classify the arterial diseases into normal, stenosis and occlusion categories, support vector machine (SVM) classifier is found to provide 97.91% accuracy on the unknown testing dataset. Moreover, variations of diagnostic parameters around the proximal and distal arterial segments define the zone of significant stenosis. The degree of stenosis is determined to quantify the severity of obstruction and the accuracy for stenosis greater than 50% is found to be 96.83%. Finally, smartphone application is implemented to provide a cost-effective, portable, user-friendly solution for Point-of-Care US system.
ER  - 
TY  - JOUR
T1  - Human stress classification during public speaking using physiological signals
A1  - Arsalan, Aamir
A1  - Majid, Muhammad
Y1  - 2021///
KW  - Human stress wearable sensors physiological signals multimodal fusionPublic speaking
JF  - Computers in Biology and Medicine
VL  - 133
SP  - 104377
EP  - 104377
DO  - https://doi.org/10.1016/j.compbiomed.2021.104377
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521001712
N2  - Public speaking is a common type of social evaluative situation and a significant amount of the population feel uneasy with it. It is of utmost importance to detect public speaking stress so that appropriate action can be taken to minimize its impacts on human health. In this study, a multimodal human stress classification scheme in response to real-life public speaking activity is proposed. Electroencephalography (EEG), galvanic skin response (GSR), and photoplethysmography (PPG) signals of forty participants are acquired in rest-state and during public speaking activity to divide data into a stressed and non-stressed group. Frequency domain features from EEG and time-domain features from GSR and PPG signals are extracted. The selected set of features from all modalities are fused to classify the stress into two classes. Classification is performed via a leave-one-out cross-validation scheme by using five different classifiers. The highest accuracy of 96.25% is achieved using a support vector machine classifier with radial basis function. Statistical analysis is performed to examine the significance of EEG, GSR, and PPG signals between the two phases of the experiment. Statistical significance is found in certain EEG frequency bands as well as GSR and PPG data recorded before and after public speaking supporting the fact that brain activity, skin conductance, and blood volumetric flow are credible measures of human stress during public speaking activity.
ER  - 
TY  - JOUR
T1  - An ordered search with a large margin classifier for feature selection
A1  - Moraes Villela, Saulo
A1  - de Castro Leite, Saul
A1  - Elias Xavier, Adilson
A1  - Fonseca Neto, Raul
Y1  - 2021///
KW  -  Large margin classifier
KW  -  Ordered search
KW  -  Projected margin
KW  -  Wrapper method
KW  - Feature selection
JF  - Applied Soft Computing
VL  - 100
SP  - 106930
EP  - 106930
DO  - https://doi.org/10.1016/j.asoc.2020.106930
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620308681
N2  - In this paper we propose a novel feature selection method for binary classification problems based on an ordered search process to explore the space of feature subsets. The method, called Admissible Ordered Search (AOS), uses a monotone evaluation function based on margin values calculated by a large margin classifier with an arbitrary Lp norm. This includes large margin classifiers with the L∞ norm, which minimize the L1 norm and are very useful in feature selection, since they produce sparse solutions. An important contribution of this paper is the development of the projected margin concept. This value is computed as the maximal margin vector projected into a lower-dimensional subspace and it is used as an upper bound for the hypothesis value. This enables great economy in runtime and consequently efficiency in the search process as a whole. AOS was tested on several problems and its results were compared to other feature selection methods. The experiments demonstrate the competitive performance of the proposed method in terms of generalization power and computational efficiency.
ER  - 
TY  - JOUR
T1  - Complex-valued wavelet artificial neural network for Doppler signals classifying
A1  - Özbay, Yüksel
A1  - Kara, Sadık
A1  - Latifoğlu, Fatma
A1  - Ceylan, Rahime
A1  - Ceylan, Murat
Y1  - 2007///
KW  -  Atherosclerosis
KW  -  Carotid artery
KW  -  Doppler signals
KW  -  Leave-one-out cross-validation
KW  -  Wavelet neural network
KW  - Complex-valued wavelet artificial neural network
JF  - Artificial Intelligence in Medicine
VL  - 40
IS  - 2
SP  - 143
EP  - 156
DO  - https://doi.org/10.1016/j.artmed.2007.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S0933365707000140
N2  - Summary
Objective
In this paper, the new complex-valued wavelet artificial neural network (CVWANN) was proposed for classifying Doppler signals recorded from patients and healthy volunteers. CVWANN was implemented on four different structures (CVWANN-1, -2, -3 and -4).
Materials and methods
In this study, carotid arterial Doppler ultrasound signals were acquired from left carotid arteries of 38 patients and 40 healthy volunteers. The patient group had an established diagnosis of the early phase of atherosclerosis through coronary or aortofemoropopliteal angiographies. In implemented structures in this paper, Haar wavelet and Mexican hat wavelet functions were used as real and imaginary parts of activation function on different sequence in hidden layer nodes. CVWANN-1, -2 -3 and -4 were implemented by using Haar-Haar, Mexican hat-Mexican hat, Haar-Mexican hat, Mexican hat-Haar as real-imaginary parts of activation function in hidden layer nodes, respectively.
Results and conclusion
In contrast to CVWANN-2, which reached classification rates of 24.5%, CVWANN-1, -3 and -4 classified 40 healthy and 38 unhealthy subjects for both training and test phases with 100% correct classification rate using leave-one-out cross-validation. These networks have 100% sensitivity, 100% specifity and average detection rate is calculated as 100%. In addition, positive predictive value and negative predictive value were obtained as 100% for these networks. These results shown that CVWANN-1, -3 and -4 succeeded to classify Doppler signals. Moreover, training time and processing complexity were decreased considerable amount by using CVWANN-3. As conclusion, using of Mexican hat wavelet function in real and imaginary parts of hidden layer activation function (CVWANN-2) is not suitable for classifying healthy and unhealthy subjects with high accuracy rate. The cause of unsuitability (obtaining the poor results in CVWANN-2) is lack of harmony between type of activation function in hidden layer and type of input signals in neural network.
ER  - 
TY  - JOUR
T1  - Pharmacophore-enabled virtual screening, molecular docking and molecular dynamics studies for identification of potent and selective histone deacetylase 8 inhibitors
A1  - Kashyap, Kriti
A1  - Kakkar, Rita
Y1  - 2020///
KW  -  3D QSAR model
KW  -  Molecular docking
KW  -  Molecular dynamics
KW  -  Pharmacophore
KW  -  Selective inhibition
KW  -  Virtual screening
KW  - Histone deacetylase 8
JF  - Computers in Biology and Medicine
VL  - 123
SP  - 103850
EP  - 103850
DO  - https://doi.org/10.1016/j.compbiomed.2020.103850
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520302092
N2  - Histone deacetylases (HDACs) play important roles in various biological processes, but are also notorious for their over-expression in numerous cancers and neurological disorders. Therefore, the development of isoform selective HDAC inhibitors is crucial in order to prevent any side effects of pan inhibition. This work focuses on identifying novel inhibitors for the selective inhibition of HDAC8, an isoform implicated in fatal diseases like T-cell lymphoma, colon cancer and childhood neuroblastoma. Virtual screening of the ‘In-trials’ subset of ZINC database has been carried out with the help of two pharmacophore models signifying potent and selective HDAC8 inhibition. A detailed molecular docking strategy, followed by molecular dynamics simulations and post-scoring with MM-GBSA calculations, has led to the identification of six promising molecules that have excellent binding with the HDAC8 active site. In order to establish the selectivity profile of these molecules, their binding to off-target HDAC isoforms has also been evaluated. Substitution analyses of the proposed inhibitors suggest that aromatic substituents that access the adjacent hydrophobic pocket of the HDAC8 active site have the potential to further enhance the HDAC8 selectivity.
ER  - 
TY  - JOUR
T1  - Cumulative link models for deep ordinal classification
A1  - Vargas, Víctor Manuel
A1  - Gutiérrez, Pedro Antonio
A1  - Hervás-Martínez, César
Y1  - 2020///
KW  -  Cumulative link models
KW  -  Kappa index
KW  -  Ordinal regression
KW  - Deep learning
JF  - Neurocomputing
VL  - 401
SP  - 48
EP  - 58
DO  - https://doi.org/10.1016/j.neucom.2020.03.034
UR  - https://www.sciencedirect.com/science/article/pii/S0925231220303805
N2  - This paper proposes a deep convolutional neural network model for ordinal regression by considering a family of probabilistic ordinal link functions in the output layer. The link functions are those used for cumulative link models, which are traditional statistical linear models based on projecting each pattern into a 1-dimensional space. A set of ordered thresholds splits this space into the different classes of the problem. In our case, the projections are estimated by a non-linear deep neural network. To further improve the results, we combine these ordinal models with a loss function that takes into account the distance between the categories, based on the weighted Kappa index. Three different link functions are studied in the experimental study, and the results are contrasted with a statistical analysis. The experiments run over two different ordinal classification problems and the statistical tests confirm that these models improve the results of a nominal model and outperform other robust proposals considered in the literature.
ER  - 
TY  - JOUR
T1  - Detection of mental fatigue state with wearable ECG devices
A1  - Huang, Shitong
A1  - Li, Jia
A1  - Zhang, Pengzhu
A1  - Zhang, Weiqiang
Y1  - 2018///
KW  -  ECG
KW  -  Feature selection
KW  -  HRV
KW  -  Machine learning
KW  -  Wearable devices
KW  - Mental fatigue
JF  - International Journal of Medical Informatics
VL  - 119
SP  - 39
EP  - 46
DO  - https://doi.org/10.1016/j.ijmedinf.2018.08.010
UR  - https://www.sciencedirect.com/science/article/pii/S1386505618305677
N2  - Overwork-related disorders, such as cerebrovascular/cardiovascular diseases (CCVD) and mental disorders due to overwork, are a major occupational and public health issue worldwide, particularly in East Asian countries. Since wearable smart devices are inexpensive, convenient, popular and widely available today, we were interested in investigating the possibility of using wearable smart electrocardiogram (ECG) devices to detect the mental fatigue state. In total, 35 healthy participants were recruited from a public university in East China. Throughout the entire experiment, each participant wore a wearable device that was further linked to a smartphone to upload the data based on Bluetooth transmission. To manipulate the fatigue state, each participant was asked to finish a quiz, which lasted for approximately 80 min, with 30 logical referential and computing problems and 25 memory tests. Eight heart rate variability (HRV) indicators namely NN.mean (mean of normal to normal interval), rMSSD (root mean square of successive differences), PNN50 (the proportion of NN50 divided by total number of NNs), TP (total spectral power), HF (high frequency from 0.15 Hz to 0.4 Hz), LF (low frequency from 0.04 Hz to 0.15 Hz), VLF (very low frequency from 0.0033 Hz to 0.04 Hz) and the LF/HF ratio were collected at intervals of 5 min throughout the entire experiment. After the feature selection was performed, six indicators remained for further analysis, which were the NN.mean, rMSSD, PNN50, TP, LF, and VLF. Four algorithms, support vector machine (SVM), K-nearest neighbor (KNN), naïve Bayes (NB), and logistic regression (LR), were used to build classifiers that automatically detected the fatigue state. The best performance was achieved by KNN, which had a CV accuracy of 75.5%. The NN.mean, PNN50, TP and LF were the most important HRV indicators for mental fatigue detection. KNN performed the best among the four algorithms and had an average CV accuracy of 65.37% for all of the possible feature combinations.
ER  - 
TY  - JOUR
T1  - Detecting genetic associations with brain imaging phenotypes in Alzheimer’s disease via a novel structured SCCA approach
A1  - Du, Lei
A1  - Liu, Kefei
A1  - Yao, Xiaohui
A1  - Risacher, Shannon L
A1  - Han, Junwei
A1  - Saykin, Andrew J
A1  - Guo, Lei
A1  - Shen, Li
Y1  - 2020///
KW  -  Fused pairwise group Lasso
KW  -  Graph guided pairwise group Lasso
KW  -  Sparse canonical correlation analysis (SCCA)
KW  - Brain imaging genetics
JF  - Medical Image Analysis
VL  - 61
SP  - 101656
EP  - 101656
DO  - https://doi.org/10.1016/j.media.2020.101656
UR  - https://www.sciencedirect.com/science/article/pii/S1361841520300232
N2  - Brain imaging genetics becomes an important research topic since it can reveal complex associations between genetic factors and the structures or functions of the human brain. Sparse canonical correlation analysis (SCCA) is a popular bi-multivariate association identification method. To mine the complex genetic basis of brain imaging phenotypes, there arise many SCCA methods with a variety of norms for incorporating different structures of interest. They often use the group lasso penalty, the fused lasso or the graph/network guided fused lasso ones. However, the group lasso methods have limited capability because of the incomplete or unavailable prior knowledge in real applications. The fused lasso and graph/network guided methods are sensitive to the sign of the sample correlation which may be incorrectly estimated. In this paper, we introduce two new penalties to improve the fused lasso and the graph/network guided lasso penalties in structured sparse learning. We impose both penalties to the SCCA model and propose an optimization algorithm to solve it. The proposed SCCA method has a strong upper bound of grouping effects for both positively and negatively highly correlated variables. We show that, on both synthetic and real neuroimaging genetics data, the proposed SCCA method performs better than or equally to the conventional methods using fused lasso or graph/network guided fused lasso. In particular, the proposed method identifies higher canonical correlation coefficients and captures clearer canonical weight patterns, demonstrating its promising capability in revealing biologically meaningful imaging genetic associations.
ER  - 
TY  - JOUR
T1  - A Bayesian network decision model for supporting the diagnosis of dementia, Alzheimer׳s disease and mild cognitive impairment
A1  - Seixas, Flávio Luiz
A1  - Zadrozny, Bianca
A1  - Laks, Jerson
A1  - Conci, Aura
A1  - Muchaluat Saade, Débora Christina
Y1  - 2014///
KW  -  Alzheimer׳s disease
KW  -  Bayesian network
KW  -  Dementia
KW  -  Mild cognitive impairment
KW  - Clinical decision support system
JF  - Computers in Biology and Medicine
VL  - 51
SP  - 140
EP  - 158
DO  - https://doi.org/10.1016/j.compbiomed.2014.04.010
UR  - https://www.sciencedirect.com/science/article/pii/S0010482514000961
N2  - Population aging has been occurring as a global phenomenon with heterogeneous consequences in both developed and developing countries. Neurodegenerative diseases, such as Alzheimer׳s Disease (AD), have high prevalence in the elderly population. Early diagnosis of this type of disease allows early treatment and improves patient quality of life. This paper proposes a Bayesian network decision model for supporting diagnosis of dementia, AD and Mild Cognitive Impairment (MCI). Bayesian networks are well-suited for representing uncertainty and causality, which are both present in clinical domains. The proposed Bayesian network was modeled using a combination of expert knowledge and data-oriented modeling. The network structure was built based on current diagnostic criteria and input from physicians who are experts in this domain. The network parameters were estimated using a supervised learning algorithm from a dataset of real clinical cases. The dataset contains data from patients and normal controls from the Duke University Medical Center (Washington, USA) and the Center for Alzheimer׳s Disease and Related Disorders (at the Institute of Psychiatry of the Federal University of Rio de Janeiro, Brazil). The dataset attributes consist of predisposal factors, neuropsychological test results, patient demographic data, symptoms and signs. The decision model was evaluated using quantitative methods and a sensitivity analysis. In conclusion, the proposed Bayesian network showed better results for diagnosis of dementia, AD and MCI when compared to most of the other well-known classifiers. Moreover, it provides additional useful information to physicians, such as the contribution of certain factors to diagnosis.
ER  - 
TY  - JOUR
T1  - Minimum class spread constrained support vector machine
A1  - Tao, JianWen
A1  - Wang, Shitong
A1  - Hu, Wenjun
Y1  - 2015///
KW  -  Kernel trick
KW  -  Novelty detection
KW  -  Support vector machines (SVMs)
KW  -  Within-class spread
KW  - Pattern classification
JF  - Neurocomputing
VL  - 151
SP  - 481
EP  - 500
DO  - https://doi.org/10.1016/j.neucom.2014.09.017
UR  - https://www.sciencedirect.com/science/article/pii/S0925231214011758
N2  - While classical classification methods such as support vector machine and its extensions (SVMs) obtain strong generalization capability by maximizing the separation margin between binary classes, they are usually sensitive to affine (or scaling) transformation of data. The optimal solutions of SVMs may be misled by the spread of data and preferentially separate classes along large margin directions. To this end, in this paper, we propose a novel minimum class spread constrained support vector machine (MCSSVM) for pattern classification problems by simultaneously considering the maximization of inter-class separation margin and the minimization of within-class spread of data, which corresponds to a distribution-dependent regularization on the classification function by constraining the within-class spread of input data. The basic idea of MCSSVM is to find a class distribution constrained hyperplane such that one class (or normal class) can be enclosed in a minimum q-spread tube, while the other class (or abnormal class) is farthest from this tube. MCSSVM can simultaneously achieve both maximum inter-class margin and minimum within-class spread so as to enhance the generalization capability of the proposed classifier. Moreover, the proposed method only requires simple extensions to existing maximum margin formulations such as SVMs and still preserves their computational efficiency. Generalization bound for MCSSVM is theoretically derived and the validity of MCSSVM is examined by classification of toy and real-world classification problems, which demonstrate the superiority of our method in comparison to other related state-of-the-art algorithms.
ER  - 
TY  - JOUR
T1  - Modeling hybrid rough set-based classification procedures to identify hemodialysis adequacy for end-stage renal disease patients
A1  - Chen, You-Shyang
Y1  - 2013///
KW  -  Classification model
KW  -  Feature selection
KW  -  HemoDialysis (HD) adequacy
KW  -  Rough Set Theory (RST)
KW  -  Urea Reduction Ratio (URR)
KW  - End-Stage Renal Disease (ESRD)
JF  - Computers in Biology and Medicine
VL  - 43
IS  - 10
SP  - 1590
EP  - 1605
DO  - https://doi.org/10.1016/j.compbiomed.2013.08.001
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513002151
N2  - Healthcare problems observed in the majority of end-stage renal disease (ESRD) patients regarding hemodialysis (HD) treatment are serious issues for the Taiwanese healthcare services, and an interesting topic is thus the adequacy of HD therapy. This study successfully models a hybrid procedure to measure HD adequacy to assess therapeutic effects and to explore the relationship between accuracy and coverage for interested parties. The proposed model has better accuracy, a lower standard deviation, and fewer attributes than the listed methods under various evaluation criteria. The study results are useful to subsequent researchers to develop suitable applications, and to ESRD patients and their doctors to ensure satisfactory medical quality.
ER  - 
TY  - JOUR
T1  - Multiple comparison procedures applied to model selection
A1  - Pizarro, Joaquı́n
A1  - Guerrero, Elisa
A1  - Galindo, Pedro L
Y1  - 2002///
KW  -  Generalization
KW  -  Multiple comparison procedures
KW  -  Network size
KW  -  Problem complexity
KW  - Model selection
JF  - Neurocomputing
VL  - 48
IS  - 1
SP  - 155
EP  - 173
DO  - https://doi.org/10.1016/S0925-2312(01)00653-1
UR  - https://www.sciencedirect.com/science/article/pii/S0925231201006531
N2  - This paper presents a new approach to model selection based on hypothesis testing. We first describe a procedure to generate different scores for any candidate model from a single sample of training data and then discuss how to apply multiple comparison procedures (MCP) to model selection. MCP statistical tests allow us to compare three or more groups of data while controlling the probability of making at least one Type I error. The complete procedure is illustrated on several model selection tasks, including the determination of the number of hidden units for feed-forward neural networks and the number of kernels for RBF networks.
ER  - 
TY  - JOUR
T1  - PPG signal motion artifacts correction algorithm based on feature estimation
A1  - Sun, Bin
A1  - Wang, Chengchao
A1  - Chen, Xiaohui
A1  - Zhang, Yongfang
A1  - Shao, Hanyu
Y1  - 2019///
KW  -  Gauss function
KW  -  Motion artifacts
KW  -  Reflection photoelectric detector
KW  -  Signal estimation
KW  - PPG signal
JF  - Optik
VL  - 176
SP  - 337
EP  - 349
DO  - https://doi.org/10.1016/j.ijleo.2018.09.085
UR  - https://www.sciencedirect.com/science/article/pii/S0030402618313883
N2  - In order to reduce the interference of human motion artifacts in PPG signals, when a reflective detector is used to extract signals, a novel motion artifacts correction algorithm based on Gaussian function decomposition and minimum mean square error estimation is proposed. Firstly, band pass filter and notch filter are used to reduce signal burr and interference of power frequency signal to get more accurate and smooth PPG signal, at same time, Kurtosis, Skewness, Standard Deviation are used to evaluate whether motion artifacts exist in a long series of PPG signals. Secondly, an improved sliding window method is used to detect the wave peaks and troughs of PPG signal and, then, take them as features of the PPG signal. Thirdly, according to extracted temporal features of quality PPG signals, the characteristics of the disturbed PPG signals by motion artifacts are estimated employing minimum mean square error estimation and corrected high-quality PPG signals are synthesized using the Gauss function. The experiment are carried out with human physiological signal extraction system designed by ourselves to show that the algorithm proposed in this paper can eliminate motion artifacts in PPG signal sequence well and human physiological parameters are calculated more accurate.
ER  - 
TY  - JOUR
T1  - Class imbalance and the curse of minority hubs
A1  - Tomašev, Nenad
A1  - Mladenić, Dunja
Y1  - 2013///
KW  -  -Nearest neighbor
KW  -  Class overlap
KW  -  Classification
KW  -  Curse of dimensionality
KW  -  Hubness
KW  - Class imbalance
JF  - Knowledge-Based Systems
VL  - 53
SP  - 157
EP  - 172
DO  - https://doi.org/10.1016/j.knosys.2013.08.031
UR  - https://www.sciencedirect.com/science/article/pii/S0950705113002682
N2  - Most machine learning tasks involve learning from high-dimensional data, which is often quite difficult to handle. Hubness is an aspect of the curse of dimensionality that was shown to be highly detrimental to k-nearest neighbor methods in high-dimensional feature spaces. Hubs, very frequent nearest neighbors, emerge as centers of influence within the data and often act as semantic singularities. This paper deals with evaluating the impact of hubness on learning under class imbalance with k-nearest neighbor methods. Our results suggest that, contrary to the common belief, minority class hubs might be responsible for most misclassification in many high-dimensional datasets. The standard approaches to learning under class imbalance usually clearly favor the instances of the minority class and are not well suited for handling such highly detrimental minority points. In our experiments, we have evaluated several state-of-the-art hubness-aware kNN classifiers that are based on learning from the neighbor occurrence models calculated from the training data. The experiments included learning under severe class imbalance, class overlap and mislabeling and the results suggest that the hubness-aware methods usually achieve promising results on the examined high-dimensional datasets. The improvements seem to be most pronounced when handling the difficult point types: borderline points, rare points and outliers. On most examined datasets, the hubness-aware approaches improve the classification precision of the minority classes and the recall of the majority class, which helps with reducing the negative impact of minority hubs. We argue that it might prove beneficial to combine the extensible hubness-aware voting frameworks with the existing class imbalanced kNN classifiers, in order to properly handle class imbalanced data in high-dimensional feature spaces.
ER  - 
TY  - JOUR
T1  - On probabilistic inference by weighted model counting
A1  - Chavira, Mark
A1  - Darwiche, Adnan
Y1  - 2008///
KW  -  Compilation
KW  -  Exact inference
KW  -  Weighted model counting
KW  - Bayesian networks
JF  - Artificial Intelligence
VL  - 172
IS  - 6
SP  - 772
EP  - 799
DO  - https://doi.org/10.1016/j.artint.2007.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S0004370207001889
N2  - A recent and effective approach to probabilistic inference calls for reducing the problem to one of weighted model counting (WMC) on a propositional knowledge base. Specifically, the approach calls for encoding the probabilistic model, typically a Bayesian network, as a propositional knowledge base in conjunctive normal form (CNF) with weights associated to each model according to the network parameters. Given this CNF, computing the probability of some evidence becomes a matter of summing the weights of all CNF models consistent with the evidence. A number of variations on this approach have appeared in the literature recently, that vary across three orthogonal dimensions. The first dimension concerns the specific encoding used to convert a Bayesian network into a CNF. The second dimensions relates to whether weighted model counting is performed using a search algorithm on the CNF, or by compiling the CNF into a structure that renders WMC a polytime operation in the size of the compiled structure. The third dimension deals with the specific properties of network parameters (local structure) which are captured in the CNF encoding. In this paper, we discuss recent work in this area across the above three dimensions, and demonstrate empirically its practical importance in significantly expanding the reach of exact probabilistic inference. We restrict our discussion to exact inference and model counting, even though other proposals have been extended for approximate inference and approximate model counting.
ER  - 
TY  - JOUR
T1  - Nonlinear dynamic approaches to identify atrial fibrillation progression based on topological methods
A1  - Safarbali, Bahareh
A1  - Hashemi Golpayegani, Seyed Mohammad Reza
Y1  - 2019///
KW  -  Dynamical system theory
KW  -  Fractal dimension
KW  -  Nonlinear signal processing
KW  -  Topological data analysis
KW  - Atrial fibrillation
JF  - Biomedical Signal Processing and Control
VL  - 53
SP  - 101563
EP  - 101563
DO  - https://doi.org/10.1016/j.bspc.2019.101563
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419301375
N2  - In recent years, atrial fibrillation (AF) development from paroxysmal to persistent or permanent forms has become an important issue in cardiovascular disorders. Information about AF pattern of presentation (paroxysmal, persistent, or permanent) was useful in the management of algorithms in each category. This management is aimed at reducing symptoms and stopping severe problems associated with AF. AF classification has been based on time duration and episodes until now. In particular, complexity changes in Heart Rate Variation (HRV) may contain clinically relevant signals of imminent systemic dysregulation. A number of nonlinear methods based on phase space and topological properties can give more insight into HRV abnormalities such as fibrillation. Aiming to provide a nonlinear tool to qualitatively classify AF stages, we proposed two geometrical indices (fractal dimension and persistent homology) based on HRV phase space, which can successfully replicate the changes in AF progression. The study population includes 38 lone AF patients and 20 normal subjects, which are collected from the Physio-Bank database. “Time of Life (TOL)” is proposed as a new feature based on the initial and final Čech radius in the persistent homology diagram. A neural network was implemented to prove the effectiveness of both TOL and fractal dimension as classification features. The accuracy of classification performance was 93%. The proposed indices provide a signal representation framework useful to understand the dynamic changes in AF cardiac patterns and to classify normal and pathological rhythms.
ER  - 
TY  - JOUR
T1  - Visual BMI estimation from face images using a label distribution based method
A1  - Jiang, Min
A1  - Guo, Guodong
A1  - Mu, Guowang
Y1  - 2020///
KW  -  Label distribution based method
KW  -  Two-stage learning framework
KW  - Visual BMI estimation
JF  - Computer Vision and Image Understanding
VL  - 197-198
SP  - 102985
EP  - 102985
DO  - https://doi.org/10.1016/j.cviu.2020.102985
UR  - https://www.sciencedirect.com/science/article/pii/S107731422030059X
N2  - Body mass index (BMI) analysis from face images is an interesting and challenging topic in machine learning and computer vision. Recent research shows that facial adiposity is associated with BMI prediction. In this work, we investigate the problem of visual BMI estimation from face images by a two-stage learning framework. BMI-related facial features are learned from the first stage. Then a label distribution based BMI estimator is learned by an optimization procedure that is implemented by projecting the features and assigned labels to a new domain which maximizing the correlation between them. Two label assignment strategies are analyzed for modeling the single BMI value as a discrete probability distribution over a range of BMIs. Extensive experiments are conducted on FIW-BMI, Morph II and VIP_attribute datasets. The experimental results show that the two-stage learning framework improves the performance step by step. More importantly, the proposed BMI estimator efficiently reduces the error. It outperforms regression based methods, two label distribution methods and two deep learning methods in most cases.
ER  - 
TY  - JOUR
T1  - Conditional random fields for clinical named entity recognition: A comparative study using Korean clinical texts
A1  - Lee, Wangjin
A1  - Kim, Kyungmo
A1  - Lee, Eun Young
A1  - Choi, Jinwook
Y1  - 2018///
KW  -  Conditional random field
KW  -  Discharge summary
KW  -  Medical history
KW  -  String matching
KW  - Clinical named entity recognition
JF  - Computers in Biology and Medicine
VL  - 101
SP  - 7
EP  - 14
DO  - https://doi.org/10.1016/j.compbiomed.2018.07.019
UR  - https://www.sciencedirect.com/science/article/pii/S0010482518302105
N2  - Background
This study demonstrates clinical named entity recognition (NER) methods on the clinical texts of rheumatism patients in South Korea. Despite the recent increase in the adoption rate of the electronic health record (EHR) system in global health institutions, health information technologies for handling and acquisition of information from numerous unstructured texts in the EHR system are still in their developing stages. The aim of this study is to verify the conventional named entity recognition (NER) methods, namely dictionary-lookup-based string matching and conditional random fields (CRFs).
Methods
We selected discharge summaries for 200 rheumatic patients from the EHR system of the Seoul National University Hospital and attempted to identify heterogeneous semantic types present in the clinical notes of each patient's history.
Results
CRFs outperform string matching in extracting most semantic types (median F1 = 0.761, minimum = 0.705, maximum = 0.906). String matching is found to be better suited for identifying hospital visit information. The performance of both methods is comparable for identifying medications. The 10-fold cross-validation shows that CRFs had median F1 = 0.811 (minimum = 0.752, maximum = 0.918), and exhibited good performance even when trained with simple features.
Conclusion
CRFs are a good candidate for implementing clinical NER in Korean clinical narrative documents. Increasing the training data and incorporating sophisticated feature engineering might improve the accuracy of identifying health information, enabling automated patient history summarization in the future.
ER  - 
TY  - JOUR
T1  - Discovering instance and process spanning constraints from process execution logs
A1  - Winter, Karolin
A1  - Stertz, Florian
A1  - Rinderle-Ma, Stefanie
Y1  - 2020///
KW  -  Constraint mining
KW  -  Instance spanning constraints
KW  -  Process mining
KW  - Digitalized compliance management
JF  - Information Systems
VL  - 89
SP  - 101484
EP  - 101484
DO  - https://doi.org/10.1016/j.is.2019.101484
UR  - https://www.sciencedirect.com/science/article/pii/S0306437919305368
N2  - Instance spanning constraints (ISC) are the instrument to establish controls across multiple instances of one or several processes. A multitude of applications crave for ISC support. Consider, for example, the bundling and unbundling of cargo across several instances of a logistics process or dependencies between examinations in different medical treatment processes. Non-compliance with ISC can lead to severe consequences and penalties, e.g., dangerous effects due to undesired drug interactions. ISC might stem from regulatory documents, extracted by domain experts. Another source for ISC are process execution logs. Process execution logs store execution information for process instances, and hence, inherently, the effects of ISC. Discovering ISC from process execution logs can support ISC design and implementation (if the ISC was not known beforehand) and the validation of the ISC during its life time. This work contributes a categorization of ISC as well as four discovery algorithms for ISC candidates from process execution logs. The discovered ISC candidates are put into context of the associated processes and can be further validated with domain experts. The algorithms are prototypically implemented and evaluated based on artificial and real-world process execution logs. The results facilitate ISC design as well as validation and hence contribute to a digitalized ISC and compliance management.
ER  - 
TY  - JOUR
T1  - Role of OpenEHR as an open source solution for the regional modelling of patient data in obstetrics
A1  - Pahl, Christina
A1  - Zare, Mojtaba
A1  - Nilashi, Mehrbakhsh
A1  - de Faria Borges, Marco Aurélio
A1  - Weingaertner, Daniel
A1  - Detschew, Vesselin
A1  - Supriyanto, Eko
A1  - Ibrahim, Othman
Y1  - 2015///
KW  -  Demographic data
KW  -  Dual model approach
KW  -  Electronic health
KW  -  Health record
KW  -  Information system
KW  -  Knowledge management
KW  -  Semantic interoperability
KW  - Archetypes
JF  - Journal of Biomedical Informatics
VL  - 55
SP  - 174
EP  - 187
DO  - https://doi.org/10.1016/j.jbi.2015.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415000714
N2  - This work investigates, whether openEHR with its reference model, archetypes and templates is suitable for the digital representation of demographic as well as clinical data. Moreover, it elaborates openEHR as a tool for modelling Hospital Information Systems on a regional level based on a national logical infrastructure. OpenEHR is a dual model approach developed for the modelling of Hospital Information Systems enabling semantic interoperability. A holistic solution to this represents the use of dual model based Electronic Healthcare Record systems. Modelling data in the field of obstetrics is a challenge, since different regions demand locally specific information for the process of treatment. Smaller health units in developing countries like Brazil or Malaysia, which until recently handled automatable processes like the storage of sensitive patient data in paper form, start organizational reconstruction processes. This archetype proof-of-concept investigation has tried out some elements of the openEHR methodology in cooperation with a health unit in Colombo, Brazil. Two legal forms provided by the Brazilian Ministry of Health have been analyzed and classified into demographic and clinical data. LinkEHR-Ed editor was used to read, edit and create archetypes. Results show that 33 clinical and demographic concepts, which are necessary to cover data demanded by the Unified National Health System, were identified. Out of the concepts 61% were reused and 39% modified to cover domain requirements. The detailed process of reuse, modification and creation of archetypes is shown. We conclude that, although a major part of demographic and clinical patient data were already represented by existing archetypes, a significant part required major modifications. In this study openEHR proved to be a highly suitable tool in the modelling of complex health data. In combination with LinkEHR-Ed software it offers user-friendly and highly applicable tools, although the complexity built by the vast specifications requires expert networks to define generally excepted clinical models. Finally, this project has pointed out main benefits enclosing high coverage of obstetrics data on the Clinical Knowledge Manager, simple modelling, and wide network and support using openEHR. Moreover, barriers described are enclosing the allocation of clinical content to respective archetypes, as well as stagnant adaption of changes on the Clinical Knowledge Manager leading to redundant efforts in data contribution that need to be addressed in future works.
ER  - 
TY  - JOUR
T1  - Ontology integration: Experience with medical terminologies
A1  - Lee, Yugyung
A1  - Supekar, Kaustubh
A1  - Geller, James
Y1  - 2006///
KW  -  Ontology
KW  -  Ontology integration
KW  -  Semantic enrichment
KW  -  Semantic integration
KW  -  Semantic refinement
KW  -  Semantics
KW  - Terminology
JF  - Computers in Biology and Medicine
VL  - 36
IS  - 7
SP  - 893
EP  - 919
DO  - https://doi.org/10.1016/j.compbiomed.2005.04.013
UR  - https://www.sciencedirect.com/science/article/pii/S0010482505000843
N1  - Special Issue on Medical Ontologies
N2  - To build a common controlled vocabulary is a formidable challenge in medical informatics. Due to vast scale and multiplicity in interpretation of medical data, it is natural to face overlapping terminologies in the process of practicing medical informatics [A. Rector, Clinical terminology: why is it so hard? Methods Inf. Med. 38 (1999) 239–252]. A major concern lies in the integration of seemingly overlapping terminologies in the medical domain and this issue has not been well addressed. In this paper, we describe a novel approach for medical ontology integration that relies on the theory of Algorithmic Semantic Refinement we previously developed. Our approach simplifies the task of matching pairs of corresponding concepts derived from a pair of ontologies, which is vital to terminology mapping. A formal theory and algorithm for our approach have been devised and the application of this method to two medical terminologies has been developed. The result of our work is an integrated medical terminology and a methodology and implementation ready to use for other ontology integration tasks.
ER  - 
TY  - JOUR
T1  - Grey wolf assisted dragonfly-based weighted rule generation for predicting heart disease and breast cancer
A1  - Moturi, Sireesha
A1  - Rao, S.N.Tirumala
A1  - Vemuru, Srikanth
Y1  - 2021///
KW  -  Hybrid classification
KW  -  Optimal feature selection
KW  -  Weighted rule generation
KW  - Disease prediction
JF  - Computerized Medical Imaging and Graphics
VL  - 91
SP  - 101936
EP  - 101936
DO  - https://doi.org/10.1016/j.compmedimag.2021.101936
UR  - https://www.sciencedirect.com/science/article/pii/S0895611121000859
N2  - Disease prediction plays a significant role in the life of people, as predicting the threat of diseases is necessary for citizens to live life in a healthy manner. The current development of data mining schemes has offered several systems that concern on disease prediction. Even though the disease prediction system includes more advantages, there are still many challenges that might limit its realistic use, such as the efficiency of prediction and information protection. This paper intends to develop an improved disease prediction model, which includes three phases: Weighted Coalesce rule generation, Optimized feature extraction, and Classification. At first, Coalesce rule generation is carried out after data transformation that involves normalization and sequential labeling. Here, rule generation is done based on the weights (priority level) assigned for each attribute by the expert. The support of each rule is multiplied with the proposed weighted function, and the resultant weighted support is compared with the minimum support for selecting the rules. Further, the obtained rule is subject to the optimal feature selection process. The hybrid classifiers that merge Support Vector Machine (SVM), and Deep Belief Network (DBN) takes the role of classification, which characterizes whether the patient is affected with the disease or not. In fact, the optimized feature selection process depends on a new hybrid optimization algorithm by linking the Grey Wolf Optimization (GWO) with Dragonfly Algorithm (DA) and hence, the presented model is termed as Grey Wolf Levy Updated-DA (GWU-DA). Here, the heart disease and breast cancer data are taken, where the efficiency of the proposed model is validated by comparing over the state-of-the-art models. From the analysis, the proposed GWU-DA model for accuracy is 65.98 %, 53.61 %, 42.27 %, 35.05 %, 34.02 %, 11.34 %, 13.4 %, 10.31 %, 9.28 % and 9.89 % better than CBA + CPAR, MKL + ANFIS, RF + EA, WCBA, IQR + KNN + PSO, NL-DA + SVM + DBN, AWFS-RA, HCS-RFRS, ADS-SM-DNN and OSSVM-HGSA models at 60th learning percentage.
ER  - 
TY  - JOUR
T1  - Machine learning in diagnosis and disability prediction of multiple sclerosis using optical coherence tomography
A1  - Montolío, Alberto
A1  - Martín-Gallego, Alejandro
A1  - Cegoñino, José
A1  - Orduna, Elvira
A1  - Vilades, Elisa
A1  - Garcia-Martin, Elena
A1  - del Palomar, Amaya Pérez
Y1  - 2021///
KW  -  Expanded disability status scale
KW  -  Machine learning
KW  -  Optical coherence tomography
KW  -  Retinal nerve fiber layer
KW  - Multiple sclerosis
JF  - Computers in Biology and Medicine
VL  - 133
SP  - 104416
EP  - 104416
DO  - https://doi.org/10.1016/j.compbiomed.2021.104416
UR  - https://www.sciencedirect.com/science/article/pii/S0010482521002109
N2  - Background
Multiple sclerosis (MS) is a neurodegenerative disease that affects the central nervous system, especially the brain, spinal cord, and optic nerve. Diagnosis of this disease is a very complex process and generally requires a lot of time. In addition, treatments are applied without any information on the disability course in each MS patient. For these two reasons, the objective of this study was to improve the MS diagnosis and predict the long-term course of disability in MS patients based on clinical data and retinal nerve fiber layer (RNFL) thickness, measured by optical coherence tomography (OCT).
Material and methods
A total of 104 healthy controls and 108 MS patients, 82 of whom had a 10-year follow-up, were enrolled. Classification algorithms such as multiple linear regression (MLR), support vector machines (SVM), decision tree (DT), k-nearest neighbours (k-NN), Naïve Bayes (NB), ensemble classifier (EC) and long short-term memory (LSTM) recurrent neural network were tested to develop two predictive models: MS diagnosis model and MS disability course prediction model.
Results
For MS diagnosis, the best result was obtained using EC (accuracy: 87.7%; sensitivity: 87.0%; specificity: 88.5%; precision: 88.7%; AUC: 0.8775). In line with this good performance, the accuracy was 85.4% using k-NN and 84.4% using SVM. And, for long-term prediction of MS disability course, LSTM recurrent neural network was the most appropriate classifier (accuracy: 81.7%; sensitivity: 81.1%; specificity: 82.2%; precision: 78.9%; AUC: 0.8165). The use of MLR, SVM and k-NN also showed a good performance (AUC ≥ 0.8).
Conclusions
This study demonstrated that machine learning techniques, using clinical and OCT data, can help establish an early diagnosis and predict the course of MS. This advance could help clinicians select more specific treatments for each MS patient. Therefore, our findings underscore the potential of RNFL thickness as a reliable MS biomarker.
ER  - 
TY  - JOUR
T1  - Diagnosis of urinary tract infection based on artificial intelligence methods
A1  - Ozkan, Ilker Ali
A1  - Koklu, Murat
A1  - Sert, Ibrahim Unal
Y1  - 2018///
KW  - Artificial intelligence methods
KW  - Artificial neural networks
KW  - Decision tree
KW  - Medical decision support systems
KW  - Random forest
KW  - Support vector machine
KW  - Urinary tract infection
JF  - Computer Methods and Programs in Biomedicine
VL  - 166
SP  - 51
EP  - 59
DO  - https://doi.org/10.1016/j.cmpb.2018.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S0169260718302803
N2  - Background and Objective
Urinary tract infection (UTI) is a common disease affecting the vast majority of people. UTI involves a simple infection caused by urinary tract inflammation as well as a complicated infection that may be caused by an inflammation of other urinary tract organs. Since all of these infections have similar symptoms, it is difficult to identify the cause of primary infection. Therefore, it is not easy to diagnose a UTI with routine examination procedures. Invasive methods that require surgery could be necessary. This study aims to develop an artificial intelligence model to support the diagnosis of UTI with complex symptoms.
Methods
Firstly, routine examination data and definitive diagnosis results for 59 UTI patients gathered and composed as a UTI dataset. Three classification models namely; decision tree (DT), support vector machine (SVM), random forest (RF) and artificial neural network (ANN), which are widely used in medical diagnosis systems, were created to model the definitive diagnosis results using the composed UTI dataset. Accuracy, specificity and sensitivity statistical measurements were used to determine the performance of created models.
Results
DT, SVM, RF and ANN models have 93.22%, 96.61%, 96.61%, 98.30% accuracy, 95.55%, 97.77%, 95.55%, 97.77% sensitivity and 85.71%, 92.85%, 100%, 100% specificy results, respectively.
Conclusions
ANN has the highest accuracy result of 98.3% for UTI diagnosis within the proposed models. Although several symptoms, laboratory findings, and ultrasound results are needed for clinical UTI diagnosis, this ANN model only needs pollacuria, suprapubic pain symptoms and erythrocyte to get the same diagnosis with such accuracy. This proposed model is a successful medical decision support system for UTI with complex symptoms. Usage of this artificial intelligence method has its advantages of lower diagnosis cost, lower diagnosis time and there is no need for invasive methods.
ER  - 
TY  - JOUR
T1  - Learning patient-specific predictive models from clinical data
A1  - Visweswaran, Shyam
A1  - Angus, Derek C
A1  - Hsieh, Margaret
A1  - Weissfeld, Lisa
A1  - Yealy, Donald
A1  - Cooper, Gregory F
Y1  - 2010///
KW  -  Algorithm
KW  -  Bayesian model averaging
KW  -  Bayesian networks
KW  -  Markov blanket
KW  -  Population-wide
KW  -  Prediction
KW  - Patient-specific
JF  - Journal of Biomedical Informatics
VL  - 43
IS  - 5
SP  - 669
EP  - 685
DO  - https://doi.org/10.1016/j.jbi.2010.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046410000560
N2  - We introduce an algorithm for learning patient-specific models from clinical data to predict outcomes. Patient-specific models are influenced by the particular history, symptoms, laboratory results, and other features of the patient case at hand, in contrast to the commonly used population-wide models that are constructed to perform well on average on all future cases. The patient-specific algorithm uses Markov blanket (MB) models, carries out Bayesian model averaging over a set of models to predict the outcome for the patient case at hand, and employs a patient-specific heuristic to locate a set of suitable models to average over. We evaluate the utility of using a local structure representation for the conditional probability distributions in the MB models that captures additional independence relations among the variables compared to the typically used representation that captures only the global structure among the variables. In addition, we compare the performance of Bayesian model averaging to that of model selection. The patient-specific algorithm and its variants were evaluated on two clinical datasets for two outcomes. Our results provide support that the performance of an algorithm for learning patient-specific models can be improved by using a local structure representation for MB models and by performing Bayesian model averaging.
ER  - 
TY  - JOUR
T1  - Partially Lazy Classification of Cardiovascular Risk via Multi-way Graph Cut Optimization
A1  - Fathalla, Karma M
A1  - Ekárt, Anikó
A1  - Gherghel, Doina
Y1  - 2018///
KW  -  Retinal Vessel Analysis
KW  -  genetic algorithm
KW  -  graph cut optimization
KW  -  lazy classification
KW  - Cardiovascular disease
JF  - Procedia Computer Science
VL  - 126
SP  - 576
EP  - 585
DO  - https://doi.org/10.1016/j.procs.2018.07.292
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918312687
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia
N2  - Cardiovascular disease (CVD) is considered a leading cause of human mortality with rising trends worldwide. Therefore, early identification of seemingly healthy subjects at risk is a priority. For this purpose, we propose a novel classification algorithm that provides a sound individual risk prediction, based on a non-invasive assessment of retinal vascular function. So-called lazy classification methods offer reduced time complexity by saving model construction time and better adapting to newly available instances, when compared to well-known eager methods. Lazy methods are widely used due to their simplicity and competitive performance. However, traditional lazy approaches are more vulnerable to noise and outliers, due to their full reliance on the instances’ local neighbourhood for classification. In this work, a learning method based on Graph Cut Optimization called GCO mine is proposed, which considers both the local arrangements and the global structure of the data, resulting in improved performance relative to traditional lazy methods. We compare GCO mine coupled with genetic algorithms (hGCO mine) with established lazy and eager algorithms to predict cardiovascular risk based on Retinal Vessel Analysis (RVA) data. The highest accuracy of 99.52% is achieved by hGCO mine. The performance of GCO mine is additionally demonstrated on 12 benchmark medical datasets from the UCI repository. In 8 out of 12 datasets, GCO mine outperforms its counterparts. GCO mine is recommended for studies where new instances are expected to be acquired over time, as it saves model creation time and allows for better generalization compared to state of the art methods.
ER  - 
TY  - JOUR
T1  - A belief rule-based decision support system for clinical risk assessment of cardiac chest pain
A1  - Kong, Guilan
A1  - Xu, Dong-Ling
A1  - Body, Richard
A1  - Yang, Jian-Bo
A1  - Mackway-Jones, Kevin
A1  - Carley, Simon
Y1  - 2012///
KW  -  Belief rule base
KW  -  Clinical risk assessment
KW  -  Evidential reasoning approach
KW  -  OR in medicine
KW  -  Uncertainty modeling
KW  - Decision support systems
JF  - European Journal of Operational Research
VL  - 219
IS  - 3
SP  - 564
EP  - 573
DO  - https://doi.org/10.1016/j.ejor.2011.10.044
UR  - https://www.sciencedirect.com/science/article/pii/S0377221711009842
N1  - Feature Clusters
N2  - This paper describes a prototype clinical decision support system (CDSS) for risk stratification of patients with cardiac chest pain. A newly developed belief rule-based inference methodology-RIMER was employed for developing the prototype. Based on the belief rule-based inference methodology, the prototype CDSS can deal with uncertainties in both clinical domain knowledge and clinical data. Moreover, the prototype can automatically update its knowledge base via a belief rule base (BRB) learning module which can adjust BRB through accumulated historical clinical cases. The domain specific knowledge used to construct the knowledge base of the prototype was learned from real patient data. We simulated a set of 1000 patients in cardiac chest pain to validate the prototype. The belief rule-based prototype CDSS has been found to perform extremely well. Firstly, the system can provide more reliable and informative diagnosis recommendations than manual diagnosis using traditional rules when there are clinical uncertainties. Secondly, the diagnostic performance of the system can be significantly improved after training the BRB through accumulated clinical cases.
ER  - 
TY  - JOUR
T1  - Linear tree
A1  - Gama, João
A1  - Brazdil, Pavel
Y1  - 1999///
KW  -  Constructive induction
KW  -  Machine learning
KW  - Multivariate decision trees
JF  - Intelligent Data Analysis
VL  - 3
IS  - 1
SP  - 1
EP  - 22
DO  - https://doi.org/10.1016/S1088-467X(99)00002-5
UR  - https://www.sciencedirect.com/science/article/pii/S1088467X99000025
N2  - In this paper we present system Ltree for propositional supervised learning. Ltree is able to define decision surfaces both orthogonal and oblique to the axes defined by the attributes of the input space. This is done combining a decision tree with a linear discriminant by means of constructive induction. At each decision node Ltree defines a new instance space by insertion of new attributes that are projections of the examples that fall at this node over the hyper-planes given by a linear discriminant function. This new instance space is propagated down through the tree. Tests based on those new attributes are oblique with respect to the original input space. Ltree is a probabilistic tree in the sense that it outputs a class probability distribution for each query example. The class probability distribution is computed at learning time, taking into account the different class distributions on the path from the root to the actual node. We have carried out experiments on twenty one benchmark datasets and compared our system with other well known decision tree systems (orthogonal and oblique) like C4.5, OC1, LMDT, and CART. On these datasets we have observed that our system has advantages in what concerns accuracy and learning times at statistically significant confidence levels.
ER  - 
TY  - JOUR
T1  - Learning and inference in knowledge-based probabilistic model for medical diagnosis
A1  - Jiang, Jingchi
A1  - Li, Xueli
A1  - Zhao, Chao
A1  - Guan, Yi
A1  - Yu, Qiubin
Y1  - 2017///
KW  -  First-order knowledge
KW  -  Gradient descent
KW  -  Markov logic network
KW  -  Markov network
KW  - Probabilistic model
JF  - Knowledge-Based Systems
VL  - 138
SP  - 58
EP  - 68
DO  - https://doi.org/10.1016/j.knosys.2017.09.030
UR  - https://www.sciencedirect.com/science/article/pii/S0950705117304495
N2  - Based on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for creating a medical knowledge network (MKN) in medical diagnosis. When a set of evidence is activated for a specific patient, we can generate a ground medical knowledge network that is composed of evidence nodes and potential disease nodes. By incorporating a Boltzmann machine into the potential function of a Markov network, we investigated the joint probability distribution of the MKN. To consider numerical evidence, a multivariate inference model is presented that uses conditional probability. In addition, the weights for the knowledge graph are efficiently learned from manually annotated Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). In our experiments, we found numerically that an improved expression of evidence variables is necessary for medical diagnosis. Our experimental results comparing a Markov logic network and six kinds of classic machine learning algorithms on the actual CEMR database and BER database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis.
ER  - 
TY  - JOUR
T1  - Ophthalmic diagnosis using deep learning with fundus images – A critical review
A1  - Sengupta, Sourya
A1  - Singh, Amitojdeep
A1  - Leopold, Henry A
A1  - Gulati, Tanmay
A1  - Lakshminarayanan, Vasudevan
Y1  - 2020///
KW  -  Classification
KW  -  Deep learning
KW  -  Fundus image datasets
KW  -  Image segmentation
KW  -  Ophthalmology
KW  -  Retina
KW  - Fundus photos
JF  - Artificial Intelligence in Medicine
VL  - 102
SP  - 101758
EP  - 101758
DO  - https://doi.org/10.1016/j.artmed.2019.101758
UR  - https://www.sciencedirect.com/science/article/pii/S0933365719305858
N2  - An overview of the applications of deep learning for ophthalmic diagnosis using retinal fundus images is presented. We describe various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, optic cup, blood vessels as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma, and diabetic retinopathy are also discussed. Important critical insights and future research directions are given.
ER  - 
TY  - JOUR
T1  - Studies in the use of data mining, prediction algorithms, and a universal exchange and inference language in the analysis of socioeconomic health data
A1  - Robson, Barry
A1  - Boray, S
Y1  - 2019///
KW  -  Bayes net
KW  -  Data analytics
KW  -  Data mining
KW  -  Decision support
KW  -  Hyperbolic Dirac net
KW  -  Inference net
KW  -  Socioeconomic
KW  -  Sparse data
KW  - Population health
JF  - Computers in Biology and Medicine
VL  - 112
SP  - 103369
EP  - 103369
DO  - https://doi.org/10.1016/j.compbiomed.2019.103369
UR  - https://www.sciencedirect.com/science/article/pii/S001048251930246X
N2  - While clinical and biomedical information in digital form has been escalating, it is socioeconomic factors that are important determinants of health on the national and global scale. We show how collective use of data mining and prediction algorithms to analyze socioeconomic population health data can stand beside classical correlation analysis in routine data analysis. The underlying theoretical basis is the Dirac notation and algebra that is a scientific standard but unusual outside of the physical sciences, combined with a theory of expected information first developed for analyzing sparse data but still largely confined to bioinformatics. The latter was important here because the records analyzed (which are for US counties and equivalents, not patients) are very few by contemporary data mining standards. The approach is very unlikely to be familiar to socioeconomic researchers, so the theory and the advantages of our inference nets over the Bayes Net are reviewed here, mostly using socioeconomic examples. While our expertise and focus is in regard to novel analytical methods rather than socioeconomics per se, a significant negative (countertrending) relationship between population health and equity was initially surprising, at least to the present authors. This encouraged deeper exploration including that of the relationship between our data mining methods and traditional Pearson's correlation. The latter is susceptible to giving wrong conclusions if a phenomenon called Simpson's paradox applies, so this is also investigated. Also discussed is that, even for very few records, associative data mining can still demand significant computational resources due to a combinatorial explosion.
ER  - 
TY  - JOUR
T1  - A novel approach for discretization of continuous attributes in rough set theory
A1  - Jiang, Feng
A1  - Sui, Yuefei
Y1  - 2015///
KW  -  Cuts
KW  -  Discretization
KW  -  Multivariate
KW  -  Supervised
KW  - Rough sets
JF  - Knowledge-Based Systems
VL  - 73
SP  - 324
EP  - 334
DO  - https://doi.org/10.1016/j.knosys.2014.10.014
UR  - https://www.sciencedirect.com/science/article/pii/S0950705114003815
N2  - Discretization of continuous attributes is an important task in rough sets and many discretization algorithms have been proposed. However, most of the current discretization algorithms are univariate, which may reduce the classification ability of a given decision table. To solve this problem, we propose a supervised and multivariate discretization algorithm — SMDNS in rough sets, which is derived from the traditional algorithm naive scaler (called Naive). Given a decision table DT=(U,C,D,V,f), since SMDNS uses both class information and the interdependence among various condition attributes in C to determine the discretization scheme, the cuts obtained by SMDNS are much less than those obtained by Naive, while the classification ability of DT remains unchanged after discretization. Experimental results show that SMDNS is efficient in terms of the classification accuracy and the number of generated cuts. In particular, our algorithm can obtain a satisfactory compromise between the number of cuts and the classification accuracy.
ER  - 
TY  - JOUR
T1  - A reliable ensemble based approach to semi-supervised learning
A1  - de Vries, Sjoerd
A1  - Thierens, Dirk
Y1  - 2021///
KW  -  Out-of-bag error
KW  -  Ranking
KW  -  Self-training
KW  -  Semi-supervised learning
KW  -  Wrapper
KW  - Ensemble learning
JF  - Knowledge-Based Systems
VL  - 215
SP  - 106738
EP  - 106738
DO  - https://doi.org/10.1016/j.knosys.2021.106738
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121000010
N2  - Semi-supervised learning (SSL) methods attempt to achieve better classification of unseen data through the use of unlabeled data than can be achieved by learning from the available labeled data alone. Most SSL methods require the user to familiarize themselves with novel, complex concepts and to ensure the underlying assumptions made by these methods match the problem structure, or they risk a decrease in predictive performance. In this paper, we present the reliable semi-supervised ensemble learning (RESSEL) method, which exploits unlabeled data by using it to generate diverse classifiers through self-training and combines these classifiers into an ensemble for prediction. Our method functions as a wrapper around a supervised base classifier and refrains from introducing additional problem dependent assumptions. We conduct experiments on a number of commonly used data sets to prove its merit. The results show RESSEL improves significantly upon the supervised alternatives, provided that the base classifier which is used is able to produce adequate probability-based rankings. It is shown that RESSEL is reliable in that it delivers results comparable to supervised learning methods if this requirement is not met, while the method also broadens the range of good parameter values. Furthermore, RESSEL is demonstrated to outperform existing self-labeled wrapper approaches.
ER  - 
TY  - JOUR
T1  - Impact of image spatial, temporal, and velocity resolutions on cardiovascular indices derived from color-Doppler echocardiography
A1  - Rojo-Álvarez, José Luis
A1  - Bermejo, Javier
A1  - Rodríguez-González, Ana Belén
A1  - Martínez-Fernández, Andrés
A1  - Yotti, Raquel
A1  - García-Fernández, Miguel A
A1  - Carlos Antoranz, José
Y1  - 2007///
KW  -  Euler equation
KW  -  Intracardiac pressure gradient
KW  -  Myocardial strain
KW  -  Resolution
KW  -  Tissue Doppler imaging
KW  - Color-Doppler echocardiography
JF  - Medical Image Analysis
VL  - 11
IS  - 6
SP  - 513
EP  - 525
DO  - https://doi.org/10.1016/j.media.2007.04.004
UR  - https://www.sciencedirect.com/science/article/pii/S1361841507000357
N2  - Quantitative processing of color-Doppler echocardiographic images has substantially improved noninvasive assessment of cardiac physiology. Many indices are computed from the velocity fields derived either from color-Doppler tissue imaging (DTI), such as acceleration, strain and strain-rate, or from blood-flow color-Doppler, such as intracardiac pressure gradients (ICPG). All of these indices are dependent on the finite resolution of the ultrasound scanner. Therefore, we developed an image-dependent method for assessing the influence of temporal, spatial, and velocity resolutions, on cardiovascular parameters derived from velocity images. In order to focus our study on the spatial, temporal, and velocity resolutions of the digital image, we did not consider the effect of other sources of noise such as the interaction between ultrasound and tissue. A simple first-order Taylor’s expansion was used to establish the functional relationship between the acquired image velocity and the calculated cardiac index. Resolutions were studied on: (a) myocardial acceleration, strain, and strain-rate from DTI, and (b) ICPG from blood-flow color-Doppler. The performance of Taylor’s-based error bounds (TBEB) was demonstrated on simulated models and illustrated on clinical images. Velocity and temporal resolution were highly relevant for the accuracy of DTI-derived parameters and ICPGs. TBEB allow to assess the effects of ideal digital image resolution on quantitative cardiovascular indices derived from velocity measurements obtained by cardiac imaging techniques.
ER  - 
TY  - JOUR
T1  - A Genetic Programming approach for feature selection in highly dimensional skewed data
A1  - Viegas, Felipe
A1  - Rocha, Leonardo
A1  - Gonçalves, Marcos
A1  - Mourão, Fernando
A1  - Sá, Giovanni
A1  - Salles, Thiago
A1  - Andrade, Guilherme
A1  - Sandin, Isac
Y1  - 2018///
KW  -  Classification
KW  -  Genetic Programming
KW  - Feature selection
JF  - Neurocomputing
VL  - 273
SP  - 554
EP  - 569
DO  - https://doi.org/10.1016/j.neucom.2017.08.050
UR  - https://www.sciencedirect.com/science/article/pii/S0925231217314716
N2  - High dimensionality, also known as the curse of dimensionality, is still a major challenge for automatic classification solutions. Accordingly, several feature selection (FS) strategies have been proposed for dimensionality reduction over the years. However, they potentially perform poorly in face of unbalanced data. In this work, we propose a novel feature selection strategy based on Genetic Programming, which is resilient to data skewness issues, in other words, it works well with both, balanced and unbalanced data. The proposed strategy aims at combining the most discriminative feature sets selected by distinct feature selection metrics in order to obtain a more effective and impartial set of the most discriminative features, departing from the hypothesis that distinct feature selection metrics produce different (and potentially complementary) feature space projections. We evaluated our proposal in biological and textual datasets. Our experimental results show that our proposed solution not only increases the efficiency of the learning process, reducing up to 83% the size of the data space, but also significantly increases its effectiveness in some scenarios.
ER  - 
TY  - JOUR
T1  - Automated enhancement of description logic-defined terminologies to facilitate mapping to ICD9-CM
A1  - Elkin, P L
A1  - Brown, S H
Y1  - 2002///
KW  - Controlled health terminologies
KW  - Description logic
KW  - ICD9-CM
KW  - Medical informatics
KW  - SNOMED
JF  - Journal of Biomedical Informatics
VL  - 35
IS  - 5
SP  - 281
EP  - 288
DO  - https://doi.org/10.1016/S1532-0464(03)00019-4
UR  - https://www.sciencedirect.com/science/article/pii/S1532046403000194
N2  - Compositional (post-coordinated) terminologies are one potential solution to the problem of content completeness. However, they have the potential to render data incomparable. For computers to determine that compositional expressions are comparable, the relations between the composed components that are understood implicitly by human readers must be represented explicitly for computer manipulation. We discuss a technique for discovering and formalizing the implicit semantic relationships in two vocabularies: the International Classification of Disease Version 9 Clinical Modification (ICD9-CM), and SNOMED-Reference Terminology (SNOMED-RT). The results of this technique are used to augment the existing SNOMED-RT relation ontology [1], which is a necessary step in automated concept mapping between systems. The reference terminology must contain all the semantics implicit in the classification in order to map concepts between the two representations. We also provide an explicit representation of the implied semantics of ICD9-CM. This tabulation will be useful for other knowledge engineering efforts involving ICD9-CM.
ER  - 
TY  - JOUR
T1  - Topological Analysis of Large-scale Biomedical Terminology Structures
A1  - Bales, Michael E
A1  - Lussier, Yves A
A1  - Johnson, Stephen B
Y1  - 2007///
JF  - Journal of the American Medical Informatics Association
VL  - 14
IS  - 6
SP  - 788
EP  - 797
DO  - https://doi.org/10.1197/jamia.M2080
UR  - https://www.sciencedirect.com/science/article/pii/S1067502707002113
N2  - Objective
To characterize global structural features of large-scale biomedical terminologies using currently emerging statistical approaches.
Design
Given rapid growth of terminologies, this research was designed to address scalability. We selected 16 terminologies covering a variety of domains from the UMLS Metathesaurus, a collection of terminological systems. Each was modeled as a network in which nodes were atomic concepts and links were relationships asserted by the source vocabulary. For comparison against each terminology we created three random networks of equivalent size and density.
Measurements
Average node degree, node degree distribution, clustering coefficient, average path length.
Results
Eight of 16 terminologies exhibited the small-world characteristics of a short average path length and strong local clustering. An overlapping subset of nine exhibited a power law distribution in node degrees, indicative of a scale-free architecture. We attribute these features to specific design constraints. Constraints on node connectivity, common in more synthetic classification systems, localize the effects of changes and deletions. In contrast, small-world and scale-free features, common in comprehensive medical terminologies, promote flexible navigation and less restrictive organic-like growth.
Conclusion
While thought of as synthetic, grid-like structures, some controlled terminologies are structurally indistinguishable from natural language networks. This paradoxical result suggests that terminology structure is shaped not only by formal logic-based semantics, but by rules analogous to those that govern social networks and biological systems. Graph theoretic modeling shows early promise as a framework for describing terminology structure. Deeper understanding of these techniques may inform the development of scalable terminologies and ontologies.
ER  - 
TY  - JOUR
T1  - Optimizing wavelet neural networks using modified cuckoo search for multi-step ahead chaotic time series prediction
A1  - Ong, Pauline
A1  - Zainuddin, Zarita
Y1  - 2019///
KW  -  Cuckoo search algorithm
KW  -  Metaheuristic algorithm
KW  -  Translation vector
KW  -  Wavelet neural networks
KW  - Chaotic time series
JF  - Applied Soft Computing
VL  - 80
SP  - 374
EP  - 386
DO  - https://doi.org/10.1016/j.asoc.2019.04.016
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619302078
N2  - Determining the optimal number of hidden nodes and their proper initial locations are essentially crucial before the wavelet neural networks (WNNs) start their learning process. In this paper, a novel strategy known as the modified cuckoo search algorithm (MCSA), is proposed for WNNs initialization in order to improve its generalization performance. The MCSA begins with an initial population of cuckoo eggs, which represent the translation vectors of the wavelet hidden nodes, and subsequently refines their locations by imitating the breeding mechanism of cuckoos. The resulting solutions from the MCSA are then used as the initial translation vectors for the WNNs. The feasibility of the proposed method is evaluated by forecasting a benchmark chaotic time series, and its superior prediction accuracy compared with that of conventional WNNs demonstrates its potential benefit.
ER  - 
TY  - JOUR
T1  - Patient-recognition data-mining model for BCG-plus interferon immunotherapy bladder cancer treatment
A1  - Shah, Shital C
A1  - Kusiak, Andrew
A1  - O’Donnell, Michael A
Y1  - 2006///
KW  -  BCG/IFN- immunotherapy
KW  -  Bladder cancer
KW  -  Patient recognition model
KW  - Data mining
JF  - Computers in Biology and Medicine
VL  - 36
IS  - 6
SP  - 634
EP  - 655
DO  - https://doi.org/10.1016/j.compbiomed.2005.03.007
UR  - https://www.sciencedirect.com/science/article/pii/S0010482505000557
N2  - Bladder cancer is the fifth most common malignant disease in the United States with an annual incidence of around 63,210 new cases and 13,180 deaths. The cost for providing care for patients with bladder cancer disease is high. Bladder cancer treatment options such as immunotherapy, chemotherapy, radiation therapy, transurethral resection, and cystectomy, are used with varying success rates. In this research, data from a nationwide bacillus Calmette-Gue´rin (BCG) plus interferon-alpha (IFN-α) immunotherapy clinical trial was considered. Data mining algorithms were used to analyze the effectiveness of immunotherapy treatment and to understand the prominent parameters and their interactions. The extracted knowledge was used to build a patient recognition model for prediction of treatment outcomes. The data was analyzed to understand the impact of various parameters on the treatment outcome. A list of significant parameters such as cumulative tumor size, presence of residual disease, stages of prior bladder cancer, current state of bladder cancer, and the presence of current bladder cancer (T1) is provided. The decision-making approach outlined in the paper supplemented with additional knowledge bases will lead to a comprehensive analytical road map of the BCG/IFN-α immunotherapy treatment. It will provide individualized guidelines for each stage of the treatment as well as measure the success of the treatment.
ER  - 
TY  - JOUR
T1  - Highlighting heterogeneous samples to support vector machines’ training
A1  - Yang, Chan-Yun
Y1  - 2008///
KW  -  Classification
KW  -  Loss function
KW  -  Pattern recognition
KW  -  Training accuracy
KW  - Support vector machines
JF  - Neurocomputing
VL  - 72
IS  - 1
SP  - 218
EP  - 230
DO  - https://doi.org/10.1016/j.neucom.2008.01.021
UR  - https://www.sciencedirect.com/science/article/pii/S0925231208001422
N1  - Machine Learning for Signal Processing (MLSP 2006) / Life System Modelling, Simulation, and Bio-inspired Computing (LSMS 2007)
N2  - Since the loss function is so important in statistical learning, this paper proposes the concept of adding heavier penalties to the heterogeneous examples of a dataset to achieve a stricter convex loss function for optimization. The concept was realized by changing the class labels of support vector machines (SVM) into greater real values. Using the magnified real-valued class labels to convey the additional penalties, an elementary stage-wise classifier was developed to achieve a high training accuracy. In this article, the original theory and induced corresponding properties of the stage-wise classifier are presented for further applications. Two types of re-weighting rules were devised in the connection of consecutive stages to produce the heavier penalties. Compared to a qualified underlying prototype, the empirical results showed that the classification complexity of the proposed classifier was increased accordingly as the accuracy of the classifier was improved due to various additional penalties. Although the stricter penalties might cause an undesirable over-fitting, the flexible re-weighting strategy is still beneficial for some application.
ER  - 
TY  - JOUR
T1  - RENAL DISEASE HEALTHCARE CENTER BASED ON A COMPUTER PHYSIOLOGICAL IMAGE
A1  - PATIENT, O F T H E
A1  - Prado, M
A1  - Roa, L
A1  - Reina-Tosina, J
A1  - Palma, A
A1  - Milán, J A
Y1  - 2002///
KW  -  dynamic modelling
KW  -  knowledge-based systems
KW  -  programming approaches
KW  -  simulation
KW  -  telematics
KW  - Medical Systems
JF  - IFAC Proceedings Volumes
VL  - 35
IS  - 1
SP  - 169
EP  - 173
DO  - https://doi.org/10.3182/20020721-6-ES-1901.01337
UR  - https://www.sciencedirect.com/science/article/pii/S1474667015397585
N1  - 15th IFAC World Congress
N2  - The Patient Physiological Image (PPI) is a novel concept that manages the knowledge of Virtual Center for Renal Support (VCRS), currently being developed by the Biomedical Engineering Group of the University of Seville (GIB). PPI is a “virtual” replica of a patient, built by means of a mathematical model. From a technical point of view, PPI is a component-oriented software module based on cutting-edge of Modeling and Simulation Technology. In this paper we first state the features that the PPI component must fulfil to support the VCRS functionality. From those, the VCRS-PPI software architecture is described.
ER  - 
TY  - JOUR
T1  - The multiscale bowler-hat transform for blood vessel enhancement in retinal images
A1  - Sazak, Çiğdem
A1  - Nelson, Carl J
A1  - Obara, Boguslaw
Y1  - 2019///
KW  -  Blood vessel enhancement
KW  -  Bowler-hat transform
KW  -  Mathematical morphology
KW  - Image enhancement
JF  - Pattern Recognition
VL  - 88
SP  - 739
EP  - 750
DO  - https://doi.org/10.1016/j.patcog.2018.10.011
UR  - https://www.sciencedirect.com/science/article/pii/S0031320318303558
N2  - Enhancement, followed by segmentation, quantification and modelling of blood vessels in retinal images plays an essential role in computer-aided retinopathy diagnosis. In this paper, we introduce the bowler-hat transform method a new approach based on mathematical morphology for vessel enhancement. The proposed method combines different structuring elements to detect innate features of vessel-like structures. We evaluate the proposed method qualitatively and quantitatively and compare it with the state-of-the-art methods using both synthetic and real datasets. Our results establish that the proposed method achieves high-quality vessel-like structure enhancement in both synthetic examples and clinically relevant retinal images. The bowler-hat transform is shown to be able to detect fine vessels while still remaining robust at junctions.
ER  - 
TY  - JOUR
T1  - Can k-NN imputation improve the performance of C4.5 with small software project data sets? A comparative evaluation
A1  - Song, Qinbao
A1  - Shepperd, Martin
A1  - Chen, Xiangru
A1  - Liu, Jun
Y1  - 2008///
KW  -  C4.5
KW  -  Data imputation
KW  -  Missing data toleration
KW  -  Software project cost prediction
KW  - Missing data
JF  - Journal of Systems and Software
VL  - 81
IS  - 12
SP  - 2361
EP  - 2370
DO  - https://doi.org/10.1016/j.jss.2008.05.008
UR  - https://www.sciencedirect.com/science/article/pii/S0164121208000988
N1  - Best papers from the 2007 Australian Software Engineering Conference (ASWEC 2007), Melbourne, Australia, April 10-13, 2007
N2  - Missing data is a widespread problem that can affect the ability to use data to construct effective prediction systems. We investigate a common machine learning technique that can tolerate missing values, namely C4.5, to predict cost using six real world software project databases. We analyze the predictive performance after using the k-NN missing data imputation technique to see if it is better to tolerate missing data or to try to impute missing values and then apply the C4.5 algorithm. For the investigation, we simulated three missingness mechanisms, three missing data patterns, and five missing data percentages. We found that the k-NN imputation can improve the prediction accuracy of C4.5. At the same time, both C4.5 and k-NN are little affected by the missingness mechanism, but that the missing data pattern and the missing data percentage have a strong negative impact upon prediction (or imputation) accuracy particularly if the missing data percentage exceeds 40%.
ER  - 
TY  - JOUR
T1  - Sparsity in optimal randomized classification trees
A1  - Blanquero, Rafael
A1  - Carrizosa, Emilio
A1  - Molero-Río, Cristina
A1  - Romero Morales, Dolores
Y1  - 2020///
KW  -  Global and local sparsity
KW  -  Nonlinear programming
KW  -  Optimal classification trees
KW  - Data mining
JF  - European Journal of Operational Research
VL  - 284
IS  - 1
SP  - 255
EP  - 272
DO  - https://doi.org/10.1016/j.ejor.2019.12.002
UR  - https://www.sciencedirect.com/science/article/pii/S0377221719309865
N2  - Decision trees are popular Classification and Regression tools and, when small-sized, easy to interpret. Traditionally, a greedy approach has been used to build the trees, yielding a very fast training process; however, controlling sparsity (a proxy for interpretability) is challenging. In recent studies, optimal decision trees, where all decisions are optimized simultaneously, have shown a better learning performance, especially when oblique cuts are implemented. In this paper, we propose a continuous optimization approach to build sparse optimal classification trees, based on oblique cuts, with the aim of using fewer predictor variables in the cuts as well as along the whole tree. Both types of sparsity, namely local and global, are modeled by means of regularizations with polyhedral norms. The computational experience reported supports the usefulness of our methodology. In all our data sets, local and global sparsity can be improved without harming classification accuracy. Unlike greedy approaches, our ability to easily trade in some of our classification accuracy for a gain in global sparsity is shown.
ER  - 
TY  - JOUR
T1  - ACO-based hybrid classification system with feature subset selection and model parameters optimization
A1  - Huang, Cheng-Lung
Y1  - 2009///
KW  -  Feature selection
KW  -  Support vector machine
KW  - Ant colony optimization
JF  - Neurocomputing
VL  - 73
IS  - 1
SP  - 438
EP  - 448
DO  - https://doi.org/10.1016/j.neucom.2009.07.014
UR  - https://www.sciencedirect.com/science/article/pii/S0925231209002781
N1  - Timely Developments in Applied Neural Computing (EANN 2007) / Some Novel Analysis and Learning Methods for Neural Networks (ISNN 2008) / Pattern Recognition in Graphical Domains
N2  - This work presents a novel hybrid ACO-based classifier model that combines ant colony optimization (ACO) and support vector machines (SVM) to improve classification accuracy with a small and appropriate feature subset. To simultaneously optimize the feature subset and the SVM kernel parameters, the feature importance and the pheromones are used to determine the transition probability; the classification accuracy and the weight vector of the feature provided by the SVM classifier are both considered to update the pheromone. The experimental results indicate that the hybridized approach can correctly select the discriminating input features and also achieve high classification accuracy.
ER  - 
TY  - JOUR
T1  - Utilizing IoT wearable medical device for heart disease prediction using higher order Boltzmann model: A classification approach
A1  - Al-Makhadmeh, Zafer
A1  - Tolba, Amr
Y1  - 2019///
KW  -  Automatic systems
KW  -  Complex data
KW  -  Energy function
KW  -  Higher order Boltzmann deep belief neural network
KW  -  Internet of Things
KW  - Heart disease
JF  - Measurement
VL  - 147
SP  - 106815
EP  - 106815
DO  - https://doi.org/10.1016/j.measurement.2019.07.043
UR  - https://www.sciencedirect.com/science/article/pii/S0263224119306724
N2  - Globally, the prognosis of heart disease can be improved by early diagnosis and treatment. However, existing automatic systems for diagnosing heart disease are hampered by the requisite big data. This paper introduces an Internet of Things-based medical device for collecting patients’ heart details before and after heart disease. The information, which is continuously transmitted to the health care center, is processed using the higher order Boltzmann deep belief neural network (HOBDBNN). The deep learning method learns heart disease features from past analysis, and achieves efficiency by the effective manipulation of complex data. Following experiments, the performance of the system is evaluated based on characteristics such as f-measure, sensitivity, specificity, loss function, and receiver operating characteristic (ROC) curve. The HOBDBNN method and IoT-based analysis recognize heart disease with 99.03% accuracy with minimum time complexity (8.5 s), effectively minimizing heart disease mortality by reducing the complexity of diagnosing heart disease.
ER  - 
TY  - JOUR
T1  - Suggestions for a Web based universal exchange and inference language for medicine
A1  - Robson, Barry
A1  - Caruso, Thomas P
A1  - Balis, Ulysses G J
Y1  - 2013///
KW  -  Bayes inference
KW  -  Best practice
KW  -  Hyperbolic complex algebra
KW  -  Probabilistic knowledge representation
KW  -  Semantic networks
KW  - Dirac
JF  - Computers in Biology and Medicine
VL  - 43
IS  - 12
SP  - 2297
EP  - 2310
DO  - https://doi.org/10.1016/j.compbiomed.2013.09.010
UR  - https://www.sciencedirect.com/science/article/pii/S0010482513002618
N2  - Mining biomedical and pharmaceutical data generates huge numbers of interacting probabilistic statements for inference, which can be supported by mining Web text sources. This latter can also be probabilistic, in a sense described in this report. However, the diversity of tools for probabilistic inference is troublesome, suggesting a need for a unifying best practice. Physicists often claim that quantum mechanics is the universal best practice for probabilistic reasoning. We discuss how the Dirac notation and algebra suggest the form and algebraic and semantic meaning of XML-like Web tags for a clinical and biomedical universal exchange language formulated to make sense directly to the eye of the physician and biomedical researcher.
ER  - 
TY  - JOUR
T1  - Applying formalized rules for treatment procedures to data delivered by personal medical devices
A1  - Skałkowski, Kornel
A1  - Zieliński, Krzysztof
Y1  - 2013///
KW  -  Event processing
KW  -  Personal medical devices
KW  -  Rule-based reasoning
KW  -  Service Oriented Architecture (SOA)
KW  - Remote healthcare
JF  - Journal of Biomedical Informatics
VL  - 46
IS  - 3
SP  - 530
EP  - 540
DO  - https://doi.org/10.1016/j.jbi.2013.04.005
UR  - https://www.sciencedirect.com/science/article/pii/S1532046413000488
N2  - The paper presents a novel approach to online application of formalized rules for medical treatment procedures when processing data from personal medical devices. The rules are formalized by using a rule-based reasoning approach and are applied in order to enhance patient safety and support physicians in their daily work. The presented approach relies on dividing data processing into two stages: (1) the event processing stage and (2) the knowledge application stage. At the event processing stage raw data produced by personal medical devices is transformed into an aggregated/correlated form, as required by the rules for treatment procedures. At the knowledge application stage formalized rules are applied to transformed data, resulting in execution of various support actions. This paper describes how rules for treatment of patients suffering from cardiovascular diseases can be expressed in terms of an event processing statement set and a rule engine knowledge base. The technical feasibility of the proposed approach is supported by a detailed description of the TeleCARE remote healthcare framework – an implementation of the proposed approach along with evaluation performed using a large number of simulated personal medical devices.
ER  - 
TY  - JOUR
T1  - Agarose hydrogel doped with gold nanobipyramids(AuNBPs@AG)as colorful height readout device for sensing hydrogen peroxide in complex sample matrix
A1  - Shou, Wen
A1  - Yang, Shuangting
A1  - Wang, Yueliang
A1  - Qiu, Bin
A1  - Lin, Zhenyu
A1  - Guo, Longhua
Y1  - 2021///
KW  -  Agarose hydrogel
KW  -  AuNBPs
KW  -  HO
KW  - Height-readout sensor
JF  - Sensors and Actuators B: Chemical
VL  - 344
SP  - 130059
EP  - 130059
DO  - https://doi.org/10.1016/j.snb.2021.130059
UR  - https://www.sciencedirect.com/science/article/pii/S0925400521006286
N2  - The portable and instrument-free sensing device for quantitative detection is of significant applications in resource constrained countries and regions. Here, a colorful height readout device formed by agarose hydrogel doped with gold nanobipyramids (AuNBPs@AG) is developed for the determination of hydrogen peroxide (H2O2) content. The responsive mechanism is based on H2O2 guided etching of AuNBPs, which causes the plasmon-mediated color change of the AuNBPs@AG hydrogel filled in plexiglass tube from top to bottom. We demonstrate the height of color-changed hydrogel is quantitatively related to the concentration of H2O2. The agarose hydrogel, as the carrier material for loading AuNBPs, guarantees the stability of AuNBPs and promotes the separation of complex samples. The method shows a good linear relationship in four ranges of H2O2 concentrations of 20 − 100 μM, 200 − 1000 μM, 1 − 10 mM and 10 − 100 mM. Compared with previous methods, using this inexpensive and easy to operate device for H2O2 detection can effectively reduce the interference of the color background, overcome the problem of the nanoparticles aggregation, and eliminate the need for expensive and sophisticated detection instruments. The practicality of the method is demonstrated via the analysis of commercial products such as contact lens solution and hair dye with satisfactory results. Our work provides a novel path for developing the height-readout quantitative devices with integrating hydrogels and plasmonic nanomaterials.
ER  - 
TY  - JOUR
T1  - Detection of the optic disc in fundus images by combining probability models
A1  - Harangi, Balazs
A1  - Hajdu, Andras
Y1  - 2015///
KW  -  Ensemble-based system
KW  -  Information fusion
KW  -  Naïve Bayes
KW  -  Object detection
KW  - Optic disc
JF  - Computers in Biology and Medicine
VL  - 65
SP  - 10
EP  - 24
DO  - https://doi.org/10.1016/j.compbiomed.2015.07.002
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515002449
N2  - In this paper, we propose a combination method for the automatic detection of the optic disc (OD) in fundus images based on ensembles of individual algorithms. We have studied and adapted some of the state-of-the-art OD detectors and finally organized them into a complex framework in order to maximize the accuracy of the localization of the OD. The detection of the OD can be considered as a single-object detection problem. This object can be localized with high accuracy by several algorithms extracting single candidates for the center of the OD and the final location can be defined using a single majority voting rule. To include more information to support the final decision, we can use member algorithms providing more candidates which can be ranked based on the confidence ordered by the algorithms. In this case, a spatial weighted graph is defined where the candidates are considered as its nodes, and the final OD position is determined in terms of finding a maximum-weighted clique. Now, we examine how to apply in our ensemble-based framework all the accessible information supplied by the member algorithms by making them return confidence values for each image pixel. These confidence values inform us about the probability that a given pixel is the center point of the object. We apply axiomatic and Bayesian approaches, as in the case of aggregation of judgments of experts in decision and risk analysis, to combine these confidence values. According to our experimental study, the accuracy of the localization of OD increases further. Besides single localization, this approach can be adapted for the precise detection of the boundary of the OD. Comparative experimental results are also given for several publicly available datasets.
ER  - 
TY  - JOUR
T1  - Assessment of 3D DCE-MRI of the kidneys using non-rigid image registration and segmentation of voxel time courses
A1  - Zöllner, Frank G
A1  - Sance, Rosario
A1  - Rogelj, Peter
A1  - Ledesma-Carbayo, María J
A1  - Rørvik, Jarle
A1  - Santos, Andrés
A1  - Lundervold, Arvid
Y1  - 2009///
KW  -  -means clustering
KW  -  Kidney
KW  -  Multi-modality non-rigid image registration
KW  -  Renal function
KW  -  Time series analysis
KW  - DCE-MRI
JF  - Computerized Medical Imaging and Graphics
VL  - 33
IS  - 3
SP  - 171
EP  - 181
DO  - https://doi.org/10.1016/j.compmedimag.2008.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S0895611108001274
N2  - We have applied automated image analysis methods in the assessment of human kidney perfusion based on 3D dynamic contrast-enhanced MRI data. This approach consists of non-rigid 3D image registration of the moving kidney followed by k-means clustering of the voxel time courses with split between left and right kidney. This method was applied to four data sets acquired from healthy volunteers, using 1.5T (2 exams) and 3 T scanners (2 exams). The proposed registration method reduced motion artifacts in the image time series and improved further analysis of the DCE-MRI data. The subsequent clustering to segment the kidney compartments was in agreement with manually delineations (similarity score of 0.96) in the same motion corrected images. The resulting mean intensity time curves clearly show the successive transition of contrast agent through kidney compartments (cortex, medulla, and pelvis). The proposed method for motion correction and kidney compartment segmentation might improve the validity and usefulness of further model-based pharmacokinetic analysis of kidney function in patients.
ER  - 
TY  - JOUR
T1  - Delivering home healthcare through a Cloud-based Smart Home Environment (CoSHE)
A1  - Pham, Minh
A1  - Mengistu, Yehenew
A1  - Do, Ha
A1  - Sheng, Weihua
Y1  - 2018///
KW  - Healthcare
KW  - IOT
KW  - Smart home
JF  - Future Generation Computer Systems
VL  - 81
SP  - 129
EP  - 140
DO  - https://doi.org/10.1016/j.future.2017.10.040
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X17302194
N2  - The dramatic increase of senior population worldwide is challenging the existing healthcare and support systems. Recently, smart home environments are utilized for ubiquitous health monitoring, allowing patients to stay at the comfort of their homes. In this paper we presented a Cloud-Based Smart Home Environment (CoSHE) for home healthcare. CoSHE collects physiological, motion and audio signals through non-invasive wearable sensors and provides contextual information in terms of the resident’s daily activity and location in the home. This enables healthcare professionals to study daily activities, behavioral changes and monitor rehabilitation and recovery processes. A smart home environment is set up with environmental sensors to provide contextual information. The sensor data are processed in a smart home gateway and sent to a private cloud, which provides real time data access for remote caregivers. Our case study shows that we can successfully integrate contextual information to health data and this comprehensive information can help better understand caretaker’s health status.
ER  - 
TY  - JOUR
T1  - Establishment and evaluation of a multicenter collaborative prediction model construction framework supporting model generalization and continuous improvement: A pilot study
A1  - Tian, Yu
A1  - Chen, Weiguo
A1  - Zhou, Tianshu
A1  - Li, Jun
A1  - Ding, Kefeng
A1  - Li, Jingsong
Y1  - 2020///
KW  -  Model generalization
KW  -  Prognosis prediction
KW  -  Transfer learning
KW  - Multicenter collaborative research
JF  - International Journal of Medical Informatics
VL  - 141
SP  - 104173
EP  - 104173
DO  - https://doi.org/10.1016/j.ijmedinf.2020.104173
UR  - https://www.sciencedirect.com/science/article/pii/S1386505620301362
N2  - Background and Objective
In recent years, an increasing number of clinical prediction models have been developed to serve clinical care. Establishing a data-driven prediction model based on large-scale electronic health record (EHR) data can provide a more empirical basis for clinical decision making. However, research on model generalization and continuous improvement is insufficiently focused, which also hinders the application and evaluation of prediction models in real clinical environments. Therefore, this study proposes a multicenter collaborative prediction model construction framework to build a prediction model with greater generalizability and continuous improvement capabilities while preserving patient data security and privacy.
Materials and Methods
Based on a multicenter collaborative research network, such as the Observational Health Data Sciences and Informatics (OHDSI), a multicenter collaborative prediction model construction framework is proposed. Based on the idea of multi-source transfer learning, in each source hospital, a base classifier was trained according to the model research setting. Then, in the target hospital with missing calibration data, a prediction model was established through weighted integration of base classifiers from source hospitals based on the smoothness assumption. Moreover, a passive-aggressive online learning algorithm was used for continuous improvement of the prediction model, which can help to maintain a high predictive performance to provide reliable clinical decision-making abilities. To evaluate the proposed prediction model construction framework, a prototype system for colorectal cancer prognosis prediction was developed. To evaluate the performance of models, 70,906 patients were screened, including 70,090 from 5 US hospital-specific datasets and 816 from a Chinese hospital-specific dataset. The area under the receiver operating characteristic curve (AUC) and the estimated calibration index (ECI) were used to evaluate the discrimination and calibration of models.
Results
Regarding the colorectal cancer prognosis prediction in our prototype system, compared with the reference models, our model achieved a better performance in model calibration (ECI = 9.294 [9.146, 9.441]) and a similar ability in model discrimination (AUC = 0.783 [0.780, 0.786]). Furthermore, the online learning process provided in this study can continuously improve the performance of the prediction model when patient data with specified labels arrive (the AUC value increased from 0.709 to 0.715 and the ECI value decreased from 13.013 to 9.634 after 650 patient instances with specified labels from the Chinese hospital arrived), enabling the prediction model to maintain a good predictive performance during clinical application.
Conclusions
This study proposes and evaluates a multicenter collaborative prediction model construction framework that can support the construction of prediction models with better generalizability and continuous improvement capabilities without the need to aggregate multicenter patient-level data.
ER  - 
TY  - JOUR
T1  - Histogram distance-based Bayesian Network structure learning: A supervised classification specific approach
A1  - Sierra, B
A1  - Lazkano, E
A1  - Jauregi, E
A1  - Irigoien, I
Y1  - 2009///
KW  -  Histogram distance
KW  -  Machine learning
KW  -  Structure learning
KW  -  Supervised classification
KW  - Bayesian Network
JF  - Decision Support Systems
VL  - 48
IS  - 1
SP  - 180
EP  - 190
DO  - https://doi.org/10.1016/j.dss.2009.07.010
UR  - https://www.sciencedirect.com/science/article/pii/S0167923609001742
N1  - Information product markets
N2  - In this work we introduce a methodology based on histogram distances for the automatic induction of Bayesian Networks (BN) from a file containing cases and variables related to a supervised classification problem. The main idea consists of learning the Bayesian Network structure for classification purposes taking into account the classification itself, by comparing the class distribution histogram distances obtained by the Bayesian Network after classifying each case. The structure is learned by applying eight different measures or metrics: the Cooper and Herskovits metric for a general Bayesian Network and seven different statistical distances between pairs of histograms. The results obtained confirm the hypothesis of the authors about the convenience of having a BN structure learning method which takes into account the existence of the special variable (the one corresponding to the class) in supervised classification problems.
ER  - 
TY  - JOUR
T1  - EMR2vec: Bridging the gap between patient data and clinical trial
A1  - Dhayne, Houssein
A1  - Kilany, Rima
A1  - Haque, Rafiqul
A1  - Taher, Yehia
Y1  - 2021///
KW  -  Clinical trial
KW  -  Medical data integration
KW  -  Neural network
KW  -  Semantic web
KW  - EMR
JF  - Computers & Industrial Engineering
VL  - 156
SP  - 107236
EP  - 107236
DO  - https://doi.org/10.1016/j.cie.2021.107236
UR  - https://www.sciencedirect.com/science/article/pii/S0360835221001406
N2  - The human suffering from diseases caused by life-threatening viruses such as SARS, Ebola, and COVID-19 motivated many of us to study and discover the best means to harness the potential of data integration to assist clinical researchers to curb these viruses. Integrating patients data with clinical trials data is enormously promising as it provides a comprehensive knowledge base that accelerates the clinical research response-ability to tackle emerging infectious disease outbreaks. This work introduces EMR2vec, a platform that customises advanced NLP, machine learning and semantic web techniques to link potential patients to suitable clinical trials. Linking these two different but complementary datasets allows clinicians and researchers to compare patients to clinical research opportunities or to automatically select patients for personalized clinical care. The platform derives a ’bag of medical terms’ (BoMT) from eligibility criteria by normalizing extracted entities through SNOMED-CT ontology. With the usage of BoMT, an ontological reasoning method is proposed to represent EMR and clinical trials in a vector space model. The platform presents a matching process that reduces vector dimensionality using a neural network, then applies orthogonality projection to measure the similarity between vectors. Finally, the proposed EMR2vec platform is evaluated with an extendable prototype based on Big data tools.
ER  - 
TY  - JOUR
T1  - Towards actionable risk stratification: A bilinear approach
A1  - Wang, Xiang
A1  - Wang, Fei
A1  - Hu, Jianying
A1  - Sorrentino, Robert
Y1  - 2015///
KW  -  Bilinear model
KW  -  Dimensionality reduction
KW  -  Logistic regression
KW  -  Matrix factorization
KW  - Risk stratification
JF  - Journal of Biomedical Informatics
VL  - 53
SP  - 147
EP  - 155
DO  - https://doi.org/10.1016/j.jbi.2014.10.004
UR  - https://www.sciencedirect.com/science/article/pii/S153204641400224X
N2  - Risk stratification is instrumental to modern clinical decision support systems. Comprehensive risk stratification should be able to provide the clinicians with not only the accurate assessment of a patient’s risk but also the clinical context to be acted upon. However, existing risk stratification techniques mainly focus on predicting the risk score for individual patients; at the cohort level, they offer little insight beyond a flat score-based segmentation. This essentially reduces a patient to a score and thus removes him/her from his/her clinical context. To address this limitation, in this paper we propose a bilinear model for risk stratification that simultaneously captures the three key aspects of risk stratification: (1) it predicts the risk of each individual patient; (2) it stratifies the patient cohort based on not only the risk score but also the clinical characteristics; and (3) it embeds all patients into clinical contexts with clear interpretation. We apply our model to a cohort of 4977 patients, 1127 among which were diagnosed with Congestive Heart Failure (CHF). We demonstrate that our model cannot only accurately predict the onset risk of CHF but also provide rich and actionable clinical insights into the patient cohort.
ER  - 
TY  - JOUR
T1  - Anticipative Hybrid Extreme Rotation Forest
A1  - Ayerdi, Borja
A1  - Graña, Manuel
Y1  - 2016///
KW  -  heterogeneous ensembles
KW  - ELM
JF  - Procedia Computer Science
VL  - 80
SP  - 1671
EP  - 1681
DO  - https://doi.org/10.1016/j.procs.2016.05.507
UR  - https://www.sciencedirect.com/science/article/pii/S1877050916309942
N1  - International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA
N2  - This paper introduces an improvement on the recently published Hybrid Extreme Rotation Forest (HERF), consisting in the anticipative determination of the the fraction of each classifier architecture included in the ensemble. We call it AHERF. Both HERF and AHERF are heterogeneous classifier ensembles, which aim to profit from the diverse problem domain specificities of each classifier architecture in order to achieve improved generalization over a larger spectrum of problem domains. In this paper AHERF are built from a pool of Decision Trees (DT), Extreme Learning Machines (ELM), Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Adaboost, Random Forests (RF), and Gaussian Naive Bayes (GNB) classifiers. Given a problem dataset, the process of anticipative determination of the ensemble composition is as follows: First, we estimate the performance of each classifier architecture by independent pilot cross-validation experiments on a small subsample of the data. Next, classifier architectures are ranked according to their accuracy results. A probability distribution of classifier architectures appearing in the ensemble is built from this ranking. Finally, the type of each individual classifier is decided by sampling this probability distribution. Computational experiments on a collection of benchmark classification problems shows improvement on the original HERF, and other state-of-the-art approaches.
ER  - 
TY  - JOUR
T1  - Accurate detection of COVID-19 patients based on distance biased Naïve Bayes (DBNB) classification strategy
A1  - Shaban, Warda M
A1  - Rabie, Asmaa H
A1  - Saleh, Ahmed I
A1  - Abo-Elsoud, M A
Y1  - 2021///
KW  -  Classification
KW  -  Feature selection
KW  -  NB
KW  -  Optimization
KW  -  Particle swarm
KW  -  Wrapper
KW  - COVID-19
JF  - Pattern Recognition
VL  - 119
SP  - 108110
EP  - 108110
DO  - https://doi.org/10.1016/j.patcog.2021.108110
UR  - https://www.sciencedirect.com/science/article/pii/S0031320321002971
N2  - COVID-19, as an infectious disease, has shocked the world and still threatens the lives of billions of people. Early detection of COVID-19 patients is an important issue for treating and controlling the disease from spreading. In this paper, a new strategy for detecting COVID-19 infected patients will be introduced, which is called Distance Biased Naïve Bayes (DBNB). The novelty of DBNB as a proposed classification strategy is concentrated in two contributions. The first is a new feature selection technique called Advanced Particle Swarm Optimization (APSO) which elects the most informative and significant features for diagnosing COVID-19 patients. APSO is a hybrid method based on both filter and wrapper methods to provide accurate and significant features for the next classification phase. The considered features are extracted from Laboratory findings for different cases of people, some of whom are COVID-19 infected while some are not. APSO consists of two sequential feature selection stages, namely; Initial Selection Stage (IS2) and Final Selection Stage (FS2). IS2 uses filter technique to quickly select the most important features for diagnosing COVID-19 patients while removing the redundant and ineffective ones. This behavior minimizes the computational cost in FS2, which is the next stage of APSO. FS2 uses Binary Particle Swarm Optimization (BPSO) as a wrapper method for accurate feature selection. The second contribution of this paper is a new classification model, which combines evidence from statistical and distance based classification models. The proposed classification technique avoids the problems of the traditional NB and consists of two modules; Weighted Naïve Bayes Module (WNBM) and Distance Reinforcement Module (DRM). The proposed DBNB tries to accurately detect infected patients with the minimum time penalty based on the most effective features selected by APSO. DBNB has been compared with recent COVID-19 diagnose strategies. Experimental results have shown that DBNB outperforms recent COVID-19 diagnose strategies as it introduce the maximum accuracy with the minimum time penalty.
ER  - 
TY  - JOUR
T1  - A deep neural network based classifier for brain tumor diagnosis
A1  - Kumar, Ambeshwar
A1  - Ramachandran, Manikandan
A1  - Gandomi, Amir H
A1  - Patan, Rizwan
A1  - Lukasik, Szymon
A1  - Soundarapandian, Ravichandran Kattur
Y1  - 2019///
KW  -  Bayesian multivariate linear regression
KW  -  Brain tumor
KW  -  Feature selection
KW  -  Iteratively reweighted least squares
KW  -  Least absolute deviations
KW  -  Medical features
KW  -  Weighted correlation
KW  - Deep neural network
JF  - Applied Soft Computing
VL  - 82
SP  - 105528
EP  - 105528
DO  - https://doi.org/10.1016/j.asoc.2019.105528
UR  - https://www.sciencedirect.com/science/article/pii/S1568494619302984
N2  - Classification process plays a key role in diagnosing brain tumors. Earlier research works are intended for identifying brain tumors using different classification techniques. However, the False Alarm Rates (FARs) of existing classification techniques are high. To improve the early-stage brain tumor diagnosis via classification the Weighted Correlation Feature Selection Based Iterative Bayesian Multivariate Deep Neural Learning (WCFS-IBMDNL) technique is proposed in this work. The WCFS-IBMDNL algorithm considers medical dataset for classifying the brain tumor diagnosis at an early stage. At first, the WCFS-IBMDNL technique performs Weighted Correlation-Based Feature Selection (WC-FS) by selecting subsets of medical features that are relevant for classification of brain tumors. After completing the feature selection process, the WCFS-IBMDNL technique uses Iterative Bayesian Multivariate Deep Neural Network (IBMDNN) classifier for reducing the misclassification error rate of brain tumor identification. The WCFS-IBMDNL technique was evaluated in JAVA language using Disease Diagnosis Rate (DDR), Disease Diagnosis Time (DDT), and FAR parameter through the epileptic seizure recognition dataset.
ER  - 
TY  - JOUR
T1  - Azidothymidine (AZT) leads to arterial stiffening and intima-media thickening in mice
A1  - Hansen, Laura
A1  - Parker, Ivana
A1  - Monet Roberts, LaDeidra
A1  - Sutliff, Roy L
A1  - Platt, Manu O
A1  - Gleason, Rudolph L
Y1  - 2013///
KW  -  Arterial remodeling
KW  -  Atherosclerosis
KW  -  HAART
KW  -  Non-AIDS defining illnesses
KW  - HIV
JF  - Journal of Biomechanics
VL  - 46
IS  - 9
SP  - 1540
EP  - 1547
DO  - https://doi.org/10.1016/j.jbiomech.2013.03.021
UR  - https://www.sciencedirect.com/science/article/pii/S0021929013001371
N2  - HIV positive patients on highly active antiretroviral therapy (HAART) have shown elevated incidence of a number of non-AIDS defining co-morbidities, including cardiovascular disease. Given that HAART regimens contain a combination of at least three drugs, that disease management often requires adjustment of these regimens, and HIV, independent of HAART, also plays a role in development of co-morbidities, determining the role of specific HAART drugs and HIV infection itself from clinical data remains challenging. To characterize specific mediators and underlying mechanisms of disease, in vitro and in vivo animal models are required, in parallel with clinical data. Given its low cost azidothymidine (AZT) contributes to the backbone of a large proportion of HAART treated patients in the developing world where much of the global burden of HIV resides. The goal of this study was to test the hypothesis that AZT can lead to proatherogenic changes including the subclinical markers of arterial stiffening and intima-media thickening in mice. AZT (100mg/kg) or vehicle was administered to wild-type FVB/N mice via oral gavage for 35 days. Cylindrical biaxial biomechanical tests on the common carotid arteries and suprarenal aortas exhibited arterial stiffening in AZT mice compared to controls. Multiphoton microscopy and histology demonstrated that AZT led to increased intima-media thickness. These data correlated with decreased elastin content and increased protease activity as measured by cathepsin zymography; no differences were observed in collagen content or organization, in vivo axial stretch, or opening angle. Thus, this study suggests the drug AZT has significant effects on the development of subclinical markers of atherosclerosis.
ER  - 
TY  - JOUR
T1  - Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques
A1  - Syed, Liyakathunisa
A1  - Jabeen, Saima
A1  - S., Manimala
A1  - Alsaeedi, Abdullah
Y1  - 2019///
KW  - Ambient Assisted Living (AAL)
KW  - Big data analytics
KW  - Internet of Medical Things (IoMT)
KW  - Machine learning techniques
KW  - Physical activities
KW  - Wearable sensors
JF  - Future Generation Computer Systems
VL  - 101
SP  - 136
EP  - 151
DO  - https://doi.org/10.1016/j.future.2019.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18321071
N2  - In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.
ER  - 
TY  - JOUR
T1  - L-Seg: An end-to-end unified framework for multi-lesion segmentation of fundus images
A1  - Guo, Song
A1  - Li, Tao
A1  - Kang, Hong
A1  - Li, Ning
A1  - Zhang, Yujun
A1  - Wang, Kai
Y1  - 2019///
KW  -  Class-imbalance
KW  -  Diabetic retinopathy
KW  -  Fundus image
KW  - Multi-lesion segmentation
JF  - Neurocomputing
VL  - 349
SP  - 52
EP  - 63
DO  - https://doi.org/10.1016/j.neucom.2019.04.019
UR  - https://www.sciencedirect.com/science/article/pii/S0925231219305430
N2  - Diabetic retinopathy and diabetic macular edema are the two leading causes for blindness in working-age people, and the quantitative and qualitative diagnosis of these two diseases usually depends on the presence and areas of lesions in fundus images. The main related lesions include soft exudates, hard exudates, microaneurysms, and haemorrhages. However, segmentation of these four kinds of lesions is difficult due to their uncertainty in size, contrast, and high interclass similarity. Therefore, we aim to design a multi-lesion segmentation model. We have designed the first small object segmentation network (L-Seg) that can segment the four kinds of lesions simultaneously. Taking into account that small lesion regions could not response at high level of network, we propose a multi-scale feature fusion method to handle this problem. In addition, when considering the cases of both class-imbalance and loss-imbalance problems, we propose a multi-channel bin loss. We have evaluated L-Seg on three fundus datasets including two publicly available datasets - IDRiD and e-ophtha and one private dataset - DDR. Extensive experiments have demonstrated that L-Seg achieves better performance in small lesion segmentation than other deep learning models and traditional methods. Specially, the mAUC score of L-Seg is over 16.8%, 1.51% and 3.11% higher than that of DeepLab v3+ on IDRiD, e-ophtha and DDR datasets, respectively. Moreover, our framework shows competitive performance compared with top-3 teams in IDRiD challenge. The source code of L-Seg is available at: https://github.com/guomugong/L-Seg.
ER  - 
TY  - JOUR
T1  - COPri v.2 — A core ontology for privacy requirements
A1  - Gharib, Mohamad
A1  - Giorgini, Paolo
A1  - Mylopoulos, John
Y1  - 2021///
KW  -  Conceptual modeling
KW  -  PbD
KW  -  Privacy by Design
KW  -  Privacy requirements
KW  -  Requirements engineering
KW  - Privacy ontology
JF  - Data & Knowledge Engineering
VL  - 133
SP  - 101888
EP  - 101888
DO  - https://doi.org/10.1016/j.datak.2021.101888
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X2100015X
N2  - Nowadays, most enterprises collect, store, and manage personal information of customers to deliver their services. In such a setting, privacy has emerged as a key concern since companies often neglect or even misuse personal data. In response to multiple massive breaches of personal data, governments around the world have enacted laws and regulations for privacy protection. These laws dictate privacy requirements for any system that acquires and manages personal data. Unfortunately, these requirements are often incomplete and/or inaccurate as many RE practitioners are insufficiently versed with privacy requirements and how are they different from other requirements, such as security. To tackle this problem, we developed a comprehensive ontology for privacy requirements. In particular, the contributions of this work include the derivation of an ontology from a previously conducted systematic literature review, an implementation using an ontology definition tool (Protégé), a demonstration of its coverage through an extensive example on Ambient Assisted Living, and a validation through competency questions. Also, we evaluate the ontology against the common pitfalls for ontologies with the help of some software tools, lexical semantics experts, and privacy and security researchers. The ontology presented herein (COPri v.2) has been enhanced with extensions motivated by the feedback received from privacy and security experts.
ER  - 
TY  - JOUR
T1  - Plantar ROI Characterization during the Stance Phase of Gait Based on a Low-cost Pressure Acquisition Platform
A1  - Mei, Zhanyong
A1  - Zhao, Guoru
A1  - Zhu, Qingsong
A1  - Wang, Lei
Y1  - 2012///
KW  -  Fisher discrimination
KW  -  ROIs
KW  -  foot biomechanics
KW  -  inter-frame difference
KW  - dynamic plantar pressure
JF  - Journal of Bionic Engineering
VL  - 9
IS  - 3
SP  - 343
EP  - 352
DO  - https://doi.org/10.1016/S1672-6529(11)60128-5
UR  - https://www.sciencedirect.com/science/article/pii/S1672652911601285
N2  - Plantar Region of Interest (ROI) detection is important for the early diagnosis and treatment of morphologic defects of the foot and foot bionic research. Conventional methods have employed complex procedures and expensive instruments which prohibit their widespread use in healthcare. In this paper an automatic plantar ROIs detection method using a customized low-cost pressure acquisition device is proposed. Plantar pressure data and 3D motion capture data were collected from 28 subjects (14 healthy subjects and 14 subjects with hallux valgus). The maximal inter-frame difference during the stance phase was calculated. Consequently, the ROIs were defined by the first-order difference in combination with prior anatomic knowledge. The anatomic locations were determined by the maximal inter-frame difference and second maximal inter-frame difference, which nearly coincided. Our system can achieve average recognition accuracies of 92.90%, 89.30%, 89.30%, 92.90%, 92.90%, and 89.30% for plantar ROIs hallux and metatarsi I-V, respectively, as compared with the annotations using the 3D motion capture system. The maximal difference of metatarsus heads II-V, and the impulse of the medial and lateral heel features made a significant contribution to the classification of hallux valgus and healthy subjects with ≥ 80% sensitivity and specificity. Furthermore, the plantar pressure acquisition system is portable and convenient to use, thus can be used in home- or community-based healthcare applications.
ER  - 
TY  - JOUR
T1  - Breast Cancer Prediction system
A1  - Kumari, Madhu
A1  - Singh, Vijendra
Y1  - 2018///
KW  -  classification
KW  -  knowledge mining
KW  -  prediction system
KW  - WCBD
JF  - Procedia Computer Science
VL  - 132
SP  - 371
EP  - 376
DO  - https://doi.org/10.1016/j.procs.2018.05.197
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918309323
N1  - International Conference on Computational Intelligence and Data Science
N2  - Breast cancer became the major source of mortality between women. The accessibility of healthcare datasets and data analysis promote the researchers to apply study in extracting unknown pattern from healthcare datasets. The intention of this study is to design a prediction system that can predict the incidence of the breast cancer at early stage by analyzing smallest set of attributes that has been selected from the clinical dataset. Wisconsin breast cancer dataset (WBCD) have been used to conduct the proposed experiment. The potential of the proposed method is obtained using classification accuracy which was obtained by comparing actual to predicted values. The outcome confirms that the maximum classification accuracy (99.28%) is achieved for this study.
ER  - 
TY  - JOUR
T1  - Process mining in healthcare: A literature review
A1  - Rojas, Eric
A1  - Munoz-Gama, Jorge
A1  - Sepúlveda, Marcos
A1  - Capurro, Daniel
Y1  - 2016///
KW  -  Case studies
KW  -  Literature review
KW  -  Process mining
KW  -  Processes
KW  - Healthcare
JF  - Journal of Biomedical Informatics
VL  - 61
SP  - 224
EP  - 236
DO  - https://doi.org/10.1016/j.jbi.2016.04.007
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416300296
N2  - Process Mining focuses on extracting knowledge from data generated and stored in corporate information systems in order to analyze executed processes. In the healthcare domain, process mining has been used in different case studies, with promising results. Accordingly, we have conducted a literature review of the usage of process mining in healthcare. The scope of this review covers 74 papers with associated case studies, all of which were analyzed according to eleven main aspects, including: process and data types; frequently posed questions; process mining techniques, perspectives and tools; methodologies; implementation and analysis strategies; geographical analysis; and medical fields. The most commonly used categories and emerging topics have been identified, as well as future trends, such as enhancing Hospital Information Systems to become process-aware. This review can: (i) provide a useful overview of the current work being undertaken in this field; (ii) help researchers to choose process mining algorithms, techniques, tools, methodologies and approaches for their own applications; and (iii) highlight the use of process mining to improve healthcare processes.
ER  - 
TY  - JOUR
T1  - Explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation
A1  - Dong, Hang
A1  - Suárez-Paniagua, Víctor
A1  - Whiteley, William
A1  - Wu, Honghan
Y1  - 2021///
KW  -  Attention Mechanisms
KW  -  Deep learning
KW  -  Explainability
KW  -  Label correlation
KW  -  Multi-label classification
KW  -  Natural Language Processing
KW  - Automated medical coding
JF  - Journal of Biomedical Informatics
VL  - 116
SP  - 103728
EP  - 103728
DO  - https://doi.org/10.1016/j.jbi.2021.103728
UR  - https://www.sciencedirect.com/science/article/pii/S1532046421000575
N2  - Background
Diagnostic or procedural coding of clinical notes aims to derive a coded summary of disease-related information about patients. Such coding is usually done manually in hospitals but could potentially be automated to improve the efficiency and accuracy of medical coding. Recent studies on deep learning for automated medical coding achieved promising performances. However, the explainability of these models is usually poor, preventing them to be used confidently in supporting clinical practice. Another limitation is that these models mostly assume independence among labels, ignoring the complex correlations among medical codes which can potentially be exploited to improve the performance.
Methods
To address the issues of model explainability and label correlations, we propose a Hierarchical Label-wise Attention Network (HLAN), which aimed to interpret the model by quantifying importance (as attention weights) of words and sentences related to each of the labels. Secondly, we propose to enhance the major deep learning models with a label embedding (LE) initialisation approach, which learns a dense, continuous vector representation and then injects the representation into the final layers and the label-wise attention layers in the models. We evaluated the methods using three settings on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS (National Health Service) COVID-19 (Coronavirus disease 2019) shielding codes. Experiments were conducted to compare the HLAN model and label embedding initialisation to the state-of-the-art neural network based methods, including variants of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).
Results
HLAN achieved the best Micro-level AUC and F1 on the top-50 code prediction, 91.9% and 64.1%, respectively; and comparable results on the NHS COVID-19 shielding code prediction to other models: around 97% Micro-level AUC. More importantly, in the analysis of model explanations, by highlighting the most salient words and sentences for each label, HLAN showed more meaningful and comprehensive model interpretation compared to the CNN-based models and its downgraded baselines, HAN and HA-GRU. Label embedding (LE) initialisation significantly boosted the previous state-of-the-art model, CNN with attention mechanisms, on the full code prediction to 52.5% Micro-level F1. The analysis of the layers initialised with label embeddings further explains the effect of this initialisation approach. The source code of the implementation and the results are openly available at https://github.com/acadTags/Explainable-Automated-Medical-Coding.
Conclusion
We draw the conclusion from the evaluation results and analyses. First, with hierarchical label-wise attention mechanisms, HLAN can provide better or comparable results for automated coding to the state-of-the-art, CNN-based models. Second, HLAN can provide more comprehensive explanations for each label by highlighting key words and sentences in the discharge summaries, compared to the n-grams in the CNN-based models and the downgraded baselines, HAN and HA-GRU. Third, the performance of deep learning based multi-label classification for automated coding can be consistently boosted by initialising label embeddings that captures the correlations among labels. We further discuss the advantages and drawbacks of the overall method regarding its potential to be deployed to a hospital and suggest areas for future studies.
ER  - 
TY  - JOUR
T1  - Reliability analysis of psoriasis decision support system in principal component analysis framework
A1  - Shrivastava, Vimal K
A1  - Londhe, Narendra D
A1  - Sonawane, Rajendra S
A1  - Suri, Jasjit S
Y1  - 2016///
KW  -  Classification
KW  -  Feature space
KW  -  PCA
KW  -  Reliability
KW  -  Stability
KW  - Dermatology
JF  - Data & Knowledge Engineering
VL  - 106
SP  - 1
EP  - 17
DO  - https://doi.org/10.1016/j.datak.2016.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X1630177X
N2  - Reliability and accuracy are essential components in any decision support system. These become even more important with a rising number of features during the classification process in a machine learning paradigm. Further, the selection of an optimal feature set is of paramount importance for the best performance, reliable and stable decision support systems. This paper presents a dermatology decision support system used for the classification of psoriasis images into diseased and healthy skin. A comprehensive grayscale and color feature space with 87 features are explored. The classification system consists of a machine learning paradigm embedded with principal component analysis-based optimal feature selection. The system consists of both offline training classifier and online testing classifier phases. The training parameters are estimated using unique feature space and ground truth, a priori derived by the dermatologist. The training phase generates the offline coefficients using a training classifier which is then used for transforming the online test features for prediction of two skin classes: diseased vs. healthy. The proposed system using principal component analysis shows the best classification accuracy of 99.39% for a 10-fold cross-validation using polynomial kernel of order-2 on a set of 540 images. We validate our system by computing the reliability and stability indices. The results demonstrate a mean reliability index of 98.71% for 11 distinct data sizes, and meeting the stability criteria within 2% tolerance. The ability to retain the dominant features by inclusion of increasing set of features is 90.52%. Thus proposed system shows the encouraging results with higher accuracy, reliability, stability and retaining power of dominant features.
ER  - 
TY  - JOUR
T1  - A novelty detection machine and its application to bank failure prediction
A1  - Li, Shukai
A1  - Tung, Whye Loon
A1  - Ng, Wee Keong
Y1  - 2014///
KW  -  Bank failure prediction
KW  -  Cluster assumption
KW  - Novelty detection
JF  - Neurocomputing
VL  - 130
SP  - 63
EP  - 72
DO  - https://doi.org/10.1016/j.neucom.2013.02.043
UR  - https://www.sciencedirect.com/science/article/pii/S0925231213006942
N1  - Track on Intelligent Computing and Applications Complex Learning in Connectionist Networks
N2  - Novelty detection has been well-studied for many years and has found a wide range of applications, but correctly identifying the outliers is still a hard problem because of the diverse variation and the small quantity of such outliers. We address the problem using several distinct characteristics of the outliers and the normal patterns. First, normal patterns are usually grouped together, forming clusters in the high density regions of the data space. Second, outliers are characteristically very different from the normal patterns, and hence tend to be located far away from the normal patterns in the data space. Third, the number of outliers is generally very small in a given dataset. Based on these observations, we can envisage that the appropriate decision boundary segregating the outliers and the normal patterns usually lies in some low density regions of the data space. This is referred to as cluster assumption. The resultant optimization problem to learn the decision function can be solved using the mixed integer programming approach. Following that, we present a cutting plane algorithm together with a multiple kernel learning technique to solve the convex relaxation of the optimization problem. Specifically, we make use of the scarcity of the outliers to find a violating solution to the cutting plane algorithm. Experimental results with several benchmark datasets show that our proposed novelty detection method outperforms existing hyperplane and density estimation-based novelty detection techniques. We subsequently apply our method to the prediction of banking failures to identify potential bank failures or high risk banks through the traits of financial distress.
ER  - 
TY  - JOUR
T1  - Challenges in clinical natural language processing for automated disorder normalization
A1  - Leaman, Robert
A1  - Khare, Ritu
A1  - Lu, Zhiyong
Y1  - 2015///
KW  -  Electronic health records
KW  -  Information extraction
KW  - Natural language processing
JF  - Journal of Biomedical Informatics
VL  - 57
SP  - 28
EP  - 37
DO  - https://doi.org/10.1016/j.jbi.2015.07.010
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001501
N2  - Background
Identifying key variables such as disorders within the clinical narratives in electronic health records has wide-ranging applications within clinical practice and biomedical research. Previous research has demonstrated reduced performance of disorder named entity recognition (NER) and normalization (or grounding) in clinical narratives than in biomedical publications. In this work, we aim to identify the cause for this performance difference and introduce general solutions.
Methods
We use closure properties to compare the richness of the vocabulary in clinical narrative text to biomedical publications. We approach both disorder NER and normalization using machine learning methodologies. Our NER methodology is based on linear-chain conditional random fields with a rich feature approach, and we introduce several improvements to enhance the lexical knowledge of the NER system. Our normalization method – never previously applied to clinical data – uses pairwise learning to rank to automatically learn term variation directly from the training data.
Results
We find that while the size of the overall vocabulary is similar between clinical narrative and biomedical publications, clinical narrative uses a richer terminology to describe disorders than publications. We apply our system, DNorm-C, to locate disorder mentions and in the clinical narratives from the recent ShARe/CLEF eHealth Task. For NER (strict span-only), our system achieves precision=0.797, recall=0.713, f-score=0.753. For the normalization task (strict span+concept) it achieves precision=0.712, recall=0.637, f-score=0.672. The improvements described in this article increase the NER f-score by 0.039 and the normalization f-score by 0.036. We also describe a high recall version of the NER, which increases the normalization recall to as high as 0.744, albeit with reduced precision.
Discussion
We perform an error analysis, demonstrating that NER errors outnumber normalization errors by more than 4-to-1. Abbreviations and acronyms are found to be frequent causes of error, in addition to the mentions the annotators were not able to identify within the scope of the controlled vocabulary.
Conclusion
Disorder mentions in text from clinical narratives use a rich vocabulary that results in high term variation, which we believe to be one of the primary causes of reduced performance in clinical narrative. We show that pairwise learning to rank offers high performance in this context, and introduce several lexical enhancements – generalizable to other clinical NER tasks – that improve the ability of the NER system to handle this variation. DNorm-C is a high performing, open source system for disorders in clinical text, and a promising step toward NER and normalization methods that are trainable to a wide variety of domains and entities. (DNorm-C is open source software, and is available with a trained model at the DNorm demonstration website: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/tmTools/#DNorm.)
ER  - 
TY  - JOUR
T1  - Electronic word of mouth analysis for new product positioning evaluation
A1  - Gregoriades, Andreas
A1  - Pampaka, Maria
Y1  - 2020///
KW  -  Product positioning
KW  -  Sentiment Analysis
KW  -  Topic modelling
KW  - Micro-blogs
JF  - Electronic Commerce Research and Applications
VL  - 42
SP  - 100986
EP  - 100986
DO  - https://doi.org/10.1016/j.elerap.2020.100986
UR  - https://www.sciencedirect.com/science/article/pii/S1567422320300636
N2  - People increasingly choose to express themselves online through electronic word of mouth (eWOM), generating large amounts of data, making eWOM a valuable source of information through big data analytics. This enables organizations to gain insights directly from customers’ opinions for better decision making. This work presents a new methodology for evaluating an organisation’s product-positioning strategy through eWOM analytics. A product’s mispositioning has significant negative effects and there is strong interest in identifying ways to avoid it. Current methods that utilize eWOM for product positioning evaluation mostly use post-product release reviews and do not statistically evaluate the effect of time on the product positioning; nor do they provide any means to diagnose the cause of mispositioning. The temporal aspect of positioning, however, provides valuable insights into which product features are more time-invariant and accordingly makes it possible to plan for product redesign or repositioning to maximize profitability. A case study is presented in the context of smartphones using design science research, utilizing Twitter data regarding the release of a new product, collected using a custom Android application. The research questions addressed in this paper are: (1) How do consumers’ preferences change over time with regards to the product’s positioning? (2) Which product features positively influence product positioning and which negatively? To answer these questions, we compared the product-positioning strategy and consumers’ opinions before and after the release of a new product to identify possible discrepancies between expected and actual positioning of the product. This work constitutes a methodological contribution with demonstrated implications for new product positioning strategy evaluation using tweet analysis.
ER  - 
TY  - JOUR
T1  - Clinical information extraction applications: A literature review
A1  - Wang, Yanshan
A1  - Wang, Liwei
A1  - Rastegar-Mojarad, Majid
A1  - Moon, Sungrim
A1  - Shen, Feichen
A1  - Afzal, Naveed
A1  - Liu, Sijia
A1  - Zeng, Yuqun
A1  - Mehrabi, Saeed
A1  - Sohn, Sunghwan
A1  - Liu, Hongfang
Y1  - 2018///
KW  -  Application
KW  -  Clinical notes
KW  -  Electronic health records
KW  -  Natural language processing
KW  - Information extraction
JF  - Journal of Biomedical Informatics
VL  - 77
SP  - 34
EP  - 49
DO  - https://doi.org/10.1016/j.jbi.2017.11.011
UR  - https://www.sciencedirect.com/science/article/pii/S1532046417302563
N2  - Background
With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text.
Objectives
In this literature review, we present a review of recent published research on clinical information extraction (IE) applications.
Methods
A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process & Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library.
Results
A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations.
Conclusions
Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.
ER  - 
TY  - JOUR
T1  - Ontology-based automatic identification of public health-related Turkish tweets
A1  - Küçük, Emine Ela
A1  - Yapar, Kürşad
A1  - Küçük, Dilek
A1  - Küçük, Doğan
Y1  - 2017///
KW  -  Automatic text processing
KW  -  Health informatics
KW  -  Social media analysis
KW  -  Twitter
KW  - Public health
JF  - Computers in Biology and Medicine
VL  - 83
SP  - 1
EP  - 9
DO  - https://doi.org/10.1016/j.compbiomed.2017.02.001
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517300318
N2  - Social media analysis, such as the analysis of tweets, is a promising research topic for tracking public health concerns including epidemics. In this paper, we present an ontology-based approach to automatically identify public health-related Turkish tweets. The system is based on a public health ontology that we have constructed through a semi-automated procedure. The ontology concepts are expanded through a linguistically motivated relaxation scheme as the last stage of ontology development, before being integrated into our system to increase its coverage. The ultimate lexical resource which includes the terms corresponding to the ontology concepts is used to filter the Twitter stream so that a plausible tweet subset, including mostly public-health related tweets, can be obtained. Experiments are carried out on two million genuine tweets and promising precision rates are obtained. Also implemented within the course of the current study is a Web-based interface, to track the results of this identification system, to be used by the related public health staff. Hence, the current social media analysis study has both technical and practical contributions to the significant domain of public health.
ER  - 
TY  - JOUR
T1  - An automated pattern recognition system for classifying indirect immunofluorescence images of HEp-2 cells and specimens
A1  - Manivannan, Siyamalan
A1  - Li, Wenqi
A1  - Akbar, Shazia
A1  - Wang, Ruixuan
A1  - Zhang, Jianguo
A1  - McKenna, Stephen J
Y1  - 2016///
KW  -  Cell classification
KW  -  Ensemble SVM
KW  -  HEp-2 cells
KW  -  Multi-resolution local patterns
KW  -  Subcellular fluorescence patterns
KW  - Anti-nuclear antibody test
JF  - Pattern Recognition
VL  - 51
SP  - 12
EP  - 26
DO  - https://doi.org/10.1016/j.patcog.2015.09.015
UR  - https://www.sciencedirect.com/science/article/pii/S0031320315003465
N2  - Immunofluorescence antinuclear antibody tests are important for diagnosis and management of autoimmune conditions; a key step that would benefit from reliable automation is the recognition of subcellular patterns suggestive of different diseases. We present a system to recognize such patterns, at cellular and specimen levels, in images of HEp-2 cells. Ensembles of SVMs were trained to classify cells into six classes based on sparse encoding of texture features with cell pyramids, capturing spatial, multi-scale structure. A similar approach was used to classify specimens into seven classes. Software implementations were submitted to an international contest hosted by ICPR 2014 (Performance Evaluation of Indirect Immunofluorescence Image Analysis Systems). Mean class accuracies obtained on heldout test data sets were 87.1% and 88.5% for cell and specimen classification respectively. These were the highest achieved in the competition, suggesting that our methods are state-of-the-art. We provide detailed descriptions and extensive experiments with various features and encoding methods.
ER  - 
TY  - JOUR
T1  - Infinite max-margin factor analysis via data augmentation
A1  - Zhang, Xuefeng
A1  - Chen, Bo
A1  - Liu, Hongwei
A1  - Zuo, Lei
A1  - Feng, Bo
Y1  - 2016///
KW  -  Classification and rejection performance
KW  -  Dirichlet process mixture
KW  -  Factor analysis
KW  - Latent variable support vector machine
JF  - Pattern Recognition
VL  - 52
SP  - 17
EP  - 32
DO  - https://doi.org/10.1016/j.patcog.2015.10.020
UR  - https://www.sciencedirect.com/science/article/pii/S0031320315003994
N2  - This paper addresses the Bayesian estimation of the discriminative probabilistic latent models, especially the mixture models. We develop the max-margin factor analysis (MMFA) model, which utilizes the latent variable support vector machine (LVSVM) as the classification criterion in the latent space to learn a discriminative subspace with max-margin constraint. Furthermore, to deal with multimodally distributed data, we further extend MMFA to infinite Gaussian mixture model and develop the infinite max-margin factor analysis (iMMFA) model, via the consideration of Dirichlet process mixtures (DPM). It jointly learns clustering, max-margin classifiers and the discriminative latent space in a united framework to improve the prediction performance. Moreover, both of MMFA and iMMFA are natural to handle outlier rejection problem, since the observations are described by a single or a mixture of Gaussian distributions. Additionally, thanks to the conjugate property, the parameters in the two models can be inferred efficiently via the simple Gibbs sampler. Finally, we implement our models on synthesized and real-world data, including multimodally distributed datasets and measured radar echo data, to validate the classification and rejection performance of the proposed models.
ER  - 
TY  - JOUR
T1  - A probabilistic topic model for clinical risk stratification from electronic health records
A1  - Huang, Zhengxing
A1  - Dong, Wei
A1  - Duan, Huilong
Y1  - 2015///
KW  -  Electronic health record
KW  -  Joint patient sub-profile-risk modeling
KW  -  Latent Dirichlet Allocation
KW  -  Patient sub-profile
KW  -  Probabilistic topic model
KW  - Risk stratification
JF  - Journal of Biomedical Informatics
VL  - 58
SP  - 28
EP  - 36
DO  - https://doi.org/10.1016/j.jbi.2015.09.005
UR  - https://www.sciencedirect.com/science/article/pii/S1532046415001975
N2  - Background and objective
Risk stratification aims to provide physicians with the accurate assessment of a patient’s clinical risk such that an individualized prevention or management strategy can be developed and delivered. Existing risk stratification techniques mainly focus on predicting the overall risk of an individual patient in a supervised manner, and, at the cohort level, often offer little insight beyond a flat score-based segmentation from the labeled clinical dataset. To this end, in this paper, we propose a new approach for risk stratification by exploring a large volume of electronic health records (EHRs) in an unsupervised fashion.
Methods
Along this line, this paper proposes a novel probabilistic topic modeling framework called probabilistic risk stratification model (PRSM) based on Latent Dirichlet Allocation (LDA). The proposed PRSM recognizes a patient clinical state as a probabilistic combination of latent sub-profiles, and generates sub-profile-specific risk tiers of patients from their EHRs in a fully unsupervised fashion. The achieved stratification results can be easily recognized as high-, medium- and low-risk, respectively. In addition, we present an extension of PRSM, called weakly supervised PRSM (WS-PRSM) by incorporating minimum prior information into the model, in order to improve the risk stratification accuracy, and to make our models highly portable to risk stratification tasks of various diseases.
Results
We verify the effectiveness of the proposed approach on a clinical dataset containing 3463 coronary heart disease (CHD) patient instances. Both PRSM and WS-PRSM were compared with two established supervised risk stratification algorithms, i.e., logistic regression and support vector machine, and showed the effectiveness of our models in risk stratification of CHD in terms of the Area Under the receiver operating characteristic Curve (AUC) analysis. As well, in comparison with PRSM, WS-PRSM has over 2% performance gain, on the experimental dataset, demonstrating that incorporating risk scoring knowledge as prior information can improve the performance in risk stratification.
Conclusions
Experimental results reveal that our models achieve competitive performance in risk stratification in comparison with existing supervised approaches. In addition, the unsupervised nature of our models makes them highly portable to the risk stratification tasks of various diseases. Moreover, patient sub-profiles and sub-profile-specific risk tiers generated by our models are coherent and informative, and provide significant potential to be explored for the further tasks, such as patient cohort analysis. We hypothesize that the proposed framework can readily meet the demand for risk stratification from a large volume of EHRs in an open-ended fashion.
ER  - 
TY  - JOUR
T1  - Online heart monitoring systems on the internet of health things environments: A survey, a reference model and an outlook
A1  - Santos, Marcus A G
A1  - Munoz, Roberto
A1  - Olivares, Rodrigo
A1  - Filho, Pedro P Rebouças
A1  - Ser, Javier Del
A1  - de Albuquerque, Victor Hugo C
Y1  - 2020///
KW  -  Bio sensors
KW  -  Heart
KW  -  Online monitoring
KW  -  Reference model
KW  - Internet of health things
JF  - Information Fusion
VL  - 53
SP  - 222
EP  - 239
DO  - https://doi.org/10.1016/j.inffus.2019.06.004
UR  - https://www.sciencedirect.com/science/article/pii/S1566253518309035
N2  - The Internet of Health Things promotes personalized and higher standards of care. Its application is diverse and attracts the attention of a substantial section of the scientific community. This approach has also been applied by people looking to enhance quality of life by using this technology. In this paper, we perform a survey that aims to present and analyze the advances of the latest studies based on medical care and assisted environment. We focus on articles for online monitoring, detection, and support of the diagnosis of cardiovascular diseases. Our research covers published manuscripts in scientific journals and recognized conferences since the year 2015. Also, we present a reference model based on the evaluation of the resources used from the selected studies. Finally, our proposal aims to help future enthusiasts to discover and enumerate the required factors for the development of a prototype for online heart monitoring purposes.
ER  - 
TY  - JOUR
T1  - Deep insight: Convolutional neural network and its applications for COVID-19 prognosis
A1  - Khanday, Nadeem Yousuf
A1  - Sofi, Shabir Ahmad
Y1  - 2021///
KW  -  Applications
KW  -  COVID-19
KW  -  Challenges
KW  -  Pandemic
KW  - Convolutional neural network
JF  - Biomedical Signal Processing and Control
VL  - 69
SP  - 102814
EP  - 102814
DO  - https://doi.org/10.1016/j.bspc.2021.102814
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421004110
N2  - Background and objective
SARS-CoV-2, a novel strain of coronavirus’ also called coronavirus disease 19 (COVID-19), a highly contagious pathogenic respiratory viral infection emerged in December 2019 in Wuhan, a city in China's Hubei province without an obvious cause. Very rapidly it spread across the globe (over 200 countries and territories) and finally on 11 March 2020 World Health Organisation characterized it as a “pandemic”. Although it has low mortality of around 3% as of 18 May 2021 it has already infected 164,316,270 humans with 3,406,027 unfortunate deaths. Undoubtedly the world was rocked by the COVID-19 pandemic, but researchers rose to all manner of challenges to tackle this pandemic by adopting the shreds of evidence of ML and AI in previous epidemics to develop novel models, methods, and strategies. We aim to provide a deeper insight into the convolutional neural network which is the most notable and extensively adopted technique on radiographic visual imagery to help expert medical practitioners and researchers to design and finetune their state-of-the-art models for their applicability in the arena of COVID-19.
Method
In this study, a deep convolutional neural network, its layers, activation and loss functions, regularization techniques, tools, methods, variants, and recent developments were explored to find its applications for COVID-19 prognosis. The pipeline of a general architecture for COVID-19 prognosis has also been proposed.
Result
This paper highlights recent studies of deep CNN and its applications for better prognosis, detection, classification, and screening of COVID-19 to help researchers and expert medical community in multiple directions. It also addresses a few challenges, limitations, and outlooks while using such methods for COVID-19 prognosis.
Conclusion
The recent and ongoing developments in AI, MI, and deep learning (Deep CNN) has shown promising results and significantly improved performance metrics for screening, prediction, detection, classification, forecasting, medication, treatment, contact tracing, etc. to curtail the manual intervention in medical practice. However, the research community of medical experts is yet to recognize and label the benchmark of the deep learning framework for effective detection of COVID-19 positive cases from radiology imagery.
ER  - 
TY  - JOUR
T1  - A decision-making mechanism for assessing risk factor significance in cardiovascular diseases
A1  - Hsu, Wei-Yen
Y1  - 2018///
KW  -  Cardiovascular diseases
KW  -  Feature ranking
KW  -  Medical decision making
KW  -  Radial basis function network
KW  - Decision support systems
JF  - Decision Support Systems
VL  - 115
SP  - 64
EP  - 77
DO  - https://doi.org/10.1016/j.dss.2018.09.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167923618301556
N2  - Cardiovascular diseases (CVDs) are severe diseases whose growing incidence worldwide has spurred increased national healthcare spending. Despite numerous diagnostic and treatment suggestions, CVDs continue to merit investigation due to their diverse risk factors, some of which are positively, negatively, or not correlated. To assist doctors and researchers in identifying the significance of CVD risk factors, in this study we propose a novel ranking and attribute (or feature) selection algorithm. We applied seven popular machine learning technologies to generate attribute-ranked datasets in order to identify the ideal number of factors/attributes for each classifier. Above all, the results of the comparisons indicate that the performance of parts of factors after ranking and attribute selection was significantly better than the performance of whole factors and that of several state-of-the-art algorithms. Since such knowledge can aid the proper selection of factors of CVD patients and thereby assist doctors in making better decisions in diagnostics and treatment, our results can reduce treatment costs and thus lower the economic burden of healthcare.
ER  - 
TY  - JOUR
T1  - A hybrid novelty score and its use in keystroke dynamics-based user authentication
A1  - Kang, Pilsung
A1  - Cho, Sungzoon
Y1  - 2009///
KW  -  Incremental learning
KW  -  Keystroke dynamics-based user authentication
KW  -  Nearest-neighbor learning
KW  -  Topological relation
KW  - Novelty detection
JF  - Pattern Recognition
VL  - 42
IS  - 11
SP  - 3115
EP  - 3127
DO  - https://doi.org/10.1016/j.patcog.2009.04.009
UR  - https://www.sciencedirect.com/science/article/pii/S0031320309001502
N2  - The purpose of novelty detection is to detect (novel) patterns that are not generated by the identical distribution of the normal class. A distance-based novelty detector classifies a new data pattern as “novel” if its distance from “normal” patterns is large. It is intuitive, easy to implement, and fits naturally with incremental learning. Its performance is limited, however, because it relies only on distance. In this paper, we propose considering topological relations as well. We compare our proposed method with 13 other novelty detectors based on 21 benchmark data sets from two sources. We then apply our method to a real-world application in which incremental learning is necessary: keystroke dynamics-based user authentication. The experimental results are promising. Not only does our method improve the performance of distance-based novelty detectors, but it also outperforms the other non-distance-based algorithms. Our method also allows efficient model updates.
ER  - 
TY  - JOUR
T1  - Automated detection of coronary artery disease using different durations of ECG segments with convolutional neural network
A1  - Acharya, U Rajendra
A1  - Fujita, Hamido
A1  - Lih, Oh Shu
A1  - Adam, Muhammad
A1  - Tan, Jen Hong
A1  - Chua, Chua Kuang
Y1  - 2017///
KW  -  CNN
KW  -  ECG
KW  -  Feature
KW  -  Heart
KW  -  Testing
KW  -  Training
KW  - CAD
JF  - Knowledge-Based Systems
VL  - 132
SP  - 62
EP  - 71
DO  - https://doi.org/10.1016/j.knosys.2017.06.003
UR  - https://www.sciencedirect.com/science/article/pii/S0950705117302769
N2  - Coronary artery disease (CAD) is caused due by the blockage of inner walls of coronary arteries by plaque. This constriction reduces the blood flow to the heart muscles resulting in myocardial infarction (MI). The electrocardiogram (ECG) is commonly used to screen the cardiac health. The ECG signals are nonstationary and nonlinear in nature whereby the transient disease indicators may appear randomly on the time scale. Therefore, the procedure to diagnose the abnormal beat is arduous, time consuming and prone to human errors. The automated diagnosis system overcomes these problems. In this study, convolutional neural network (CNN) structures comprising of four convolutional layers, four max pooling layers and three fully connected layers are proposed for the diagnosis of CAD using two and five seconds durations of ECG signal segments. Deep CNN is able to differentiate between normal and abnormal ECG with an accuracy of 94.95%, sensitivity of 93.72%, and specificity of 95.18% for Net 1 (two seconds) and accuracy of 95.11%, sensitivity of 91.13% and specificity of 95.88% for Net 2 (5s). The proposed system can help the clinicians in their accurate and reliable decision making of CAD using ECG signals.
ER  - 
TY  - JOUR
T1  - ARC-SL: Association rule-based classification with soft labels
A1  - Geng, Xiaojiao
A1  - Liang, Yan
A1  - Jiao, Lianmeng
Y1  - 2021///
KW  -  Belief functions
KW  -  Imprecise classification association rule
KW  -  Soft labels
KW  - Association classification
JF  - Knowledge-Based Systems
VL  - 225
SP  - 107116
EP  - 107116
DO  - https://doi.org/10.1016/j.knosys.2021.107116
UR  - https://www.sciencedirect.com/science/article/pii/S0950705121003798
N2  - As one of the most promising classification schemes, association classification (AC) effectively integrates classification with association rule discovery, but it requires precisely labelled data which is usually expensive or hard to obtain in practice. This paper extends the AC to solve the classification of data with soft labels represented as belief functions defined over the set of classes. To characterize the imprecision of labels, a more general rule, called imprecise classification association rule (ICAR), is first introduced so that the consequent is more flexible, being any subset of the class set. Meanwhile, measures of support and confidence are proposed for ICAR by taking into account the belief functions encoded in soft labels. With the new rule structure, an association rule-based soft labelled classification method, called ARC-SL, is then developed to build a more accurate classification model containing three phases: entropy-based adaptive partition for deriving fuzzy regions of continuous attributes, Apriori-based rule mining for generating a set of ICARs with variable support and confidence thresholds, and rule pruning for discarding redundant or poor rules. Finally, a belief reasoning procedure is presented to classify each input instance through combining the activated rules in the framework of belief functions. Experiments on benchmark datasets and an application to facial expression recognition demonstrate the effectiveness of the proposed method.
ER  - 
TY  - JOUR
T1  - Current Application of Digital Diagnosing Systems for Retinopathy of Prematurity
A1  - Bao, Yuekun
A1  - Ming, Wai-Kit
A1  - Mou, Zhi-Wei
A1  - Kong, Qi-Hang
A1  - Li, Ang
A1  - Yuan, Ti-Fei
A1  - Mi, Xue-Song
Y1  - 2021///
KW  -  computer-based image analysis
KW  -  deep learning
KW  -  machine learning
KW  -  multiple instance learning
KW  - retinopathy of prematurity
JF  - Computer Methods and Programs in Biomedicine
VL  - 200
SP  - 105871
EP  - 105871
DO  - https://doi.org/10.1016/j.cmpb.2020.105871
UR  - https://www.sciencedirect.com/science/article/pii/S0169260720317041
N2  - Background and Objective
Retinopathy of prematurity (ROP), a proliferative vascular eye disease, is one of the leading causes of blindness in childhood and prevails in premature infants with low-birth-weight. The recent progress in digital image analysis offers novel strategies for ROP diagnosis. This paper provides a comprehensive review on the development of digital diagnosing systems for ROP to software researchers. It may also be adopted as a guide to ophthalmologists for selecting the most suitable diagnostic software in the clinical setting, particularly for the remote ophthalmic support.
Methods
We review the latest literatures concerning the application of digital diagnosing systems for ROP. The diagnosing systems are analyzed and categorized. Articles published between 1998 and 2020 were screened with the two searching engines Pubmed and Google Scholar.
Results
Telemedicine is a method of remote image interpretation that can provide medical service to remote regions, and yet requires training to local operators. On the basis of image collection in telemedicine, computer-based image analytical systems for ROP were later developed. So far, the aforementioned systems have been mainly developed by virtue of classic machine learning, deep learning (DL) and multiple machine learning. During the past two decades, various computer-aided systems for ROP based on classic machine learning (e.g. RISA, ROPtool, CAIER) became available and have achieved satisfactory performance. Further, automated systems for ROP diagnosis based on DL are developed for clinical applications and exhibit high accuracy. Moreover, multiple instance learning is another method to establish an automated system for ROP detection besides DL, which, however, warrants further investigation in future.
Conclusion
At present, the incorporation of computer-based image analysis with telemedicine potentially enables the detection, supervision and in-time treatment of ROP for the preterm babies.
ER  - 
TY  - JOUR
T1  - Blockchain technology in the healthcare industry: Trends and opportunities
A1  - Hussien, Hassan Mansur
A1  - Yasin, Sharifah Md
A1  - Udzir, Nur Izura
A1  - Ninggal, Mohd Izuan Hafez
A1  - Salman, Sadeq
Y1  - 2021///
KW  -  Blockchain technology
KW  -  Healthcare industries
KW  -  Privacy protection
KW  -  Prospects
KW  -  information security
KW  - Bibliometric
JF  - Journal of Industrial Information Integration
VL  - 22
SP  - 100217
EP  - 100217
DO  - https://doi.org/10.1016/j.jii.2021.100217
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X21000170
N2  - The growth in the use of blockchain technology in healthcare is remarkable and has a significant impact on the healthcare industry. In this work, the gap between the healthcare industry and blockchain technologies was addressed by evaluating previous activities. Bibliometric analysis of dataset distribution, venues, keywords and citations was conducted to identify the trend of blockchain technology in healthcare. Case studies of telecare medicine information system and E-health were also reviewed and evaluated in terms of security and privacy. This study discussed potential future challenges such as scalability and storage capacity, blockchain size, universal interoperability and standardisation. This work highlighted the motivations of employing blockchain technology in the healthcare industry. Prospects in health data and sharing process, clinical trials, the pharmaceutical industry, big data, artificial intelligence, 5 G ultrasonic device, security and privacy were highlighted.
ER  - 
TY  - JOUR
T1  - Prediction and pattern analysis of medication refill adherence through electronic health records and dispensation data
A1  - Galozy, Alexander
A1  - Nowaczyk, Slawomir
Y1  - 2020///
KW  -  Electronic health records
KW  -  Prediction
KW  -  Refill patterns
KW  -  Simulation
KW  - Medication refill adherence
JF  - Journal of Biomedical Informatics
VL  - 112
SP  - 100075
EP  - 100075
DO  - https://doi.org/10.1016/j.yjbinx.2020.100075
UR  - https://www.sciencedirect.com/science/article/pii/S2590177X20300093
N1  - Articles initially published in Journal of Biomedical Informatics: X 5-8, 2020
N2  - Background and purpose
Low adherence to medication in chronic disease patients leads to increased morbidity, mortality, and healthcare costs. The widespread adoption of electronic prescription and dispensation records allows a more comprehensive overview of medication utilization. In combination with electronic health records (EHR), such data provides new opportunities for identifying patients at risk of nonadherence and provide more targeted and effective interventions. The purpose of this article is to study the predictability of medication adherence for a cohort of hypertensive patients, focusing on healthcare utilization factors under various predictive scenarios. Furthermore, we discover common proportion of days covered patterns (PDC-patterns) for patients with index prescriptions and simulate medication-taking behaviours that might explain observed patterns.
Procedures
We predict refill adherence focusing on factors of healthcare utilization, such as visits, prescription information and demographics of patient and prescriber. We train models with machine learning algorithms, using four different data splits: stratified random, patient, temporal forward prediction with and without index patients. We extract frequent, two-year long PDC-patterns using K-means clustering and investigate five simple models of medication-taking that can generate such PDC-patterns.
Findings
Model performance varies between data splits (AUC test set: 0.77–0.89). Including historical information increases the performance slightly in most cases (approx. 1–2% absolute AUC uplift). Models show low predictive performance (AUC test set: 0.56–0.66) on index-prescriptions and patients with sudden drops in PDC (Recall: 0.58–0.63). We find 21 distinct two-year PDC-patterns, ranging from good adherence to intermittent gaps and early discontinuation in the first or second year. Simulations show that observed PDC-patterns can only be explained by specific medication consumption behaviours.
Conclusions
Prediction models developed using EHR exhibit bias towards patients with high healthcare utilization. Even though actual medication-taking is not observable, consumption patterns may not be as arbitrary, provided that medication refilling and consumption is linked.
ER  - 
TY  - JOUR
T1  - An analytic approach to better understanding and management of coronary surgeries
A1  - Delen, Dursun
A1  - Oztekin, Asil
A1  - Tomak, Leman
Y1  - 2012///
KW  -  Clinical decision support systems
KW  -  Coronary artery bypass surgery (CABG)
KW  -  Data mining
KW  -  Machine learning
KW  -  Sensitivity analysis
KW  -  Survival prediction
KW  - Heart disease
JF  - Decision Support Systems
VL  - 52
IS  - 3
SP  - 698
EP  - 705
DO  - https://doi.org/10.1016/j.dss.2011.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S0167923611002089
N2  - Demand for high-quality, affordable healthcare services increasing with the aging population in the US. In order to cope with this situation, decision makers in healthcare (managerial, administrative and/or clinical) need to be increasingly more effective and efficient at what they do. Along with expertise, information and knowledge are the other key sources for better decisions. Data mining techniques are becoming a popular tool for extracting information/knowledge hidden deep into large healthcare databases. In this study, using a large, feature-rich, nationwide inpatient databases along with four popular machine learning techniques, we developed predictive models; and using an information fusion based sensitivity analysis on these models, we explained the surgical outcome of a patient undergoing a coronary artery bypass grafting. In this study, support vector machines produced the best prediction results (87.74%) followed by decision trees and neural networks. Studies like this illustrate the fact that accurate prediction and better understanding of such complex medical interventions can potentially lead to more favorable outcomes and optimal use of limited healthcare resources.
ER  - 
TY  - JOUR
T1  - Mining time dependency patterns in clinical pathways
A1  - Lin, Fu-ren
A1  - Chou, Shien-chao
A1  - Pan, Shung-mei
A1  - Chen, Yao-mei
Y1  - 2001///
KW  -  Association
KW  -  Data mining
KW  -  Sequence pattern
KW  -  Time dependency pattern
KW  - Clinical pathways
JF  - International Journal of Medical Informatics
VL  - 62
IS  - 1
SP  - 11
EP  - 25
DO  - https://doi.org/10.1016/S1386-5056(01)00126-5
UR  - https://www.sciencedirect.com/science/article/pii/S1386505601001265
N2  - Clinical pathways are widely adopted by many large hospitals around the world in order to provide high-quality patient treatment and reduce the length of hospital stay of each patient. The development of clinical pathways is a lengthy process, and may require the collaboration among physicians, nurses, and staffs in a hospital. However, the individual differences cause great variances in the execution of clinical pathways. It calls for a more dynamic and adaptive process to improve the performance of clinical pathways. This paper reports a data mining technique we have developed to discover the time dependency pattern of clinical pathways for managing brain stroke. The mining of time dependency pattern is to discover patterns of process execution sequences and to identify the dependent relation between activities in a majority of cases. By obtaining the time dependency patterns, we can predict the paths for new patients when he/she is admitted into a hospital; in turn, the health care procedure will be more effective and efficient.
ER  - 
TY  - JOUR
T1  - A neural network approach to classify carotid disorders from Heart Rate Variability analysis
A1  - Verde, Laura
A1  - De Pietro, Giuseppe
Y1  - 2019///
KW  -  Artificial neural networks
KW  -  Correlation-based feature selection
KW  -  HRV analysis
KW  -  Signal processing
KW  - Carotid diseases
JF  - Computers in Biology and Medicine
VL  - 109
SP  - 226
EP  - 234
DO  - https://doi.org/10.1016/j.compbiomed.2019.04.036
UR  - https://www.sciencedirect.com/science/article/pii/S0010482519301441
N2  - Background
Atherosclerosis is a progressive process responsible for most heart diseases and ischemic stroke. It constitutes, in fact, the most common cause of stroke in middle-aged people. To avoid or, at least, limit the disabling deficits that may derive from a carotid disease, a prompt and early diagnosis is necessary. The diagnostic technique used to detect a carotid disease is the eco-color Doppler. Unfortunately, this method is not free from errors, due to manufacturer mistakes or its operator dependence.
Methods
In this study, we propose an automated methodology capable of identifying the presence of a carotid disease from the Heart Rate Variability analysis of electrocardiographic signals. A Correlation-based Feature Selector for data reduction and Artificial Neural Networks are used to distinguish between pathological and healthy subjects.
Results
A series of tests has been realized to evaluate the proposed approach by using electrocardiographic signals selected from an available database in order to analyse the classification ability in comparison with other algorithms existing in literature. The results obtained show that the proposed approach provides values of accuracy, sensitivity, specificity, precision, F-measure and ROC area, respectively equal to 90.5%, 97.7%, 72.9%, 89.7%, 93.5% and 0.957, better than those achieved by other algorithms.
Conclusions
Considering the achieved accuracy, our methodology is more effective than any of the main algorithm existing in literature. It is important to note that this approach is proposed as a support for the diagnosis of a carotid disorder through a non-invasive approach.
ER  - 
TY  - JOUR
T1  - Perceptual relativity-based local hyperplane classification
A1  - Wen, Guihua
A1  - Jiang, Lijun
A1  - Wen, Jun
A1  - Wei, Jia
A1  - Yu, Zhiwen
Y1  - 2012///
KW  -  Classification
KW  -  Nearest neighbors
KW  -  Perceptual relativity
KW  - Local hyperplane
JF  - Neurocomputing
VL  - 97
SP  - 155
EP  - 163
DO  - https://doi.org/10.1016/j.neucom.2012.03.018
UR  - https://www.sciencedirect.com/science/article/pii/S0925231212003967
N2  - The k-local hyperplane distance nearest neighbors classification (HKNN) builds a non-linear decision surface with maximal local margin in the input space, with invariance inferred from the local neighborhood rather than the prior knowledge, so that it performs very well in many applications. However, it still cannot be comparable with human being in classification on the noisy, the sparse, and the imbalance data. This paper proposes a new approach,called relative local hyperplane classifier(RLHC),to overcome this problem by utilizing the perceptual relativity to HKNN. It finds k nearest neighbors for the query sample from each class and then performs the relative transformation over all these nearest neighbors to build the relative space. Subsequently, each local hyperplane is constructed in the relative space, which is then applied to perform the classification. Experimental results on both real and simulated data suggest that the proposed approach often gives the better results in classification and robustness.
ER  - 
TY  - JOUR
T1  - Improving the transfer ability of prediction models for electronic noses
A1  - Yan, Ke
A1  - Zhang, David
Y1  - 2015///
KW  -  Breath analysis
KW  -  Calibration transfer
KW  -  Device standardization
KW  -  Tikhonov regularization
KW  - Electronic nose
JF  - Sensors and Actuators B: Chemical
VL  - 220
SP  - 115
EP  - 124
DO  - https://doi.org/10.1016/j.snb.2015.05.060
UR  - https://www.sciencedirect.com/science/article/pii/S0925400515006796
N2  - Calibration transfer is attracting more and more attention in the field of electronic noses (e-noses). It aims at making the prediction model trained on one device transferable to other devices, which is important for the large-scale deployment of e-noses, especially when the cost of sample collection is high. In this paper, the transfer ability of prediction models is improved in two steps. First, windowed piecewise direct standardization (WPDS) is used to standardize the slave device, i.e. to transform the variables from the slave device to match the master one. Then, data from the master device are used to develop prediction models with a novel strategy named standardization error based model improvement (SEMI). Finally, the standardized slave data can be predicted by the models with a better accuracy. The proposed WPDS is a generalization of the widely used PDS algorithm. The main idea of SEMI is to make the trained models rely more on variables with small standardization errors, thus less sensitive to the inconsistency of the devices. It links the standardization step and the prediction step. To evaluate the algorithms, three e-noses specialized for breath analysis are adopted to collect a dataset, which contains pure chemicals and breath samples. Experiments show that WPDS outperforms previous methods in the sense of standardization error and prediction accuracy; SEMI consistently enhances the accuracy of the master model applied to standardized slave data. This study provides effective and extensible methods for model transfer of e-noses.
ER  - 
TY  - JOUR
T1  - A novel odor filtering and sensing system combined with regression analysis for chemical vapor quantification
A1  - Jha, Sunil K
A1  - Hayashi, Kenshi
Y1  - 2014///
KW  -  Adsorbing material
KW  -  E-nose
KW  -  MOS sensor
KW  -  Support vector regression
KW  -  VOCs concentration estimation.
KW  - Odor filter
JF  - Sensors and Actuators B: Chemical
VL  - 200
SP  - 269
EP  - 287
DO  - https://doi.org/10.1016/j.snb.2014.04.022
UR  - https://www.sciencedirect.com/science/article/pii/S0925400514004237
N2  - An advanced odor filtering and sensing system based on polymers, carbon molecular sieves, micro-ceramic heaters and metal oxide semiconductor (MOS) gas sensor array has been designed for quantitative identification of volatile organic chemicals (VOCs). MOS sensor resistance due to chemical vapor adsorption in filtering material and after desorption are measured for five target VOCs including acetone, benzene, ethanol, pentanal, and propenoic acid at distinct concentrations in between 3 and 500 parts per million (ppm). Two kinds of regression methods specifically linear regression analysis based on least square criterion and kernel function based support vector regression (SVR) have been employed to model sensor resistance with VOCs concentration. Scatter plot and Spearman's rank correlation coefficient (ρ) are used to investigate the strength of dependence of sensor resistance on vapor concentration and to search optimal filtering material for VOCs quantification prior to the regression analysis. Quantitative recognition efficiency of regression methods have been evaluated on the basis of coefficient of determination R2 (R-squared) and correlation values. MOS sensor resistance after vapor desorption with carbon molecular sieve (carboxen–1012) as filtering material results the maximum values of R-squared (R2=0.9957) and correlation (ρ=1.00) between the actual and estimated concentration for propenoic acid using radial basis kernel based SVR method.
ER  - 
TY  - JOUR
T1  - The impact of fine-tuning of optical recognition system on database reliability
A1  - Modesti, P A
A1  - Massetti, L
A1  - Bamoshmoosh, M
A1  - Baldereschi, M
A1  - Cambi, G E
A1  - Rapi, S
Y1  - 2012///
KW  -  Automatic data processing
KW  -  Clinical trial
KW  -  Data management system
KW  -  Optical readers
KW  - Data entry
JF  - Computers in Biology and Medicine
VL  - 42
IS  - 7
SP  - 778
EP  - 783
DO  - https://doi.org/10.1016/j.compbiomed.2012.05.003
UR  - https://www.sciencedirect.com/science/article/pii/S0010482512000820
N2  - Although optical reading systems are important tools to transfer data from a paper form to electronic databases, the impact of system fine-tuning on the final error rate is not usually considered. At the end of a multi-step process involving paper form design training of operators, and fine-tuning procedure, the final rate of error can be reduced from 0.65% to 0.05%. Fine-tuning should be introduced as a standard procedure while using optical reading systems.
ER  - 
TY  - JOUR
T1  - The effects of data sources, cohort selection, and outcome definition on a predictive model of risk of thirty-day hospital readmissions
A1  - Walsh, Colin
A1  - Hripcsak, George
Y1  - 2014///
KW  -  Electronic health record
KW  -  Predictive analytics
KW  -  Regularized Logistic Regression
KW  -  Risk modeling
KW  -  Text mining
KW  - Readmissions
JF  - Journal of Biomedical Informatics
VL  - 52
SP  - 418
EP  - 426
DO  - https://doi.org/10.1016/j.jbi.2014.08.006
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414001877
N1  - Special Section: Methods in Clinical Research Informatics
N2  - Background
Hospital readmission risk prediction remains a motivated area of investigation and operations in light of the hospital readmissions reduction program through CMS. Multiple models of risk have been reported with variable discriminatory performances, and it remains unclear how design factors affect performance.
Objectives
To study the effects of varying three factors of model development in the prediction of risk based on health record data: (1) reason for readmission (primary readmission diagnosis); (2) available data and data types (e.g. visit history, laboratory results, etc); (3) cohort selection.
Methods
Regularized regression (LASSO) to generate predictions of readmissions risk using prevalence sampling. Support Vector Machine (SVM) used for comparison in cohort selection testing. Calibration by model refitting to outcome prevalence.
Results
Predicting readmission risk across multiple reasons for readmission resulted in ROC areas ranging from 0.92 for readmission for congestive heart failure to 0.71 for syncope and 0.68 for all-cause readmission. Visit history and laboratory tests contributed the most predictive value; contributions varied by readmission diagnosis. Cohort definition affected performance for both parametric and nonparametric algorithms. Compared to all patients, limiting the cohort to patients whose index admission and readmission diagnoses matched resulted in a decrease in average ROC from 0.78 to 0.55 (difference in ROC 0.23, p value 0.01). Calibration plots demonstrate good calibration with low mean squared error.
Conclusion
Targeting reason for readmission in risk prediction impacted discriminatory performance. In general, laboratory data and visit history data contributed the most to prediction; data source contributions varied by reason for readmission. Cohort selection had a large impact on model performance, and these results demonstrate the difficulty of comparing results across different studies of predictive risk modeling.
ER  - 
TY  - JOUR
T1  - Adapting machine learning techniques to censored time-to-event health record data: A general-purpose approach using inverse probability of censoring weighting
A1  - Vock, David M
A1  - Wolfson, Julian
A1  - Bandyopadhyay, Sunayan
A1  - Adomavicius, Gediminas
A1  - Johnson, Paul E
A1  - Vazquez-Benitez, Gabriela
A1  - O’Connor, Patrick J
Y1  - 2016///
KW  -  Electronic health data
KW  -  Inverse probability weighting
KW  -  Machine learning
KW  -  Risk prediction
KW  -  Survival analysis
KW  - Censored data
JF  - Journal of Biomedical Informatics
VL  - 61
SP  - 119
EP  - 131
DO  - https://doi.org/10.1016/j.jbi.2016.03.009
UR  - https://www.sciencedirect.com/science/article/pii/S1532046416000496
N2  - Models for predicting the probability of experiencing various health outcomes or adverse events over a certain time frame (e.g., having a heart attack in the next 5years) based on individual patient characteristics are important tools for managing patient care. Electronic health data (EHD) are appealing sources of training data because they provide access to large amounts of rich individual-level data from present-day patient populations. However, because EHD are derived by extracting information from administrative and clinical databases, some fraction of subjects will not be under observation for the entire time frame over which one wants to make predictions; this loss to follow-up is often due to disenrollment from the health system. For subjects without complete follow-up, whether or not they experienced the adverse event is unknown, and in statistical terms the event time is said to be right-censored. Most machine learning approaches to the problem have been relatively ad hoc; for example, common approaches for handling observations in which the event status is unknown include (1) discarding those observations, (2) treating them as non-events, (3) splitting those observations into two observations: one where the event occurs and one where the event does not. In this paper, we present a general-purpose approach to account for right-censored outcomes using inverse probability of censoring weighting (IPCW). We illustrate how IPCW can easily be incorporated into a number of existing machine learning algorithms used to mine big health care data including Bayesian networks, k-nearest neighbors, decision trees, and generalized additive models. We then show that our approach leads to better calibrated predictions than the three ad hoc approaches when applied to predicting the 5-year risk of experiencing a cardiovascular adverse event, using EHD from a large U.S. Midwestern healthcare system.
ER  - 
TY  - JOUR
T1  - Two-group classification via a biobjective margin maximization model
A1  - Carrizosa, Emilio
A1  - Martin-Barragan, Belen
Y1  - 2006///
KW  -  Biobjective
KW  -  Classification
KW  -  Data mining
KW  -  ROC curve
KW  -  Support vector machines
KW  - Multiple objective programming
JF  - European Journal of Operational Research
VL  - 173
IS  - 3
SP  - 746
EP  - 761
DO  - https://doi.org/10.1016/j.ejor.2005.06.059
UR  - https://www.sciencedirect.com/science/article/pii/S0377221705006934
N2  - In this paper we propose a biobjective model for two-group classification via margin maximization, in which the margins in both classes are simultaneously maximized. The set of Pareto-optimal solutions is described, yielding a set of parallel hyperplanes, one of which is just the solution of the classical SVM approach. In order to take into account different misclassification costs or a priori probabilities, the ROC curve can be used to select one out of such hyperplanes by expressing the adequate tradeoff for sensitivity and specificity. Our result gives a theoretical motivation for using the ROC approach in case misclassification costs in the two groups are not necessarily equal.
ER  - 
TY  - JOUR
T1  - EPTs-TL: A two-level approach for efficient event prediction in healthcare
A1  - Mehrmolaei, Soheila
Y1  - 2021///
KW  -  Data-point weighting
KW  -  EPTs-TL
KW  -  Fuzzy logic concept
KW  -  Healthcare database
KW  -  Metaheuristics
KW  - Event prediction
JF  - Artificial Intelligence in Medicine
VL  - 111
SP  - 101999
EP  - 101999
DO  - https://doi.org/10.1016/j.artmed.2020.101999
UR  - https://www.sciencedirect.com/science/article/pii/S0933365720312641
N2  - Recently, the event prediction on time series (EPTs) was discussed as one of the important and interesting research trends that its usage is growing for taking proper decisions in the various sciences. In the real-world, time series event-based analysis can pose as one of the challenging prediction problems in healthcare, which have a direct impact and a key role in supporting health management. In this paper, an efficient approach of two-level (TL) is proposed to the EPTs problem in healthcare, which named EPTs-TL. At the first level, unseen time series data is predicted by using an enhanced hybrid model based on soft computing technology. Then, a new feature extraction-based method is proposed for fuzzy detection of future events in two-level. The EPTs -TL approach employed concepts of three components: weighting, fuzzy logic, and metaheuristics in two-level of the proposed approach. The empirical results demonstrate the excellent performance of the EPTs -TL approach in comparison to conventional prediction models in healthcare and medicine. Also, the proposed approach can be introduced as a strong tool to handle the complex and uncertain behaviors of time series, analyze unusual variations of those, forewarn the possible critical situations in the society, and fuzzy predict event in healthcare.
ER  - 
TY  - JOUR
T1  - 1000× faster than PLINK: Combined FPGA and GPU accelerators for logistic regression-based detection of epistasis
A1  - Wienbrandt, Lars
A1  - Kässens, Jan Christian
A1  - Hübenthal, Matthias
A1  - Ellinghaus, David
Y1  - 2019///
KW  -  FPGA GPU hardware accelerator
KW  -  Gene-gene (G × G) interaction
KW  -  Heterogeneous architecture
KW  -  Hybrid computing
KW  - Genome-wide association study (GWAS)
JF  - Journal of Computational Science
VL  - 30
SP  - 183
EP  - 193
DO  - https://doi.org/10.1016/j.jocs.2018.12.013
UR  - https://www.sciencedirect.com/science/article/pii/S1877750318310184
N2  - Logistic regression as implemented in PLINK is a powerful and commonly used framework for assessing gene-gene interactions. However, fitting regression models for each pair of markers in a genome-wide dataset is a computationally intensive task, for which reason pre-filtering techniques and fast epistasis screenings are applied to reduce the computational burden. We demonstrate that employing a combination of a Xilinx UltraScale FPGA with an Nvidia Tesla GPU leads to runtimes of only minutes for logistic regression tests on a genome-wide scale, resulting in a speedup of more than 1000 up to 1600 when compared to multi-threaded PLINK on a server-grade computing platform. This article is an extended version of our conference paper [1].
ER  - 
TY  - JOUR
T1  - Study of impaired cardiovascular and respiratory coupling during orthostatic stress based on joint symbolic dynamics
A1  - Reulecke, S
A1  - Charleston-Villalobos, S
A1  - González-Hermosillo, J
A1  - González-Camarena, R
A1  - Voss, A
A1  - Gaitán-González, M
A1  - Hernández-Pacheco, G
A1  - Schroeder, R
A1  - Aljama-Corrales, T
Y1  - 2018///
KW  -  Cardiorespiratory coupling
KW  -  Joint symbolic dynamics
KW  -  Orthostatic stress
KW  -  Vascular–respiratory coupling
KW  -  Vasovagal syncope
KW  - Cardiovascular coupling
JF  - Medical Engineering & Physics
VL  - 61
SP  - 51
EP  - 60
DO  - https://doi.org/10.1016/j.medengphy.2018.08.006
UR  - https://www.sciencedirect.com/science/article/pii/S1350453318301437
N2  - The present study investigates the instantaneous coupling among the cardiac, vascular, and respiratory systems, using the heart rate, respiration, and systolic and diastolic blood pressure variability in 12 healthy and 16 vasovagal syncope female subjects during a head-up tilt (HUT) testing protocol at 70° This study contributes to the coupling analysis by using a nonlinear joint symbolic dynamics (JSD) in a high-temporal resolution scheme, based on 5 min segments of the time series that are shifted every minute. For each segment, a bivariate JSD matrix was constructed to obtain global and local coupling indices in accordance to Shannon's entropy and the probability of occurrence of various bivariate words, respectively. The novel approach revealed important findings in the coupling dynamics of the systems, thus allowing the detection of group differences during the early orthostatic phase, and during the HUT test, before the occurrence of any pre-syncopal symptoms. In patients, the global indices indicated a significant decrease of cardiovascular coupling, starting at 10 min after the tilt-up, manifested by reduced baroreflex sensitivity and cardiorespiratory coupling that was initiated 8 min after the onset of the orthostatic phase (OP). A decreased autonomic control on cardiovascular–respiratory couplings was further evidenced by increased alterations of the JSD indices during the OP compared to the supine position in patients compared to controls. Furthermore, findings based on local indices demonstrated that female patients showed reductions and disengagements in cardiovascular (p < 0.001) and cardiorespiratory (p < 0.01) couplings, as early as the first 5 min and during the complete OP.
ER  - 
TY  - JOUR
T1  - Unsupervised Classification based Analysis of the Temporal Pattern of Insulin Sensitivity and Modelling Noise of Patient Groups under Tight Glycemic Control
A1  - Benyó, Balázs
A1  - Paláncz, Béla
A1  - Szlávecz, Ákos
A1  - Stewart, Kent
A1  - Homlok, József
A1  - Pretty, Christopher G
A1  - Chase, J Geoffrey
Y1  - 2018///
KW  -  Intensive Control Insulin-Nutrition-Glucose (ICIN
KW  -  Stochastic TARgeted Control (STAR)
KW  -  blood-glucose dynamics
KW  -  critical care
KW  -  stochastic differential equation
KW  -  stochastic noise
KW  -  temporal pattern
KW  -  unsupervised classification
KW  - clycaemic control
JF  - IFAC-PapersOnLine
VL  - 51
IS  - 27
SP  - 62
EP  - 67
DO  - https://doi.org/10.1016/j.ifacol.2018.11.619
UR  - https://www.sciencedirect.com/science/article/pii/S2405896318333287
N1  - 10th IFAC Symposium on Biological and Medical Systems BMS 2018
N2  - Background: Glycaemic control (GC) of critical care patients with abnormal blood glucose (BG) level can reduce mortality and improve clinical outcomes. Model based GC protocol allows personalised and effective control of BG level of the patients. As a part of the protocol the patient’s state is predicted by a stochastic model. Improving accuracy of patient state prediction would enable to develop more effective model-based GC algorithms. Methods: The temporal behaviour of the metabolic system of intensive care patients under glycaemic control was analysed and three patient cohorts from three geographically distant hospitals were compared with each other. The three hospitals used the same glycaemic control protocol, but provided different treatment environment. The patients, based on the time function of their state changes - described by the insulin sensitivity parameter (SI(t)) - were classified and the distribution of the patients from different cohorts were examined. Results: In the study no major differences were found in the distribution of the geographically distinct patient cohorts. As the SI value describes the metabolic state of the patient this result suggests that the temporal pattern of the metabolic state changes is similar in each patient cohorts. The patient state descriptor parameter (SI) is identified by using a physiological model. The accuracy of the model and the temporal changes in the accuracy are also analysed by a similar classification methodology than the one used for patient state change classification. The classified time function was the modelling noise identified by a stochastic model. The patients from different hospitals were distributed evenly between the resulted classes, thus modelling accuracy is found to be similar in the three patient cohorts. These results confirms previous studies, however in the previous studies mainly statistical comparison were made rather than the comparison of the temporal pattern of the state descriptor parameters. The correlation between the patient state and the modelling accuracy based classification is also analysed by comparing the classes resulted in by the above described two studies. As high portion of the patients are classified into the same classes by the two classification study we can state that the temporal pattern of the state change correlates with the temporal pattern of the modelling error.
ER  - 
TY  - JOUR
T1  - FIFS: A data mining method for informative marker selection in high dimensional population genomic data
A1  - Kavakiotis, Ioannis
A1  - Samaras, Patroklos
A1  - Triantafyllidis, Alexandros
A1  - Vlahavas, Ioannis
Y1  - 2017///
KW  -  Ancestry informative marker
KW  -  Big data
KW  -  Data mining
KW  -  Feature selection
KW  -  Frequent pattern mining
KW  -  Machine learning
KW  -  Population genomics
KW  -  Single nucleotide polymorphism
KW  - Bioinformatics
JF  - Computers in Biology and Medicine
VL  - 90
SP  - 146
EP  - 154
DO  - https://doi.org/10.1016/j.compbiomed.2017.09.020
UR  - https://www.sciencedirect.com/science/article/pii/S0010482517303189
N2  - Background and objective
Single Nucleotide Polymorphism (SNPs) are, nowadays, becoming the marker of choice for biological analyses involving a wide range of applications with great medical, biological, economic and environmental interest. Classification tasks i.e. the assignment of individuals to groups of origin based on their (multi-locus) genotypes, are performed in many fields such as forensic investigations, discrimination between wild and/or farmed populations and others. Τhese tasks, should be performed with a small number of loci, for computational as well as biological reasons. Thus, feature selection should precede classification tasks, especially for Single Nucleotide Polymorphism (SNP) datasets, where the number of features can amount to hundreds of thousands or millions.
Methods
In this paper, we present a novel data mining approach, called FIFS – Frequent Item Feature Selection, based on the use of frequent items for selection of the most informative markers from population genomic data. It is a modular method, consisting of two main components. The first one identifies the most frequent and unique genotypes for each sampled population. The second one selects the most appropriate among them, in order to create the informative SNP subsets to be returned.
Results
The proposed method (FIFS) was tested on a real dataset, which comprised of a comprehensive coverage of pig breed types present in Britain. This dataset consisted of 446 individuals divided in 14 sub-populations, genotyped at 59,436 SNPs. Our method outperforms the state-of-the-art and baseline methods in every case. More specifically, our method surpassed the assignment accuracy threshold of 95% needing only half the number of SNPs selected by other methods (FIFS: 28 SNPs, Delta: 70 SNPs Pairwise FST: 70 SNPs, In: 100 SNPs.)
Conclusion
Our approach successfully deals with the problem of informative marker selection in high dimensional genomic datasets. It offers better results compared to existing approaches and can aid biologists in selecting the most informative markers with maximum discrimination power for optimization of cost-effective panels with applications related to e.g. species identification, wildlife management, and forensics.
ER  - 
TY  - JOUR
T1  - Accuracy of rule extraction using a recursive-rule extraction algorithm with continuous attributes combined with a sampling selection technique for the diagnosis of liver disease
A1  - Hayashi, Yoichi
A1  - Fukunaga, Kazuhiro
Y1  - 2016///
KW  -  BUPA liver disorder dataset
KW  -  Child-Pugh score
KW  -  Hepatitis dataset
KW  -  Re-RX algorithm
KW  -  Sampling selection technique
KW  - Rule extraction
JF  - Informatics in Medicine Unlocked
VL  - 5
SP  - 26
EP  - 38
DO  - https://doi.org/10.1016/j.imu.2016.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S2352914816300272
N2  - Although liver cancer is the second most common cause of death from cancer worldwide, because of the limited accuracy and interpretability of extracted classification rules, the diagnosis of liver disease remains difficult. In addition, hepatitis, which is inflammation of the liver, can progress to fibrosis, cirrhosis, or even liver cancer. Numerous methods for diagnosing liver disease have been applied, but most current diagnostic methods are black box models that cannot adequately reveal information hidden in the data. In the medical setting, extracted rules must be not only highly accurate, but also highly interpretable. The Recursive-Rule eXtraction (Re-RX) algorithm is a white box model that generates highly accurate and interpretable classification rules on the basis of both discrete and continuous attributes; however, it tends to generate more rules than other rule extraction algorithms. The objectives of this study were to use a new rule extraction algorithm, Continuous Re-RX combined with sampling selection techniques (Sampling-Continuous Re-RX), to achieve highly accurate and interpretable diagnostic rules for the BUPA and Hepatitis datasets and to quantify the associations between the presence and severity of ascites and serum biomarkers with the risk of developing hepatitis in consideration of Child-Pugh scores. The performance of Sampling-Continuous Re-RX was compared with existing techniques, and as a result, it was found to extract more accurate, concise, and interpretable rules for the BUPA and Hepatitis datasets compared with previous extraction algorithms. In addition, the rules extracted using the proposed method were close to the trade-off curve, which indicated that they were more accurate and interpretable, and therefore more suitable in the medical setting.
ER  - 
TY  - JOUR
T1  - Evidence-based model for real-time surveillance of ARDS
A1  - Taoum, Aline
A1  - Mourad-Chehade, Farah
A1  - Amoud, Hassan
Y1  - 2019///
KW  -  Evidence-based theory
KW  -  Linear and nonlinear parameters
KW  -  Real-time surveillance
KW  - Acute respiratory distress syndrome
JF  - Biomedical Signal Processing and Control
VL  - 50
SP  - 83
EP  - 91
DO  - https://doi.org/10.1016/j.bspc.2019.01.016
UR  - https://www.sciencedirect.com/science/article/pii/S1746809419300163
N2  - Real-time health surveillance becomes important and necessary with the increase of the elderly population to preserve their quality of life. Real-time models aim to provide alerts before the severe illness occurs. Acute respiratory distress syndrome is a crucial disease of the respiratory system that threats the health of the elderly. This paper proposes a real-time model for the surveillance of ARDS based on belief functions theory. Non-invasive physiological signals are considered such as heart rate, respiratory rate, oxygen saturation and mean airway blood pressure. Different linear and nonlinear parameters are extracted from these signals; then a parameters selection procedure is performed to reduce their dimensionality. Afterwards, classifiers are constructed using parameters distributions defined in the evidence framework. Real-time prediction is then performed by combining all classifiers decisions. As results, high performances are obtained over the testing sets with performances of 77% and 71% for sensitivity and specificity, respectively.
ER  - 
TY  - JOUR
T1  - Automatic Segmentation of Diffuse Retinal Thickening Edemas Using Optical Coherence Tomography Images
A1  - Samagaio, Gabriela
A1  - de Moura, Joaquim
A1  - Novo, Jorge
A1  - Ortega, Marcos
Y1  - 2018///
KW  -  diffuse retinal thickening region
KW  -  optical coherence tomography
KW  -  segmentation
KW  - Computer-aided diagnosis
JF  - Procedia Computer Science
VL  - 126
SP  - 472
EP  - 481
DO  - https://doi.org/10.1016/j.procs.2018.07.281
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918312572
N1  - Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia
N2  - Diabetic retinopathy is one of the leading causes of vision impairment that is commonly associated to the Macular Edema (ME) disease. The Diffuse Retinal Thickening (DRT) is a ME type derived from the local intraretinal fluid accumulation in the lower retinal layers, producing significant morphological alterations in the eye fundus. The presence and properties of these intraretinal fluids are used by the ophthalmologists as significant indicators of the clinical stage of the ME disease. Given that, the precise identification and segmentation of the DRT edema type allow the early diagnosis of the ME disease which, therefore, permits a better adjustment of the treatments, reducing their costs as well as improving the life quality of the patients. This paper proposes a novel methodology for the automatic identification and segmentation of the DRT edemas using Optical Coherence Tomography (OCT) images as source of information. Firstly, the method identifies four of the principal retinal layers that are used as reference to delimit the outer retina, region where the DRT edemas are typically originated. Inside this region, a large and heterogeneous set of features was defined to recognize the characteristic “sponge-like” patterns of the DRT edema, using intensity, texture and clinically-defined features. For this analysis, four representative classifiers were employed with the best subsets of previously selected features. This methodology was tested using 70 OCT images from where 560 samples were extracted with the presence and absence of DRT edemas. The best results were achieved by the 7-kNN classifier, reaching in the detection stage an accuracy of 0.9366, whereas in the segmentation stage obtained values of 0.6625 and 0.7899 for the Jaccard and Dice coefficients, respectively.
ER  - 
TY  - JOUR
T1  - Wavelet-based segmentation of renal compartments in DCE-MRI of human kidney: Initial results in patients and healthy volunteers
A1  - Li, Sheng
A1  - Zöllner, Frank G
A1  - Merrem, Andreas D
A1  - Peng, Yinghong
A1  - Roervik, Jarle
A1  - Lundervold, Arvid
A1  - Schad, Lothar R
Y1  - 2012///
KW  -  -means
KW  -  Kidney
KW  -  Segmentation
KW  -  Wavelet analysis
KW  - DCE-MRI
JF  - Computerized Medical Imaging and Graphics
VL  - 36
IS  - 2
SP  - 108
EP  - 118
DO  - https://doi.org/10.1016/j.compmedimag.2011.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S0895611111000838
N2  - Renal diseases can lead to kidney failure that requires life-long dialysis or renal transplantation. Early detection and treatment can prevent progression towards end stage renal disease. MRI has evolved into a standard examination for the assessment of the renal morphology and function. We propose a wavelet-based clustering to group the voxel time courses and thereby, to segment the renal compartments. This approach comprises (1) a nonparametric, discrete wavelet transform of the voxel time course, (2) thresholding of the wavelet coefficients using Stein's Unbiased Risk estimator, and (3) k-means clustering of the wavelet coefficients to segment the kidneys. Our method was applied to 3D dynamic contrast enhanced (DCE-) MRI data sets of human kidney in four healthy volunteers and three patients. On average, the renal cortex in the healthy volunteers could be segmented at 88%, the medulla at 91%, and the pelvis at 98% accuracy. In the patient data, with aberrant voxel time courses, the segmentation was also feasible with good results for the kidney compartments. In conclusion wavelet based clustering of DCE-MRI of kidney is feasible and a valuable tool towards automated perfusion and glomerular filtration rate quantification.
ER  - 
TY  - JOUR
T1  - Ensemble learning model for diagnosing COVID-19 from routine blood tests
A1  - AlJame, Maryam
A1  - Ahmad, Imtiaz
A1  - Imtiaz, Ayyub
A1  - Mohammed, Ameer
Y1  - 2020///
KW  -  Diagnostic model
KW  -  Ensemble
KW  -  Machine learning
KW  -  Routine blood tests
KW  - COVID-19
JF  - Informatics in Medicine Unlocked
VL  - 21
SP  - 100449
EP  - 100449
DO  - https://doi.org/10.1016/j.imu.2020.100449
UR  - https://www.sciencedirect.com/science/article/pii/S2352914820305992
N2  - Background and objectives
The pandemic of novel coronavirus disease 2019 (COVID-19) has severely impacted human society with a massive death toll worldwide. There is an urgent need for early and reliable screening of COVID-19 patients to provide better and timely patient care and to combat the spread of the disease. In this context, recent studies have reported some key advantages of using routine blood tests for initial screening of COVID-19 patients. In this article, first we present a review of the emerging techniques for COVID-19 diagnosis using routine laboratory and/or clinical data. Then, we propose ERLX which is an ensemble learning model for COVID-19 diagnosis from routine blood tests.
Method
The proposed model uses three well-known diverse classifiers, extra trees, random forest and logistic regression, which have different architectures and learning characteristics at the first level, and then combines their predictions by using a second level extreme gradient boosting (XGBoost) classifier to achieve a better performance. For data preparation, the proposed methodology employs a KNNImputer algorithm to handle null values in the dataset, isolation forest (iForest) to remove outlier data, and a synthetic minority oversampling technique (SMOTE) to balance data distribution. For model interpretability, features importance are reported by using the SHapley Additive exPlanations (SHAP) technique.
Results
The proposed model was trained and evaluated by using a publicly available data set from Albert Einstein Hospital in Brazil, which consisted of 5644 data samples with 559 confirmed COVID-19 cases. The ensemble model achieved outstanding performance with an overall accuracy of 99.88% [95% CI: 99.6–100], AUC of 99.38% [95% CI: 97.5–100], a sensitivity of 98.72% [95% CI: 94.6–100] and a specificity of 99.99% [95% CI: 99.99–100].
Discussion
The proposed model revealed better performance when compared against existing state-of-the-art studies (Banerjee et al., 2020; de Freitas Barbosa et al., 2020; de Moraes Batista et al., 2020; Soares et al., 2020) [3,22,56,71] for the same set of features employed by them. As compared to the best performing Bayes Net model (de Freitas Barbosa et al., 2020) [22] average accuracy of 95.159%, ERLX achieved an average accuracy of 99.94%. In comparison with AUC of 85% reported by the SVM model (de Moraes Batista et al., 2020) [56], ERLX obtained AUC of 99.77% in addition to improvements in sensitivity, and specificity. As compared with ER-COV model (Soares et al., 2020) [71] average sensitivity of 70.25% and specificity of 85.98%, ERLX model achieved sensitivity of 99.47% and specificity of 99.99%. The ERLX model obtained a considerably higher score as compared with ANN model (Banerjee et al., 2020) [3] in all performance metrics. Therefore, the model presented is robust and can be deployed for reliable early and rapid screening of COVID-19 patients.
ER  - 
TY  - JOUR
T1  - SME2EM: Smart mobile end-to-end monitoring architecture for life-long diseases
A1  - Serhani, Mohamed Adel
A1  - Menshawy, Mohamed El
A1  - Benharref, Abdelghani
Y1  - 2016///
KW  -  Data as a service
KW  -  SOA
KW  -  Smart mobile monitoring
KW  -  Visualization as a service
KW  - Model checking
JF  - Computers in Biology and Medicine
VL  - 68
SP  - 137
EP  - 154
DO  - https://doi.org/10.1016/j.compbiomed.2015.11.009
UR  - https://www.sciencedirect.com/science/article/pii/S0010482515003777
N2  - Monitoring life-long diseases requires continuous measurements and recording of physical vital signs. Most of these diseases are manifested through unexpected and non-uniform occurrences and behaviors. It is impractical to keep patients in hospitals, health-care institutions, or even at home for long periods of time. Monitoring solutions based on smartphones combined with mobile sensors and wireless communication technologies are a potential candidate to support complete mobility-freedom, not only for patients, but also for physicians. However, existing monitoring architectures based on smartphones and modern communication technologies are not suitable to address some challenging issues, such as intensive and big data, resource constraints, data integration, and context awareness in an integrated framework. This manuscript provides a novel mobile-based end-to-end architecture for live monitoring and visualization of life-long diseases. The proposed architecture provides smartness features to cope with continuous monitoring, data explosion, dynamic adaptation, unlimited mobility, and constrained devices resources. The integration of the architecture׳s components provides information about diseases׳ recurrences as soon as they occur to expedite taking necessary actions, and thus prevent severe consequences. Our architecture system is formally model-checked to automatically verify its correctness against designers׳ desirable properties at design time. Its components are fully implemented as Web services with respect to the SOA architecture to be easy to deploy and integrate, and supported by Cloud infrastructure and services to allow high scalability, availability of processes and data being stored and exchanged. The architecture׳s applicability is evaluated through concrete experimental scenarios on monitoring and visualizing states of epileptic diseases. The obtained theoretical and experimental results are very promising and efficiently satisfy the proposed architecture׳s objectives, including resource awareness, smart data integration and visualization, cost reduction, and performance guarantee.
ER  - 
TY  - JOUR
T1  - Predicting heart transplantation outcomes through data analytics
A1  - Dag, Ali
A1  - Oztekin, Asil
A1  - Yucel, Ahmet
A1  - Bulur, Serkan
A1  - Megahed, Fadel M
Y1  - 2017///
KW  -  Health-care analytics
KW  -  Medical decision making
KW  -  Transplants
KW  -  Unbalanced data
KW  -  United Network for Organ Sharing (UNOS)
KW  - Data mining
JF  - Decision Support Systems
VL  - 94
SP  - 42
EP  - 52
DO  - https://doi.org/10.1016/j.dss.2016.10.005
UR  - https://www.sciencedirect.com/science/article/pii/S0167923616301816
N2  - Predicting the survival of heart transplant patients is an important, yet challenging problem since it plays a crucial role in understanding the matching procedure between a donor and a recipient. Data mining models can be used to effectively analyze and extract novel information from large/complex transplantation datasets. The objective of this study is to predict the 1-, 5-, and 9-year patient's graft survival following a heart transplant surgery via the deployment of analytical models that are based on four powerful classification algorithms (i.e. decision trees, artificial neural networks, support vector machines, and logistic regression). Since the datasets used in this study has a much larger number of survival cases than deaths for 1- and 5-year survival analysis and vice versa for 9-year survival analysis, random under sampling (RUS) and synthetic minority over-sampling (SMOTE) are employed to overcome the data-imbalance problems. The results indicate that logistic regression combined with SMOTE achieves the best classification for the 1-, 5-, and 9-year outcome prediction, with area-under-the-curve (AUC) values of 0.624, 0.676, and 0.838, respectively. By applying sensitivity analysis to the data analytical models, the most important predictors and their associated contribution for the 1-, 5-, and 9-year graft survival of heart transplant patients are identified. By doing so, variables, whose importance changes over time, are differentiated. Not only this proposed hybrid approach gives superior results over the literature but also the models and identification of the variables present important retrospective findings, which can be the basis for a prospective medical study.
ER  - 
TY  - JOUR
T1  - A double-layer ELM with added feature selection ability using a sparse Bayesian approach
A1  - Kiaee, Farkhondeh
A1  - Gagné, Christian
A1  - Sheikhzadeh, Hamid
Y1  - 2016///
KW  -  Binary classification
KW  -  Feature selection
KW  -  Sparse Bayesian learning
KW  -  Subnetwork architecture
KW  - Extreme learning machine
JF  - Neurocomputing
VL  - 216
SP  - 371
EP  - 380
DO  - https://doi.org/10.1016/j.neucom.2016.08.011
UR  - https://www.sciencedirect.com/science/article/pii/S0925231216308323
N2  - The Sparse Bayesian Extreme Learning Machine (SBELM) has been recently proposed to reduce the number of units activated on the hidden layer. To deal with high-dimensional data, a novel sparse Bayesian Double-Layer ELM (DL-ELM) is proposed in this paper. The first layer of the proposed DL-ELM is based on a set of SBELM subnetworks which are separately applied to the features on the input layer. The second layer consists in a set of weight parameters to determine the contribution of each feature in the output. Adopting a Bayesian approach and using Gaussian priors, the proposed method is sparse in both the hidden layer (of SBELM subnetworks) and the input layer. Sparseness in the input layer (i.e. pruning of irrelevant features) is achieved by decaying weights on the second layer to zero, such that the contribution of the corresponding input feature is deactivated. The proposed framework then enables simultaneous feature selection and classifier design at the training time. Experimental comparisons on real benchmark data sets show that the proposed method benefits from efficient feature selection ability while providing a compact classification model of good accuracy and generalization properties.
ER  - 
TY  - JOUR
T1  - Scale-space approximated convolutional neural networks for retinal vessel segmentation
A1  - Noh, Kyoung Jin
A1  - Park, Sang Jun
A1  - Lee, Soochahn
Y1  - 2019///
KW  -  Convolutional neural networks
KW  -  Multi-scale representation
KW  -  Scale-space approximation
KW  - Retinal vessel segmentation
JF  - Computer Methods and Programs in Biomedicine
VL  - 178
SP  - 237
EP  - 246
DO  - https://doi.org/10.1016/j.cmpb.2019.06.030
UR  - https://www.sciencedirect.com/science/article/pii/S0169260719302226
N2  - Background and objective: Retinal fundus images are widely used to diagnose retinal diseases and can potentially be used for early diagnosis and prevention of chronic vascular diseases and diabetes. While various automatic retinal vessel segmentation methods using deep learning have been proposed, they are mostly based on common CNN structures developed for other tasks such as classification.
Methods
We present a novel and simple multi-scale convolutional neural network (CNN) structure for retinal vessel segmentation. We first provide a theoretical analysis of existing multi-scale structures based on signal processing. In previous structures, multi-scale representations are achieved through downsampling by subsampling and decimation. By incorporating scale-space theory, we propose a simple yet effective multi-scale structure for CNNs using upsampling, which we term scale-space approximated CNN (SSANet). Based on further analysis of the effects of the SSA structure within a CNN, we also incorporate residual blocks, resulting in a multi-scale CNN that outperforms current state-of-the-art methods.
Results
Quantitative evaluations are presented as the area-under-curve (AUC) of the receiver operating characteristic (ROC) curve and the precision-recall curve, as well as accuracy, for four publicly available datasets, namely DRIVE, STARE, CHASE_DB1, and HRF. For the CHASE_DB1 set, the SSANet achieves state-of-the-art AUC value of 0.9916 for the ROC curve. An ablative analysis is presented to analyze the contribution of different components of the SSANet to the performance improvement.
Conclusions
The proposed retinal SSANet achieves state-of-the-art or comparable accuracy across publicly available datasets, especially improving segmentation for thin vessels, vessel junctions, and central vessel reflexes.
ER  - 
TY  - JOUR
T1  - Precision health data: Requirements, challenges and existing techniques for data security and privacy
A1  - Thapa, Chandra
A1  - Camtepe, Seyit
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Ethical guidelines
KW  -  Legal requirements
KW  -  Privacy
KW  -  Security
KW  - Precision health
JF  - Computers in Biology and Medicine
VL  - 129
SP  - 104130
EP  - 104130
DO  - https://doi.org/10.1016/j.compbiomed.2020.104130
UR  - https://www.sciencedirect.com/science/article/pii/S0010482520304613
N2  - Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.
ER  - 
TY  - JOUR
T1  - Regularized discriminant entropy analysis
A1  - Zhao, Haitao
A1  - Wong, W K
Y1  - 2014///
KW  -  Discriminant entropy analysis
KW  -  Entropy-based learning
KW  - Regularized discriminant entropy
JF  - Pattern Recognition
VL  - 47
IS  - 2
SP  - 806
EP  - 819
DO  - https://doi.org/10.1016/j.patcog.2013.08.020
UR  - https://www.sciencedirect.com/science/article/pii/S0031320313003427
N2  - In this paper, we propose the regularized discriminant entropy (RDE) which considers both class information and scatter information on original data. Based on the results of maximizing the RDE, we develop a supervised feature extraction algorithm called regularized discriminant entropy analysis (RDEA). RDEA is quite simple and requires no approximation in theoretical derivation. The experiments with several publicly available data sets show the feasibility and effectiveness of the proposed algorithm with encouraging results.
ER  - 
TY  - JOUR
T1  - Supervised Adaptive Incremental Clustering for data stream of chunks
A1  - Zheng, Laiwen
A1  - Huo, Hong
A1  - Guo, Yiyou
A1  - Fang, Tao
Y1  - 2017///
KW  -  Adaptive update
KW  -  Classification
KW  -  Data stream of chunks
KW  -  Supervised clustering
KW  - Automatic clustering
JF  - Neurocomputing
VL  - 219
SP  - 502
EP  - 517
DO  - https://doi.org/10.1016/j.neucom.2016.09.054
UR  - https://www.sciencedirect.com/science/article/pii/S092523121631089X
N2  - Many supervised clustering algorithms have been developed to find the optimal clusters for static datasets by presetting some parameters, but they are seldom suitable for dynamic datasets, such as the data stream of chunks. To find the optimal clusters of the data stream of chunks, a novel Supervised Adaptive Incremental Clustering (SAIC) algorithm is proposed. SAIC can cluster dynamic datasets of arbitrary shapes and sizes automatically. It includes learning and post-processing phases. In the learning phase, each cluster updates adaptively according to its learning rate that is calculated from its counter value. All data points are shuffled at each iteration in order to make SAIC insensitive to the input order of data points. In the post-processing phase, the outliers or boundary points are eliminated according to the counter value of each cluster and the number of iterations. Four synthetic datasets and fourteen UCI datasets are used to evaluate the performance of SAIC, respectively. The experiments on UCI datasets show that SAIC reaches to or outperforms some other supervised clustering algorithms and several unsupervised incremental clustering algorithms. In addition, three data stream of chunks are used to evaluate SAIC from different aspects, which shows SAIC has the scalability and incremental learning ability for the clustering of data streams of chunks.
ER  - 
TY  - JOUR
T1  - Quantitative Trait Specific Differential Expression (qtDE)
A1  - Sarkar, Mrityunjay
A1  - Majumder, Aurpan
Y1  - 2015///
KW  -  Correlation
KW  -  Differentially expressed genes
KW  -  MutualInfoAdjacency
KW  -  Pathway
KW  -  T-statistics
KW  - Quantitative trait
JF  - Procedia Computer Science
VL  - 46
SP  - 706
EP  - 718
DO  - https://doi.org/10.1016/j.procs.2015.02.131
UR  - https://www.sciencedirect.com/science/article/pii/S1877050915001957
N1  - Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace &amp; Island Resort, Kochi, India
N2  - Natural selection based on phenotypic traits may lead to genetic changes. To understand the relationship between phenotypic traits and gene expression profiles it is important to study the concordance between the two. In this work we have developed a simple procedure to find the differentially expressed (DE) genes across various tissues between phenotypes through linear correlation as well as non linear mutual information and polynomial regression between quantitative-trait and the gene expression profiles. Here we are making the use of mice gene expression data to find the differentially expressed genes between the male and female phenotypes exploring the dependency between the gene expression profiles of four tissues (brain, muscle, liver, adipose) and quantitative trait (weight). To prove the effectiveness of the method we have tested our results with a popular DE tool (DEGseq). In the results we have shown that mutual information based trait-specific DE genes are biologically more significant compared to the polynomial regression and linear correlative counterparts.
ER  - 
TY  - JOUR
T1  - Determination of body fat percentage by electrocardiography signal with gender based artificial intelligence
A1  - Uçar, Muhammed Kürşad
A1  - Uçar, Zeliha
A1  - Uçar, Kübra
A1  - Akman, Mehmet
A1  - Bozkurt, Mehmet Recep
Y1  - 2021///
KW  -  Artificial intelligence
KW  -  Body composition
KW  -  Body fat percentage
KW  -  Gender based body fat percentage
KW  -  Machine learning
KW  - Electrocardiography signal
JF  - Biomedical Signal Processing and Control
VL  - 68
SP  - 102650
EP  - 102650
DO  - https://doi.org/10.1016/j.bspc.2021.102650
UR  - https://www.sciencedirect.com/science/article/pii/S1746809421002470
N2  - Background and purpose
Body fat percentage (BFP) is a frequently used parameter in the assessment of body composition. The body is made up of fat, muscle and lean body tissues. Excess fat tissue in the body causes obesity. Obesity is a treatable disease that decreases the quality of life. Obesity can trigger ailments such as psychological disorders, cardiovascular diseases and respiratory and digestive problems. Dual energy X-ray absorptiometry gold standard method is laborious, costly and time consuming. For this reason, more practical methods are needed. The aim of this study is to develop BFP prediction models with gender-based electrocardiography (ECG) signal and machine learning methods.
Methods
In the study, 25 features were extracted from seven different QRS bands and filtered and unfiltered ECG signals. In addition, age, height and weight were used as features. Spearman feature selection algorithm was used to increase the performance.
Results
The BFP prediction models developed have performance values of R=0.94 for men and R=0.93 for women and R=0.91 for all individuals. Feature selection algorithm helped increase performance.
Conclusion:
According to the results, it is thought that ECG based BFP prediction models can be used in practice.
ER  - 
TY  - JOUR
T1  - Diversity-aware classifier ensemble selection via f-score
A1  - Visentini, Ingrid
A1  - Snidaro, Lauro
A1  - Foresti, Gian Luca
Y1  - 2016///
KW  -  Classifiers fusion
KW  -  Classifiers selection
KW  -  Online tracking
KW  -  Tracking via classification
KW  - F-score
JF  - Information Fusion
VL  - 28
SP  - 24
EP  - 43
DO  - https://doi.org/10.1016/j.inffus.2015.07.003
UR  - https://www.sciencedirect.com/science/article/pii/S1566253515000688
N2  - The primary effect of using a reduced number of classifiers is a reduction in the computational requirements during learning and classification time. In addition to this obvious result, research shows that the fusion of all available classifiers is not a guarantee of best performance but good results on the average. The much researched issue of whether it is more convenient to fuse or to select has become even more of interest in recent years with the development of the Online Boosting theory, where a limited set of classifiers is continuously updated as new inputs are observed and classifications performed. The concept of online classification has recently received significant interest in the computer vision community. Classifiers can be trained on the visual features of a target, casting the tracking problem into a binary classification one: distinguishing the target from the background. Here we discuss how to optimize the performance of a classifier ensemble employed for target tracking in video sequences. In particular, we propose the F-score measure as a novel means to select the members of the ensemble in a dynamic fashion. For each frame, the ensemble is built as a subset of a larger pool of classifiers selecting its members according to their F-score. We observed an overall increase in classification accuracy and a general tendency in redundancy reduction among the members of an f-score optimized ensemble. We carried out our experiments both on benchmark binary datasets and standard video sequences.
ER  - 
TY  - JOUR
T1  - Impacts of reference points and reference lines on the slope- and area-based heart rate asymmetry analysis
A1  - Yan, Chang
A1  - Li, Peng
A1  - Yao, Lianke
A1  - Karmakar, Chandan
A1  - Liu, Changchun
Y1  - 2019///
KW  -  Heart rate asymmetry (HRA)
KW  -  Poincaré plot
KW  -  Slope index
KW  - Area index
JF  - Measurement
VL  - 137
SP  - 515
EP  - 526
DO  - https://doi.org/10.1016/j.measurement.2019.01.062
UR  - https://www.sciencedirect.com/science/article/pii/S0263224119300715
N2  - Heart rate asymmetry (HRA) could capture valuable dynamical properties from the electrocardiographic RR interval time-series that are helpful for evaluating the cardiovascular functioning. Several metrics derived from the Poincaré plot have been established for assessing HRA such as the slope index (SI) and the area index (AI). In the current study, we aimed to examine how different reference points and reference lines affect the calculations of SI and AI. To understand their performance, two case studies that were to classify subjects with (1) arrhythmias and (2) congestive heart failure, respectively, from normal controls were performed. To examine whether these effects depend on data lengths, the case studies were performed on both long-term heartbeat interval time-series and short-term segments. Our results showed that different reference points or reference lines could strongly affect the performance of both SI and AI, especially when short-term data were being analyzed. Using the minimum of data as the reference point might be a conservative solution in application but a spectrum of SI or AI measurements with multiple reference points and reference lines are highly recommended.
ER  - 
TY  - JOUR
T1  - A medical diagnostic tool based on radial basis function classifiers and evolutionary simulated annealing
A1  - Alexandridis, Alex
A1  - Chondrodima, Eva
Y1  - 2014///
KW  -  Evolutionary computation
KW  -  Medical diagnosis
KW  -  Neural networks
KW  -  Radial basis function
KW  -  Simulated annealing
KW  - Decision support systems
JF  - Journal of Biomedical Informatics
VL  - 49
SP  - 61
EP  - 72
DO  - https://doi.org/10.1016/j.jbi.2014.03.008
UR  - https://www.sciencedirect.com/science/article/pii/S1532046414000653
N2  - Objective
The profusion of data accumulating in the form of medical records could be of great help for developing medical decision support systems. The objective of this paper is to present a methodology for designing data-driven medical diagnostic tools, based on neural network classifiers.
Methods
The proposed approach adopts the radial basis function (RBF) neural network architecture and the non-symmetric fuzzy means (NSFM) training algorithm, which presents certain advantages including better approximation capabilities and shorter computational times. The novelty in this work consists of adapting the NSFM algorithm to train RBF classifiers, and suitably tailoring the evolutionary simulated annealing (ESA) technique to optimize the produced RBF models. The integration of ESA is critical as it helps the optimization procedure to escape from local minima, which could arise from the application of the traditional simulated annealing algorithm, and thus discover improved solutions. The resulting method is evaluated in nine different medical benchmark datasets, where the common objective is to train a suitable classifier. The evaluation includes a comparison with two different schemes for training classifiers, including a standard RBF training technique and support vector machines (SVMs). Accuracy% and the Matthews Correlation Coefficient (MCC) are used for comparing the performance of the three classifiers.
Results
Results show that the use of ESA helps to greatly improve the performance of the NSFM algorithm and provide satisfactory classification accuracy. In almost all benchmark datasets, the best solution found by the ESA-NSFM algorithm outperforms the results produced by the SFM algorithm and SVMs, considering either the accuracy% or the MCC criterion. Furthermore, in the majority of datasets, the average solution of the ESA-NSFM population is statistically significantly higher in terms of accuracy% and MCC at the 95% confidence level, compared to the global optimum solution that its rivals could achieve. As far as computational times are concerned, the proposed approach was found to be faster compared to SVMs.
Conclusions
The results of this study suggest that the ESA-NSFM algorithm can form the basis of a generic method for knowledge extraction from data originating from different kinds of medical records. Testing the proposed approach on a number of benchmark datasets, indicates that it provides increased diagnostic accuracy in comparison with two different classifier training methods.
ER  - 
TY  - JOUR
T1  - Nonsmooth nonconvex optimization approach to clusterwise linear regression problems
A1  - Bagirov, Adil M
A1  - Ugon, Julien
A1  - Mirzayeva, Hijran
Y1  - 2013///
KW  -  Incremental algorithm
KW  -  Späth algorithm
KW  - Clusterwise linear regression
JF  - European Journal of Operational Research
VL  - 229
IS  - 1
SP  - 132
EP  - 142
DO  - https://doi.org/10.1016/j.ejor.2013.02.059
UR  - https://www.sciencedirect.com/science/article/pii/S0377221713002087
N2  - Clusterwise regression consists of finding a number of regression functions each approximating a subset of the data. In this paper, a new approach for solving the clusterwise linear regression problems is proposed based on a nonsmooth nonconvex formulation. We present an algorithm for minimizing this nonsmooth nonconvex function. This algorithm incrementally divides the whole data set into groups which can be easily approximated by one linear regression function. A special procedure is introduced to generate a good starting point for solving global optimization problems at each iteration of the incremental algorithm. Such an approach allows one to find global or near global solution to the problem when the data sets are sufficiently dense. The algorithm is compared with the multistart Späth algorithm on several publicly available data sets for regression analysis.
ER  - 
TY  - JOUR
T1  - A method for the detection and classification of diabetic retinopathy using structural predictors of bright lesions
A1  - Amin, Javeria
A1  - Sharif, Muhammad
A1  - Yasmin, Mussarat
A1  - Ali, Hussam
A1  - Fernandes, Steven Lawrence
Y1  - 2017///
KW  -  Diabetic retinopathy
KW  -  Exudates
KW  -  Features extraction
KW  -  Gabor filter
KW  - Classification
JF  - Journal of Computational Science
VL  - 19
SP  - 153
EP  - 164
DO  - https://doi.org/10.1016/j.jocs.2017.01.002
UR  - https://www.sciencedirect.com/science/article/pii/S1877750317300546
N2  - Diabetic burden around the world with a consequence of diabetic retinopathy can lead to permanent blindness in patients. Exudates detection in fundus images through an automated method is a vital task that has many applications in diabetic retinopathy screening. Realizing it important, a system being proposed in this paper automatically classifies exudates and non-exudates regions in retinal images. Presented technique is based on pre-processing for candidate lesion extraction, features extraction and classification. In pre-processing, Gabor filter is applied to the gray scale image which makes it useful for lesion enhancement. Segmentation of candidate lesion is based on mathematical morphology. A features set is selected for each candidate lesion using a combination of statistical and geometric features. Presented method is evaluated via publicly accessible datasets with the help of performance parameters such as true positive, false positive and area under curve for statistical analysis. Publicly available datasets such as e-ophtha, HRIS, MESSIDOR, DIARETDB1, VDIS, DRIVE, HRF and one local dataset are used to test the suggested system. The achieved results show an average AUC of 0.98 and accuracy as high as 98.58% which are substantially higher than the existing methods.
ER  - 
TY  - JOUR
T1  - A general kernelization framework for learning algorithms based on kernel PCA
A1  - Zhang, Changshui
A1  - Nie, Feiping
A1  - Xiang, Shiming
Y1  - 2010///
KW  -  Kernel PCA
KW  -  Learning algorithm
KW  -  Two-stage framework
KW  - Kernel method
JF  - Neurocomputing
VL  - 73
IS  - 4
SP  - 959
EP  - 967
DO  - https://doi.org/10.1016/j.neucom.2009.08.014
UR  - https://www.sciencedirect.com/science/article/pii/S092523120900335X
N1  - Bayesian Networks / Design and Application of Neural Networks and Intelligent Learning Systems (KES 2008 / Bio-inspired Computing: Theories and Applications (BIC-TA 2007)
N2  - In this paper, a general kernelization framework for learning algorithms is proposed via a two-stage procedure, i.e., transforming data by kernel principal component analysis (KPCA), and then directly performing the learning algorithm with the transformed data. It is worth noting that although a very few learning algorithms were also kernelized by this procedure before, why and under what condition this procedure is feasible have not been further studied. In this paper, we explicitly present this kernelization framework, and give a rigorous justification to reveal that under some mild conditions, the kernelization under this framework is equivalent to traditional kernel method. We show that these mild conditions are usually satisfied in most of learning algorithms. Therefore, most of learning algorithms can be kernelized under this framework without having to reformulate it into inner product form, which is a common yet vital step in traditional kernel methods. Enlightened by this framework, we also propose a novel kernel method based on the low-rank KPCA, which could be used to remove the noise in the feature space, speed up the kernel algorithm and improve the numerical stability for the kernel algorithm. Experiments are presented to verify the validity and effectiveness of the proposed methods.
ER  - 
TY  - JOUR
T1  - Minimum deviation distribution machine for large scale regression
A1  - Liu, Ming-Zeng
A1  - Shao, Yuan-Hai
A1  - Wang, Zhen
A1  - Li, Chun-Na
A1  - Chen, Wei-Jie
Y1  - 2018///
KW  -  Dual coordinate descend algorithm
KW  -  Minimum deviation distribution machine
KW  -  Stochastic gradient algorithm
KW  -  Support vector machine
KW  - Regression
JF  - Knowledge-Based Systems
VL  - 146
SP  - 167
EP  - 180
DO  - https://doi.org/10.1016/j.knosys.2018.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S0950705118300534
N2  - In this paper, by introducing the statistics of training data into support vector regression (SVR), we propose a minimum deviation distribution regression (MDR). Rather than just minimizing the structural risk, MDR also minimizes both the regression deviation mean and the regression deviation variance, which is able to deal with the different distribution of boundary data and noises. The formulation of minimizing the first and second order statistics in MDR leads to a strongly convex quadratic programming problem (QPP). An efficient dual coordinate descend algorithm is adopted for small sample problem, and an average stochastic gradient algorithm for large scale one. Both theoretical analysis and experimental results illustrate the efficiency and effectiveness of the proposed method.
ER  - 
TY  - JOUR
T1  - Countering imbalanced datasets to improve adverse drug event predictive models in labor and delivery
A1  - Taft, L M
A1  - Evans, R S
A1  - Shyu, C R
A1  - Egger, M J
A1  - Chawla, N
A1  - Mitchell, J A
A1  - Thornton, S N
A1  - Bray, B
A1  - Varner, M
Y1  - 2009///
KW  -  Data-mining
KW  -  Labor and delivery
KW  -  Oversampling
KW  -  Pregnancy
KW  - Adverse drug events
JF  - Journal of Biomedical Informatics
VL  - 42
IS  - 2
SP  - 356
EP  - 364
DO  - https://doi.org/10.1016/j.jbi.2008.09.001
UR  - https://www.sciencedirect.com/science/article/pii/S1532046408001214
N2  - Background
The IOM report, Preventing Medication Errors, emphasizes the overall lack of knowledge of the incidence of adverse drug events (ADE). Operating rooms, emergency departments and intensive care units are known to have a higher incidence of ADE. Labor and delivery (L&D) is an emergency care unit that could have an increased risk of ADE, where reported rates remain low and under-reporting is suspected. Risk factor identification with electronic pattern recognition techniques could improve ADE detection rates.
Objective
The objective of the present study is to apply Synthetic Minority Over Sampling Technique (SMOTE) as an enhanced sampling method in a sparse dataset to generate prediction models to identify ADE in women admitted for labor and delivery based on patient risk factors and comorbidities.
Results
By creating synthetic cases with the SMOTE algorithm and using a 10-fold cross-validation technique, we demonstrated improved performance of the Naïve Bayes and the decision tree algorithms. The true positive rate (TPR) of 0.32 in the raw dataset increased to 0.67 in the 800% over-sampled dataset.
Conclusion
Enhanced performance from classification algorithms can be attained with the use of synthetic minority class oversampling techniques in sparse clinical datasets. Predictive models created in this manner can be used to develop evidence based ADE monitoring systems.
ER  - 
TY  - JOUR
T1  - A novel approach to modeling multifactorial diseases using Ensemble Bayesian Rule classifiers
A1  - Balasubramanian, Jeya Balaji
A1  - Boes, Rebecca D
A1  - Gopalakrishnan, Vanathi
Y1  - 2020///
KW  -  Ensemble methods
KW  -  Interpretability
KW  -  Rule learning
KW  - Bayesian methods
JF  - Journal of Biomedical Informatics
VL  - 107
SP  - 103455
EP  - 103455
DO  - https://doi.org/10.1016/j.jbi.2020.103455
UR  - https://www.sciencedirect.com/science/article/pii/S1532046420300836
N2  - Modeling factors influencing disease phenotypes, from biomarker profiling study datasets, is a critical task in biomedicine. Such datasets are typically generated from high-throughput ’omic’ technologies, which help examine disease mechanisms at an unprecedented resolution. These datasets are challenging because they are high-dimensional. The disease mechanisms they study are also complex because many diseases are multifactorial, resulting from the collective activity of several factors, each with a small effect. Bayesian rule learning (BRL) is a rule model inferred from learning Bayesian networks from data, and has been shown to be effective in modeling high-dimensional datasets. However, BRL is not efficient at modeling multifactorial diseases since it suffers from data fragmentation during learning. In this paper, we overcome this limitation by implementing and evaluating three types of ensemble model combination strategies with BRL— uniform combination (UC; same as Bagging), Bayesian model averaging (BMA), and Bayesian model combination (BMC)— collectively called Ensemble Bayesian Rule Learning (EBRL). We also introduce a novel method to visualize EBRL models, called the Bayesian Rule Ensemble Visualizing tool (BREVity), which helps extract interpret the most important rule patterns guiding the predictions made by the ensemble model. Our results using twenty-five public, high-dimensional, gene expression datasets of multifactorial diseases, suggest that, both EBRL models using UC and BMC achieve better predictive performance than BMA and other classic machine learning methods. Furthermore, BMC is found to be more reliable than UC, when the ensemble includes sub-optimal models resulting from the stochasticity of the model search process. Together, EBRL and BREVity provides researchers a promising and novel tool for modeling multifactorial diseases from high-dimensional datasets that leverages strengths of ensemble methods for predictive performance, while also providing interpretable explanations for its predictions.
ER  - 
TY  - JOUR
T1  - A novel approach to multiclass psoriasis disease risk stratification: Machine learning paradigm
A1  - Shrivastava, Vimal K
A1  - Londhe, Narendra D
A1  - Sonawane, Rajendra S
A1  - Suri, Jasjit S
Y1  - 2016///
KW  -  Color features
KW  -  Machine learning
KW  -  Multiclass
KW  -  Psoriasis skin disease
KW  -  Texture features
KW  - Dermatology
JF  - Biomedical Signal Processing and Control
VL  - 28
SP  - 27
EP  - 40
DO  - https://doi.org/10.1016/j.bspc.2016.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S1746809416300337
N2  - The stage and grade of psoriasis severity is clinically relevant and important for dermatologists as it aids them lead to a reliable and an accurate decision making process for better therapy. This paper proposes a novel psoriasis risk assessment system (pRAS) for stratification of psoriasis severity from colored psoriasis skin images having Asian Indian ethnicity. Machine learning paradigm is adapted for risk stratification of psoriasis disease grades utilizing offline training and online testing images. We design four kinds of pRAS systems. It uses two kinds of classifiers (support vector machines (SVM) and decision tree (DT)) during training and testing phases and two kinds of feature selection criteria (Principal Component Analysis (PCA) and Fisher Discriminant Ratio (FDR)), thus, leading to an exhaustive comparison between these four systems. Our database consisted of 848 psoriasis images with five severity grades: healthy, mild, moderate, severe and very severe, consisting of 383, 47, 245, 145, and 28 images respectively. The pRAS system computes 859 colored and grayscale image features. Using cross-validation protocol with K-fold procedure, the pRAS system utilizing the SVM with FDR combination with combined color and grayscale feature set gives an accuracy of 99.92%. Several performance evaluation parameters such as: feature retaining power, aggregated feature effect and system reliability is computed meeting our assumptions and hypothesis. Our results demonstrate promising results and pRAS system is able to stratify the psoriasis disease.
ER  - 
TY  - JOUR
T1  - Distributed learning from multiple EHR databases: Contextual embedding models for medical events
A1  - Li, Ziyi
A1  - Roberts, Kirk
A1  - Jiang, Xiaoqian
A1  - Long, Qi
Y1  - 2019///
KW  -  Contextual embedding models
KW  -  Diagnoses prediction
KW  -  EHR data
KW  - Distributed computing
JF  - Journal of Biomedical Informatics
VL  - 92
SP  - 103138
EP  - 103138
DO  - https://doi.org/10.1016/j.jbi.2019.103138
UR  - https://www.sciencedirect.com/science/article/pii/S1532046419300565
N2  - Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases.
ER  - 
TY  - JOUR
T1  - Symptoms based predict treatment medicine suggestion system using data mining techniques
A1  - Sathish Kumar, P J
A1  - Jagadeesh Kannan, R
Y1  - 2017///
PB  - Institute of Advanced Scientific Research, Inc.
JF  - Journal of Advanced Research in Dynamical and Control Systems
VL  - 9
IS  - Special Issue 6
SP  - 2012
EP  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037366379&partnerID=40&md5=dde5f650cd6241d2119f20c8706738cb
N1  - cited By 0
N2  - As extensive measure of information is created in medicinal associations (healing centers, therapeutic focuses) yet as this information is not appropriately utilized. Here, information mining is the way toward dissecting diverse parts of information and amassing it into helpful data. As of late, Machine Learning systems have been effectively used in an alternate applications incorporating to help with therapeutic finding. It is exceptionally easy and on time to prepare for patients to break down illness in view of clinical and lab side effects with suitable information and gives more proficient outcome for particular ailment. At that point classification is an information mining assignment by and large utilized as a part of therapeutic information mining. In this manuscript study of the execution measure of a machine learning strategy, neural system and Naive Bayes Classifier calculation for grouping side effects based anticipation for drug treatment is executed. Utilizing Naive Bayes, the general speed and precision of the calculation expanded and remove high quality information set from an unstructured content. The Naive Bayes is a standout amongst the best order calculations. In numerous basic leadership frameworks, positioning execution is a fascinating and attractive idea than just arrangement. So to augment conventional Naive Bayes, and to enhance its execution, community oriented channel and preprocessing idea is consolidated. The manifestations bringing about diabetes and treatment drug are recognized and are connected to the expectation display in view of which the forecast is finished. It is utilizing two manifestations based dataset preparing. Gathering dataset after the application of preprocessing for expels commotion and neural system procedure for preparing and testing of dataset. The examination module breaks down the research center test reports of the manifestations levels of the patient and gives appropriate consciousness messages to the patient through outcomes. The objective here is to find new and helpful examples to give significant and valuable data about diabetes to the patient. © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.
ER  - 
TY  - JOUR
T1  - An efficient technique for retinal vessel segmentation and denoising using modified isodata and CLAHE
A1  - Khan, K B
A1  - Khaliq, A A
A1  - Shahid, M
A1  - Khan, S
Y1  - 2016///
PB  - International Islamic University Malaysia-IIUM
JF  - IIUM Engineering Journal
VL  - 17
IS  - 2
SP  - 31
EP  - 46
DO  - 10.31436/iiumej.v17i2.611
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027714223&doi=10.31436%2Fiiumej.v17i2.611&partnerID=40&md5=a5676f00ed6c66072056f5960c255126
N1  - cited By 16
N2  - Retinal damage caused due to complications of diabetes is known as a Diabetic Retinopathy (DR). In this case, the vision is obscured due to damage of tiny retinal blood vessels. These tiny blood vessels may cause leakage that affect the vision and can lead to complete blindness. Identification of these new retinal vessels and their structure is an essential for analysis of DR. Automatic blood vessel segmentation plays a significant role to assist subsequent automatic methodologies that aid to such analysis. In literature, most authors have used computationally-hungry strong preprocessing steps followed by a simple thresholding and postprocessing steps. This paper proposed an arrangement of simple preprocessing steps that consist of Contrast Limited Adaptive Histogram Equalization (CLAHE) for contrast enhancement and a difference image of green channel from its Gaussian blur filtered image to remove local noise or geometrical objects. The proposed Modified Iterative Self Organizing Data Analysis Technique (MISODATA) has been used for segmentation of vessel and non-vessel pixels based on global and local thresholding. Finally, postprocessing steps have been applied using region properties (area, eccentricity) to eliminate the unwanted regions/segments, nonvessel pixels, and noise. A novel postprocessing steps are used to reject misclassified foreground pixels. The strategy has been tested on the openly accessible DRIVE (Digital Retinal Images for Vessel Extraction) and STARE (STructured Analysis of the REtina) databases. The average accuracy rates of 0.952 and 0.957 with average sensitivity rates 0.780 and 0.745 along with average specificity rates of 0.972 and 0.974 were obtained on DRIVE and STARE datasets, respectively. The performance of the proposed technique has been assessed comprehensively. The acquired accuracy, robustness, low complexity, and high efficiency make the method an efficient tool for an automatic retinal image analysis. The proposed technique perform well as compared to the existing strategies on the online available databases in term of accuracy, sensitivity, specificity, false positive rate, true positive rate, and area under receiver operating characteristic (ROC) curve.
ER  - 
TY  - JOUR
T1  - Machine Learning Approach for Postprandial Blood Glucose Prediction in Gestational Diabetes Mellitus
A1  - Pustozerov, E A
A1  - Tkachuk, A S
A1  - Vasukova, E A
A1  - Anopova, A D
A1  - Kokina, M A
A1  - Gorelova, I V
A1  - Pervunina, T M
A1  - Grineva, E N
A1  - Popova, P V
Y1  - 2020///
KW  -  Blood
KW  -  Blood glucose curves; Blood glucose level; Data preprocessing; Diabetes management; Feature engineerings; Gestational diabetes; Gradient boosting; Machine learning approaches
KW  - Behavioral research; Decision trees; Forecasting; Glucose; Machine learning; Nutrition; Predictive analytics; Surveys; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Access
DO  - 10.1109/ACCESS.2020.3042483
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097932870&doi=10.1109%2FACCESS.2020.3042483&partnerID=40&md5=3b7cf385ffd7801004c2007a7dc6ec3a
N1  - cited By 8
N2  - Postprandial blood glucose prediction is a crucial part of diabetes management. Recently, this topic has been of great interest, resulting in many research projects and published papers. Although different input parameters that might be beneficial for blood glucose prediction models were comprehensively discussed, specific data preprocessing, feature engineering and model tuning steps were not explained in detail in many of these papers. In this work, we developed and comprehensively described a data-driven blood glucose model based on a decision tree gradient boosting algorithm to predict different characteristics of postprandial glycemic responses; the model utilized meal-related data derived from a mobile app diary (including information on the glycemic index), food context (information on previous meals), characteristics of the individual patients and patient behavioral questionnaires. A set of rules was defined and implemented to detect incorrect meal records and to filter faulty data, and analyses were conducted on the overall food diary data and in particular, the data on the current meal for which the postprandial blood glucose response was calculated. Different gradient boosting models were trained and evaluated with parameters selected via random search cross-validation. The best models for the prediction of the incremental area under the blood glucose curve two hours after food intake had the following characteristics: R=0.631, MAE=0.373 mmol/L*h for the model not using data on current blood glucose; R=0.644, MAE=0.371 mmol/L*h for the model using data on the current blood glucose levels; and R=0.704, MAE=0.341 mmol/L*h for the model utilizing data on the continuous blood glucose trends before the meal. The impact of features was evaluated using Shapley values. The meal glycemic load, amount of carbohydrates in the meal, type of meal (e.g., breakfast), amount of starch and amount of food consumed 6 hours before the current meal were the most important contributors in the models. CCBY
ER  - 
TY  - CONF
T1  - Evaluation of Rough Sets Data Preprocessing on Context-Driven Semantic Analysis with RNN
A1  - Xie, H
A1  - Ahmadon, M A B
A1  - Yamaguchi, S
Y1  - 2018///
KW  -  Application examples; Data preprocessing; Medical; Medical literatures; Natural language learning; Semantic analysis; Semantic information; Training process
KW  -  Long short-term memory
KW  - Natural language processing systems; Rough set theory; Semantics
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2018 IEEE 7th Global Conference on Consumer Electronics, GCCE 2018
SP  - 272
EP  - 275
SN  - 9781538663097
DO  - 10.1109/GCCE.2018.8574653
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060306885&doi=10.1109%2FGCCE.2018.8574653&partnerID=40&md5=b727f2239e6eccd5fee0742a97fbe46a
N1  - cited By 5
N2  - In the application examples of NLP (natural language learning), the rich semantic information in medical literature can extract characteristic target words through the training of RNN-LSTM (recurrent neural network-long short-term memory). In the process of extracting these target words, we often encounter some wrong target words which cause RNN to reduce the hit rate and extend the training time. In this paper, we take Diabetes in medical research as an example, the data preprocessing of rough sets, and the word vector tagging for target word can improve the hit efficiency of the target words in the RNN-LSTM training process. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Mask Region-oriented Diabetic Retinopathies Detection in Ophthalmic Medical Images via Non-local Attention
A1  - Zhao, H
A1  - Peng, H
A1  - Gao, Z
A1  - Zheng, S
Y1  - 2021///
KW  -  Color similarity; Color textures; Cotton-wool spots; Diabetic retinopathy; Haemorrage; Lesion detection; Mask region-oriented CNN; Multi-lesion detection; Non-local attention; Nonlocal
KW  -  Eye protection
KW  - Diagnosis; Medical imaging; Textures
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the International Joint Conference on Neural Networks
VL  - 2021-July
SN  - 9780738133669
DO  - 10.1109/IJCNN52387.2021.9534118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116472254&doi=10.1109%2FIJCNN52387.2021.9534118&partnerID=40&md5=0bf7aa65d55eda4d1a034cbeb24917e4
N1  - cited By 0
N2  - Accurate lesions detection on retinopathy images is crucial for the diagnosis of diabetes. However, it is always hampered by various characteristics of lesions such as shape, color, texture, and similarities. Most advanced algorithms still cannot automatically detect common lesions, e.g. exudate, hemorrhage, and cotton-wool spots, being used for comprehensive analysis of disease state. To this end, we present a multi-functional detection model for diabetic retinopathies and further analyze disease mechanisms overall. Specifically, this paper attempts to implement a multi-lesion detector via modified Mask region-oriented CNN, which can be used for the aforementioned retinopathies. Meanwhile, a non-local attention module is introduced to the detector as a spatial attention mechanism for handling the global information missing problem. In addition, to boost the effectiveness of the detector, the dilated operation is implemented for dataset preprocessing. Improvement is achieved both algorithmically and architecturally, via investigating thoroughly the most probable lesion category with a novel ensemble learning framework. Extensive experiments on standard datasets for three different tasks evidence the superior performance of the proposed method over state-of-the-art methods. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Diabetes diagnosis using different data pre processing techniques
A1  - Singh, S N
A1  - Kathuria, K
Y1  - 2018///
KW  -  Blood sugar levels; Conventional approach; Data cleaning; Data preparation; Data preprocessing; Diabetes diagnosis; Fatal disease; Medical record
KW  -  Data handling
KW  - Cleaning; Data mining; Data reduction; Decision trees; Diagnosis; Medical education; Medical problems; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2018 4th International Conference on Computing Communication and Automation, ICCCA 2018
SN  - 9781538669471
DO  - 10.1109/CCAA.2018.8777332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070380895&doi=10.1109%2FCCAA.2018.8777332&partnerID=40&md5=d2dc1f17ec9d8a6897a108f284ecf4e6
N1  - cited By 2
N2  - Diabetes is one of the chronic and fatal disease that occurs due to variations in the production of insulin and its use in the body which leads to high blood sugar levels and some long term complications. According to IDF report 2017, 425 million adults have been affected by diabetes worldwide. The effective Diabetes diagnosis can be made through the knowledge discovery of available medical records. The conventional approaches focus only on data mining techniques but lacks in proper data preparation and attribute feature selection. This paper aims to find solutions for the efficient data preparation technique with Decision trees for improved accuracy. © 2018 IEEE.
ER  - 
TY  - JOUR
T1  - Thirty-day re-hospitalization rate prediction of diabetic patients
A1  - Shih, D.-H.
A1  - Huang, F.-C.
A1  - Weng, C.-L.
A1  - Shih, P.-Y.
A1  - Yen, D C
Y1  - 2020///
KW  -  Apriori algorithms; Classification results; Data mining technology; Feature reduction; Health care costs; Health-care system; Prediction accuracy; UCI machine learning repository
KW  -  Data mining
KW  - Classification (of information); Forecasting; Health care; Hospitals; Support vector machines
PB  - Taiwan Academic Network Management Committee
JF  - Journal of Internet Technology
VL  - 21
IS  - 7
SP  - 2065
EP  - 2074
DO  - 10.3966/160792642020122107021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107235716&doi=10.3966%2F160792642020122107021&partnerID=40&md5=0c128270d324c64595cadc495c0e75c3
N1  - cited By 0
N2  - Diabetes is a serious global health problem, and rehospitalization is usually associated with increased mortality and excessive medical burden. With the increasing cost of diabetes to the health care system, rehospitalization is recommended as a goal to reduce health care costs. This paper aims to use data mining technology to accurately predict the 30-day re-hospitalization of diabetic patients. We use the data set from UCI machine learning repository, preprocessing, use feature reduction method to find out the classification results of rehospitalization, and then use frequent set and Apriori algorithm to find the association rules between diabetes mellitus patients and re-hospitalization related variables. The experimental results show that the recursive feature reduction method is effective in combined with SVM can get a better prediction accuracy. © 2020 Taiwan Academic Network Management Committee. All rights reserved.
ER  - 
TY  - CONF
T1  - Identification of Cardiovascular Diseases Risk Factors among Diabetes Patients Using Ontological Data Mining Techniques
A1  - Qrenawi, M I
A1  - Al Sarraj, W
Y1  - 2018///
KW  -  Cardio-vascular disease; cardiovascular; Data mining algorithm; Frequent pattern discovery; Healthcare environments; Medical knowledge discovery; Pre-processing stages; T2DM
KW  -  Data mining
KW  - Cardiology; Diseases; Medical education; Ontology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2018 International Conference on Promising Electronic Technologies, ICPET 2018
SP  - 129
EP  - 134
SN  - 9781538656976
DO  - 10.1109/ICPET.2018.00030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058386469&doi=10.1109%2FICPET.2018.00030&partnerID=40&md5=e5926303d5863cf354583f4f0b5f44a5
N1  - cited By 2
N2  - Healthcare environment is rich of data, but still needs knowledge extraction that is necessarily important for saving people lives. Medical Knowledge discovery is a process of extracting knowledge patterns from biomedical data, which is useful and crucial for making effective decisions especially in developing strategies and policies of preventive medical treatments. Data mining methods are the best-known way to recognize the hidden data standards. Ontology engineering used to improve knowledge domain representation, and further is considered for the enhancement and refinement of the mining techniques based on the discovered patterns driven from ontological data mining. In this paper, we apply ontology driven data mining techniques on a data set of diabetes patients who have cardiovascular disease. That process performed to identify the relationship between type two diabetes mellitus patients and their important laboratory tests specified by doctors. Doctors aim to investigate the probability of cardiovascular disease occurrence and stroke happening. Ontology driven Data mining techniques also used in experimental study as well as rule induction, association rules methods. In a late phase, we used frequent pattern discovery and rules induction method using ontological data mining algorithm (RMonto). The findings of this study reveals that the use of ontologies minimizes the number of attributes in the preprocessing stage and helps in all data mining stages; in addition to its important role in ontological data mining, we have a higher learning accuracy ratio exceeding 90%. The results of data mining methods and ontological data mining shows that the significance of some laboratory tests like: LP(a),CRP,HDL,FBG,TG,LDH and Chol to predict CVD risk among T2DM patients with a high accuracy. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Automated detection of Polycystic Ovarian Syndrome using follicle recognition
A1  - Deshpande, S S
A1  - Wakankar, A
Y1  - 2015///
KW  -  Contrast Enhancement; Morphological approach; Polycycstic Ovarian syndrome; Polycystic ovarian syndrome; Reproductive age group; Support vector machine algorithm; Type 2 diabetes mellitus; Ultrasound image processing
KW  -  Feature extraction
KW  - Health; Image enhancement; Image segmentation; Support vector machines; Ultrasonic applications
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of 2014 IEEE International Conference on Advanced Communication, Control and Computing Technologies, ICACCCT 2014
SP  - 1341
EP  - 1346
SN  - 9781479939145
DO  - 10.1109/ICACCCT.2014.7019318
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923270482&doi=10.1109%2FICACCCT.2014.7019318&partnerID=40&md5=40bbebe5fa0763df5931c8bd479b60e2
N1  - cited By 12
N2  - Polycystic Ovarian Syndrome (PCOS) is one of the most common hormonal disorder present in females in reproductive age group. Early detection and treatment of PCOS is important since it is often associated with obesity, type 2 diabetes mellitus, and high cholesterol levels. In this paper, automated detection of PCOS is done by calculating no of follicles in ovarian ultrasound image and then incorporating clinical, biochemical and imaging parameters to classify patients in two groups i.e. normal and PCOS affected. Number of follicles are detected by ovarian ultrasound image processing using preprocessing which includes contrast enhancement and filtering, feature extraction using Multiscale morphological approach and segmentation. Support Vector Machine algorithm is used for classification which takes into account all the parameters such as body mass index (BMI), hormonal levels, menstrual cycle length and no of follicles detected in ovarian ultrasound image processing. The results obtained are verified by doctors and compared with manual detection. The accuracy obtained for the proposed method is 95%. © 2014 IEEE.
ER  - 
TY  - CONF
T1  - Cancer prediction for type-2 diabetes using machine learning
A1  - Malathy, V
A1  - Shilpa, N
A1  - Anand, M
A1  - Swathi, N
A1  - Elavarasi, R
ED  - Rajasri Reddy I., Mahender K
Y1  - 2020///
PB  - IOP Publishing Ltd
JF  - IOP Conference Series: Materials Science and Engineering
VL  - 981
IS  - 3
DO  - 10.1088/1757-899X/981/3/032011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099518602&doi=10.1088%2F1757-899X%2F981%2F3%2F032011&partnerID=40&md5=b7878daca48977d74dc84baf46ec2249
N1  - cited By 0
N2  - Type-2 diabetic patients may easily get suffered from cancer. The presented work in this paper predicts whether the diabetic patient has got cancer problem or not. If cancer is identified in the patient, the type of cancer such as breast cancer, colon cancer and liver cancer is classified. The parameters associated with cancer and diabetes are recognized and collected. Then the collected dataset is preprocessed. During preprocessing the null values in the dataset are replaced with mean values. The dataset is now ready for training. Cancer disease prediction is done by breast cancer, colon cancer and liver cancer. To classify the cancer type, random forest algorithm is used. © Published under licence by IOP Publishing Ltd.
ER  - 
TY  - CONF
T1  - DR-NET: A Stacked Convolutional Classifier Framework for Detection of Diabetic Retinopathy
A1  - Chakravarthy, S N
A1  - Singhal, H
A1  - Yadav, N R P
Y1  - 2019///
KW  -  Classification tasks; Convolutional neural network; Diabetic retinopathy; Digital fundus images; Early warning signs; Pre-processing method; Vision impairments; Visualization technique
KW  -  Eye protection
KW  - Biomedical engineering; Convolution; Deep learning; Deep neural networks; Feature extraction; Flow visualization; Image classification; Neural networks; Visualization
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the International Joint Conference on Neural Networks
VL  - 2019-July
SN  - 9781728119854
DO  - 10.1109/IJCNN.2019.8852011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073215736&doi=10.1109%2FIJCNN.2019.8852011&partnerID=40&md5=23d23660ef400ad2b2888431e81b0862
N1  - cited By 2
N2  - Diabetic Retinopathy is the main cause of vision impairment among people suffering from diabetes and often leads to blindness. It has no early warning signs. Hence, it is of utmost importance to detect it as early as possible in order to provide adequate treatment. Most of the current research in this field focuses on a manual process of feature extraction such as annotation of lesions and optic disk segmentation so as to detect the presence of DR. In this paper, a framework DR-NET using stacked convolutional neural networks for diabetic retinopathy detection from digital fundus images is proposed. A network consisting of convolutional layers with different filters stacked in parallel, the output of which is concatenated and global max pooling performed on it, is developed. This architecture helps extract intricate features during the classification task along with minimizing the learnable parameters and reduces overfitting, thus, improving the overall performance of the model. Various preprocessing methods were applied to further improve accuracy. Visualization techniques were also used to gain insights into the learning of the model. The experimental results were performed on about 12,000 images which were an ensemble of various online datasets, yielded an accuracy of 81% and a kappa score of 0.6. © 2019 IEEE.
ER  - 
TY  - JOUR
T1  - Chemometric analysis of diffuse reflectance spectral data using singular value decomposition for blood glucose detection
A1  - Suryakala, S V
A1  - Prince, S
Y1  - 2018///
KW  -  Blood glucose; Blood glucose detection; Diffuse reflectance spectroscopy; Glucose measuring systems; Measurement techniques; Near infrared diffuse reflectance; Proportional relationships; Standard normal variates
KW  -  Singular value decomposition
KW  -  adult; aged; Article; chemometric analysis; clinical article; decomposition; diabetic patient; diffuse reflectance spectroscopy; female; glucose absorption; glucose blood level; human; male; middle aged; near infrared spectroscopy; particle size; venous blood
KW  -  glucose
KW  - Blood; Glucose; Infrared devices; Reflection; Spectrometers; Spectroscopy
PB  - World Scientific Publishing Co. Pte Ltd
JF  - Biomedical Engineering - Applications, Basis and Communications
VL  - 30
IS  - 5
DO  - 10.4015/S1016237218500278
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049577673&doi=10.4015%2FS1016237218500278&partnerID=40&md5=ed44b804f1ad9ac66560284ab90c1db3
N1  - cited By 2
N2  - Diabetes mellitus is a metabolic disorder that affects the production or usage of insulin by the body. Diabetes prevails in the body as a long-term condition which causes several other disorders if left unnoticed. Proper control of Diabetes needs continuous monitoring. The current measurement technique is invasive in nature and requires the withdrawal of blood from the body. Periodic quantification of blood glucose leads to pain and discomfort for the subject. This paper presents a non-invasive glucose measuring system using near-infrared diffuse reflectance spectroscopy (DRS). This work attempts to determine the blood glucose value from the diffuse reflected spectra in the NIR region. The study is executed with the spectral signatures of 33 diabetic subjects collected non-invasively using diffuse reflectance spectrometer from a diabetic centre. Blood glucose level of the same subjects are also recorded using the clinical method. The spectral information is subjected to standard normal variate (SNV) preprocessing method to remove baseline drift and then dimension reduction using singular value decomposition (SVD) is applied to the preprocessed data. The extracted singular values when compared with the clinically measured blood glucose is found to have a proportional relationship. The proposed study using singular value decomposition paves us the way for estimating the blood glucose value non-invasively with the obtained set of clinical blood glucose and the corresponding singular value table as a standard reference set. © 2018 National Taiwan University.
ER  - 
TY  - CONF
T1  - Hemorrhage diabetic retinopathy detection based on fundus image using neural network and FCM segmentation
A1  - Purwanithami, H A
A1  - Atika Sari, C
A1  - Rachmawanto, E H
A1  - Rosal Ignatius Moses Setiadi, D
Y1  - 2020///
KW  -  Contrast Enhancement; Detection accuracy; Diabetic retinopathy; Fundus image; Neural network method; RGB images
KW  -  Neural networks
KW  - Blood; Blood vessels; Eye protection; Image enhancement; Image segmentation
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2020 International Seminar on Application for Technology of Information and Communication: IT Challenges for Sustainability, Scalability, and Security in the Age of Digital Disruption, iSemantic 2020
SP  - 45
EP  - 49
SN  - 9781728190686
DO  - 10.1109/iSemantic50169.2020.9234226
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096876063&doi=10.1109%2FiSemantic50169.2020.9234226&partnerID=40&md5=8c45345ef92879075f71a9e8b943d748
N1  - cited By 2
N2  - Hemorrhage Diabetic Retinopathy is a type of diabetes that attacks the blood vessels of the retina. This disease can cause blindness, but early treatment can minimize this. This research proposes a method of detecting blood vessels in the retina caused by Hemorrhage Diabetic Retinopathy. Detection is based on the Fundus image based on several stages of preprocessing, segmentation, and detection. At the preprocessing stage, the fundus image with the RGB image format is taken the green channel to do a contrast enhancement operation with CLAHE and segmentation with FCM. Then the detection is done using the Neural Network method. At the experimental stage, 100 testing images are used which are divided into two classes, namely Hemorrhage and Non-Hemorrhage. Detection results showed from 100 images, only one image was detected incorrectly, so it can be concluded that the detection accuracy reached 99%. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - Identifying Latent Classes of Non-Communicable Disease Comorbidities in Clinical Laboratory Data
A1  - Rivera, M A C
A1  - Carpio, J T
A1  - Vinluan, A A
Y1  - 2021///
KW  -  Clinical laboratory data; Clusterings; Comorbidities; Laboratory datum; Latent class; Latent class analysis; Model evaluation; Model Selection; Non-communicable disease
KW  -  Patient treatment
KW  - Clustering algorithms; Diseases; Epidemiology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2021 International Conference on Big Data Engineering and Education, BDEE 2021
SP  - 16
EP  - 22
SN  - 9781665439572
DO  - 10.1109/BDEE52938.2021.00009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123488004&doi=10.1109%2FBDEE52938.2021.00009&partnerID=40&md5=edacfa70eb76ddf07de4888265dfe709
N1  - cited By 0
N2  - Non-communicable diseases (NCDs) make up 7 of the world's top 10 causes of mortality. Detecting these diseases in patient populations can allow early intervention, prevention, and management. This study is a work-in-progress that intends to identify latent classes of potential NCD comorbidities without other clinical criteria by detecting patterns in clinical laboratory data. The latent classes will be determined using Latent Class Analysis (LCA). The first phase included data extraction and preprocessing. The second phase will consist of model selection and evaluation and system development. Log-likelihood, Bayesian Information Criterion (BIC), and Akaike Information Criterion (AIC)) fit measures will be used in model selection. Model evaluation will use posterior probability and entropy scores. The chosen model will be used in a system to help identify comorbidity patterns in new patient records. Preliminary experiments on a model selection with diabetes mellitus (DM) indicators revealed three distinct latent classes, wherein one of the classes indicated its presence. The researchers hope that primary care physicians and local government health centers could use this system to tailor comorbidity treatment and management programs for their patients. Future work includes the integration of this study's results into a customized laboratory information system. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - Retinal vessel segmentation using multi-scale line detection
A1  - Gao, X
Y1  - 2013///
JF  - International Review on Computers and Software
VL  - 8
IS  - 2
SP  - 613
EP  - 619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877317723&partnerID=40&md5=edf4d8b5c9d333d3beb80f61ed172471
N1  - cited By 3
N2  - Inspection of the retinal vasculature may reveal precursors of serious diseases such as hypertension, diabetes, cardiovascular disease and stroke. In this paper an effective method for automatically extracting the vascular network in retinal images is presented. The proposed method is based on a multi-scale line detection, which is the line responses at varying scales. Linearly combining these line responses produces the final segmentation for each retinal image. The multi-scale line detection is applied on a vessel enhanced image whose noise and optic disc is removed and the contrast of blood vessels (including thin vessels) is enhanced by top-hat transformation and line detector filter. The preprocessing with retinal image results in the response of thin vessels with multi-scale detection is more sensitive and free from the influence of the optic disc, so the proposed method can get the very detail vascular tree in segmentation results. The performance of the proposed method was evaluated on two publicly available DRIVE, STARE databases. Experimental results have also shown the proposed method achieves high local accuracy (a measure to assess the accuracy at regions around the vessels) and approximates the average accuracy of a human observer. Moreover, the method is simple, fast, and robust to noise, so suitable for being integrated into a computer-assisted diagnostic system for ophthalmic disorders. © 2013 Praise Worthy Prize S.r.l. - All rights reserved.
ER  - 
TY  - CONF
T1  - Using Machine Learning to Predict Diabetes Complications
A1  - Jian, Y
A1  - Pasquier, M
A1  - Sagahyroon, A
A1  - Aloul, F
Y1  - 2021///
KW  -  Chronic disease; Classification algorithm; Diabetes complication; Diabetes mellitus; Diabetes prediction; Diabetic retinopathy; Dyslipidemias; Machine-learning; Metabolic syndromes; Supervised classification
KW  -  Forecasting
KW  - Eye protection; Supervised learning
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - BioSMART 2021 - Proceedings: 4th International Conference on Bio-Engineering for Smart Technologies
SN  - 9781665408103
DO  - 10.1109/BioSMART54244.2021.9677649
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125341610&doi=10.1109%2FBioSMART54244.2021.9677649&partnerID=40&md5=ad86c1affa726982db525a59da675a33
N1  - cited By 0
N2  - Diabetes Mellitus (DM) is a chronic disease that is considered to be life threatening. It can affect any part of the body over time, resulting in more serious complications such as Dyslipidemia, Neuropathy and Retinopathy. In this work, different supervised classification algorithms were applied to build several models to predict and diagnose eight diabetes complications. The complications include: Metabolic Syndrome, Dyslipidemia, Neuropathy, Nephropathy, Diabetic Foot, Hypertension, Obesity, and Retinopathy. For this study, a dataset collected by the Rashid Centre for Diabetes and Research (RCDR) located in Ajman, UAE, was utilized. The dataset contains 884 records with 79 features. Some essential preprocessing steps were applied to handle the missing values and unbalanced data problems. Multiple solutions were tested and evaluated. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy using image processing and deep learning
A1  - Swain, D
A1  - Bijawe, S S
A1  - Akolkar, P P
A1  - Shinde, A
A1  - Mahajani, M V
Y1  - 2021///
KW  -  CNN; Convolutional neural network; Deep learning; Diabetic retinopathy; Disease diagnosis; Gaussian blur; Gaussian blur filter; Images processing; K-means++ clustering; Machine intervention; Softmax
KW  -  Gabor filters
KW  - Convolution; Convolutional neural networks; Deep learning; Diagnosis; Eye protection; Grading; Image processing; K-means clustering; Network architecture; Ophthalmology
PB  - Inderscience Publishers
JF  - International Journal of Computing Science and Mathematics
VL  - 14
IS  - 4
SP  - 397
EP  - 409
DO  - 10.1504/IJCSM.2021.120686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124800099&doi=10.1504%2FIJCSM.2021.120686&partnerID=40&md5=581dc8dc74eebe35cdc257d1002e274e
N1  - cited By 0
N2  - Diabetic retinopathy is one of the most non-uniform and confront regions to diagnose as it is exceptionally perplexing. In the circle of retinopathy, the number of times intensive assessments are required to be done to determine upon the diabetes mellitus or blindness that patient might be facing. Various professionals may take different amount of time to recognise diabetic retinopathy. So, a framework is required that can effectively and precisely analyse the retinal conditions with no of such limitations. This paper presents a two- stage method to effectively predict the level grading of diabetic retinopathy. The first stage involves preprocessing the retinal image and reducing the noise from an image. The second stage involves building a convolutional neural network architecture for predicting diabetic retinopathy level. It is a hurdle of diabetes that can affect the retinal nervous and lead to total or partial loss of vision. © 2021 Inderscience Enterprises Ltd.
ER  - 
TY  - JOUR
T1  - Methodological comparisons of heart rate variability analysis in patients with type 2 diabetes and angiotensin converting enzyme polymorphism
A1  - Marzbanrad, F
A1  - Khandoker, A H
A1  - Hambly, B D
A1  - Ng, E
A1  - Tamayo, M
A1  - Lu, Y
A1  - Matthews, S
A1  - Karmakar, C
A1  - Palaniswami, M
A1  - Jelinek, H F
A1  - McLachlan, C
Y1  - 2016///
KW  -  ACE protein
KW  -  Adaptive preprocessing; Angiotensin-converting enzyme; Heart rate variability; Heart rate variability analysis; Interval time series; Methodological comparison; Preprocessing approaches; Type 2 diabetes mellitus
KW  -  Aged; Diabetes Mellitus
KW  -  Genetic
KW  -  Heart
KW  -  Type 2; Female; Heart Rate; Humans; Male; Middle Aged; Peptidyl-Dipeptidase A; Polymorphism
KW  -  Type 2; female; genetic polymorphism; genetics; heart rate; human; male; middle aged; pathophysiology; physiology
KW  -  aged; Diabetes Mellitus
KW  -  human; dipeptidyl carboxypeptidase
KW  - Enzymes; Polymorphism
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Journal of Biomedical and Health Informatics
VL  - 20
IS  - 1
SP  - 55
EP  - 63
DO  - 10.1109/JBHI.2015.2480778
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971321459&doi=10.1109%2FJBHI.2015.2480778&partnerID=40&md5=ac535f1ab70d7ea67d69dd1289b13644
N1  - cited By 10
N2  - Angiotensin converting enzyme (ACE) polymorphism has been shown to be important in hypertension progression and also in diabetes complications, especially associated with heart disease. Heart rate variability (HRV) is an established measure for classification of autonomic function regulating heart rate, based on the interbeat interval time series derived from a raw ECG recording. Results of this paper show that the length (number of interbeat intervals) and preprocessing of the tachogram affect the HRV analysis outcome. The comparison was based on tachogram lengths of 250, 300, 350, and 400 RR-intervals and five preprocessing approaches. An automated adaptive preprocessing method for the heart rate biosignal and tachogram length of 400 interbeat intervals provided the best classification. HRV results differed for the Type 2 Diabetes Mellitus (T2DM) group between the I/I genotype and the I/D and D/D genotypes, whereas for controls there was no significant difference in HRV between genotypes. Selecting an appropriate length of recording and automated preprocessing has confirmed that there is an effect of ACE polymorphism including the I/I genotype and that I/I should not be combined with I/D genotype in determining the extent of autonomic modulation of the heart rate. © 2015 IEEE.
ER  - 
TY  - JOUR
T1  - Automatic Classification of Preliminary Diabetic Retinopathy Stages using CNN
A1  - Khaled, O
A1  - ElSahhar, M
A1  - El-Dine, M A
A1  - Talaat, Y
A1  - Hassan, Y M I
A1  - Hamdy, A
Y1  - 2021///
KW  -  Automatic classification; Condition; Convolutional neural network; Diabetes mellitus; Diabetic retinopathy; Eye disease; Images processing; Retinal disease
KW  -  Eye protection
KW  - Classification (of information); Convolution; Convolutional neural networks; Image classification; Object detection; Ophthalmology
PB  - Science and Information Organization
JF  - International Journal of Advanced Computer Science and Applications
VL  - 12
IS  - 2
SP  - 713
EP  - 721
DO  - 10.14569/IJACSA.2021.0120289
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102028530&doi=10.14569%2FIJACSA.2021.0120289&partnerID=40&md5=38c0db2185cfa1d1ca8312c2617b7efd
N1  - cited By 0
N2  - Diabetes Mellitus is one of the modern world’s most prominent and dominant maladies. This condition later on leads to a menacing eye disease called Diabetic Retinopathy (DR). Diabetic Retinopathy is a retinal disease that is caused by high blood sugar levels in the retina, and can naturally progress to irreversible vision loss (blindness). The primary purpose of this imperative research is the early detection and classification of this hazardous condition, to try and prevent any threatening complications in the future. In the course of recent years, Convolutional Neural Networks (CNNs) turned out to be exceptionally famous and fruitful in solving and unraveling image processing and object detection problems for enormous datasets. Throughout this pivotal research, a model was proposed to detect the presence of (DR) and classify it into 5 distinct stages, factoring in an immense and substantial dataset. The model starts by applying preprocessing techniques such as normalization, to maintain the same dimensions for all the images before proceeding to the main processing stage. Furthermore, diverse sampling methods such as “Resize & Crop”, “Rotation”, and “Flipping” have been tested out, so as to pinpoint the best augmentation technique. Finally, the normalized images were fed into a Convolutional Neural Network (CNN), to predict whether a person suffers from DR or not, and classify the level/stage of the disease. The proposed method was utilized on 88,700 retinal fundus images, which are a parcel of the full (EyePACS) dataset, and finally achieved 81.12%, 89.16%, and 84.16% for sensitivity, specificity, and accuracy, respectively. © 2021. All Rights Reserved.
ER  - 
TY  - CONF
T1  - Diagnosis of Diabetes by Tongue Analysis
A1  - Srividhya, E
A1  - Muthukumaravel, A
Y1  - 2019///
KW  -  Classifier models; Color and texture features; Color image segmentation; Diabetic diagnosis; Image processing technique; Optimization techniques; Segmentation methods; Tongue
KW  -  Image texture
KW  - Artificial intelligence; Color; Color image processing; Diagnosis; Image classification; Image segmentation; Textures
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of 2019 International Conference on Computational Intelligence and Knowledge Economy, ICCIKE 2019
SP  - 256
EP  - 259
SN  - 9781728137780
DO  - 10.1109/ICCIKE47802.2019.9004394
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080940225&doi=10.1109%2FICCIKE47802.2019.9004394&partnerID=40&md5=33f4e998109c79f4abbb7b9d9b41d20c
N1  - cited By 0
N2  - Tongue image is used to identify any disease based on its shape, color, size and texture. We proposed the image segmentation method; the segmented study of the tongue reflects the presence of diabetes of a person in addition optimization technique is used to obtain the best result. We are explaining computerized model to examine the changes in tongue, this will be useful in finding different diseases that will take place in the patient's body. First we have to take the samples of image and introduce the image processing techniques to differentiate the color and texture features. With this technique we can classify the tongue image by using Classifier model. The proposed design represents an input image, preprocessing that image, identifying the color, gist feature, implemented and identifies the problem effectively and the result achieved better performance when compared to existing techniques. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - Chronic Kidney Disease Survival Prediction with Artificial Neural Networks
A1  - Zhang, H
A1  - Hung, C.-L.
A1  - Chu, W.C.-C.
A1  - Chiu, P.-F.
A1  - Tang, C Y
ED  - Schmidt H. Griol D., Wang H Baumbach J Zheng H Callejas Z Hu X Dickerson J Zhang L
Y1  - 2019///
KW  -  Artificial neural network models; Chronic kidney disease; Computational capability; Computational results; High blood pressures; LASSO; Medical intervention; Technological advancement
KW  -  Diagnosis
KW  - Bioinformatics; Blood pressure; Classification (of information); Clinical research; Complex networks; Deep learning; Forecasting; Neural networks; Patient treatment
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018
SP  - 1351
EP  - 1356
SN  - 9781538654880
DO  - 10.1109/BIBM.2018.8621294
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062550699&doi=10.1109%2FBIBM.2018.8621294&partnerID=40&md5=269efa6116ec65d72ea8435d49cdca99
N1  - cited By 8
N2  - The main objective of this work is to investigate the performance of Artificial Neural Network (ANN) models while applying to the survivability prediction on Chronic Kidney Disease (CKD) patients. Chronic kidney disease patients suffer from losing the primary function of the kidney, i.e., blood filtration, gradually over time. At the end stage, regular hemodialysis or kidney transplant are required to survive. During the process of disease development, patients may also suffer from complications of acidosis, anemia, diabetes, high blood pressure or neuropathy, etc., which in turn affects patients' quotidian life. It is reported that the median survival time of late-stage patients is only about 3 years. Evaluating precisely the condition of patients is of great importance as it would greatly help to decide appropriate care, medications or medical interventions needed, which among them have a complex interrelationship and influence the outcome of the individual patient. An accurate prediction model would hopefully be able to fit into that role and may be used to revise current treatment. However, due to the complex nature of the problem, as multiple interrelated factors may influence the patient's survival, finding such a model is a challenging task. Recently, artificial intelligence (AI), especially the deep learning technique has become a thriving field, owing to the rise of computational capability. As the approach needs no human specialist in specifying explicit knowledge in advance, but gathers knowledge automatically from amounts of data by building a hierarchy of concepts with complicated ones upon simpler ones, it has already been applied successfully in intuitive problems, like understanding speeches or images, making diagnoses in medicine or self-driving cars, etc. And conversely, technological advancements have been made available through practical applications, profiting from which we may develop prediction models for chronic kidney disease survivability. In this research, data preprocessing, data transformations, and artificial neural networks are used to establish the mapping from many clinical factors to the patient's survival. The computational results are also reported in the paper. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Svm ranking with backward search for feature selection in type II diabetes databases
A1  - Balakrishnan, S
A1  - Narayanaswamy, R
A1  - Savarimuthu, N
A1  - Samikannu, R
Y1  - 2008///
KW  -  Classifiers; Control theory; Cybernetics; Data mining; Data processing; Database systems; Diagnosis; Learning algorithms; Support vector machines
KW  -  Medical computing
KW  - Backward search; Classification accuracy; False negative rate; Feature selection; Predictive accuracy; SVM
JF  - Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics
SP  - 2628
EP  - 2633
DO  - 10.1109/ICSMC.2008.4811692
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949136657&doi=10.1109%2FICSMC.2008.4811692&partnerID=40&md5=2d35cae3eab3f088bb4cb028618cdd2b
N1  - cited By 30
N2  - Clinical databases have accumulated large quantities of information about patients and their clinical history. Data mining is the search for relationships and patterns within this data that could provide useful knowledge for effective decision-making. Classification analysis is one of the widely adopted data mining techniques for healthcare applications to support medical diagnosis, improving quality of patient care, etc. Usually medical databases are high dimensional in nature. If a training dataset contains irrelevant features (i.e., attributes), classification analysis may produce less accurate results. Data pre-processing is required to prepare the data for data mining and machine learning to increase the predictive accuracy. Feature selection is a preprocessing technique commonly used on high-dimensional data and its purposes include reducing dimensionality, removing irrelevant and redundant features, reducing the amount of data needed for learning, improving algorithms' predictive accuracy, and increasing the constructed models' comprehensibility. Much research work in data mining has gone into improving the predictive accuracy of the classifiers by applying the techniques of feature selection. The importance of feature selection in medical data mining is appreciable as the diagnosis of the disease could be done in this patient-care activity with minimum number of features. Feature selection may provide us with the means to reduce the number of clinical measures made while still maintaining or even enhancing accuracy and reducing false negative rates. In medical diagnosis, reduction in false negative rate can, literally, be the difference between life and death. In this paper we propose a feature selection approach for finding an optimum feature subset that enhances the classification accuracy of Naive Bayes classifier. Experiments were conducted on the Pima Indian Diabetes Dataset to assess the effectiveness of our approach. The results confirm that SVM Ranking with Backward Search approach leads to promising improvement on feature selection and enhances classification accuracy. © 2008 IEEE.
ER  - 
TY  - CONF
T1  - Diabetes Prediction Using Quantum Neurons with Preprocessing Based on Hypercomplex Numbers
A1  - Monteiro, C A
A1  - De Paula Neto, F M
Y1  - 2021///
KW  -  Accuracy rate; Diabetes prediction; Hypercomplex number; Machine-learning; Quantum Computing; Quantum machine learning; Quantum machines; Quantum neuron; Single quantum
KW  -  Health care
KW  - Learning algorithms; Machine learning; Neural networks; Neurons; Quantum efficiency; Quantum optics
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 IEEE Symposium Series on Computational Intelligence, SSCI 2021 - Proceedings
SN  - 9781728190488
DO  - 10.1109/SSCI50451.2021.9660028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125801942&doi=10.1109%2FSSCI50451.2021.9660028&partnerID=40&md5=475db5590dfcc9cc24d64566643148f7
N1  - cited By 0
N2  - The use of properties that are intrinsic to quantum mechanics has made it possible to build quantum algorithms with greater efficiency than classical algorithms to solve problems whose classically efficient solution either does not exist or is not known. There are quantum neurons that can carry an exponential amount of information to a linear number of quantum information units (qubits) using the quantum property of superposition. In this paper, we compare the performance of three of these quantum neuron models applied to the diabetes classification problem. We also propose the use of different data preprocessing strategies. Quantum neurons were simulated using the IBM Qiskit tool. We compare the preprocessing approaches applied to two toy problems (1) simulating the XOR operator and (2) solving a generic nonlinear problem. The results of the experiments shows that a single quantum neuron is capable of achieving an accuracy rate of 100% in the XOR problem and an accuracy rate of 100% in a non-linear dataset, demonstrating that the quantum neurons with real weights are capable of modeling non-linearly separable problems. In the problem of diagnosing diabetes, quantum neurons achieved an accuracy rate of 76% and AUC-ROC of 88%, while its classic version, the perceptron, reached only 63% accuracy and the artificial neural network reached 80% AUC-ROC. These results indicate that a single quantum neuron performs better than its classical version and even the artificial neural network for AUC-ROC, demonstrating potential for use in healthcare applications in the near future. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Comparative Analysis of Data Preparation Methods Employed in Prediction of Diabetes Mellitus Diagnosis using eICU-CRD Dataset.
A1  - Sagi, S L N
A1  - Narsapuram, M
A1  - Nakarikanti, P
A1  - Sane, S
A1  - Vadisina, S S
A1  - Uddagiri, C
Y1  - 2021///
KW  -  Analysis of data; Comparative analyzes; Cross validation; Data preprocessing; Diabetes mellitus; EICU-CRD dataset; Feature engineerings; Lightgbm classifier; MIT GOSSIS; Prediction accuracy
KW  -  Noninvasive medical procedures
KW  - Classification (of information); Computer aided diagnosis; Forecasting; Pipelines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 International Conference on Smart Generation Computing, Communication and Networking, SMART GENCON 2021
SN  - 9781665425032
DO  - 10.1109/SMARTGENCON51891.2021.9645891
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124036910&doi=10.1109%2FSMARTGENCON51891.2021.9645891&partnerID=40&md5=f64060814656c281de27bbf00b420172
N1  - cited By 0
N2  - In any classification system it's necessary to preprocess the source data in order to draw useful insights. In this paper, two different preprocessing approaches on WiDS Datathon 2021 Data were compared and analyzed deeply to account for the difference between resultant prediction accuracies.A comparative analysis was conducted on the preprocessing steps of both the approaches The steps applied are but not limited to - handling missing values, Feature scaling, Standardization, etc. in the process of determining whether a patient has been diagnosed with Diabetes Mellitus before. The readings in the first 24 hours from the patient's stay in ICU were used here and all the readings were acquired through invasive and noninvasive methods. LightGBM Classifier was used for the purpose of classifying the target variable.An attempt to determine the probability that a patient who is admitted to an ICU has been diagnosed with Diabetes Mellitus was made using two approaches with different machine learning pipelines. The first pipeline resulted in an accuracy score of 0.87242 and the second one gave a score of 0.86471. The boosting algorithms used in both the methods were the same but due to the differences in the missing values handling techniques and feature engineering steps, their prediction accuracies were different. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - A novel breath analysis system based on electronic olfaction
A1  - Guo, D
A1  - Zhang, D
A1  - Li, N
A1  - Zhang, L
A1  - Yang, J
Y1  - 2010///
KW  -  Airway inflammation; Breath analysis; Classification methods; Clinical conditions; Electronic nose (e-nose); Electronic olfaction; Signal preprocessing; Therapy monitoring
KW  -  Diseases
KW  -  article; breath analysis; clinical effectiveness; controlled study; diabetes mellitus; diagnostic accuracy; diagnostic error; electronic nose; hemodialysis; human; kidney disease; kidney failure; medical instrumentation; olfactometer; respiratory tract inflammation; sensor; signal processing
KW  - Biomarkers; Chemical analysis; Chemical sensors; Chromatography; Diagnosis; Disease control; Electronic nose; Gas chromatography; Pathology; Patient treatment
PB  - IEEE Computer Society
JF  - IEEE Transactions on Biomedical Engineering
VL  - 57
IS  - 11
SP  - 2753
EP  - 2763
DO  - 10.1109/TBME.2010.2055864
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958138911&doi=10.1109%2FTBME.2010.2055864&partnerID=40&md5=0e592a31b6024260c92148a7c4846507
N1  - cited By 120
N2  - Certain gases in the breath are known to be indicators of the presence of diseases and clinical conditions. These gases have been identified as biomarkers using equipments, such as gas chromatography and electronic nose (e-nose). GC is very accurate but is expensive, time consuming, and nonportable. E-nose has the advantages of low cost and easy operation, but is not particular for analyzing breath odor, and hence, has a limited application in diseases diagnosis. This paper proposes a novel system that is special for breath analysis. We selected chemical sensors that are sensitive to the biomarkers and compositions in human breath, developed the system, and introduced the odor signal preprocessing and classification method. To evaluate the system performance, we captured breath samples from healthy persons and patients known to be afflicted with diabetes, renal disease, and airway inflammation, respectively, and conducted experiments on medical treatment evaluation and disease identification. The results show that the system is not only able to distinguish between breath samples from subjects suffering from various diseases or conditions (diabetes, renal disease, and airway inflammation) and breath samples from healthy subjects, but in the case of renal failure is also helpful in evaluating the efficacy of hemodialysis (treatment for renal failure). © 2010 IEEE.
ER  - 
TY  - CONF
T1  - A Study on Recent Developments for Detection of Neovascularization
A1  - Firdausy, K
A1  - Wahyunggoro, O
A1  - Nugroho, H A
A1  - Sasongko, M B
Y1  - 2019///
KW  -  Abnormal blood vessels; Automatic screening; Diabetic retinopathy; Digital fundus images; Neo-vascularization; Pre-processing step; Retinal blood vessels; Retinal fundus images
KW  -  Eye protection
KW  - Blood; Blood vessels; Diagnosis; Engineering research; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2019 11th International Conference on Information Technology and Electrical Engineering, ICITEE 2019
SN  - 9781728140551
DO  - 10.1109/ICITEED.2019.8929941
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077960681&doi=10.1109%2FICITEED.2019.8929941&partnerID=40&md5=bd17eea4c75fb3d178a71f59d2a7c2d5
N1  - cited By 1
N2  - Diabetes and its complications are major causes of death in most countries. One of the complications of diabetes is diabetic retinopathy, the disruption of vision because of the damage to the retinal blood vessels. Automatic screening of diabetic retinopathy is needed because of the increasing number of diabetic retinopathy patients and the limited number of ophthalmologists. Neovascularization (new abnormal blood vessel) is an indicator of proliferative diabetic retinopathy, which is the most advanced stage of diabetic retinopathy. The accurate detection of neovascularization is an important step for the early detection of proliferative diabetic retinopathy. This paper examines some of the current techniques used to detect neovascularization from retinal digital fundus images. Various algorithms include preprocessing steps, and image databases are discussed. This work can be used by researchers to conduct further research in the field of neovascularization detection. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - Diabetic Retinopathy dark lesion detection: Preprocessing phase
A1  - Poostchi, H
A1  - Khakmardan, S
A1  - Pourreza, H
Y1  - 2011///
KW  -  Eye protection
KW  -  Knowledge engineering; Photography
KW  - Diabetic retinopathy; Fundus photographs; Lesion detection; preprocessing; retina
JF  - 2011 1st International eConference on Computer and Knowledge Engineering, ICCKE 2011
SP  - 177
EP  - 182
SN  - 9781467357135
DO  - 10.1109/ICCKE.2011.6413347
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874302445&doi=10.1109%2FICCKE.2011.6413347&partnerID=40&md5=69270e861318f75a5e5832977c00c291
N1  - cited By 11
N2  - Diabetic Retinopathy is the common cause of vision loss and blindness in patients with diabetes. Diabetic Retinopathy indicative lesions include dark lesions such as Microaneurysms and Hemorrhages; and bright lesions like Exudates. Automatic detection of distinct lesions in digital color fundus photographs is a critical step in development of diabetic retinopathy automatic detection systems. Dark lesions are the early signs of the disease. In addition, preprocessing phase is affectively required in any automatic dark lesion detection system for contrast enhancement around the dark lesions. Therefore, in this paper several different methods of preprocessing digital color fundus photographs, as the first critical step of dark lesions detection, are summarized. © 2011 IEEE.
ER  - 
TY  - JOUR
T1  - A comprehensive review on automated systems for severity grading of diabetic retinopathy and macular edema
A1  - Mathews, M R
A1  - Anzar, S M
Y1  - 2021///
KW  -  Automated screening; Automated systems; Clinical settings; Diabetes mellitus; Diabetic retinopathy; Evaluation metrics; Learning-based algorithms; Systematic searches
KW  -  Learning algorithms
KW  - Automation; Computer aided diagnosis; Cost effectiveness; Deep learning; Eye protection; Grading
PB  - John Wiley and Sons Inc
JF  - International Journal of Imaging Systems and Technology
VL  - 31
IS  - 4
SP  - 2093
EP  - 2122
DO  - 10.1002/ima.22574
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103419863&doi=10.1002%2Fima.22574&partnerID=40&md5=f3067a08041511d6cc4de1493f604f0c
N1  - cited By 0
N2  - Diabetes mellitus is a major medical concern worldwide. Long-term diabetes can affect the retina of the eye and lead to diabetic retinopathy (DR) and diabetic macular edema (DME). Proper screening and consultation with an ophthalmologist are necessary to prevent avoidable vision loss. As DR and DME have become more prevalent, automated screening is essential to provide cost-effective and rapid solutions with reduced human resources requirements. This paper aims to provide a comprehensive review of the literature on computer-aided diagnosis of DR and DME. We identified the studies on automated five-class grading of DR according to International Clinical Diabetic Retinopathy severity scale and three class grading of diabetic maculopathy, using fundus images. A systematic search on research repositories was conducted, and relevant studies were scrutinized and included in the review. The studies were reported in nearly 100 different journals. We have reviewed the studies in all aspects including datasets, preprocessing, non-deep learning, and deep learning-based algorithms, and evaluation metrics. Significant contributions in developing automated tools for DR/DME grading are highlighted. We have identified and discussed research gaps and challenges. This will help researchers to get an updated summary of work done in the area. Deep learning-based algorithms have outperformed the traditional algorithms in the domain. Despite their promising performance, these algorithms reveal the potential for significant improvements to become a reliable tool in clinical settings. © 2021 Wiley Periodicals LLC.
ER  - 
TY  - JOUR
T1  - A Clinical Decision Support System to Stratify the Temporal Risk of Diabetic Retinopathy
A1  - Bernardini, M
A1  - Romeo, L
A1  - Mancini, A
A1  - Frontoni, E
Y1  - 2021///
KW  -  Clinical decision support systems; Code; Diabetic retinopathy; Gradient boosting; Machine-learning; Microvascular; Predictive medicine; Predictive models; Predictive performance; Task analysis
KW  -  Eye protection
KW  - Decision support systems; Decision trees; Diagnosis; E-learning; Health risks; Machine learning; Records management; Risk assessment
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Access
VL  - 9
SP  - 151864
EP  - 151872
DO  - 10.1109/ACCESS.2021.3127274
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119418547&doi=10.1109%2FACCESS.2021.3127274&partnerID=40&md5=32fc07d51482b1b0759351fa42238d2f
N1  - cited By 0
N2  - Diabetic Retinopathy (DR) is the most common and insidious microvascular complication of diabetes, and can progress asymptomatically until a sudden loss of vision occurs. Although DR is prevalent nowadays, its prevention remains challenging. The multiple aim of this study was to predict the risk of developing DR as diabetic complication (task 1) and, subsequently, temporally stratify the DR risk (task 2) using electronic health records data. To perform these objectives, a novel preprocessing procedure was designed to select both control and pathological patients, and moreover, a novel fully annotated/standardized 120K dataset from multiple diabetologic centers was provided. Globally, although the Extreme Gradient Boosting model offers satisfying predictive performance, the Random Forest model obtained the best predictive performance to solve task 1 and task 2, reaching the best Area Under the Precision-Recall Curve of 72.43 % and 84.38 %, respectively. Also the features importance extracted from the best Machine Learning (ML) models is provided. The proposed Artificial Intelligence-based solution was proven to be capable of generalizing across different diabetologic centers while ensuring high-interpretability. Moreover, the proposed ML solution is currently being adopted as a Clinical Decision Support System in several diabetologic centers for DR screening and follow-up purposes. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Improvement of grey wolf optimizer with adaptive middle filter to adjust support vector machine parameters to predict diabetes complications
A1  - Jeyafzam, F
A1  - Vaziri, B
A1  - Suraki, M Y
A1  - Hosseinabadi, A A R
A1  - Slowik, A
Y1  - 2021///
KW  -  Classification methods; Diabetes diagnosis; Large amounts of data; Machine learning methods; Meta heuristic algorithm; Multi-layer perceptron neural networks; Parameter adjustments; Particle swarm optimization algorithm
KW  -  Support vector machines
KW  - Adaptive filtering; Adaptive filters; Data mining; Decision trees; Diagnosis; Filtration; Forecasting; Genetic algorithms; Large dataset; Learning systems; Multilayer neural networks; Particle swarm optimization (PSO); Statistical tests; Trees (mathematics)
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Neural Computing and Applications
VL  - 33
IS  - 22
SP  - 15205
EP  - 15228
DO  - 10.1007/s00521-021-06143-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107761086&doi=10.1007%2Fs00521-021-06143-y&partnerID=40&md5=ef7ee4a97c21120b98bb32c6367615cb
N1  - cited By 1
N2  - In medical science, collecting and classifying data from various diseases is a vital task. The confused and large amounts of data are problems that prevent us from achieving acceptable results. One of the major problems for diabetic patients is a failure to properly diagnose the disease. As a result of this mistake in diagnosis or failure in early diagnosis, the patient may suffer from complications such as blindness, kidney failure, and cutting off the toes. Nowadays, doctors diagnose the disease by relying on their experience and knowledge and performing complex and time-consuming tests. One of the problems with current diabetic, diagnostic methods is the lack of appropriate features to diagnose the disease and consequently the weakness in its diagnosis, especially in its early stages. Since diabetes diagnosis relies on large amounts of data with many parameters, it is necessary to use machine learning methods such as support vector machine (SVM) to predict the complications of diabetes. One of the disadvantages of SVM is its parameter adjustment, which can be accomplished using metaheuristic algorithms such as particle swarm optimization algorithm (PSO), genetic algorithm, or grey wolf optimizer (GWO). In this paper, after preprocessing and preparing the dataset for data mining, we use SVM to predict complications of diabetes based on selected parameters of a patient acquired by laboratory test using improved GWO. We improve the selection process of GWO by employing dynamic adaptive middle filter, a nonlinear filter that assigns appropriate weight to each value based on the data value. Comparison of the final results of the proposed algorithm with classification methods such as a multilayer perceptron neural network, decision tree, simple Bayes, and temporal fuzzy min–max neural network (TFMM-PSO) shows the superiority of the proposed method over the comparable ones. © 2021, The Author(s).
ER  - 
TY  - CONF
T1  - Blood vessels segmentation using thresholding approach for fundus image analysis
A1  - Karule, P T
A1  - Joshi, S
Y1  - 2018///
KW  -  Background information; Detection algorithm; Fundus image; Local thresholding; Non-trivial tasks; Pre-processing operations; Retinal blood vessels; Retinal image analysis
KW  -  Blood vessels
KW  - Computer aided analysis; Image analysis; Image enhancement; Image segmentation; Intelligent computing; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of 2017 International Conference on Intelligent Computing and Control, I2C2 2017
VL  - 2018-January
SP  - 1
EP  - 5
SN  - 9781538603741
DO  - 10.1109/I2C2.2017.8321965
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049729562&doi=10.1109%2FI2C2.2017.8321965&partnerID=40&md5=fb071a99258f7af033ad84342a7b0499
N1  - cited By 4
N2  - The principal target of blood vessels detection algorithm is to early detecting the diabetes in advanced stages by comparison of its states of retinal blood vessels. Due to relatively low contrast, potential presence of dark pathologies like microaneurysms and hemorrhages and variable size of blood vessels, segmentation of blood vessels is a nontrivial task. The dictum of work present in this paper is local thresholding approach, which can be used in computer based retinal image analysis to extract the retinal image vessels. To enhance the blood vessels and suppress the background information, we performed preprocessing operation on the retinal image. Then the enhanced image is segmented using local thresholding algorithm. The proposed approach is tested on the DRIVE dataset and is compared with alternative approaches. Experimental results obtained by the proposed approach show that it is effective when this obtained the average accuracy of 96.02% and best accuracy of 97.2%. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - Chemometric approach for improving VCSEL-based glucose predictions
A1  - Fard, S T
A1  - Chrostowski, L
A1  - Kwok, E
A1  - Amann, M.-C.
Y1  - 2010///
KW  -  Absorption; Blood; Forecasting; Light; Light absorption; Light sources; Surface emitting lasers; Transceivers
KW  -  Biological; Monitoring
KW  -  Blood Glucose; Humans; Lasers; Magnetic Resonance Spectroscopy; Models
KW  -  Computer-Assisted
KW  -  Glucose
KW  -  Physiologic; Predictive Value of Tests; Reproducibility of Results; Signal Processing
KW  -  absorption spectroscopy; accuracy; aqueous solution; article; blood; chemometric analysis; diabetic patient; glucose blood level; human; information processing; laser; physiology; prediction; signal noise ratio; spectral sensitivity; vertical cavity surface emitting laser; white light
KW  -  buffer; glucose
KW  - Aqueous solutions; Average prediction error; Blood glucose; Buffered solutions; Chemometric approach; Chemometrics; Data preprocessing; Diabetes patients; Glucose monitoring; Multivariate techniques; Optical methods; Partial least squares; Physiological range; Real-time glucose monitoring; Vertical-cavity surface emitting laser; White light
JF  - IEEE Transactions on Biomedical Engineering
VL  - 57
IS  - 3
SP  - 578
EP  - 585
DO  - 10.1109/TBME.2009.2032160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649137108&doi=10.1109%2FTBME.2009.2032160&partnerID=40&md5=8269b89b99ae8ed842699af8f32b4e02
N1  - cited By 2
N2  - Optical methods are one of the painless and promising techniques that can be used for blood glucose predictions for diabetes patients. The use of thermally tunable vertical cavity surface-emitting lasers (VCSELs) as the light source to obtain blood absorption spectra, along with the multivariate technique partial least squares for analysis and glucose estimation, has been demonstrated. With further improvements by using data preprocessing and two VCSELs, we have achieved a clinically acceptable level in the physiological range in buffered solutions. The results of previous experiments conducted using white light showed that increasing the number of wavelength intervals used in the analysis improves the accuracy of prediction. The average prediction error, using absorption spectra from one VCSEL in aqueous solution, is about 1.2 mM. This error is reduced to 0.8 mM using absorption spectra from two VCSELs. This result confirms that increasing the number of VCSELs improves the accuracy of prediction. © 2006 IEEE.
ER  - 
TY  - CONF
T1  - Diabetic retinopathy analysis using CDR technique
A1  - Kurle, S S
A1  - Maralbhavi, N P
A1  - Salunke, S U
A1  - Chandanshive, A A
Y1  - 2017///
KW  -  Cup to disc ratios; Diabetic retinopathy; Different stages; Fundus image; Human eye; Micro aneurysms; Pi models; Vascular complications
KW  -  Eye protection
KW  - Clock and data recovery circuits (CDR circuits); Control systems; Health risks; Image processing; Intelligent computing; Man machine systems
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the 2017 International Conference on Intelligent Computing and Control Systems, ICICCS 2017
VL  - 2018-January
SP  - 708
EP  - 711
SN  - 9781538627457
DO  - 10.1109/ICCONS.2017.8250555
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047624384&doi=10.1109%2FICCONS.2017.8250555&partnerID=40&md5=b054df2946709e3837ad3e2292fbb8fe
N1  - cited By 0
N2  - Diabetes has become one of the rapidly increasing health threats worldwide. Diabetes occurs when the pancreas fails to secrete enough insulin, slowly affecting the retina of the human eye. As it progresses, the vision of a patient starts deteriorating, leading to diabetic retinopathy. The diabetic retinopathy is a micro vascular complication of diabetes, causing abnormalities in the retina, and in the worst case, blindness. Typically there are no salient symptoms in the early stages of diabetic retinopathy, but their number and severity predominantly increase with time. The early stage detection of diabetic retinopathy should be done. As detection appears in the early stage the level of the disease can be observed. The hardware developed will help to detect the level of DR. The implementation will be done using fundus images of retina of diabetic retinopathy patient. Preprocessing is done on image and level will be detected along it. Earlier the detection is done by taking the images and processing can be done on it. The project helps provide different stages of diabetic retinopathy, so as to cure it and diagnose it. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - Discretization for naive bayes taking the specifics of heart data into account
A1  - Bohacik, J
A1  - Zabovsky, M
Y1  - 2019///
PB  - University of Zagreb, Faculty of Organization and Informatics
JF  - Journal of Information and Organizational Sciences
VL  - 43
IS  - 1
SP  - 1
EP  - 14
DO  - 10.31341/jios.43.1.1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068857299&doi=10.31341%2Fjios.43.1.1&partnerID=40&md5=8f567b62577b74c23c96099c8678a7d0
N1  - cited By 0
N2  - At the present time heart disease is a major cause of death. Factors such as physical inactiveness, obesity, diabetes, social isolation and aging are expected to make the situation worse. It is worsened even further with misdiagnosis of patients describing heart related issues. A probability decision support approach to diagnosis of heart disease based on Naive Bayes is discussed here as most hospitals collect patient records but these are rarely used for automatic decision support. The approach is analyzed on Statlog heart data with the focus on improving preprocessing methods. As the result, a discretization algorithm with Equal Frequency Discretization which considers the specifics of engaged heart disease patients is presented. Enhancements of achieved accuracy with the added discretization and in comparison with other machine learning algorithms are shown in experiments founded on 10-fold cross-validation. © 2019, University of Zagreb, Faculty of Organization and Informatics. All rights reserved.
ER  - 
TY  - CONF
T1  - A comprehensive exploration to the machine learning techniques for diabetes identification
A1  - Wei, S
A1  - Zhao, X
A1  - Miao, C
Y1  - 2018///
KW  -  10-fold cross-validation; Classification results; Comprehensive research; Data preprocessing; Machine learning techniques; Metabolic disorders; Pima Indian Diabetes; SVM(support vector machine)
KW  -  Classification (of information)
KW  - Artificial intelligence; Deep neural networks; Internet of things; Learning systems; Population statistics; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE World Forum on Internet of Things, WF-IoT 2018 - Proceedings
VL  - 2018-January
SP  - 291
EP  - 295
SN  - 9781467399449
DO  - 10.1109/WF-IoT.2018.8355130
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050387064&doi=10.1109%2FWF-IoT.2018.8355130&partnerID=40&md5=46cdb1f5438dfa50dd8ea0259a744f79
N1  - cited By 47
N2  - Diabetes mellitus, known as diabetes, is a group of metabolic disorders and has affected hundreds of millions of people. The detection of diabetes is of great importance, concerning its severe complications. There have been plenty of research studies about diabetes identification, many of which are based on the Pima Indian diabetes data set. It's a data set studying women in Pima Indian population started from 1965, where the onset rate for diabetes is comparatively high. Most of the research studies done before mainly focused on one or two particular complex technique to test the data, while a comprehensive research over many common techniques is missing. In this paper, we make a comprehensive exploration to the most popular techniques (e.g. DNN (Deep Neural Network), SVM (Support Vector Machine), etc.) used to identify diabetes and data preprocessing methods. Basically, we examine these techniques by the accuracy of cross-validation on the Pima Indian data set. We compare the accuracy of each classifier over several ways of data preprocessors and we modify the parameters to improve their accuracy. The best technique we find has 77.86% accuracy using 10-fold cross-validation. We also analyze the relevance between each feature with the classification result. © 2018 IEEE.
ER  - 
TY  - JOUR
T1  - Prediction of diabetes Mell-EH-Tiss using unsupervised learning method
A1  - Arora, S
A1  - Munjal, K
Y1  - 2015///
PB  - Research India Publications
JF  - International Journal of Applied Engineering Research
VL  - 10
IS  - 8
SP  - 19049
EP  - 19062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929871486&partnerID=40&md5=57a1243cc20e5ba41c343c7b63e1b2d7
N1  - cited By 0
N2  - In the medical environment, data is “information rich” but “knowledge poor” because the data which is available is not “mined” as it cannot discover the hidden patterns and information which is used for reaching at the effective decision. Also, there is a lack of effective tools for analysis which can discover the hidden correlation and trends in data. Data mining have innumerable applications in field of business and scientific domains. Precious knowledge can be determined by the application of data mining techniques in the healthcare systems. Diabetes is a disease which has attacked most of the people around the world. Almost all people are reporting their problems of diabetes whether occurred by hereditary or any other problem. People around the world are using glucometer devices for testing their diabetes unknown of the fact that their results vary if another test is conducted after the first test. This leads to the error in detection of glucose with +-10 mg/dl. The data which is hidden can be mined and extracted so as to obtain the fruitful results out of that data. The main aim of this research study is to extract the hidden knowledge and patterns from the diabetes dataset and preprocessing of that data takes place. K-means Clustering algorithm to obtain the accuracy of the prediction is applied and also the new algorithm will be designed which will be the improved version of K-means to increase the accuracy and see the effect on the prediction of diabetes. This research study is very beneficial for the doctors and patients. It will provide the user with a user friendly environment without the need of doctor or any hospital staff and will help to mine the data using new technique being designed. These algorithms being designed will define the effectiveness and efficiency of the method used to predict the diabetes mellitus. © Research India Publications.
ER  - 
TY  - CONF
T1  - Impact of Different Degree of Smoothing on Non-Local Means based Filter for Retinal Vessel Modeling
A1  - Firdausy, K
A1  - Wahyunggoro, O
A1  - Nugroho, H A
A1  - Sasongko, M B
A1  - Hidayat, R
ED  - Pratomo A.H. Pranolo A., Hernandez L Drezewski R Voliansky R Zakaria M S Akbar B M Saifullah S Akbar A T Husaini R Heriyanto H Suryotomo A P Permadi V A Tahalea S P
Y1  - 2019///
KW  -  Blood vessels
KW  -  Coefficient of determination; Diabetic retinopathy; Gaussian model; Non local means; Retinal blood vessels; Retinal fundus images; Segmentation models; Two Dimensional (2 D)
KW  - Blood; Embedded systems; Eye protection; Image denoising; Image enhancement; Noise abatement; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceeding - 2019 5th International Conference on Science in Information Technology: Embracing Industry 4.0: Towards Innovation in Cyber Physical System, ICSITech 2019
SP  - 118
EP  - 122
SN  - 9781728123806
DO  - 10.1109/ICSITech46713.2019.8987555
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080960585&doi=10.1109%2FICSITech46713.2019.8987555&partnerID=40&md5=8fdb83e009b6bf3a22b957d1b5924d38
N1  - cited By 0
N2  - One of the complications of diabetes is the disruption of vision because of the damage to the retinal blood vessels, which is known as diabetic retinopathy (DR). The change in diameter of the retinal blood vessel is an important sign of DR. Several segmentation models have been proposed for measuring the retinal blood vessel diameter, but the results are not always satisfactory. Preprocessing steps, such as image filtering for noise reduction, can improve the result of the modeling step. This paper presents the results of applying non-local means based filter for retinal image enhancement to improve the performance of retinal vessel modeling using two-dimensional (2D) Gaussian. Various degrees of smoothing (DoS) of non-local means based filter were compared and applied to the retinal images from High-Resolution Fundus (HRF) public dataset. The results show that DoS value, which is 12, gave the highest R2 (coefficient of determination) value, which is 0,9862. The proposed filtering approach yields encouraging results for retinal vessel modeling improvement. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - A computational approach of data smoothening and prediction of diabetes dataset
A1  - Jakhmola, S
A1  - Pradhan, T
ED  - Mitra S. Bedi P., McIntosh S M S R Nair I
Y1  - 2015///
KW  -  Computational approach; Data points; Data preprocessing; Information loss; Multiple regressions; Noise removal; Real-world; Smoothening
KW  -  Medical computing
KW  - Correlation methods; Data mining; Diagnosis; Forecasting; Information science
PB  - Association for Computing Machinery
JF  - ACM International Conference Proceeding Series
VL  - 10-13-August-2015
SP  - 744
EP  - 748
SN  - 9781450333610
DO  - 10.1145/2791405.2791572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960955056&doi=10.1145%2F2791405.2791572&partnerID=40&md5=8ed96d973eb5039753f64f45b2964d0f
N1  - cited By 3
N2  - Data mining when applied on medical diagnosis can help doctors to take major decisions. Diabetes is a disease which has to be monitored by the patient so as not to cause severe damage to the body. Therefore to predict diabetes is an important task that is most important for the patient. In this study, a new data smoothening technique is proposed for noise removal from the data. It is very important for the user to have control over the smoothening of the data so that the information loss can be monitored. The proposed method allows the user to control the level of data smoothening by accepting the loss percentage on the individual data points. Allowable loss is calculated and a decision is made to smoothen the value or retain it to the level which is accurate. The proposed method will enable the user to get the output based on his requirements of preprocessing. The proposed algorithm will allow the user to interact with the data preprocessing system unlike the primitive algorithms. Different levels of smoothened output are obtained by different loss percentage. This preprocessed output produced will be of a better quality and will resemble more to the real world data. Furthermore, correlation and multiple regression is applied on the preprocessed diabetes dataset and a prediction is made on this basis. © 2015 ACM.
ER  - 
TY  - CONF
T1  - Segmentation of Finger Nails Image based on Image Processing methods
A1  - Kurniastuti, I
A1  - Wulan, T D
ED  - Kuniadi D. Hendrick H., Heriyanto R Yuli Y Yaldi G Dwiharyadi A Siskawati E
Y1  - 2020///
KW  -  Finger nail; Finger nail image; Image processing - methods; Images processing; K-mean region growing; K-means; Nail image; Pre-processing step; Region growing; Segmentation images
KW  -  Median filters
KW  - Edge detection; Image enhancement; Image segmentation; Processing
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 3rd International Conference on Applied Science and Technology, iCAST 2020
SP  - 695
EP  - 699
SN  - 9781728195674
DO  - 10.1109/iCAST51016.2020.9557685
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125796065&doi=10.1109%2FiCAST51016.2020.9557685&partnerID=40&md5=a7fa60a52e0bc528db6daf7f93a30797
N1  - cited By 0
N2  - This research has aim to segmentation of finger nails image using image processing methods and k-means region growing. Finger nails image could be used as early detection of diabetes mellitus. The steps in research are preprocessing step and segmentation step. Preprocessing step consist of image grayscale conversion, median filter, edge detection sobel operator and dilation. The aim of preprocessing step is to enhancement image before segmentation process is applied in image. Therefore, segmentation step using k-means region growing approach. The result show that accuracy rate of methods is 54.67%. In the next research, segmentation of finger nails image could use other methods that show better result. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - A rule based expert system for microaneurysm detection in digital fundus images
A1  - Manjaramkar, A
A1  - Kokare, M
Y1  - 2016///
KW  -  Clinical procedure; Diabetic retinopathy; Digital fundus images; Evaluation protocol; Image preprocessing; Microaneurysms; Red lesions; Rule based expert systems
KW  -  Eye protection
KW  - Expert systems
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2016 International Conference on Computational Techniques in Information and Communication Technologies, ICCTICT 2016 - Proceedings
SP  - 137
EP  - 140
SN  - 9781509000821
DO  - 10.1109/ICCTICT.2016.7514567
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980325632&doi=10.1109%2FICCTICT.2016.7514567&partnerID=40&md5=c101931e9472f255b125cbf5a337261c
N1  - cited By 7
N2  - Inefficiency of human body to produce and consume insulin leads to diabetes. This diabetes over a period of time starts showing adverse effects on different organs. If it affects eye it is termed as diabetic retinopathy (DR). Earlier researchers have put forth many automated microaneurysms detection systems, but none of them can replace clinical procedure. Here, we propose an rule based system for microaneurysms detection in digital fundus images. The proposed system is a three step algorithm. Initially image pre-processing is done, secondly candidate microaneurysms are segmented, thirdly features are extracted from these candidates and true microaneurysms are recognized using rule based expert system. The system performance is evaluated on publicly available database DIARETDB1 which consists of evaluation protocol and ground truth collected from experts. Overall Sensitivity of 80.6% at 97.50% specificity with accuracy of 95.95% is achieved by proposed method. © 2016 IEEE.
ER  - 
TY  - JOUR
T1  - Improving diagnosis of diabetes mellitus using combination of preprocessing techniques
A1  - Asgarnezhad, R
A1  - Shekofteh, M
A1  - Boroujeni, F Z
Y1  - 2017///
PB  - Asian Research Publishing Network
JF  - Journal of Theoretical and Applied Information Technology
VL  - 95
IS  - 13
SP  - 2889
EP  - 2895
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024390270&partnerID=40&md5=e919cfc5b0a9edd4c693581aa39e0c24
N1  - cited By 4
N2  - Diabetes mellitus is one of the most common diseases among people of all age groups, affecting children, adolescents and young adults. There is an increasing interest in using machine learning techniques to diagnose these chronic diseases. However, the poor quality of most medical data sets inhibits construction of efficient models for prediction of diabetes mellitus. Without efficient preprocessing methods, dealing with these kinds of data sets leads to unreliable results. This paper presents an efficient preprocessing technique including a combination of missing value replacement and attribute subset selection methods on a well-known diabetes mellitus data set. The results show that the proposed technique can improve the performance of applied classifier and outperforms the traditional methods in terms of accuracy and precision in diabetes mellitus prediction. © 2005 – ongoing JATIT & LLS.
ER  - 
TY  - CONF
T1  - Mining biomedical data from mexrisc
A1  - Somodevilla, M J
A1  - Mendez, M
A1  - Celis, C P D
A1  - Pineda, I H
Y1  - 2013///
KW  -  Biomedical data; Classification process; Clustering; Number of datum; Support systems
KW  -  Computer science
KW  - Artificial intelligence; Classification (of information); Data mining; Decision support systems; Medical problems
PB  - IEEE Computer Society
JF  - Proceedings - 2013 Mexican International Conference on Computer Science, ENC 2013
SP  - 36
EP  - 40
SN  - 9780769550879
DO  - 10.1109/ENC.2013.11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893469672&doi=10.1109%2FENC.2013.11&partnerID=40&md5=1d135e3ce8d6c44ffbc680eb2b477f60
N1  - cited By 0
N2  - In this paper, it is presented a support system for decision making to diabetes prevention. Using MexRisc platform, a large number of data is collected, and then preprocessing data techniques are applied in order to do a classification process. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Improving the classification accuracy using recursive feature elimination with cross-validation
A1  - Misra, P
A1  - Yadav, A S
Y1  - 2020///
PB  - Research Trend
JF  - International Journal on Emerging Technologies
VL  - 11
IS  - 3
SP  - 659
EP  - 665
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086443367&partnerID=40&md5=427f9c058442abba070ff603707e4ab2
N1  - cited By 14
N2  - In Machine Learning (ML) community, researchers are proposing complex models for real-life problems to achieve higher accuracy, which requires high computing and other resources. Fields like computer vision and NLP have given rise to deep learning with complex and high computational models setting the trend to apply them in almost all the fields While they help where we have an abundance of data and complex relationships, simpler models still can do wonders and on their day can challenge these behemoths. Here feature selection plays an important role and drastically improves the model accuracy. We have proposed the Recursive Feature Elimination with Cross-Validation (RFECV) approach for Type-II diabetes prediction to improve the classification accuracy. The major challenge with this approach is to deal with overfitting issue and improve the accuracy without unnecessary record deletion. We have applied other pre-processing methods and then have applied five different classical ML algorithms Logistic regression, Artificial Neural Networks, Naïve Bayes, Support Vector Machine, and Decision Tree (DT) to predict diabetes onset. LR provided the best accuracy (84%), and the rest of the models remains very close to each other. © 2020, Research Trend. All rights reserved.
ER  - 
TY  - JOUR
T1  - A Novel Algorithm for Hyperspectral Image Denoising in Medical Application
A1  - Nageswaran, K
A1  - Nagarajan, K
A1  - Bandiya, R
Y1  - 2019///
KW  -  Algorithms; Color; Humans; Image Processing
KW  -  Computer-Assisted; Linear Models; Phantoms
KW  -  Imaging; Principal Component Analysis; Signal-To-Noise Ratio; Wavelet Analysis
KW  - algorithm; Article; decomposition; diabetes mellitus; multidimensional wavelet transform; multiple linear regression analysis; noise reduction; retinopathy; skin allergy; wavelet transformation; color; human; image processing; imaging phantom; principal component analysis; procedures; signal noise ratio; statistical model; wavelet analysis
PB  - Springer New York LLC
JF  - Journal of Medical Systems
VL  - 43
IS  - 9
DO  - 10.1007/s10916-019-1403-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069687954&doi=10.1007%2Fs10916-019-1403-5&partnerID=40&md5=cb7838adf5393d1b8985faf498ebdae5
N1  - cited By 4
N2  - The one of the preprocessing step for hyperspectral imagery is noise reduction. The images are received by the detector and this can be degraded by several factors like atmospherical things and device noises which emit temperature noise, processing noise and explosion noise. There are several strategies are developed already to cut back the signal to noise magnitude relation of the hyperspectral image. However, the stationary noise of the many denoising ways developed cannot be applied on to the gauge boson noise. Thus, the each gauge boson and thermal noise square measure gift within the captured hyperspectral image (HSI). during this paper, we tend to projected a replacement denoising framework known as tensor-based filtering employing a PARAFAC tensor decomposition methodology for scale back each noise. The proposed technique is performs higher in removing noise as compared with different strategies like Multiple linear regression (MLR) algorithm and combined algorithm called multidimensional wavelet transforms with multiway wiener filter (MWPT-MWF) technique. The performance analysis of the new denoising framework has more efficient for reducing signal dependent (PN) and signal independent noise (TN) as compared with other conventional method. Hence this novel denoising approach would be more beneficial for detection of skin allergy and also this algorithm will be very useful for detection of retinal exudates and diagnosis of diabetes mellitus and retinopathy disease in medical application. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Auto-MeDiSine: an auto-tunable medical decision support engine using an automated class outlier detection method and AutoMLP
A1  - Jahangir, M
A1  - Afzal, H
A1  - Ahmed, M
A1  - Khurshid, K
A1  - Amjad, M F
A1  - Nawaz, R
A1  - Abbas, H
Y1  - 2020///
KW  -  Data analysis techniques; Distance based algorithm; Medical decision supports; Pima Indian Diabetes; Pre-processing method; State-of-the-art methods; Tuning of parameters; World Health Organization
KW  -  Learning systems
KW  - Anomaly detection; Artificial intelligence; Classification (of information); Data handling; Decision support systems; Diagnosis; Forecasting; Multilayer neural networks; Multilayers; Statistics
PB  - Springer
JF  - Neural Computing and Applications
VL  - 32
IS  - 7
SP  - 2621
EP  - 2633
DO  - 10.1007/s00521-019-04137-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064447553&doi=10.1007%2Fs00521-019-04137-5&partnerID=40&md5=2911223da4704fb56eb6319897e3461d
N1  - cited By 6
N2  - With advanced data analysis techniques, efforts for more accurate decision support systems for disease prediction are on the rise. According to the World Health Organization, diabetes-related illnesses and mortalities are on the rise. Hence, early diagnosis is particularly important. In this paper, we present a framework, Auto-MeDiSine, that comprises an automated version of enhanced class outlier detection using a distance-based algorithm (AutoECODB), combined with an ensemble of automatic multilayer perceptron (AutoMLP). AutoECODB is built upon ECODB by automating the tuning of parameters to optimize outlier detection process. AutoECODB cleanses the dataset by removing outliers. Preprocessed dataset is then used to train a prediction model using an ensemble of AutoMLPs. A set of experiments is performed on publicly available Pima Indian Diabetes Dataset as follows: (1) Auto-MeDiSine is compared with other state-of-the-art methods reported in the literature where Auto-MeDiSine realized an accuracy of 88.7%; (2) AutoMLP is compared with other learners including individual (focusing on neural network-based learners) and ensemble learners; and (3) AutoECODB is compared with other preprocessing methods. Furthermore, in order to validate the generality of the framework, Auto-MeDiSine is tested on another publicly available BioStat Diabetes Dataset where it outperforms the existing reported results, reaching an accuracy of 97.1%. © 2019, Springer-Verlag London Ltd., part of Springer Nature.
ER  - 
TY  - CONF
T1  - Computerized information system using stacked generalization for diagnosis of diabetes mellitus
A1  - Vijayan, V V
A1  - Anjali, C
Y1  - 2016///
KW  -  AdaBoost algorithm; Decision stumps; Naive Bayes classifiers; Preprocessing; Stacked generalization
KW  -  Data mining
KW  - Adaptive boosting; Artificial intelligence; Classifiers; Decision support systems; Decision trees; Diagnosis; Information systems; Information use; Support vector machines; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2015 IEEE Recent Advances in Intelligent Computational Systems, RAICS 2015
SP  - 173
EP  - 178
SN  - 9781467366700
DO  - 10.1109/RAICS.2015.7488409
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978953954&doi=10.1109%2FRAICS.2015.7488409&partnerID=40&md5=6548ac4bb7c5d2db081a5dfd858e89c0
N1  - cited By 1
N2  - Diabetes mellitus is considered to be a severe health issue which is caused due to the presence of higher amount of plasma/glucose in the blood. A number of decision support systems were introduced to help medical experts for analyzing different factors that cause diabetes. Here a computerized information system is designed using Stacked Generalization for predicting diabetes. The classifiers under consideration are Decision Stump, Decision Tree, Naive Bayes and Support Vector Machine. The selection of Stacked Generalization is done after conducting a detail performance evaluation of individual classifiers and AdaBoost algorithm. The level of accuracy was varied from a lower value of 75 % to a higher value of 82 % by using Stacking algorithm. © 2015 IEEE.
ER  - 
TY  - CONF
T1  - Enhancement in the Identification of Slough Tissue in Chronic Wound Assessment
A1  - Nizam, K
A1  - Fauzi, M F A
A1  - Ahmad, N N
A1  - Logeswaran, R
A1  - Nair, H K R
Y1  - 2019///
KW  -  Chronic wounds; Contrast Enhancement; Diagnosis methods; Digital image; Fully automated; Histogram equalizations; Middle-income countries; Reference points
KW  -  Tissue
KW  - Cell death; Granulation; Image analysis; Image enhancement; Plants (botany); Tissue engineering
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the 2019 IEEE International Conference on Signal and Image Processing Applications, ICSIPA 2019
SP  - 154
EP  - 158
SN  - 9781728133775
DO  - 10.1109/ICSIPA45851.2019.8977785
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084731413&doi=10.1109%2FICSIPA45851.2019.8977785&partnerID=40&md5=a21ddf5ec5381f28d1d2ee2caba4688d
N1  - cited By 0
N2  - The prevalence of chronic ulcer wound is steadily increasing not only in predominantly rich nations but most markedly, in the world's middle-income countries. In addition to the dire consequences on the health and well-being of a patient, diabetes and its complications impact harshly on the finances of individuals and their families, and the economies of nations. The current diagnosis methods utilized by the diagnosticians are expert-oriented, vision-dependant, time-consuming, have interob-server variations and cause discomfort to the patient. Therefore, in an effort to improve capacity for diagnosis, a fully-automated wound tissue characterization system has been offered that would analyze the digital images of chronic wounds to identify the tissue types namely, granulation, slough, necrosis, and epithelial. In our previous research, the three tissue types (granulation, necrosis, and epithelial) were identified with higher accuracy in 301 images. In this paper, the slough identification has been enhanced by adding reference points and contrast enhancement to evaluate which method demonstrates better experimental results. Quantitative analysis of the results proves that preprocessing the images with Adaptable Histogram Equalization technique achieved the highest accuracy of 94.0% for the slough tissue. © 2019 IEEE.
ER  - 
TY  - JOUR
T1  - Raman spectroscopy as a promising tool for noninvasive point-of-care glucose monitoring
A1  - Scholtes-Timmerman, M J
A1  - Bijlsma, S
A1  - Fokkert, M J
A1  - Slingerland, R
A1  - Van Veen, S J F
Y1  - 2014///
KW  -  80 and over; Blood Glucose; Blood Glucose Self-Monitoring; Female; Humans; Male; Middle Aged; Monitoring
KW  -  Adult; Aged; Aged
KW  -  Physiologic; Point-of-Care Systems; Spectrum Analysis
KW  -  Raman
KW  -  adult; aged; Article; blood glucose monitoring; cohort analysis; female; gender; glucose blood level; human; human tissue; major clinical study; male; non invasive procedure; point of care testing; Raman spectrometry; analysis; blood glucose monitoring; devices; hospital information system; middle aged; physiologic monitoring; procedures; Raman spectrometry; very elderly
KW  - glucose; glucose blood level
PB  - SAGE Publications Inc.
JF  - Journal of Diabetes Science and Technology
VL  - 8
IS  - 5
SP  - 974
EP  - 979
DO  - 10.1177/1932296814543104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930742774&doi=10.1177%2F1932296814543104&partnerID=40&md5=ea1bc18acaf51a41448bbfdd7e9c78a5
N1  - cited By 25
N2  - Background: Self-monitoring of glucose is important for managing diabetes. Noninvasive glucose monitors are not yet available, but patients would benefit highly from such a device. Methods: We present results that may lead to a novel, point-of-care noninvasive system to measure blood glucose based on Raman spectroscopy. A hospitalized cohort of 111 subjects was measured using a custom-made Raman spectrometer system. Blood glucose reference samples were used to correlate Raman data to glucose levels, using advanced preprocessing and analysis algorithms. Results: A correlation coefficient (R2) of .83 was found correlating independent Raman-based predictions on reference blood glucose for the full cohort. Stratification of the cohort in gender-specific groups raised correlation levels to .88 (females) and .94 (males). Glucose could be measured noninvasively with average errors as low as 0.9 mM. Conclusion: We conclude that this novel system shows promising results for the advance of noninvasive, point-of-care glucose monitoring. © 2014 Diabetes Technology Society.
ER  - 
TY  - CONF
T1  - A Comparative Study with Different Machine Learning Algorithms for Diabetes Disease Prediction
A1  - Kibria, H B
A1  - Matin, A
A1  - Jahan, N
A1  - Islam, S
Y1  - 2021///
KW  -  Blood glucose; Comparatives studies; Diagnose disease; Extract informations; Human bodies; Hyperglycaemia; K near neighbor; Machine learning algorithms; Nearest-neighbour; Support vectors machine
KW  -  Support vector machines
KW  - Diagnosis; Glucose; Learning algorithms; Logistic regression; Motion compensation; Nearest neighbor search
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - CCE 2021 - 2021 18th International Conference on Electrical Engineering, Computing Science and Automatic Control
SN  - 9781665400299
DO  - 10.1109/CCE53527.2021.9633043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123852802&doi=10.1109%2FCCE53527.2021.9633043&partnerID=40&md5=71bb98f328cc5d01e331d72befeccd16
N1  - cited By 3
N2  - Diabetic is a disease that occurred when the level of blood glucose is higher than usual, which is also known as hyperglycemia. When the human body is incapable of producing enough insulin(a hormone that produces glucose from food), then this situation leads to diabetes. The rapid increase of this disease makes the researchers work much harder in this area to build a model for diagnosing diabetes efficiently. As in healthcare, the availability of data is high, so it is easy to extract information from those data to diagnose disease and develop a new model for better results. This paper aims to introduce a model that can predict diabetes efficiently with the help of machine learning algorithms. Here logistic regression, SVM, and k nearest neighbor algorithms have been used for the classification of diabetics. After data preprocessing and training, those algorithms gave a good result. Logistic regression provided the best accuracy of 83% for test data. Also, SVM and knn both performed well and showed an accuracy of 82% and 79%, respectively. The proposed model has demonstrated improved results compared with previous work. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - A Novel Approachfor the Diagnosis of Diabetic Retinopathy using Convolutional Neural Network
A1  - Hema Rajini, N
Y1  - 2019///
KW  -  Convolutional neural network; Diabetic retinopathy; Early diagnosis; High-precision; Image processing and computer vision; Intelligent diagnosis system; Preprocessing; Vision problems
KW  -  Diagnosis
KW  - Convolution; Eye protection; Feature extraction; Image processing; Neural networks
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2019 5th International Conference on Advanced Computing and Communication Systems, ICACCS 2019
SP  - 1102
EP  - 1107
SN  - 9781538695333
DO  - 10.1109/ICACCS.2019.8728506
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067983256&doi=10.1109%2FICACCS.2019.8728506&partnerID=40&md5=06e24cefa16cdbee1c5c35197e1a5edd
N1  - cited By 2
N2  - Diabetic Retinopathy (DR) is the influence of diabetes on the eye. It is the number one eye disease that causes blindness. Patients with diabetes are more likely to get influenced under such disease as they grow older. Early indications of DR are the appearance of microaneurysms, hemorrhages and exudates. There are no early indications for Diabetic Retinopathy as patients do not suffer from vision problems until the late stages of the disease when treatments could be inefficient. Therefore, an early diagnosis of DR is important and preventing the disease from developing. With the use of intelligent diagnosis systems, image processing and computer vision systems doctors can detect DR at the initial stage. Such determination is explicitly done by the characterization of a convolutional neural system which gives high precision in grouping of these diseases through spatial examination. © 2019 IEEE.
ER  - 
TY  - JOUR
T1  - Accurate Detection of Non-Proliferative Diabetic Retinopathy in Optical Coherence Tomography Images Using Convolutional Neural Networks
A1  - Ghazal, M
A1  - Ali, S S
A1  - Mahmoud, A H
A1  - Shalaby, A M
A1  - El-Baz, A
Y1  - 2020///
KW  -  CAD system; Computer aided diagnostics; Diabetic retinopathy; Imaging modality; System-setup
KW  -  Convolutional neural networks
KW  - Computer aided diagnosis; Convolution; Deep learning; Eye protection; Learning systems; Medical imaging; Optical tomography; Tomography; Transfer learning
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Access
VL  - 8
SP  - 34387
EP  - 34397
DO  - 10.1109/ACCESS.2020.2974158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080903339&doi=10.1109%2FACCESS.2020.2974158&partnerID=40&md5=84c180ed903f8b2c950e892574c4d539
N1  - cited By 30
N2  - Diabetic retinopathy (DR) is a disease that forms as a complication of diabetes. It is particularly dangerous since it often goes unnoticed and can lead to blindness if not detected early. Despite the clear importance and urgency of such an illness, there is no precise system for the early detection of DR so far. Fortunately, such system could be achieved using deep learning including convolutional neural networks (CNNs), which gained momentum in the field of medical imaging due to its capability of being effectively integrated into various systems in a manner that significantly improves the performance. This paper proposes a computer aided diagnostic (CAD) system for the early detection of non-proliferative DR (NPDR) using CNNs. The proposed system is developed for the optical coherence tomography (OCT) imaging modality. Throughout this paper, all aspects of deployment of the proposed system are studied starting from the preprocessing stage required to extract input retina patches to train the CNN without resizing the image, to the use of transfer learning principals and how to effectively combine features in order to optimize performance. This is done through investigating several scenarios for the system setup and then selecting the best one, which from the results revealed to be a two pre-trained CNNs based system, in which one of these CNNs is independently fed by nasal retina patches and the other one by temporal retina patches. The proposed transfer learning based CAD system achieves a promising accuracy of 94%. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Non-invasive Monitoring of Three Glucose Ranges Based on ECG by Using DBSCAN-CNN
A1  - Li, J
A1  - Tobore, I
A1  - Liu, Y
A1  - Kandwal, A
A1  - Wang, L
A1  - Nie, Z
Y1  - 2021///
KW  -  Adult; Blood Glucose; Blood Glucose Self-Monitoring; Electrocardiography; Glucose; Glucose Tolerance Test; Humans
KW  -  Autonomic nervous system; Blood glucose; Convolution neural network; Density based spatial clustering of applications with noise; ECG preprocessing; Non-invasive monitoring; Oral glucose tolerance tests; Visualization results
KW  -  Electrocardiography
KW  -  adrenergic system; adult; Article; autonomic nervous system; blood glucose monitoring; cholinergic system; clinical article; controlled study; convolutional neural network; diabetes mellitus; diabetes screening; diagnostic test accuracy study; electrocardiogram; glucose blood level; heart beat; homeostasis; human; human experiment; impaired glucose tolerance; mass screening; nerve cell network; oral glucose tolerance test; QRS interval; QT interval; sensitivity and specificity; blood glucose monitoring; electrocardiography; glucose blood level; glucose tolerance test
KW  -  glucose
KW  - Glucose
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Journal of Biomedical and Health Informatics
VL  - 25
IS  - 9
SP  - 3340
EP  - 3350
DO  - 10.1109/JBHI.2021.3072628
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104267008&doi=10.1109%2FJBHI.2021.3072628&partnerID=40&md5=48287591bafeaea03152269d5e51c180
N1  - cited By 11
N2  - Autonomic nervous system (ANS) can maintain homeostasis through the coordination of different organs including heart. The change of blood glucose (BG) level can stimulate the ANS, which will lead to the variation of Electrocardiogram (ECG). Considering that the monitoring of different BG ranges is significant for diabetes care, in this paper, an ECG-based technique was proposed to achieve non-invasive monitoring with three BG ranges: low glucose level, moderate glucose level, and high glucose level. For this purpose, multiple experiments that included fasting tests and oral glucose tolerance tests were conducted, and the ECG signals from 21 adults were recorded continuously. Furthermore, an approach of fusing density-based spatial clustering of applications with noise and convolution neural networks (DBSCAN-CNN) was presented for ECG preprocessing of outliers and classification of BG ranges based ECG. Also, ECG's important information, which was related to different BG ranges, was graphically visualized. The result showed that the percentages of accurate classification were 87.94% in low glucose level, 69.36% in moderate glucose level, and 86.39% in high glucose level. Moreover, the visualization results revealed that the highlights of ECG for the different BG ranges were different. In addition, the sensitivity of prediabetes/diabetes screening based on ECG was up to 98.48%, and the specificity was 76.75%. Therefore, we conclude that the proposed approach for BG range monitoring and prediabetes/diabetes screening has potentials in practical applications. © 2013 IEEE.
ER  - 
TY  - CONF
T1  - Detection of the Emergence of Exudate on the Image of Retina Using Extreme Learning Machine Method
A1  - Anggraeni, Z
A1  - Wibawa, H A
Y1  - 2019///
KW  -  Activation functions; Contrast Limited Adaptive Histogram Equalization (CLAHE); Diabetic retinopathy; Dilation and erosions; Extreme learning machine; Exudate detections; Gray level co-occurrence matrix; Retinal blood vessels
KW  -  Eye protection
KW  - Blood vessels; Discrete wavelet transforms; Erosion; Knowledge acquisition; Machine learning; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - ICICOS 2019 - 3rd International Conference on Informatics and Computational Sciences: Accelerating Informatics and Computational Research for Smarter Society in The Era of Industry 4.0, Proceedings
SN  - 9781728146102
DO  - 10.1109/ICICoS48119.2019.8982492
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081100143&doi=10.1109%2FICICoS48119.2019.8982492&partnerID=40&md5=cb826d9679efdb8ce281050e9786dcbb
N1  - cited By 1
N2  - Diabetic retinopathy is a health problem that cause damage to the retinal blood vessels and occurs in more than half of people who suffer from diabetes. It is estimated that around 28 million people experience loss of sight for this reason. Thus, the system for detecting early signs of diabetic retinopathy will be very helpful and one of first signs of the onset of symptoms of diabetic retinopathy is the appearance of exudates in the retinal image of the eye. To build an exudate emergence detection system, in this study use the method of extreme learning machine (ELM) which has a fast learning speed. This system uses the gray level co-occurrence matrix feature extraction with 6 features, namely contrast, homogeneity, correlation, ASM, energy and dissimilarity. To get the best model, six scenarios are used by distinguishing the preprocessing flow. The pre processing stage carried out by all scenarios is optic disc removal, green channel separation, contrast limited adaptive histogram equalization (CLAHE) followed by two different preprocessing lines, namely applying brightness and dilation and erosion operations. Then the second path is radon transform, top-hat filtering, discrete wavelet transform and dilation and erosion. The best model results reached the best accuracy value of 65% with a combination of multiquadric activation functions and 30 hidden neurons. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - Design, development and implementation of semi-automated CINE MR images segmentation pipeline using feature extraction and active contours
A1  - Garcia-Gomez, V J
A1  - Alberich-Bayarri, A
A1  - Sanz-Requena, R
A1  - Garcia, A M
A1  - Marti-Bonmati, L
A1  - Naranjo, V
Y1  - 2014///
KW  -  Coronary heart disease; Developed countries; High blood pressures; Image analysis software; Image preprocessing; Left ventricular myocardiums; Segmentation methods; Segmentation system
KW  -  Image segmentation
KW  - Blood pressure; Cardiology; Diseases; Feature extraction; Heart; Hough transforms; Magnetic resonance imaging; Pipelines
PB  - IEEE Computer Society
JF  - 2014 IEEE-EMBS International Conference on Biomedical and Health Informatics, BHI 2014
SP  - 161
EP  - 164
SN  - 9781479921317
DO  - 10.1109/BHI.2014.6864329
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906851389&doi=10.1109%2FBHI.2014.6864329&partnerID=40&md5=c0b06541db9f3c6da6f5f17d93ddb797
N1  - cited By 1
N2  - Coronary heart disease is a major cause of death in developed countries. There are some factors like cholesterol, high blood pressure, diabetes or obesity that increase the risk of having coronary heart disease. Early detection of the disease may have a better treatment response. Cardiac imaging methods require new processing analysis approaches. In this paper, new segmentation system has been studied in order to develop a semi-automated methodology for left ventricular myocardium segmentation from cardiac MR CINE images based on figure detection and active contours. Different image preprocessing techniques like the Hough Transform, centroid detection and snakes were integrated in a pipeline that permits the analysis and generation of structured reports with imaging biomarkers of the cardiac function in different cardiomyopathies. To implement the new segmentation method, a new segmentation and image analysis software has been created. Cardiac reports are made automatically with this tool using bull eyes and biomarkers like ejection fraction (EF), myocardial mass, cardiac output or thickening. © 2014 IEEE.
ER  - 
TY  - CONF
T1  - Diabetic retinopathy classification using downscaling algorithms and deep learning
A1  - Doshi, N
A1  - Oza, U
A1  - Kumar, P
Y1  - 2020///
KW  -  Art and science; Diabetic patient; Diabetic retinopathy; Learning network; Preprocessing phase; Retinal fundus images; State-of-the-art methods; Training and testing
KW  -  Deep learning
KW  - Eye protection; Image classification; Image enhancement; Learning algorithms; Ophthalmology; Statistical tests
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 7th International Conference on Signal Processing and Integrated Networks, SPIN 2020
SP  - 950
EP  - 955
SN  - 9781728154756
DO  - 10.1109/SPIN48934.2020.9071423
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084282809&doi=10.1109%2FSPIN48934.2020.9071423&partnerID=40&md5=e6f8a4e371ccd6d7164f48d9193803d4
N1  - cited By 5
N2  - Diabetic Retinopathy (DR) is an art and science of recording and classifying the retinal images of a diabetic patient. DR classification deals with classifying retinal fundus image into five stages on the basis of severity of diabetes. One of the major issue faced while dealing with DR classification problem is the large and varying size of images. In this paper we propose and explore the use of several downscaling algorithms before feeding the image data to a Deep Learning Network for classification. For improving training and testing; we amalgamate two datasets: Kaggle and Indian Diabetic Retinopathy Image Dataset. Our experiments have been performed on a novel Multi Channel Inception V3 architecture with a unique self crafted preprocessing phase. We report results of proposed approach using accuracy, specificity and sensitivity, which outperform the previous state of the art methods. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - A widespread study of diabetes prediction using several machine learning techniques
A1  - Saha, P K
A1  - Patwary, N S
A1  - Ahmed, I
Y1  - 2019///
KW  -  Common disease; Comprehensive research; Data classification; Glucose level; Machine learning techniques; Neural network (nn); Preprocessing techniques; Sugar levels
KW  -  Learning systems
KW  - Decision trees; Neural networks; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2019 22nd International Conference on Computer and Information Technology, ICCIT 2019
SN  - 9781728158426
DO  - 10.1109/ICCIT48885.2019.9038559
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082991762&doi=10.1109%2FICCIT48885.2019.9038559&partnerID=40&md5=b12038ad8c6a9d536d171e173a9543ec
N1  - cited By 10
N2  - Diabetes is one of the most common diseases that can affect anyone at any ages. These diseases attacked when the glucose level or sugar level is increased. Predicting diabetes is one of the most important things at this moment. There are some several techniques applied on Indian Pima Dataset. The dataset studying on woman Pima Indian population which had started in 1965. Most of the researcher trying to apply some complex techniques on dataset while many comprehensive research has many common technique missing.In our study, we have applied some very popular techniques such as Neural Network(NN), Support Vector Machine(SVM), Random Forest(RF), etc. We have applied those methods in several ways. Firstly, we have applied several algorithms in the original dataset. Then we used several preprocessing techniques to identify diabetes. Finally, we applied those techniques to compare and get the best accuracy. Neural Network was given the best accuracy(80.4%) than any other techniques. © 2019 IEEE.
ER  - 
TY  - JOUR
T1  - Towards noninvasive and fast detection of Glycated hemoglobin levels based on ECG using convolutional neural networks with multisegments fusion and Varied-weight
A1  - Li, J
A1  - Lu, J
A1  - Tobore, I
A1  - Liu, Y
A1  - Kandwal, A
A1  - Wang, L
A1  - Zhou, J
A1  - Nie, Z
Y1  - 2021///
KW  -  Auto correlation; Convolutional neural network; Fast detections; Glycated hemoglobin a1c; Glycated hemoglobins; Gold standards; Haemoglobins; Hemoglobin levels; Multi-segment; Non-invasive detection
KW  -  Electrocardiography
KW  - Autocorrelation; Chemical detection; Convolution; Feature extraction; Hemoglobin; Long short-term memory
PB  - Elsevier Ltd
JF  - Expert Systems with Applications
VL  - 186
DO  - 10.1016/j.eswa.2021.115846
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114695116&doi=10.1016%2Fj.eswa.2021.115846&partnerID=40&md5=e01709a0b17fe6cd8819ab977ebadf25
N1  - cited By 0
N2  - Glycated hemoglobin A1c (HbA1c) is regarded as a gold standard to evaluate long-term blood glucose control, and it is also a crucial metric in diabetes screening, diagnosis, and management. However, thus far, the HbA1c measurement methods are invasive and painful. Considering that HbA1c levels are associated with cardiovascular autonomic neuropathy, in this paper, a novel Electrocardiogram (ECG)-based approach was presented for noninvasive and fast detection of HbA1c levels using 60-second, single-lead ECG waveform. For this purpose, a total of 317,105 ECG datasets encompassing 370 patients with diabetes were obtained using wearable devices. Furthermore, the ECG preprocessing was based on autocorrelation analysis. The convolutional neural networks with multisegment fusion and varied-weight (CNN-MFVW) were proposed to achieve ECG feature extraction and HbA1c detection. The results showed that the average accuracy, precision, recall, and F1-score of the proposed algorithm were 0.9015, 0.9051, 0.8991 and 0.9013 respectively. Moreover, the area under the curve (AUC) was up to 0.9899, which was higher than other algorithms of conventional CNN and CNN-LSTM. Therefore, we conclude that the proposed approach for noninvasive and fast detection of HbA1c levels has potentials in practical applications. © 2021 Elsevier Ltd
ER  - 
TY  - JOUR
T1  - Measurements and quality assessments of near-infrared plasma glucose spectra in the combination band region using a scanning filter spectrometer
A1  - Saptari, V
A1  - Youcef-Toumi, K
Y1  - 2005///
KW  -  Algorithms; Blood Glucose; Equipment Design; Equipment Failure Analysis; Humans; Quality Control; Reproducibility of Results; Sensitivity and Specificity; Spectrophotometry
KW  -  Biomedical engineering
KW  -  Diabetes; Near-infrared measurements; Plasma glucose spectra; Scanning filter spectrometer
KW  -  Infrared
KW  -  algorithm; article; equipment; equipment design; evaluation; glucose blood level; human; infrared spectrophotometry; instrumentation; methodology; quality control; reproducibility; sensitivity and specificity
KW  - Algorithms; Glucose; Infrared spectroscopy; Spectrometers; Spectrum analysis
JF  - Journal of Biomedical Optics
VL  - 10
IS  - 6
DO  - 10.1117/1.2141934
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645241128&doi=10.1117%2F1.2141934&partnerID=40&md5=809bc3371b64608e9bde3343810a276f
N1  - cited By 2
N2  - Near-infrared measurements of glucose in human plasma are performed using a custom, rapid, high-throughput filter-based spectrometer covering a spectral range between 2080 and 2315 nm. Quality of the measured glucose signals is quantified through the use of two figures of merit: selectivity and limit of detection. Selectivity measures the uniqueness of the glucose spectrum from among the interfering spectra. Limit of detection measures the smallest glucose concentration change detectable. The proposed system, which includes the spectroscopic hardware and a spectral preprocessing algorithm, is shown to produce a selectivity value of 0.57, with zero being nonselective and one being fully selective, and a limit of detection value of 2.2 mM. Prediction of an independent dataset is also performed using net analyte signal-based and partial least-squares multi-variate calibration techniques, which produce standard error of prediction values of 1.14 and 1.45 mM, respectively. © 2005 Society of Photo-Optical Instrumentation Engineers.
ER  - 
TY  - CONF
T1  - Application of interactive multivariate data visualization to the analysis of patients findings in metabolic research
A1  - Koehler, Carsta
A1  - Koenig, Andreas
A1  - Temelkova-Kurktschiev, Theodora
A1  - Hanefeld, Markolf
Y1  - 1999///
KW  -  Medical computing
KW  -  Multivariate data visualization
KW  - Database systems; Feature extraction; Metabolism; Visualization
PB  - IEEE, Piscataway, NJ, United States
JF  - International Conference on Knowledge-Based Intelligent Electronic Systems, Proceedings, KES
SP  - 397
EP  - 402
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033350568&partnerID=40&md5=ff827372b8a5faffa8f20ffaaf560010
N1  - cited By 2
N2  - In this paper, methods for multivariate data projection, resulting feature map display, and interactive visual analysis are applied in an interdisciplinary cooperation for the analysis of patients findings in metabolic research. From a medical database, collected in corresponding research, data sets were subject to preprocessing, projection, and visual analysis. Globally significant parameters could be identified by this work, that coincide with certain patient groups, e.g. characterized by the attributes sex and diabetes. Further, the visualization techniques opened a way to find local correlations and interdependencies between parameters within subgroups. The medical analysis is supported by a recent dedicated PC-based tool that allows the processing and interactive visualization of the medical databases by means of novel features and techniques.
ER  - 
TY  - JOUR
T1  - Diabetes Mellitus Prediction and Severity Level Estimation Using OWDANN Algorithm
A1  - Annamalai, R
A1  - Nedunchelian, R
Y1  - 2021///
KW  -  Diabetes mellitus; Early diagnosis; F measure; Pre-processed data; Risk factors; State-of-the-art methods; Training time
KW  -  Diagnosis
KW  - Forecasting
PB  - Hindawi Limited
JF  - Computational Intelligence and Neuroscience
VL  - 2021
DO  - 10.1155/2021/5573179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114113567&doi=10.1155%2F2021%2F5573179&partnerID=40&md5=cb8480030aa5783d796a2c718e3231c6
N1  - cited By 0
N2  - Today, diabetes is one of the most prevalent, chronic, and deadly diseases in the world owing to some complications. If accurate early diagnosis is feasible, the risk factor and incidence of diabetes may be greatly decreased. Diabetes prediction is stable and reliable, since there are only minimal labelling evidence and outliers found in the datasets of diabetes. Numerous works coped with diabetes disease prediction and provided the solution. But the existing methods proffered low accuracy detection and consumed more training time. So, this paper proposed an OWDANN algorithm for diabetes mellitus disease prediction and severity level estimation. The proposed system mainly consists of two phases, namely, disease prediction and severity level estimation phase. In the disease prediction phase, the preprocessing is performed for the Pima dataset. Then, the features are extracted from the preprocessed data, and finally, the classification step is performed by using OWDANN. In the severity level estimation phase, the diabetes positive dataset is preprocessed first. Then, the features are extracted, and lastly, the severity level is predicted using GDHC. The extensive experimental results showed that the proposed system outperforms with 98.97% accuracy, 94.98% sensitivity, 95.62% specificity, 97.02% precision, 93.84% recall, 9404% f-measure, 0.094% FDR, and 0.023% FPR compared with the state-of-the-art methods. © 2021 Annamalai R and Nedunchelian R.
ER  - 
TY  - CONF
T1  - Evaluation of Feature Selection Using Wrapper for Numeric Dataset with Random Forest Algorithm
A1  - Nugroho, A
A1  - Fanani, A Z
A1  - Shidik, G F
Y1  - 2021///
KW  -  Backward selection; Dimensionality reduction; Feature extraction and selection; Features selection; Forward selection; Random forest algorithm; Random forests; Relevant features; Wrapper; Wrapper methods
KW  -  Feature extraction
KW  - Decision trees; Random forests
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2021 International Seminar on Application for Technology of Information and Communication: IT Opportunities and Creativities for Digital Innovation and Communication within Global Pandemic, iSemantic 2021
SP  - 179
EP  - 183
SN  - 9781665428040
DO  - 10.1109/iSemantic52711.2021.9573249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118929554&doi=10.1109%2FiSemantic52711.2021.9573249&partnerID=40&md5=fb3659d613fc10a755f5e77d379eacf4
N1  - cited By 0
N2  - Preprocessing is more than half of machine learning process. Dimensionality reduction is one of the preprocessing task, which included feature extraction and selection. Feature selection used for identify relevant and remove not relevant feature. The goal of this research is to select relevant feature using wrapper method for early diabetes prediction dataset which has been transformed to numeric dataset previously. Forward and backward selection are used in wrapper method, that's combine with random forest and cross validation. Random forest is decision tree enhancement, which is group of trees that can produce difference or same result at each tree. The most results are made as final result. The final result from feature selection with wrapper method can make higher accuracy than without feature selection for numeric dataset and the number of feature can be reduced. With features selection which is sequential forward selection it has 98.84 % accuracy with 11 feature selected and with sequential backward selection, it has 99.03 % accuracy with same number of features selected. With reduced features, will reduces complexity of trees and time required in mining process.. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Automated Microaneurysm Detection in Fundus Images through Region Growing
A1  - Li, L
A1  - Shan, J
Y1  - 2017///
KW  -  Detection and diagnosis; Detection methods; Diabetic retinopathy; Feature selection and classification; Image preprocessing; Microaneurysms; Region classifications; Region growing
KW  -  Feature extraction
KW  - Bioinformatics; Classification (of information); Diagnosis; Eye protection; Neural networks
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2017 IEEE 17th International Conference on Bioinformatics and Bioengineering, BIBE 2017
VL  - 2018-January
SP  - 125
EP  - 130
SN  - 9781538613245
DO  - 10.1109/BIBE.2017.00-67
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049380506&doi=10.1109%2FBIBE.2017.00-67&partnerID=40&md5=24550ef47d8c59fe9f912f7b6dbe52a8
N1  - cited By 2
N2  - Diabetic retinopathy (DR) is the leading cause of blindness if not detected and treated in time and is a serious complication of diabetes. Since DR is a progressive eye disease, the early detection and diagnosis of DR is important to prevent patients from blindness. One of the most characteristic symptoms of DR is the presence of microaneurysm (MA) - the early sign of DR, which is hard to detect manually due to its small size. In this paper, we propose an automatic MA detection method based on region growing and region classification. We solve two problems: 1) given a fundus image, how to automatically partition the image into regions that may or may not contain MAs through a region growing approach, and 2) given a region in a fundus image, how to automatically evaluate whether this region contains MA by feeding the features of the region into an artificial neural network (ANN) for classification. The proposed approach involves image preprocessing, region growing, feature selection and classification steps. In the experiment, the public dataset DIAbetic RETinopathy DataBase 1 (DIARETDB1) is used to provide training/testing data and ground truth. The proposed method can achieve the performance with sensitivity 86.6%, specificity 96.3%, and accuracy 93.9%, for automatic MA detection. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - Machine learning in healthcare diagnosis system using density based clustering with logistic regression classification model
A1  - Ilayaraja, M
A1  - Selvam, R P
A1  - Kavitha, K P
A1  - Pustokhina, I
A1  - Pustokhin, D A
A1  - Shankar, K
Y1  - 2020///
PB  - Science and Engineering Research Support Society
JF  - International Journal of Advanced Science and Technology
VL  - 29
IS  - 8 Special Issue
SP  - 730
EP  - 740
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083787221&partnerID=40&md5=18af0d63b706c06dc5e3a4f8e6f27233
N1  - cited By 0
N2  - Diabetes is considered as a commonly occurring disease affecting people under all ages over the globe. Due to massive increase in number of patients being affected by diabetes, it is essential to design an automated disease diagnosis model for diabetes. This paper introduces a new machine learning based clustering with classification model to diagnose the presence of diabetes. The proposed model involves a density based clustering (DBC) technique with LR (LR) classifier called DBC-LR model for diabetes diagnosis. The presented DBC-LR model comprises a set of four stages namely preprocessing, feature reduction, clustering and classification. The validation of the DBC-LR model takes place using Pima Indian Diabetes dataset and the results are validated under several aspects. The clustered output of the DBC-LR model on the applied dataset and the classification outcome ensures the betterment of the proposed model over the compared methods. The DBC-LR model has resulted to superior outcome with the maximum accuracy of 98.3%. © 2020 SERSC.
ER  - 
TY  - JOUR
T1  - An Intelligent Gestational Diabetes Diagnosis Model Using Deep Stacked Autoencoder
A1  - Sumathi, A
A1  - Meganathan, S
A1  - Ravisankar, B V
Y1  - 2021///
KW  -  Auto encoders; Data classification; Deep learning; Diabetes diagnosis; Diabetes mellitus; Gestational diabetes; Gestational diabetes mellitu; Hier-archical clustering; Hierarchical Clustering; Outlier Detection
KW  -  Obstetrics
KW  - Anomaly detection; Classification (of information); Computer aided diagnosis; Computer software; Data handling; Deep learning; Nearest neighbor search; Statistics
PB  - Tech Science Press
JF  - Computers, Materials and Continua
VL  - 69
IS  - 3
SP  - 3109
EP  - 3126
DO  - 10.32604/cmc.2021.017612
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115907948&doi=10.32604%2Fcmc.2021.017612&partnerID=40&md5=ff37a4d7e0696d0db697053d84ed3e87
N1  - cited By 0
N2  - Gestational Diabetes Mellitus (GDM) is one of the commonly occurring diseases among women during pregnancy. Oral Glucose Tolerance Test (OGTT) is followed universally in the diagnosis of GDM diagnosis at early pregnancy which is costly and ineffective. So, there is a need to design an effective and automated GDMdiagnosis and classification model. The recent developments in the field of Deep Learning (DL) are useful in diagnosing different diseases. In this view, the current research article presents a new outlier detection with deep-stacked Autoencoder (OD-DSAE) model forGDMdiagnosis and classification. The goal of the proposed OD-DSAE model is to find out those mothers with high risks and make them undergo earlier diagnosis, monitoring, and treatment compared to low-risk women. The presented ODDSAE model involves three major processes namely, preprocessing, outlier detection, and classification. In the first step i.e., data preprocessing, there exists three stages namely, format conversion, class labelling, andmissing value replacement using k-nearest neighbors (KNN) model. Outliers are superior values which considerably varies from other data observations. So, it might represent the variability in measurement, experimental errors or novelty too. So, Hierarchical Clustering (HC)-based outlier detection technique is incorporated in OD-DSAE model, and thereby classification performance can be improved. The proposed model was simulated using Python 3.6.5 on a dataset collected by the researcher themselves. A series of experiments was conducted and the results were investigated under different aspects. The experimental outcomes inferred that the OD-DSAE model has outperformed the compared methods and achieved high precision of 96.17%, recall of 98.69%, specificity of 89.50%, accuracy of 96.18%, and F-score of 97.41%. © 2021 Tech Science Press. All rights reserved.
ER  - 
TY  - JOUR
T1  - Diabetes diagnostic method based on tongue image using ANN & CNN classifier
A1  - Srividhya, E
A1  - Muthukumaravel, A
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Recent Technology and Engineering
VL  - 8
IS  - 1 Special Issue 4
SP  - 284
EP  - 288
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069171597&partnerID=40&md5=838ef3612eec72ab0efd8b01d82773e5
N1  - cited By 1
N2  - In this paper diagnosing diabetic using tongue image is classified based on the machine learning and deep learning concepts. For machine learning Artificial Neural Network (ANN) and for deep learning Convolution Neural Network (CNN) are used to classify the diabetic patients tongue. There is a strong relationship between the characteristics of tongue and human health diagnosis for any diseases. In this proposed method we are going to get the input image, preprocessing the image for noise reduction and segment the image with size, shape and color, then we have to classify whether that image is diabetic or healthy tongue image. If it is a diabetic image again we have to classify for Diabetic Mellitus types that istype 1 and type 2 based on the severity in the image. The proposed method is compared with SVM classifier for better accuracy. As the experiment results in 98% of accuracy in diagnosing the diabetic diseases. © 2019, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.
ER  - 
TY  - CONF
T1  - Standardization in IRIS diagnosis
A1  - Perner, P
ED  - Jedrzejowicz P. Nguyen N.T., Hong T.-P. Czarnowski I
Y1  - 2015///
KW  -  Colored Spot Recognition; Iris images; Iris recognition; Object matching; Pupil Recognition; Topological Matching
KW  -  Diagnosis
KW  - Biometrics; Diseases; Image acquisition; Mining; Object recognition; Standardization; Topology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2015 IEEE 2nd International Conference on Cybernetics, CYBCONF 2015
SP  - 212
EP  - 217
SN  - 9781479983223
DO  - 10.1109/CYBConf.2015.7175934
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947968788&doi=10.1109%2FCYBConf.2015.7175934&partnerID=40&md5=4f01ab2e64b7b157a26ba4ab4f3d6caa
N1  - cited By 1
N2  - Molecular image-based techniques are widely used in medicine to detect specific diseases. The analysis of the eye plays an important role in order to detect specific diseases. Eye background analysis is used in order to detect certain forms of diabetes and others diseases. In the alternative medicine plays the diagnosis of the iris an important role. One understands by iris diagnosis (Iridology) the investigation and analysis of the colored part of the eye, the iris, to discover factors which play an important role for the prevention and treatment of illnesses, but al-so for the preservation of an optimum health. Although alternative practitioner describe substantial success with the iris diagnosis. The conventional medicine is not convinced of the diagnosis method. A big drawback of the method is the subjective interpretation of what is seen in the iris image. An automatic system would pave the way for much wider use of the iris diagnosis for the diagnosis of illnesses and for the purpose of individual health protection. With this paper we de-scribe our work towards an automatic iris diagnosis system. We describe the image acquisition and the problems with it. Different ways of image acquisition and image preprocessing are explained. We describe the image analysis method for the detection of the iris. This method is based on our novel case-based object recognition and case mining method. Results for the recognition of the iris are given. We describe how to detect the pupil and not wanted lamp spots. We explain how to recognize orange blue spots in the iris and match them against the topological map of the iris. Finally, we give an outlook for further work. © 2015 IEEE.
ER  - 
TY  - CONF
T1  - Automatic detection of exudates in retinal images using histogram analysis
A1  - Sharath Kumar, P N
A1  - Kumar, R R
A1  - Sathar, A
A1  - Sahasranamam, V
Y1  - 2013///
KW  -  Blood vessels; Computational complexity; Eye protection; Luminance; Statistical methods
KW  -  Graphic methods
KW  - Diabetic retinopathy; exudates; False-positive eliminations; Fundus image; Histogram analysis; preprocessing
JF  - 2013 IEEE Recent Advances in Intelligent Computational Systems, RAICS 2013
SP  - 277
EP  - 281
SN  - 9781479921782
DO  - 10.1109/RAICS.2013.6745487
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896744612&doi=10.1109%2FRAICS.2013.6745487&partnerID=40&md5=1739edd68340eb110acfbdef95c42b86
N1  - cited By 18
N2  - Diabetic Retinopathy (DR) is the major cause of blindness caused by the damage to the blood vessels in the retina from diabetes. It cannot be prevented but early detection through fundus imaging by an ophthalmologist can prevent further vision loss. Presence of microaneurysms, hemorrhages, cotton-wool spots and exudates are the symptoms of mild DR. Of these, the detection of exudates is one of the important factors in the early diagnosis of DR. Exudates are fatty deposits on the retina which appear as yellowish regions in fundus image. Fundus images show considerable variation in brightness which makes automatic detection of exudates difficult. In this study, we are proposing a new method for preprocessing and false positive elimination towards the reliable detection of exudates. The brightness of the fundus image was changed by the nonlinear curve with brightness values of the hue saturation value (HSV) space. To emphasize brighter yellow regions (exudates), gamma correction was performed on each red and green components of the image. Subsequently, the histograms of each red and green component were extended. After that, the exudates candidates were detected using histogram analysis. Finally, false positives were removed by using multi-channel histogram analysis. To evaluate the new method for the detection of exudates, we examined 158 fundus images, including 84 abnormal images with exudates and 74 normal images. The sensitivity and specificity for the detection of abnormal and normal cases were 88.45% and 95.5% respectively. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - A multi-scale feature fusion method based on U-net for retinal vessel segmentation
A1  - Yang, D
A1  - Liu, G
A1  - Ren, M
A1  - Xu, B
A1  - Wang, J
Y1  - 2020///
PB  - MDPI AG
JF  - Entropy
VL  - 22
IS  - 8
DO  - 10.3390/E22080811
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089479869&doi=10.3390%2FE22080811&partnerID=40&md5=4ff541c5597ec8097940250756a634aa
N1  - cited By 17
N2  - Computer-aided automatic segmentation of retinal blood vessels plays an important role in the diagnosis of diseases such as diabetes, glaucoma, and macular degeneration. In this paper, we propose a multi-scale feature fusion retinal vessel segmentation model based on U-Net, named MSFFU-Net. The model introduces the inception structure into the multi-scale feature extraction encoder part, and the max-pooling index is applied during the upsampling process in the feature fusion decoder of an improved network. The skip layer connection is used to transfer each set of feature maps generated on the encoder path to the corresponding feature maps on the decoder path. Moreover, a cost-sensitive loss function based on the Dice coefficient and cross-entropy is designed. Four transformations-rotating, mirroring, shifting and cropping-are used as data augmentation strategies, and the CLAHE algorithm is applied to image preprocessing. The proposed framework is tested and trained on DRIVE and STARE, and sensitivity (Sen), specificity (Spe), accuracy (Acc), and area under curve (AUC) are adopted as the evaluation metrics. Detailed comparisons with U-Net model, at last, it verifies the effectiveness and robustness of the proposed model. The Sen of 0.7762 and 0.7721, Spe of 0.9835 and 0.9885, Acc of 0.9694 and 0.9537 and AUC value of 0.9790 and 0.9680 were achieved on DRIVE and STARE databases, respectively. Results are also compared to other state-of-the-art methods, demonstrating that the performance of the proposed method is superior to that of other methods and showing its competitive results. © 2020 by the authors.
ER  - 
TY  - CONF
T1  - Classification of Diabetic Retinopathy through Deep Feature Extraction and Classic Machine Learning Approach
A1  - Paradisa, R H
A1  - Sarwinda, D
A1  - Bustamam, A
A1  - Argyadiva, T
Y1  - 2020///
KW  -  Automatic Detection; Data augmentation; Diabetic retinopathy; Feature extraction and classification; Gradient boosting; K-nearest neighbors; Learning approach; Machine learning approaches
KW  -  Classification (of information)
KW  - Adaptive boosting; Convolutional neural networks; Decision trees; Deep learning; Extraction; Eye protection; Feature extraction; Image processing; Learning systems; Nearest neighbor search; Support vector machines; Text processing
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 3rd International Conference on Information and Communications Technology, ICOIACT 2020
SP  - 377
EP  - 381
SN  - 9781728173566
DO  - 10.1109/ICOIACT50329.2020.9332082
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100907188&doi=10.1109%2FICOIACT50329.2020.9332082&partnerID=40&md5=43322d8b8b719780334247959dd45772
N1  - cited By 3
N2  - Diabetic Retinopathy (DR) is a complication of diabetes, the leading cause of vision loss in working-age adults. An ophthalmologist can carry out the diagnosis of DR by examining color fundus images. However, the fundus image analysis process takes a long time. Automatic detection of DR is achallenging task. One of the deep learning approaches, Convolutional Neural Networks (CNN), is efficient in image classification tasks. In this research, a CNN architecture is used, namely ResNet-50, as feature extraction and classification. The ResNet-50 feature output at the feature extraction stage is also used as input for machine learning classifiers such as Support Vector Machine (SVM), Random Forest (RF), k-Nearest Neighbor (k-NN), and Extreme Gradient Boosting (XGBoost). The model works by using fundus images from the DIARETDBI dataset. Data augmentation and preprocessing are proposed in this study to facilitate the model in recognizing images. The performance of each classifier is evaluated based on accuracy, sensitivity, and specificity. The SVM classifier achieved 99% for accuracy and sensitivity in the 80:20 dataset composition. The k-NN classifier obtains the highest specificity for the same dataset's design by 100%. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - An automated hybrid technique for detecting the stage of non-proliferative diabetic retinopathy
A1  - Singh, N
A1  - Kumar, A
A1  - Tripathi, R C
Y1  - 2010///
KW  -  Automation; Blood; Blood vessels; Diagnosis; Extraction; Image enhancement; Mathematical morphology; Principal component analysis; Support vector machines
KW  -  Eye protection
KW  - Automated grading; Automated systems; Blood supply; Color normalization; Cotton-wool spots; Diabetic retinopathy; Early detection; Fundus image; Fuzzy C means clustering; Hard exudates; Hybrid techniques; Image features; Image processing technique; Input vector; Microaneurysms; Nearest Neighbor classifier; non-proliferative diabetic retinopathy (NPDR); SVM classifiers; Vascularization
JF  - ACM International Conference Proceeding Series
SP  - 73
EP  - 80
SN  - 9781450304085
DO  - 10.1145/1963564.1963576
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955112321&doi=10.1145%2F1963564.1963576&partnerID=40&md5=eee0a7300ec59688de0b763289cf3294
N1  - cited By 1
N2  - Diabetes is fast becoming a scourge in the modern day society both in the developing and the developed societies. Diabetes related complications lead to a lot of morbidity and Diabetic Retinopathy is fast becoming the cause of preventable blindness. Early detection and treatment with Laser will go a long way in checking this disease. Non-proliferative Diabetic Retinopathy (NPDR) is the set of early changes that take place in the Retina. It is divided into 3 categories into mild, moderate and severe. Initial changes when the microaneurysms (MA) start appearing. Then it is followed by hemorrhages. Finally appearance of cotton wool spots and hard exudates categorize it into severe NPDR. The stage of neo vascularization (NV) when new blood vessels begin to appear (to compensate for the reduced blood supply and nutrition to the retina) finally qualifies for proliferative Diabetic Retinopathy. The idea is to extract the features of NPDR and depending on their intensity and frequency they can be graded into mild, moderate and severe. This automated grading can be matched with the specialist's perception and its accuracy can be tested. In this work, we have proposed a computer based automated hybrid technique for the detection of stages of Non-Proliferative Diabetic Retinopathy (NPDR) retinopathy stage using the color fundus images. The features are extracted from the Sample images using the image processing techniques and fed to the support vector machine (SVM). After color normalization preprocessing stage, an evidence value for every pixel is calculated by SVM. Then a mathematical morphological technique, a fuzzy c-means clustering technique, PCA, a support vector machine and a nearest neighbor classifier for further processing. The SVM classifier uses features extracted by combined 2DPCA instead of explicit image features as the input vector Combined 2DPCA is proposed and virtual SVM is applied to achieve the higher accuracy of classification. We demonstrate a Sensitivity of 97.1% for the classifier with the Specificity of 98.3%. Thus, an automated system for diagnosis of NPDR can be a useful tool for the Specialist to support in screening an detection of early Diabetic Retinopathy changes and hence timely intervention leading to reduced DR(Diabetic retinopathy) related blindness. © 2010 ACM.
ER  - 
TY  - JOUR
T1  - Intelligent Data-Driven Model for Diabetes Diurnal Patterns Analysis
A1  - Eissa, M R
A1  - Good, T
A1  - Elliott, J
A1  - Benaissa, M
Y1  - 2020///
KW  -  Algorithms; Blood Glucose; Circadian Rhythm; Cluster Analysis; Diabetes Mellitus
KW  -  Automated; Unsupervised Machine Learning
KW  -  Data-driven model; Dose calculation; Influential factors; Insulin sensitivity; Intelligent data; Mobile applications; Pattern identification; Time series clustering
KW  -  Insulin
KW  -  Physiologic; Pattern Recognition
KW  -  Type 1; Humans; Insulin Infusion Systems; Monitoring
KW  -  algorithm; Article; carbohydrate intake; clinical practice; clustering algorithm; diabetes mellitus; diurnal enuresis; fitness; glucose blood level; glycemic control; glycemic index; health care personnel; health care utilization; hospital readmission; human; insulin sensitivity; intelligence quotient; k means clustering; machine learning; mathematical model; measurement accuracy; mobile application; pattern analysis; pilot study; preprocessing; self monitoring; time series analysis; unsupervised machine learning; validation process; algorithm; automated pattern recognition; blood; circadian rhythm; cluster analysis; insulin dependent diabetes mellitus; insulin infusion; metabolism; physiologic monitoring; physiology; procedures
KW  -  glucose; insulin
KW  - Carbohydrates; Mathematical instruments
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Journal of Biomedical and Health Informatics
VL  - 24
IS  - 10
SP  - 2984
EP  - 2992
DO  - 10.1109/JBHI.2020.2975927
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086143105&doi=10.1109%2FJBHI.2020.2975927&partnerID=40&md5=a106b4220fedf2a3550daf9e8f53405d
N1  - cited By 1
N2  - In type 1 diabetes, diurnal activity routines are influential factors in insulin dose calculations. Bolus advisors have been developed to more accurately suggest doses of meal-related insulin based on carbohydrate intake, according to pre-set insulin to carbohydrate levels and insulin sensitivity factors. These parameters can be varied according to the time of day and their optimal setting relies on identifying the daily time periods of routines accurately. The main issues with reporting and adjustments of daily activity routines are the reliance on self-reporting which is prone to inaccuracy and within bolus calculators, the keeping of default settings for daily time periods, such as within insulin pumps, glucose meters, and mobile applications. Moreover, daily routines are subject to change over periods of time which could go unnoticed. Hence, forgetting to change the daily time periods in the bolus calculator could contribute to sub-optimal self-management. In this paper, these issues are addressed by proposing a data-driven model for identification of diabetes diurnal patterns based on self-monitoring data. The model uses time-series clustering to achieve a meaningful separation of the patterns which is then used to identify the daily time periods and to advise of any time changes required. Further improvements in bolus advisor settings are proposed to include week/weekend or even modifiable daily time settings. The proposed model provides a quick, granular, more accurate, and personalized daily time setting profile while providing a more contextual perspective to glycemic pattern identification to both patients and clinicians. © 2013 IEEE.
ER  - 
TY  - CONF
T1  - Prediction of Diabetes with its Symptoms Based on Machine Learning
A1  - Xu, X
A1  - Huang, X
A1  - Ma, J
A1  - Luo, X
Y1  - 2021///
KW  -  Data preprocessing; Literature analysis; Machine-learning; Medical research; On-machines
KW  -  Forecasting
KW  - Data handling; Data visualization; Information analysis; Machine learning
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering, CSAIEE 2021
SP  - 147
EP  - 156
SN  - 9781665422048
DO  - 10.1109/CSAIEE54046.2021.9543343
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117806818&doi=10.1109%2FCSAIEE54046.2021.9543343&partnerID=40&md5=4bafc13016083f6eb4be78f2c0bd0574
N1  - cited By 0
N2  - As the destruction of diabetes is significant to the whole world, we want to focus on it and extract useful information from the correlation between symptoms and disease. The dataset obtained from UCI is the fundamental resource for the research. In order to ensure the accuracy of the project conclusions, three different approaches were used to verify each other: literature analysis, data analysis and machine learning. Literature part mainly contains previous work and large quantities of medical research done on diabetes. Data analysis included data preprocessing and visualization so as to unfold the concealed information of the dataset. Machine learning is to use the inspiration from the previous two parts to attain a suitable model for diabetes prediction. The project finally provides knowledge of different symptoms of diabetes and their relation with diabetes. It also elaborates how symptoms can be used to predict disease. Finally, we put forward suggestions for the prevention of diabetes and monitoring of potential disease. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - A novel holistic disease prediction tool using best fit data mining techniques
A1  - Diwani, S A
A1  - Yonah, Z O
Y1  - 2017///
PB  - University of Bahrain
JF  - International Journal of Computing and Digital Systems
VL  - 6
IS  - 2
SP  - 63
EP  - 72
DO  - 10.12785/IJCDS/060202
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053244636&doi=10.12785%2FIJCDS%2F060202&partnerID=40&md5=eadafcb6b3e1b93a388eebde7d8345fd
N1  - cited By 7
N2  - Given that, today, the healthcare ecosystem is an information rich industry, there is an increasing demand for data mining (DM) tools to improve the quantity and quality of delivered healthcare; especially in handling patients suffering from deadly diseases such as HIV, Breast Cancer, Diabetes, Tuberculosis (TB), Heart diseases and Liver disorder. Given the fatality nature of these diseases when they remain undetected until at advanced stages, there remains a demand for best classifier tools to assist in diagnosing, detecting and treatment of these life-threatening diseases at their early stages. Complementary to this demand is the fact that the healthcare industry today generates large amounts of complex data about patients, hospital resources and disease diagnosis. Consequently, the healthcare ecosystem is warehousing large amount of medical data, which is an asset for healthcare organizations if properly utilized. The large amount of patient and disease related data could be processed and analyzed for knowledge extraction that enables support for cost savings and decision making towards delivery of timely and quality healthcare. In this paper, we report on an ongoing research work to develop and test a holistic DM disease prediction (Diagnosis and prognosis) tool, equipped with processes for preprocessing patients' data and a learning procedure for selecting a disease-specific best classifier, for disease prediction and delivery of speedy and cost effective diagnostic interventions and patient follow up in a hospital environment. As diseases are diagnosed, the predictive tool helps medical doctors in decision-making about what disease case it is and suggests possible treatment strategies within a much-reduced time. Test results for breast cancer and HIV data sets are reported. Achieved from the reported work are classification accuracies of 97.0752% (Classifier acting singly); 97.6323% (fusion of three classifiers). These results are better than those reported in the literature. The results show that the proposed DM disease prediction tool has potential to greatly impact on current patient management, care and future interventions against deadly diseases. © 2017 University of Bahrain. All rights reserved.
ER  - 
TY  - CONF
T1  - DeepDR: An image guideddiabetic retinopathy detection technique using attention-based deep learning scheme
A1  - Islam, N
A1  - Saeed, U
A1  - Naz, R
A1  - Tanveer, J
A1  - Kumar, K
A1  - Shaikh, A A
Y1  - 2019///
KW  -  Eye protection
KW  -  attentionmechanism; Convolutional neural network; Diabetic retinopathy; global average pooling; Transfer learning
KW  - Blood vessels; Convolution; Cost effectiveness; Deep learning; Deep neural networks; Image enhancement; Multilayer neural networks; Network architecture
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2019 2nd International Conference on New Trends in Computing Sciences, ICTCS 2019 - Proceedings
SN  - 9781728128825
DO  - 10.1109/ICTCS.2019.8923097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077204987&doi=10.1109%2FICTCS.2019.8923097&partnerID=40&md5=7df7718e09b422c3353bd9f1ce588674
N1  - cited By 2
N2  - This paper proposes an efficient and cost effective deep learning architecture to detect the diabetic retinopathy in real time. Diabetes is a leading root cause of eye disease in patients. It illuminates eye vessels, and releases blood form vessels. Early detection of diabetic retinopathy is useful to reduce the risk of blindness or any hazard. In this paper, after some preprocessing and data augmentation, Inception V3 is used as pre-Trained model to extract the initial features set. Convolutional neural network has been used with attention layers. These additional CNN layers are added to extract the deep features to improve classification performance and accuracy. Initially, the model was proposed by Kevin Mader in Kaggle. The paper introduced additional layers in proposed model and improved the validation and testing accuracy significantly. More than 90% validation accuracy was achieved with the proposed Convolutional Neural Network model. Testing accuracy was improved up to 5%. This improvement in accuracy is very significant because the dataset is imbalanced and contains noisy images. It is concluded that global average pooling (GAP) based attention mechanism increased deep learning architecture accuracy to detect the Diabetic Retinopathy in imbalanced and noisy image dataset © 2019 IEEE.
ER  - 
TY  - JOUR
T1  - Investigations on diabetic macular edema using motion pattern estimation to prevent vision loss
A1  - Murugeswari, S
A1  - Sukanesh, R
Y1  - 2015///
PB  - Asian Research Publishing Network
JF  - ARPN Journal of Engineering and Applied Sciences
VL  - 10
IS  - 7
SP  - 2940
EP  - 2947
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928530718&partnerID=40&md5=89c44ec86e7e2c1eda2ce643262d1f66
N1  - cited By 1
N2  - Diabetic macular edema (DME) is a complication of diabetes. It can blur or distort patients' vision and make the blindness. It is categorized by the presence of lesions. Habitually the presence of lesions is detected by Ophthalmologists from the dilated retinal images captured by dropping chemical solution into an eye. This process peeves the patients. So, there is a need for an autonomous method to detect the presence of lesions using image processing algorithm from the nondilated images to help the ophthalmologists to diagnose the disease without inconvenience and irritation to the patient and thus protects patients from vision loss. In this work, Meadian filter and Contrast Limited Adaptive Histogram Equalization used in image preprocessing. Motion pattern estimation with masking process used for segmentation. To extract the feature Grey Level Co-occurrence Matrix isused. Support vector machine used to classify the severity level for disease. The proposed algorithm has produced the sensitivity of 99.743%, specificity of 97.14% and accuracy of 97.711%.It is more helpful for ophthalmologist in the detection of DME. Since this method is automated, it detects faster and this level of accuracy in result helps the ophthalmologists to diagnose the disease very easily. © 2006-2015 Asian Research Publishing Network (ARPN).
ER  - 
TY  - CONF
T1  - Retinal Fundus Identification Utilizing Supervised and Unsupervised Nature of Deep Neural Network
A1  - Chowdhury, M S
A1  - Taimy, F R
A1  - Nahid, A.-A.
A1  - Ali, M Y
A1  - Bin Ali, F
Y1  - 2019///
KW  -  Accuracy; Adversarial networks; Augmentation; CLAHE; Computer Aided Diagnosis(CAD); Digital photography; Pre-processing algorithms; State of the art
KW  -  Deep neural networks
KW  - Computer aided diagnosis; Image segmentation; Medical computing; Ophthalmology; Robotics
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 1st International Conference on Advances in Science, Engineering and Robotics Technology 2019, ICASERT 2019
SN  - 9781728134451
DO  - 10.1109/ICASERT.2019.8934640
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077983478&doi=10.1109%2FICASERT.2019.8934640&partnerID=40&md5=28dd78c4169e289d9fbacbc02c517533
N1  - cited By 1
N2  - Eye disease has become serious concern to people, specially who have been suffering from diabetes. Systematic eye diseases are manifest in around the retina of an eye. Digital photography of the retinal images provide a significant information about the disease. This kind of retinal images work as a powerful source for the doctors and ophthalmologist to take meaningful decision about the disease. This kind of Computer Aided Diagnosis(CAD) allow both the doctors and patient to take a second layer of satisfaction. Considering this issues, this paper segmented the retinal vessel images using the state-of-the- art Deep Neural Network(DNN) such as a combination of U-Net and Generative Adversarial Network (GAN) model. As the retinal images suffers due to the Contrast and Illumination problems, this paper also proposed two preprocessing algorithms to reduce this effect. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - A Survey on Computer Algorithms for Retinal image Preprocessing and Vessel Segmentation
A1  - Maheswari, M V
A1  - Murugeswari, G
Y1  - 2020///
KW  -  Blood vessels
KW  -  Computer Aided Diagnosis(CAD); Diabetic patient; Diabetic retinopathy; Preprocessing techniques; Retinal blood vessels; Retinal image analysis; Varying thickness; Vessel segmentation
KW  - Blood; Computer aided diagnosis; Eye protection; Image enhancement; Image segmentation; Ophthalmology; Risk assessment; Surveys
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the 5th International Conference on Inventive Computation Technologies, ICICT 2020
SP  - 403
EP  - 408
SN  - 9781728146850
DO  - 10.1109/ICICT48043.2020.9112470
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086993835&doi=10.1109%2FICICT48043.2020.9112470&partnerID=40&md5=f8a64415d712e2f9c9154d4a3dfc5975
N1  - cited By 2
N2  - Diabetic retinopathy is one of the common eye diseases found in most of the diabetic patients. According to the latest report by the International Diabetes Federation (IDF), in the year 2045 about 84% of the Indian population will be affected by Diabetes. Automated Computer Aided Diagnosis (CAD) system helps ophthalmologists to reduce the risk of visual defects and to reduce the cost of medical resources to a greater extent. Retina is a delicate and a thin tissue that lines back of eye. It is supplied with sufficient amount of blood by a delicate network of blood vessels. Blood vessels in the retina plays an essential role for the analysis and detection of various chronicle diseases like diabetic retinopathy, glaucoma, cataract and cardio vascular risks. Manual vessel segmentation is always a difficult process in a retinal image because of varying thickness and width of a retinal blood vessels. Retinal blood vessel segmentation has become a mandatory process for any type of retinal diseases. This survey paper briefly explains preprocessing techniques for retinal image enhancement, methods used for segmentation of blood vessels and publicly available retinal databases. The goal of the paper is to provide a deep understanding and knowledge about the existing computer algorithms used for retinal image analysis and to identify the research areas for computer researchers in the field of ophthalmology. © 2020 IEEE.
ER  - 
TY  - JOUR
T1  - Kalman smoothing for objective and automatic preprocessing of glucose data
A1  - Staal, O M
A1  - Salid, S
A1  - Fougner, A
A1  - Stavdahl, O
Y1  - 2019///
KW  -  Algorithms; Blood Chemical Analysis; Blood Glucose; Humans; Models
KW  -  Biomedical measurements; Data processing and analysis; Glucose measurements; Interstitial fluids; Noise measurements; Pre-processing step; Relative deviations; Smoothing methods
KW  -  Computer-Assisted
KW  -  Glucose
KW  -  Statistical; Signal Processing
KW  -  accuracy; algorithm; Article; blood glucose monitoring; comparative study; data processing; diabetes mellitus; dynamics; glucose blood level; human; mathematical model; signal noise ratio; theoretical model; algorithm; analysis; blood analysis; glucose blood level; procedures; signal processing; standards; statistical model
KW  -  glucose; insulin
KW  - Blood; Data handling; Homogenization method; Kalman filters; MATLAB; Spurious signal noise; Sugars; Time measurement; Time series
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Journal of Biomedical and Health Informatics
VL  - 23
IS  - 1
SP  - 218
EP  - 226
DO  - 10.1109/JBHI.2018.2811706
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042852871&doi=10.1109%2FJBHI.2018.2811706&partnerID=40&md5=e8c36e480dd9a55de0d2d5b495caad2e
N1  - cited By 13
N2  - A method for preprocessing a time series of glucose measurements based on Kalman smoothing is presented. Given a glucose data time series that may be irregularly sampled, the method outputs an interpolated time series of glucose estimates with mean and variance. The method can provide homogenization of glucose data collected from different devices by using separate measurement noise parameters for differing glucose measurement equipment. We establish a link between the ISO 15197 standard and the measurement noise variance used by the Kalman smoother for self-monitoring of blood glucose (SMBG) measurements. The method provides phaseless smoothing, and it can automatically correct errors in the original datasets like small fallouts and erroneous readings when surrounding data allow. The estimated variance can be used for deciding at which times the data are trustworthy. The method can be used as a preprocessing step in many kinds of glucose data processing and analysis tasks, such as computing the mean absolute relative deviation between measurement systems or estimating the plasma-to-interstitial fluid glucose dynamics of continuous glucose monitor or flash glucose monitor (FGM) signals. The method is demonstrated on SMBG and FGM glucose data from a clinical study. A MATLAB implementation of the method is publicly available. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Construction of a two-parameter empirical model of left ventricle wall motion using cardiac tagged magnetic resonance imaging data
A1  - Shi, J J
A1  - Alenezy, M
A1  - Smirnova, I V
A1  - Bilgen, M
Y1  - 2012///
KW  -  Biological; Movement; Reproducibility of Results; Ventricular Function
KW  -  Biomechanics; Linear transformations; Magnetic resonance imaging; Parameter estimation; Strain rate
KW  -  Computer-Assisted; Magnetic Resonance Imaging; Male; Models
KW  -  Female; Heart; Humans; Image Processing
KW  -  Heart
KW  -  article; biological model; female; heart; heart ventricle function; human; image processing; male; movement (physiology); nuclear magnetic resonance imaging; physiology; reproducibility
KW  - Cardiac cycles; Cardiac modeling; Cardio-vascular disease; Comprehensive model; Empirical model; Healthy individuals; Image preprocessing; Left ventricles; Model fitting; Model prediction; Myocardial deformation; Qualitative and quantitative analysis; Relaxation phasis; Single parameter; Tagged magnetic resonance; Tagged MRI; Temporal evolution; Temporal profile; Temporal trends; Time-dependent variables; Transformation matrices; Transverse planes; Two-component; Wall motion
JF  - BioMedical Engineering Online
VL  - 11
DO  - 10.1186/1475-925X-11-79
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867671189&doi=10.1186%2F1475-925X-11-79&partnerID=40&md5=2e6f9ca1071d5feca449d7335c8da169
N1  - cited By 8
N2  - Background: A one-parameter model was previously proposed to characterize the short axis motion of the LV wall at the mid-ventricle level. The single parameter of this model was associated with the radial contraction of myocardium, but more comprehensive model was needed to account for the rotation at the apex and base levels. The current study developed such model and demonstrated its merits and limitations with examples.Materials and methods: The hearts of five healthy individuals were visualized using cardiac tagged magnetic resonance imaging (tMRI) covering the contraction and relaxation phases. Based on the characteristics of the overall dynamics of the LV wall, its motion was represented by a combination of two components - radial and rotational. Each component was represented by a transformation matrix with a time-dependent variable α or β.Image preprocessing step and model fitting algorithm were described and applied to estimate the temporal profiles of α and β within a cardiac cycle at the apex, mid-ventricle and base levels. During this process, the tagged lines of the acquired images served as landmark reference for comparing against the model prediction of the motion. Qualitative and quantitative analyses were performed for testing the performance of the model and thus its validation.Results: The α and β estimates exhibited similarities in values and temporal trends once they were scaled by the radius of the epicardium (repi)and plotted against the time scaled by the period of the cardiac cycle (Tcardiac) of each heart measured during the data acquisition. α/repi peaked at about Δt/Tcardiac=0.4 and with values 0.34, 0.4 and 0.3 for the apex, mid-ventricle and base level, respectively. β/repi similarly maximized in amplitude at about Δt/Tcardiac=0.4, but read 0.2 for the apex and - 0.08 for the base level. The difference indicated that the apex twisted more than the base.Conclusion: It is feasible to empirically model the spatial and temporal evolution of the LV wall motion using a two-parameter formulation in conjunction with tMRI-based visualization of the LV wall in the transverse planes of the apex, mid-ventricle and base. In healthy hearts, the analytical model will potentially allow deriving biomechanical entities, such as strain, strain rate or torsion, which are typically used as diagnostic, prognostic or predictive markers of cardiovascular diseases including diabetes. © 2012 Shi et al.; licensee BioMed Central Ltd.
ER  - 
TY  - CONF
T1  - Data-driven Preprocessing Techniques for Early Diagnosis of Diabetes, Heart and Liver Diseases
A1  - Gupta, S
A1  - Namdev, U
A1  - Gupta, V
A1  - Chheda, V
A1  - Bhowmick, K
Y1  - 2021///
KW  -  Classification algorithm; Data driven; Data preprocessing; Disease classification; Early diagnosis; Heart disease; Liver disease; Machine-learning; Performance; Pre-processing techniques
KW  -  Decision trees
KW  - Cardiology; Computer aided diagnosis; Decision support systems; Diseases; Heart; Medical informatics; Nearest neighbor search; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 4th International Conference on Electrical, Computer and Communication Technologies, ICECCT 2021
SN  - 9781665414807
DO  - 10.1109/ICECCT52121.2021.9616835
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123407984&doi=10.1109%2FICECCT52121.2021.9616835&partnerID=40&md5=4078dd6d45fbd65e550877f317672cbb
N1  - cited By 0
N2  - The emergence of machine learning in medicine has revolutionized the entire procedure of detecting and treating ailments. For knowledge extraction and decision support, machine learning models have been adapted in healthcare research. The significance of data preprocessing is often overlooked in mainstream health informatics research, which focuses more on generating accurate models. This paper focuses on building robust classification models for the prediction of diabetes, heart, and liver disease using a variety of preprocessing techniques to achieve optimal results. The implementation of the models is carried out on datasets sourced from the University of California, Irvine (UCI) Machine Learning Repository. Numerous preprocessing techniques such as feature engineering, data pruning, oversampling for skewed datasets, imputation of missing values, encoding categorical variables, and feature scaling are used in this paper. These techniques help considerably augment the performance of the classification algorithms used, which include Random Forest, K-Nearest Neighbours (KNN), and Support Vector Machine (SVM) among others. The performance of these algorithms is further improved by hyperparameter tuning, significantly improving the accuracy scores. The maximum accuracies obtained for heart disease, liver disease and diabetes prediction are 90.16%, 73% and 93.23% respectively. The paper also showcases the advantages of detection of these diseases at an early stage, which could make a substantial difference in numerous cases. The performance of different classifiers has been documented using metrics such as Accuracy, Balanced Accuracy, and F-1 score. Further visualization and comparison of the performance of the classification algorithms are carried out to find the best results. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - A survey on detection of diabetic retinopathy
A1  - Shalini, R
A1  - Sasikala, S
Y1  - 2019///
KW  -  Cotton-wool spots; Diabetic retinopathy; Exudates; Hemorrhages; Lesion of Interest; Micro Aneurysms
KW  -  Image segmentation
KW  - Cotton; Eye protection; Feature extraction; Internet of things; Ophthalmology; Surveys; Wool; Yarn
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud), I-SMAC 2018
SP  - 626
EP  - 630
SN  - 9781538614426
DO  - 10.1109/I-SMAC.2018.8653694
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063471939&doi=10.1109%2FI-SMAC.2018.8653694&partnerID=40&md5=8fcec1af78dfded1572b4b7c5583d682
N1  - cited By 4
N2  - Visual perception is very important for human life. Although several medical conditions can cause retinal disease, the most common cause is diabetes. Diabetic Retinopathy (DR) can be identified using retinal fundus images. Detection and classification of deformation in Diabetic retinopathy is a challenging task since it is symptomless. Several algorithms were analyzed for the identification of abnormality. The analysis of different models in detecting the abnormalities from the image is done which includes various preprocessing techniques to standardize the image and post-processing techniques are applied for morphological adjustments, segmentation algorithms for segmenting the Lesion of Interest(LOI ) namely white lesions and red lesions, further feature extraction methods extracts the features like Micro Aneurysms, Hemorrhages, Exudates and Cotton Wool Spots and so on finally, classification methods were utilized which concludes the presence or absence of DR symptoms along with the severity based on the count of the features extracted in the given retinal image. This survey study aims to develop a novel algorithm to identify and detect types of above mentioned diseases and find out the severity of those diseases also examine with 100% accuracy. © 2018 IEEE.
ER  - 
TY  - JOUR
T1  - IoT based health—related topic recognition from emerging online health community (Med help) using machine learning technique
A1  - Sampath, P
A1  - Packiriswamy, G
A1  - Kumar, N P
A1  - Shanmuganathan, V
A1  - Song, O.-Y.
A1  - Tariq, U
A1  - Nawaz, R
Y1  - 2020///
PB  - MDPI AG
JF  - Electronics (Switzerland)
VL  - 9
IS  - 9
SP  - 1
EP  - 15
DO  - 10.3390/electronics9091469
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090633599&doi=10.3390%2Felectronics9091469&partnerID=40&md5=cf8fc8fb78a3da1afea5b324f83e4718
N1  - cited By 15
N2  - The unprompted patient’s and inimitable physician’s experience shared on online health communities (OHCs) contain a wealth of unexploited knowledge. Med Help and eHealth are some of the online health communities offering new insights and solutions to all health issues. Diabetes mellitus (DM), thyroid disorders and tuberculosis (TB) are chronic diseases increasing rapidly every year. As part of the project described in this article comments related to the diseases from Med Help were collected. The comments contain the patient and doctor discussions in an unstructured format. The sematic vision of the internet of things (IoT) plays a vital role in organizing the collected data. We pre-processed the data using standard natural language processing techniques and extracted the essential features of the words using the chi-squared test. After preprocessing the documents, we clustered them using the K-means++ algorithm, which is a popular centroid-based unsupervised iterative machine learning algorithm. A generative probabilistic model (LDA) was used to identify the essential topic in each cluster. This type of framework will empower the patients and doctors to identify the similarity and dissimilarity about the various diseases and important keywords among the diseases in the form of symptoms, medical tests and habits. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
ER  - 
TY  - CONF
T1  - The establishment of diabetes diet classification model based on BL-WSmote
A1  - Lei, X
A1  - Feng, C
Y1  - 2018///
KW  -  Attribute reduction; Classification models; Data preprocessing; Model accuracy; Random forests; SMOTE algorithm; Strong classifiers; Traditional Chinese Medicine
KW  -  Decision trees
KW  - Medical problems; Medicine
PB  - Association for Computing Machinery
JF  - ACM International Conference Proceeding Series
SP  - 111
EP  - 115
SN  - 9781450365215
DO  - 10.1145/3239283.3239323
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055688665&doi=10.1145%2F3239283.3239323&partnerID=40&md5=626840b461c84d074f194ac094eb1e96
N1  - cited By 1
N2  - This paper analyzes the daily diet recipe of diabetes from traditional Chinese medicine diet, and establishes a model to determine the type of diabetes. The process of building a model is as follows. Firstly, use improved Smote algorithm to do data preprocessing. Secondly, using GA to optimize Random Forest do attribute reduction. Finally, using Bagging establish model. We establish 3 sets of comparison experiments. The comparison of 4 different algorithms between Random Forest, GDBT, Ada Booster, Bagging 4 different algorithms; The comparison between raw data, Smote improved data, BL-Smote improved data, BL-WSmote improved; The comparison between the whole attributes and the important attributes. The model accuracy is as high as 92%, it is proved that this model has certain applicable value in determining diabetes classification. © 2018 Association for Computing Machinery.
ER  - 
TY  - JOUR
T1  - Artificial Intelligence and Internet of Things Enabled Disease Diagnosis Model for Smart Healthcare Systems
A1  - Mansour, R F
A1  - Amraoui, A E
A1  - Nouaouri, I
A1  - DIaz, V G
A1  - Gupta, D
A1  - Kumar, S
Y1  - 2021///
KW  -  Disease diagnosis; Health-care system; Healthcare sectors; Internet of Things (IOT); Key technologies; Maximum accuracies; Search optimization; Smart healthcare systems
KW  -  Internet of things
KW  - Cardiology; Classification (of information); Data acquisition; Diagnosis; Diseases; Health care; Long short-term memory
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Access
VL  - 9
SP  - 45137
EP  - 45146
DO  - 10.1109/ACCESS.2021.3066365
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103790116&doi=10.1109%2FACCESS.2021.3066365&partnerID=40&md5=c03e42c8395b805eec8540641165326d
N1  - cited By 32
N2  - The recent advancements in Internet of Things (IoT), cloud computing, and Artificial Intelligence (AI) transformed the conventional healthcare system into smart healthcare. By incorporating key technologies such as IoT and AI, medical services can be improved. The convergence of IoT and AI offers different opportunities in healthcare sector. In this view, the current research article presents a new AI and IoT convergence-based disease diagnosis model for smart healthcare system. The major goal of this article is to design a disease diagnosis model for heart disease and diabetes using AI and IoT convergence techniques. The presented model encompasses different stages namely, data acquisition, preprocessing, classification, and parameter tuning. IoT devices such as wearables and sensors permit seamless data collection while AI techniques utilize the data in disease diagnosis. The proposed method uses Crow Search Optimization algorithm-based Cascaded Long Short Term Memory (CSO-CLSTM) model for disease diagnosis. In order to achieve better classification of the medical data, CSO is applied to tune both 'weights' and 'bias' parameters of CLSTM model. Besides, isolation Forest (iForest) technique is employed in this research work to remove the outliers. The application of CSO helps in considerable improvement in the diagnostic outcomes of CLSTM model. The performance of CSO-LSTM model was validated using healthcare data. During the experimentation, the presented CSO-LSTM model accomplished the maximum accuracies of 96.16% and 97.26% in diagnosing heart disease and diabetes respectively. Therefore, the proposed CSO-LSTM model can be employed as an appropriate disease diagnosis tool for smart healthcare systems. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Improving the Accuracy of Diabetes Diagnosis Applications through a Hybrid Feature Selection Algorithm
A1  - Li, X
A1  - Zhang, J
A1  - Safara, F
Y1  - 2021///
KW  -  Artificial intelligence techniques; Feature selection and classification; Harmony search algorithms; Healthcare workers; High blood pressures; Hybrid feature selections; K-nearest neighbors; Particle swarm optimization algorithm
KW  -  Diagnosis
KW  - Artificial intelligence; Blood pressure; Classification (of information); Condition monitoring; Data mining; Decision making; Diseases; Feature extraction; Genetic algorithms; Health risks; Home health care; K-means clustering; Nearest neighbor search; Particle swarm optimization (PSO)
PB  - Springer
JF  - Neural Processing Letters
DO  - 10.1007/s11063-021-10491-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103420516&doi=10.1007%2Fs11063-021-10491-0&partnerID=40&md5=22d20fd895bb5c7cffe1fdc17776a37b
N1  - cited By 8
N2  - Artificial intelligence is a future and valuable tool for early disease recognition and support in patient condition monitoring. It can increase the reliability of the cure and decision making by developing useful systems and algorithms. Healthcare workers, especially nurses and physicians, are overworked due to a massive and unexpected increase in the number of patients during the coronavirus pandemic. In such situations, artificial intelligence techniques could be used to diagnose a patient with life-threatening illnesses. In particular, diseases that increase the risk of hospitalization and death in coronavirus patients, such as high blood pressure, heart disease and diabetes, should be diagnosed at an early stage. This article focuses on diagnosing a diabetic patient through data mining techniques. If we are able to diagnose diabetes in the early stages of the disease, we can force patients to stay home and care for their health, so the risk of being infected with the coronavirus would be reduced. The proposed method has three steps: preprocessing, feature selection and classification. Several combinations of Harmony search algorithm, genetic algorithm, and particle swarm optimization algorithm are examined with K-means for feature selection. The combinations have not examined before for diabetes diagnosis applications. K-nearest neighbor is used for classification of the diabetes dataset. Sensitivity, specificity, and accuracy have been measured to evaluate the results. The results achieved indicate that the proposed method with an accuracy of 91.65% outperformed the results of the earlier methods examined in this article. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - CONF
T1  - Smart Architecture for Diabetic Patients Using Machine Learning
A1  - Anishfathima, B
A1  - Gautham, P
A1  - Gowri Mahalakshmi, B
A1  - Jahangir Jamadar, S K
Y1  - 2021///
KW  -  Data mining
KW  -  Diabetes mellitus; Diabetic patient; Knowledge analysis; Predicting types; Preprocessing systems; Risk factors; Smart architectures; Wellbeing
KW  - Machine learning
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021
SP  - 1544
EP  - 1548
SN  - 9781665405201
DO  - 10.1109/ICACCS51430.2021.9441985
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108030231&doi=10.1109%2FICACCS51430.2021.9441985&partnerID=40&md5=b09627dbf8906554162f8bf734a4977f
N1  - cited By 2
N2  - Because of its constantly expanding event, an increasing number of Diabetes mellitus infected households. Nearly all diabetic patients care little about their quality of prosperity or the risk factors that they face when examining them. In this evaluation, we suggested a novel paradigm for predicting type 2 diabetes mellitus subject to data mining techniques (T2DM). The major challenges we are trying to solve are enhancing the consistency of the Figure model, and to make more than one dataset adaptable to the model. The model is comprised of two regions, the enhanced SMO with RF estimation, given the movement of preprocessing systems. To learn about our findings and the findings of other researchers, the Dataset from PIMA Indians Dataset for Diabetes study and the Waikato Setting for Knowledge Analysis instrument compartment were utilized. Results reveals that a 3.04 percent higher conjecture accuracy was reached by the model than those of different scientists. Also, our model makes sure that the accuracy of the dataset is adequate. We extended it to two other diabetes datasets to better test the introduction of our model. The two examinations' outcomes show acceptable execution. Subsequently, the model is demonstrated to be helpful for the reasonable wellbeing the executives of diabetes. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - An Automatic Detection of Blood Vessel in Retinal Images Using Convolution Neural Network for Diabetic Retinopathy Detection
A1  - Raja, C
A1  - Balaji, L
Y1  - 2019///
KW  -  Adaptive histogram equalization; Adaptive histogram equalizations (AHE); Blood vessel segmentation; Convolution neural network; Diabetic retinopathy; Fuzzy C means clustering; Pre-processing step; Texture feature extraction
KW  -  Feature extraction
KW  - Blood; Blood vessels; Classification (of information); Convolution; Equalizers; Extraction; Eye protection; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Graphic methods; Image enhancement; Image segmentation; Ophthalmology; Support vector machines; Textures; Vision
PB  - Pleiades Publishing
JF  - Pattern Recognition and Image Analysis
VL  - 29
IS  - 3
SP  - 533
EP  - 545
DO  - 10.1134/S1054661819030180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073068394&doi=10.1134%2FS1054661819030180&partnerID=40&md5=118e2473d5255eedce2add9be4f62da7
N1  - cited By 15
N2  - Abstract: Diabetes is a typical chronic disease that may remind to numerous complications. Since the diabetic patients, the diabetic retinopathy (DR) is standout amongst the most serious of these inconveniences and also most steady reasons of vision loss. Automatic detection of diabetic retinopathy at early stage is helping the ophthalmologist to treat the affected patient and avoid vision loss. Therefore, in this paper, we develop an efficient automatic diabetic detection in retinal images using convolution neural network. The suggested system mainly comprises of five modules such as (i) preprocessing, (ii) blood vessel segmentation, (iii) exudates segmentation, (iv) texture feature extraction, and (v) diabetic detection. At first, the preprocessing step is carried out using adaptive histogram equalization (AHE) for enhancing the input retinal image. Consequently, blood vessel segmentation and exudates segmentation are done using convolution neural network (CNN) and fuzzy c-means clustering (FCM) respectively. Then, texture features are extracted from blood vessel and exudates. After the feature extraction, the diabetic classification is done with the help of support vector machine. The experimental results demonstrate that the proposed approach accomplishes better diabetic detection result (accuracy, sensitivity, and specificity) compared to other approaches. © 2019, Pleiades Publishing, Ltd.
ER  - 
TY  - JOUR
T1  - Detection of microaneurysms in diabetic fundus images using preprocessing techniques
A1  - Reshma Chand, C P
A1  - Dheeba, J
Y1  - 2016///
PB  - Serials Publications
JF  - International Journal of Control Theory and Applications
VL  - 9
IS  - 7
SP  - 3269
EP  - 3274
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988892604&partnerID=40&md5=44ba6910a57c3129ab3abf2769770b33
N1  - cited By 0
N2  - Diabetic retinopathy is one of the important reasons for vision loss that arises due to diabetes. Non-proliferative diabetic retinopathy is the preliminary stage of diabetic retinopathy disease which shows initial symptoms in the retina of human eye. In this paper, Microaneurysms are detected using different preprocessing steps and mathematical morphology techniques. Histogram stretching and CLAHE are used to give uniform illumination to the poor quality images. Morphology techniques are used remove the optic disc which may be misclassified as noise and to provide better visibility of Microaneurysms. This method provides us with better result inorder to identify Microaneurysms clearly without any misclassification. © International Science Press.
ER  - 
TY  - CONF
T1  - Identification of diabetes in pancreatic organs using iridology
A1  - Adelina, D C
A1  - Sigit, R
A1  - Harsono, T
A1  - Rochmad, M
ED  - Bagar F.N.C. Zainudin A., Al Rasyid M U H Briantoro H Akbar Z F
Y1  - 2017///
KW  -  Adaptive median filtering; Gray level co-occurrence matrix; Hough circles; Input systems; Iridology; Region of interest; Segmentation process; Transform methods
KW  -  Intelligent computing
KW  - Adaptive filters; Blood pressure; Image processing; Image segmentation; Median filters; Medical problems
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - International Electronics Symposium on Knowledge Creation and Intelligent Computing, IES-KCIC 2017
VL  - 2017-January
SP  - 114
EP  - 119
SN  - 9781538607169
DO  - 10.1109/KCIC.2017.8228573
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046537091&doi=10.1109%2FKCIC.2017.8228573&partnerID=40&md5=47ac00f7f566a397d659b0b6822e9987
N1  - cited By 12
N2  - Diabetes is a general disease often infected in humans. Many ways to detect diabetes, one of them is checking blood pressure, but this way is not effective, because it takes blood first and take a lot of time. Iridology is one way analysis health based on the iris. Therefore we need a tool used to identify pancreatic damage as an indication of diabetes through iridology. Load image is the first step to identify pancreatic organs based on the iris. The eye image that we used as the input system comes from the eye clinic database. The next step is adaptive median filtering used in the process preprocessing to reduce the noise on the image. After that the next step is segmentation process using hough circle transform method. The results of segmentation will be normalized and take the Region of interest. ROI will be done feature extraction by using GLCM (Gray Level Co-Occurrence Matrix). To know the condition of pancreas organ using backpropagation method. © 2017 IEEE.
ER  - 
TY  - CONF
T1  - Predicting glucose levels in patients with type1 diabetes based on physiological and activity data
A1  - Vahedi, M R
A1  - MacBride, K B
A1  - Wunsik, W
A1  - Kim, Y
A1  - Fong, C
A1  - Padilla, A J
A1  - Pourhomayoun, M
A1  - Zhong, A
A1  - Kulkarni, S
A1  - Arunachalam, S
A1  - Jiang, B
Y1  - 2018///
KW  -  Blood glucose; Blood glucose level; Insulin delivery system; Missing value imputation; Nutrition informations; Physiological measurement; Prediction accuracy; Predictive modeling
KW  -  Glucose
KW  - Blood; Feature extraction; Forecasting; Glucose sensors; Health care; Insulin; Learning algorithms; Learning systems; Medical problems; Physiological models; Predictive analytics
PB  - Association for Computing Machinery, Inc
JF  - Proceedings of the 8th ACM MobiHoc 2018 Workshop on Pervasive Wireless Healthcare Workshop, MobileHealth 2018
SN  - 9781450358484
DO  - 10.1145/3220127.3220133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052215000&doi=10.1145%2F3220127.3220133&partnerID=40&md5=1b823cd8932ff82b555c3b70b8d6d703
N1  - cited By 9
N2  - Managing blood glucose levels for type 1 diabetes patients is an absolute necessity to better glycemic control. In this paper, we present a predictive model that usesphysiological measurements and physical activity to predict continuous glucoselevels and help patients reduce and prevent hyperglycemia and hypoglycemia exposure, conditions that are harmful to patient health.The data of this research includes 4 months of physiological measurements, physical activity, and nutrition information collected from 93 patients with diabetes using the Medtronic MiniMed™ 530G insulin delivery system with Enlite™ sensor. Afterdata preprocessing, missing value imputation, feature extraction, and feature selection, a set of 180 featureswere derived to represent the raw data. Then, an appropriatepredictive model was developedbased on machine-learning algorithms to predict continuous glucose levels. The prediction accuracy and error have been calculated to evaluate the performance of the system. The results demonstratedthat the predictedglucose levelsclosely followed the actual sensor glucose (SG)values measuredby subcutaneous glucose sensor. © 2018 ACM.
ER  - 
TY  - JOUR
T1  - Automated detection of diabetic retinopathy in fundus images using fused features
A1  - Bibi, I
A1  - Mir, J
A1  - Raja, G
Y1  - 2020///
KW  -  Algorithms; Databases
KW  -  Article; automation; contrast enhancement; dense scale invariant feature transform; diabetic retinopathy; histogram of oriented gradient; human; image analysis; image processing; local binary pattern; local ternary pattern; mathematical computing; mathematical model; ophthalmoscopy; process design; sensitivity and specificity; support vector machine; visual system parameters; algorithm; diabetes mellitus; eye fundus; factual database
KW  -  Automated detection; Classification accuracy; Diabetic retinopathy; Histogram of oriented gradients (HOG); Kernel based classifiers; Local binary patterns; Local ternary patterns (LTP); Scale invariant feature transforms
KW  -  Factual; Diabetes Mellitus; Diabetic Retinopathy; Fundus Oculi; Humans
KW  -  Image segmentation
KW  - Blood vessels; Classification (of information); Database systems; Eye protection; Medical imaging; Support vector machines
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Physical and Engineering Sciences in Medicine
VL  - 43
IS  - 4
SP  - 1253
EP  - 1264
DO  - 10.1007/s13246-020-00929-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091311168&doi=10.1007%2Fs13246-020-00929-5&partnerID=40&md5=395dafff03976fe06d4d0b18740fa1d2
N1  - cited By 5
N2  - Diabetic retinopathy (DR) is one of the severe eye conditions due to diabetes complication which can lead to vision loss if left untreated. In this paper, a computationally simple, yet very effective, DR detection method is proposed. First, a segmentation independent two-stage preprocessing based technique is proposed which can effectively extract DR pathognomonic signs; both bright and red lesions, and blood vessels from the eye fundus image. Then, the performance of Local Binary Patterns (LBP), Local Ternary Patterns (LTP), Dense Scale-Invariant Feature Transform (DSIFT) and Histogram of Oriented Gradients (HOG) as a feature descriptor for fundus images, is thoroughly analyzed. SVM kernel-based classifiers are trained and tested, using a 5-fold cross-validation scheme, on both newly acquired fundus image database from the local hospital and combined database created from the open-sourced available databases. The classification accuracy of 96.6% with 0.964 sensitivity and 0.969 specificity is achieved using a Cubic SVM classifier with LBP and LTP fused features for the local database. More importantly, in out-of-sample testing on the combined database, the model gives an accuracy of 95.21% with a sensitivity of 0.970 and specificity of 0.932. This indicates the proposed model is very well-fitted and generalized which is further corroborated by the presented train-test curves. © 2020, Australasian College of Physical Scientists and Engineers in Medicine.
ER  - 
TY  - JOUR
T1  - Textureless macula swelling detection with multiple retinal fundus images
A1  - Giancardo, L
A1  - Meriaudeau, F
A1  - Karnowski, T P
A1  - Tobin Jr., K W
A1  - Grisan, E
A1  - Favaro, P
A1  - Ruggeri, A
A1  - Chaum, E
Y1  - 2011///
KW  -  Algorithms; Fundus Oculi; Humans; Image Processing
KW  -  Cameras; Diagnosis; Image reconstruction; Image registration; Imaging systems; Mathematical operators; Ophthalmology; Optical data processing; Optical flows; Personnel training
KW  -  Computer-Assisted; Macula Lutea; Ophthalmoscopy; Point-of-Care Systems; Telemedicine
KW  -  Medical imaging
KW  -  algorithm; article; calibration; camera; digital fundus camera; exudate; eye fundus; eye photography; human; image processing; image reconstruction; measurement error; multiple retinal fundus image; optic flow; quantitative analysis; retina macula edema
KW  - Bio-medical image processing; diabetes; Image motion analysis; Medical diagnostics; stereo image processing
JF  - IEEE Transactions on Biomedical Engineering
VL  - 58
IS  - 3 PART 2
SP  - 795
EP  - 799
DO  - 10.1109/TBME.2010.2095852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952016087&doi=10.1109%2FTBME.2010.2095852&partnerID=40&md5=3f81758db3baa1e40b957afc8b0b768c
N1  - cited By 37
N2  - Retinal fundus images acquired with nonmydriatic digital fundus cameras are versatile tools for the diagnosis of various retinal diseases. Because of the ease of use of newer camera models and their relatively low cost, these cameras can be employed by operators with limited training for telemedicine or point-of-care (PoC) applications. We propose a novel technique that uses uncalibrated multiple-view fundus images to analyze the swelling of the macula. This innovation enables the detection and quantitative measurement of swollen areas by remote ophthalmologists. This capability is not available with a single image and prone to error with stereo fundus cameras. We also present automatic algorithms to measure features from the reconstructed image, which are useful in PoC automated diagnosis of early macular edema, e.g., before the appearance of exudation. The technique presented is divided into three parts: first, a preprocessing technique simultaneously enhances the dark microstructures of the macula and equalizes the image; second, all available views are registered using nonmorphological sparse features; finally, a dense pyramidal optical flow is calculated for all the images and statistically combined to build a naive height map of the macula. Results are presented on three sets of synthetic images and two sets of real-world images. These preliminary tests show the ability to infer a minimum swelling of 300 μ and to correlate the reconstruction with the swollen location. © 2006 IEEE.
ER  - 
TY  - CONF
T1  - An automated approach for diagnosing diabetic retinopathy in retinal fundus images
A1  - Prasannan, V
A1  - Kumar, C S
A1  - Deepa, V
Y1  - 2018///
KW  -  Adaptive histogram equalizations (AHE); Computer aided diagnosis systems; Diabetic retinopathy; Feature extraction and classification; Fundus image; K nearest neighbours (k-NN); Mean and standard deviations; Support vector machines algorithms
KW  -  Image denoising
KW  - Computer aided diagnosis; Extraction; Eye protection; Feature extraction; Gabor filters; K-means clustering; Mathematical morphology; Nearest neighbor search; Neural networks; Ophthalmology; Pixels; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2018 3rd IEEE International Conference on Recent Trends in Electronics, Information and Communication Technology, RTEICT 2018 - Proceedings
SP  - 381
EP  - 386
SN  - 9781538624401
DO  - 10.1109/RTEICT42901.2018.9012542
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081786868&doi=10.1109%2FRTEICT42901.2018.9012542&partnerID=40&md5=bc2db0b83bbdc99f81cbae53ea26611e
N1  - cited By 0
N2  - Diabetes mellitus and its associated complications, including diabetic retinopathy (DR) have been identified as a growing global public health problem. Early treatment and constant monitoring of DR can prevent 90 percent of visual loss. Manual screening of fundus images by ophthalmologists is tedious and time consuming. Hence, a computer-aided diagnosis system can significantly act as an efficient solution for the time consuming detection process. The propounded algorithm includes various stages such as pre-processing, feature extraction and classification. In literature, different approaches for preprocessing such as dual guassian, second derivative of Guassian and Gabor filter, mathematical morphology, K-mean clustering algorithms etc. are employed. In the proposed algorithm, preprocessing was done using Adaptive Histogram Equalization (AHE) and image de-noising using non-local means (NLM) filter. NLM takes the means of all the neighbouring pixels and weightage is given by checking the similarity between these pixels to the target pixel. Laplacian edge detection, by identifying the zero crossings of the second derivatives of the image intensity was used in both the methods. In the feature extraction stage, features like HOG, area, perimeter, mean and standard deviation are extracted. Lastly, Support Vector Machines (SVM) algorithm, k-nearest neighbours (KNN) and Artificial Neural Network (ANN) was used for the categorization of the images to healthy and diseased. The comparison on the functioning of AHE and NLM validates that NLM outperforms AHE. Parameters like accuracy, error rate, specificity, and precision were used to analyze the working of the classifiers. SVM with 0.875 accuracy, 0.12 error rate, 0.75 specificity and 0.80 precision outperformed well when analogized to the other methods. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Investigating image enhancement methods for better classification of retinal blood vessels into arteries and veins
A1  - Mirsharif, Q
A1  - Tajeripour, F
Y1  - 2012///
KW  -  Algorithms; Artificial intelligence; Diagnosis; Feature extraction; Ophthalmology; Signal processing
KW  -  Image enhancement
KW  - Acquisition process; Automatic classification; Automatic tools; Early diagnosis; Efficient systems; Enhancement techniques; Histogram equalizations; Multi-scale Retinex; Nonuniformity; Pre-processing step; Retinal blood vessels; Retinal image; Width ratio
JF  - AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing
SP  - 591
EP  - 597
SN  - 9781467314794
DO  - 10.1109/AISP.2012.6313815
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869166703&doi=10.1109%2FAISP.2012.6313815&partnerID=40&md5=42104a3d06508fb44d0f126b556d7a8c
N1  - cited By 9
N2  - Developing an automatic tool for classification of retinal blood vessels into arteries and veins has gone under special attention recently due to its importance in early diagnosis of several diseases namely, diabetes, hypertension and stroke. Indeed such pathologies make alternations in artery or vein vessel tree leading to an abnormal arteriolar-to-venular width ratio (AVR). To measure AVR, arteries and veins must be carefully separated. For this purpose, a few methods have been proposed in the literature most of which are based on feature extraction. However, different factors such as non-uniformity of lightness during the image acquisition process degrade the quality of retinal images which in turn affect the results of computer algorithms. In this paper, we investigate a number of image enhancement techniques for improving the quality of retinal images considering the specific characteristics of those images. Experimental results demonstrate the significant role of image enhancement as a preprocessing step in developing an efficient system for automatic classification of retinal blood vessels into arteries and veins. © 2012 IEEE.
ER  - 
TY  - CONF
T1  - The development of Diabetics-oriented Telemedical Information System
A1  - Zhao, Y
A1  - Wang, Z
A1  - Tang, Y
A1  - Zhao, M
A1  - Chen, S
A1  - Hou, J
A1  - Ke, M
Y1  - 2011///
KW  -  Condition monitoring
KW  -  Information systems; Patient treatment; Physiological models; Physiology; Telemedicine
KW  - Abnormal data; Bluetooth technology; Causes of death; Complex structure; Diabetics; Genetic factors; Health monitoring; Healthcare services; Medical services; Physiological parameters; Physiological signals; Platform systems; Pulse- wave; Telemedical information systems; Wearable sensing
JF  - 2011 IEEE International Conference on Information and Automation, ICIA 2011
SP  - 720
EP  - 725
SN  - 9781457702686
DO  - 10.1109/ICINFA.2011.5949088
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051485377&doi=10.1109%2FICINFA.2011.5949088&partnerID=40&md5=7ff4d68fe63fdb9695cbed6a75230215
N1  - cited By 0
N2  - The number of diabetes keeps increasing for many years, which has become one of the world's leading causes of death. Besides, the greatest hazards of diabetes are constituted by its complications such as hypertension and cardiovascular problems. As the treatment to diabetes usually acquires for quite a long time, the daily healthcare of those patients is a costly work. In this paper, a development of Diabetics-oriented Tele-medical Information System (DTMIS) is introduced. This DTMIS provides users with comprehensive, convenient and timely health monitoring and medical service. DTMIS can realize the real-time telemedicine monitoring of a diabetic's physiological parameters. The system is a complex structure combining the use of wearable sensing shirt, Bluetooth technology and GSM networking. And we have implemented a physiological signal preprocessing method that can detect abnormal data and provides preliminary suggestions to patients here. In the end, it can be concluded that the implementation of this platform system will provide a complete health condition monitoring and healthcare service. © 2011 IEEE.
ER  - 
TY  - JOUR
T1  - Electronic nose for detecting multilevel diabetes using optimized deep neural network
A1  - Sarno, R
A1  - Izza Sabilla, S
A1  - Rahman Wijaya, D
A1  - Hariyanto
Y1  - 2020///
KW  -  Blood test; Blood-glucose monitors; Chronic disease; Electronic nose (e-nose); Error rate; Optimal signals; Testing method; Testing procedure
KW  -  Deep neural networks
KW  - Blood; Electronic nose; Medical problems; Testing
PB  - International Association of Engineers
JF  - Engineering Letters
VL  - 28
IS  - 1
SP  - 31
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081546916&partnerID=40&md5=1ea059dc16b10251ba159b200cecc13f
N1  - cited By 19
N2  - Diabetes is a chronic disease which is still a major issue in the world. The common testing methods generally used to detect diabetes are urine dipstick, laboratory blood tests, and blood glucose monitors. However, those testing procedures are often perceived as painful and inconvenient for the patients. In this context, this study proposes an electronic nose (e-nose) for detecting three classes of diabetes (healthy, prediabetes, and diabetes) based on a patient breath. The proposed e-nose system is called DENS, which utilizes an optimized deep neural network for the classiflcation. DENS also attempts to enhance accuracy and to reduce the error rate from previous studies. Therefore, this paper has three contributions: (i) the optimal gas sensors for capturing patient breaths; (ii) the optimal signal preprocessing; (iii) the fine-tuned parameters of deep neural network (DNN) for classifying multilevel diabetes. The proposed system successfully detected multilevel diabetes with an accuracy of 96.29% and showed a minimum classiflcation error of 0.050. © 2020, International Association of Engineers. All rights reserved.
ER  - 
TY  - CONF
T1  - Automated identification of diabetic retinopathy stages using support vector machine
A1  - Du, N
A1  - Li, Y
Y1  - 2013///
KW  -  Automated identification; Computer-based approach; Diabetic retinopathy; Exudates; Fundus image; Morphological processing; Retinal blood vessels; Texture analysis method
KW  -  Eye protection
KW  - Blood vessels; Support vector machines
PB  - IEEE Computer Society
JF  - Chinese Control Conference, CCC
SP  - 3882
EP  - 3886
SN  - 9789881563835
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890462765&partnerID=40&md5=21d8ef500766dc38e4d7f1e6cdd42186
N1  - cited By 29
N2  - Diabetic retinopathy (DR) is a condition where the retina is damaged due to fluid leaking from the blood vessels into the retina. The main stages of diabetic retinopathy are non-proliferate diabetes retinopathy (NPDR) and proliferate diabetes retinopathy (PDR). Early detection of diabetic retinopathy is crucial to prevent blindness. In this work, we have proposed a computer based approach for the detection of diabetic retinopathy stage using color fundus images. Image preprocessing, morphological processing techniques and texture analysis methods are applied on the fundus images to detect the features such as area of blood vessels, hard exudates and the contrast, homogeneity. The features are fed to the support vector machine (SVM). We demonstrate a classification accuracy of 93%, sensitivity of 90% and specificity of 100%. © 2013 TCCT, CAA.
ER  - 
TY  - JOUR
T1  - Efficient diabetes mellitus prediction with grid based random forest classifier in association with natural language processing
A1  - Abokhzam, A A
A1  - Gupta, N K
A1  - Bose, D K
Y1  - 2021///
KW  -  Diabetes mellitus; Diabetic patient; Grid-search algorithm; Language processing; Medical health; NAtural language processing; Random forest classifier; Training phase
KW  -  Natural language processing systems
KW  - Artificial life; Blood; Decision trees; Diagnosis; Feature extraction; Forecasting; Glucose; Logistic regression; Random forests; Support vector regression
PB  - Springer
JF  - International Journal of Speech Technology
VL  - 24
IS  - 3
SP  - 601
EP  - 614
DO  - 10.1007/s10772-021-09825-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103186112&doi=10.1007%2Fs10772-021-09825-z&partnerID=40&md5=42be21848ffb284ebf8409ee0ac07971
N1  - cited By 5
N2  - Human body turns the food consumed into energy, but when insulin doesn’t act in its way to convert the blood glucose into energy, then the glucose remains in the bloodstream and causes a life-threatening health issue called Diabetes Mellitus or Diabetes. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future will be suffering from diabetes. With the rapid development of machine learning, it has been applied to many aspects of medical health. So, for efficiently and effectively diagnosing the Diabetes Mellitus, a method is proposed using the ML Grid Search algorithm. In this method, Pima Indian Diabetic Dataset is used. This system has two phases: The training phase includes preprocessing, feature selection and instance evaluation is done and the test phase includes preprocessing, instance evaluation and disease prediction is done. For feature selection, the random forest feature selection is used and for classification, support vector regression, logistic regression and grid based random forest classifier is used. The proposed method of predicting the diabetes, the accuracy is almost 95.7% which is higher when compared to previous methods. Additionally, the proposed system provides an ability to the users to understand the resulting scenario over any language with the help of language processing in results. The Natural Language Processing concept is adapted over the proposed approach to identify the features of the resulting text and perform the language processing and display the exact language-oriented data to the users without any hurdle. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Dimensionality reduction in data mining: A Copula approach
A1  - Houari, R
A1  - Bounceur, A
A1  - Kechadi, M.-T.
A1  - Tari, A.-K.
A1  - Euler, R
Y1  - 2016///
KW  -  Copulas; Data preprocessing; Dimensionality reduction; High dimensional spaces; Human activity recognition; Machine learning repository; Multi dimensional; Multidimensional data
KW  -  Data reduction
KW  - Artificial intelligence; Classification (of information); Data handling; Data mining; Learning systems
PB  - Elsevier Ltd
JF  - Expert Systems with Applications
VL  - 64
SP  - 247
EP  - 260
DO  - 10.1016/j.eswa.2016.07.041
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980367787&doi=10.1016%2Fj.eswa.2016.07.041&partnerID=40&md5=ea2a124d478bbf9d3ba2664d0b72292d
N1  - cited By 54
N2  - The recent trends in collecting huge and diverse datasets have created a great challenge in data analysis. One of the characteristics of these gigantic datasets is that they often have significant amounts of redundancies. The use of very large multi-dimensional data will result in more noise, redundant data, and the possibility of unconnected data entities. To efficiently manipulate data represented in a high-dimensional space and to address the impact of redundant dimensions on the final results, we propose a new technique for the dimensionality reduction using Copulas and the LU-decomposition (Forward Substitution) method. The proposed method is compared favorably with existing approaches on real-world datasets: Diabetes, Waveform, two versions of Human Activity Recognition based on Smartphone, and Thyroid Datasets taken from machine learning repository in terms of dimensionality reduction and efficiency of the method, which are performed on statistical and classification measures. © 2016 Elsevier Ltd
ER  - 
TY  - CONF
T1  - A Novel approach for classifying diabetes' patients based on imputation and machine learning
A1  - Driss, K
A1  - Boulila, W
A1  - Batool, A
A1  - Ahmad, J
Y1  - 2020///
KW  -  Accurate prediction; Diagnostic measurements; Imputation process; K-nearest neighbors; Pre-processing of data; Real-world datasets; Receiver operating characteristics; Research studies
KW  -  Diagnosis
KW  - Data Analytics; Forecasting; Machine learning; Nearest neighbor search
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 International Conference on UK-China Emerging Technologies, UCET 2020
SN  - 9781728194882
DO  - 10.1109/UCET51115.2020.9205378
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094321626&doi=10.1109%2FUCET51115.2020.9205378&partnerID=40&md5=d6c20a052e7d2ffdac0e39c9086dc775
N1  - cited By 7
N2  - Since the last decade, many research studies has been conducted on machine learning-based diabetes disease prediction using diagnostic measurement. However, the main challenge in machine learning-based diabetes disease prediction is the preprocessing of data, which contains, in most cases missing values and outliers. For data analytics and accurate prediction, data cleansing is highly desired and recommended. The goal of this study is to predict diabetic patients using realworld datasets. The proposed approach is based on three main steps: cleansing, modelling, and storytelling. In the first step, an imputation process is conducted to remove missing values. Then, k-nearest neighbor's algorithm is applied to classify patients. To evaluate the performance of the proposed approach, two criteria, namely the F1 score and the Receiver Operating Characteristic (ROC) has been used. F1 score and ROC curve show a clear distinction between diabetic and nondiabetic patients. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - Enhancing parameter precision and the minimal modeling approach in type I diabetes
A1  - Fernandez, M
A1  - Acosta, D
A1  - Villasana, M
A1  - Streja, D
Y1  - 2004///
KW  -  Data preprocessing; Insulin delivery; Minimal model; Subcutaneous route; Type I diabetes
KW  -  Mathematical models
KW  - Algorithms; Blood; Closed loop control systems; Computer software; Constraint theory; Data processing; Disease control; Glucose; Identification (control systems); Insulin; Optimization; Parameter estimation; Patient monitoring; Perturbation techniques; Sensors; Signal processing
JF  - Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings
VL  - 26 II
SP  - 797
EP  - 800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144273256&partnerID=40&md5=3d403ad0eaa10f7488608c4b5962a6f9
N1  - cited By 10
N2  - An evaluation of a simple model including external perturbations was evaluated for its usefulness in predicting diabetic patients' behavior. The model proposed has been derived from Cobelli and Marl's comprehensive model and is structurally identifiable. The optimization was carried out on data gathered using CGMS (Medtronic MiniMed) on 3 subjects. The model was also optimized after performing a model-based signal enhancement. The results obtained before and after signal enhancement showed a promising reduction in the variation coefficient of the estimated parameters. This reduction is expected to be useful in the design of a closed loop controller for subcutaneous insulin delivery.
ER  - 
TY  - CONF
T1  - Naive Bayes for statlog heart database with consideration of data specifics
A1  - Bohacik, J
A1  - Zabovsky, M
ED  - Korecko S. Szakal A., Novitzka V
Y1  - 2018///
KW  -  10-fold cross-validation; Classification algorithm; Data preprocessing technique; Discretization algorithms; Heart disease; Naive bayes; Probability approach; Supervised discretization
KW  -  Heart
KW  - Cardiology; Classification (of information); Classifiers; Diseases
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2017 IEEE 14th International Scientific Conference on Informatics, INFORMATICS 2017 - Proceedings
VL  - 2018-January
SP  - 35
EP  - 39
SN  - 9781538608890
DO  - 10.1109/INFORMATICS.2017.8327218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050635112&doi=10.1109%2FINFORMATICS.2017.8327218&partnerID=40&md5=3950a9b6620b4a51bdae81cf7c146956
N1  - cited By 2
N2  - Heart disease belongs to one of the main reasons for mortality nowadays and it is expected to become worse due to factors such as aging, diabetes and obesity. In addition, existing misdiagnosis of patients reporting heart related ailment worsens this situation even further. In the paper, a probability approach to recognition of heart disease is analyzed with the employment of Naive Bayes on Statlog Heart Database and with the search of data preprocessing techniques for its improvement. A discretization algorithm of numerical attributes which takes the specifics of given heart disease patients into account is presented. It is based on supervised discretization with consideration of Equal Frequency Discretization. Experiments making use of 10-fold cross-validation show improvements of accuracy which are measured with sensitivity, specificity and their sum and the results are also compared with other classification algorithms. © 2017 IEEE.
ER  - 
TY  - CONF
T1  - The new approach to automatic detection of Optic Disc from non-dilated retinal images
A1  - Vahabi, Z
A1  - Vafadoost, M
A1  - Gharibzadeh, Sh.
Y1  - 2010///
KW  -  Algorithms; Biomedical engineering; Blood vessels; Disks (structural components); Edge detection; Entropy; Eye protection; Image enhancement; Ophthalmology; Template matching
KW  -  Image matching
KW  - Diabetic retinopathy; Gradient; Morphlogical operation; Optic disk; Wavelet packet
JF  - 2010 17th Iranian Conference of Biomedical Engineering, ICBME 2010 - Proceedings
SN  - 9781424474844
DO  - 10.1109/ICBME.2010.5704960
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951747268&doi=10.1109%2FICBME.2010.5704960&partnerID=40&md5=8f4cea8cc1a95b001f37dbaf226fd228
N1  - cited By 7
N2  - Diabetic retinopathy is the commonest cause of blindness. Diabetes causes cataracts, Glaucoma and diabetic retinopathy. The Optic Disc is the exit point of retinal nerve fibers from the eye and the entrance and exit point for retinal blood vessels. The detection of Optic Disc is very essential to locate the various anatomical features in the retinal images. We describe a new filtering approach in the wavelet domain for image preprocessing. Sobel edge detection, Texture Analysis, Intensity and Template matching was used to detect Optic Disc. The proposed algorithm is tested on 150 images of Messidor dataset. Experimental results indicates that we are able to achieve 87.54% sensitivity, 99.76% specificity and 99.81% accuracy. © 2010 IEEE.
ER  - 
TY  - JOUR
T1  - Prediction analysis of diabetic type-II-Foot ulcer data using r and data classification algorithm
A1  - Pushpaleela, R C
A1  - Padmajavalli, R
Y1  - 2018///
PB  - Institute of Advanced Scientific Research, Inc.
JF  - Journal of Advanced Research in Dynamical and Control Systems
VL  - 10
IS  - 3 Special Issue
SP  - 1198
EP  - 1206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050364868&partnerID=40&md5=fe397eec44355e7e9f97e51ccd876df0
N1  - cited By 0
N2  - Data mining approach helps to analyze patient diseases. Diabetes Mellitus is a chronic disease to affect various organs of the human body. Early prediction can save human life and can take control over the diseases. The present work emphases on analysis of diabetes through SVM algorithm with statistical implication using R. Diabetes is a common disease with all age group on the world. The main aim of this research paper is to study and discuss about the SVM implementation using R with different kinds of medical datasets. For classification is performed by SVM in R. This paper will help for early detection of the complications which might help in timely treatment of individuals. For data preprocessing the real time medical data set used Manual, R Studio and WEKA tool. © 2018, Institute of Advanced Scientific Research, Inc. All rights reserved.
ER  - 
TY  - JOUR
T1  - Ensemble models of cutting-edge deep neural networks for blood glucose prediction in patients with diabetes
A1  - Tena, F
A1  - Garnica, O
A1  - Lanchares, J
A1  - Hidalgo, J I
Y1  - 2021///
KW  -  Blood glucose; Blood glucose prediction; Cutting edges; Deep learning; Ensemble models; Ensemble neural network; Network-based modeling; Neural-networks; Performance; Prediction horizon
KW  -  Forecasting
KW  - Blood; Deep neural networks; Glucose
PB  - MDPI
JF  - Sensors
VL  - 21
IS  - 21
DO  - 10.3390/s21217090
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117914446&doi=10.3390%2Fs21217090&partnerID=40&md5=a7f92d0b7c51a55d94c27dc3f6be177c
N1  - cited By 1
N2  - This article proposes two ensemble neural network-based models for blood glucose prediction at three different prediction horizons—30, 60, and 120 min—and compares their performance with ten recently proposed neural networks. The twelve models’ performances are evaluated under the same OhioT1DM Dataset, preprocessing workflow, and tools at the three prediction horizons using the most common metrics in blood glucose prediction, and we rank the best-performing ones using three methods devised for the statistical comparison of the performance of multiple algorithms: scmamp, model confidence set, and superior predictive ability. Our analysis provides a comparison of the state-of-the-art neural networks for blood glucose prediction, estimating the model’s error, highlighting those with the highest probability of being the best predictors, and providing a guide for their use in clinical practice. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
ER  - 
TY  - JOUR
T1  - Automated identification of diabetic retinopathy stages using digital fundus images
A1  - Nayak, J
A1  - Bhat, P S
A1  - Acharya U, R
A1  - Lim, C M
A1  - Kagathi, M
Y1  - 2008///
KW  -  Computer-Assisted; Fluorescein Angiography; Humans
KW  -  Diabetic Retinopathy; Diagnosis
KW  -  computers and data processing; blood vessel; diabetic retinopathy; diagnostic accuracy; diagnostic imaging; eye disease; eye fundus; human; image analysis; morphology; photography; sensitivity and specificity; statistical significance
KW  - article; artificial neural network; automation
JF  - Journal of Medical Systems
VL  - 32
IS  - 2
SP  - 107
EP  - 115
DO  - 10.1007/s10916-007-9113-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549123754&doi=10.1007%2Fs10916-007-9113-9&partnerID=40&md5=0f91829fb5f863c68f059e08fdbc867e
N1  - cited By 217
N2  - Diabetic retinopathy (DR) is caused by damage to the small blood vessels of the retina in the posterior part of the eye of the diabetic patient. The main stages of diabetic retinopathy are non-proliferate diabetes retinopathy (NPDR) and proliferate diabetes retinopathy (PDR). The retinal fundus photographs are widely used in the diagnosis and treatment of various eye diseases in clinics. It is also one of the main resources for mass screening of diabetic retinopathy. In this work, we have proposed a computer-based approach for the detection of diabetic retinopathy stage using fundus images. Image preprocessing, morphological processing techniques and texture analysis methods are applied on the fundus images to detect the features such as area of hard exudates, area of the blood vessels and the contrast. Our protocol uses total of 140 subjects consisting of two stages of DR and normal. Our extracted features are statistically significant (p<0.0001) with distinct mean±SD as shown in Table 1. These features are then used as an input to the artificial neural network (ANN) for an automatic classification. The detection results are validated by comparing it with expert ophthalmologists. We demonstrated a classification accuracy of 93%, sensitivity of 90% and specificity of 100%. © 2007 Springer Science+Business Media, LLC.
ER  - 
TY  - JOUR
T1  - A Data-Driven Knowledge Acquisition System: An End-to-End Knowledge Engineering Process for Generating Production Rules
A1  - Ali, M
A1  - Ali, R
A1  - Khan, W A
A1  - Han, S C
A1  - Bang, J
A1  - Hur, T
A1  - Kim, D
A1  - Lee, S
A1  - Kang, B H
Y1  - 2018///
KW  -  Algorithm selection; Decision tree classification; Decision-tree algorithm; Features rankings; Knowledge acquisition systems; Knowledge acquisition tools; Production rules; User experience
KW  -  Data mining
KW  - Automation; Classification (of information); Data acquisition; Data handling; Data structures; Decision trees; Feature extraction; Knowledge acquisition; Knowledge engineering; Production; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Access
VL  - 6
SP  - 15587
EP  - 15607
DO  - 10.1109/ACCESS.2018.2817022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044097740&doi=10.1109%2FACCESS.2018.2817022&partnerID=40&md5=950c3db01af5f4f06eb98801dbf8bc70
N1  - cited By 17
N2  - Data-driven knowledge acquisition is one of the key research fields in data mining. Dealing with large amounts of data has received a lot of attention in the field recently, and a number of methodologies have been proposed to extract insights from data in an automated or semi-automated manner. However, these methodologies generally target a specific aspect of the data mining process, such as data acquisition, data preprocessing, or data classification. However, a comprehensive knowledge acquisition method is crucial to support the end-to-end knowledge engineering process. In this paper, we introduce a knowledge acquisition system that covers all major phases of the cross-industry standard process for data mining. Acknowledging the importance of an end-to-end knowledge engineering process, we designed and developed an easy-to-use data-driven knowledge acquisition tool (DDKAT). The major features of the DDKAT are: (1) a novel unified features scoring approach for data selection; (2) a user-friendly data processing interface to improve the quality of the raw data; (3) an appropriate decision tree algorithm selection approach to build a classification model; and (4) the generation of production rules from various decision tree classification models in an automated manner. Furthermore, two diabetes studies were performed to assess the value of the DDKAT in terms of user experience. A total of 19 experts were involved in the first study and 102 students in the artificial intelligence domain were involved in the second study. The results showed that the overall user experience of the DDKAT was positive in terms of its attractiveness, as well as its pragmatic and hedonic quality factors. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Diabetic retinopathy detection through artificial intelligent techniques: a review and open issues
A1  - Ishtiaq, U
A1  - Abdul Kareem, S
A1  - Abdullah, E.R.M.F.
A1  - Mujtaba, G
A1  - Jahangir, R
A1  - Ghafoor, H Y
Y1  - 2020///
KW  -  Convolutional neural network; Diabetic retinopathy; DIARETDB1; Image preprocessing; Transfer learning
KW  -  Deep learning
KW  - Automation; Convolution; Eye protection; Image enhancement; Machine learning; Neural networks; Textures
PB  - Springer
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 21-22
SP  - 15209
EP  - 15252
DO  - 10.1007/s11042-018-7044-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059694741&doi=10.1007%2Fs11042-018-7044-8&partnerID=40&md5=78ab99181945f4e2c00419ace5b22021
N1  - cited By 18
N2  - Diabetic Retinopathy (DR) is the disease caused by uncontrolled diabetes that may lead to blindness among the patients. Due to the advancements in artificial intelligence, early detection of DR through an automated system is more beneficial over the manual detection. At present, there are several published studies on automated DR detection systems through machine learning or deep learning approaches. This study presents a review on DR detection techniques from five different aspects namely, datasets, image preprocessing techniques, machine learning-based approaches, deep learning-based approaches, and performance measures. Moreover, it also presents the authors’ observation and significance of the review findings. Furthermore, we also discuss nine new research challenges in DR detection. After a rigorous selection process, 74 primary publications were selected from eight academic databases for this review. From the selected studies, it was observed that many public datasets are available in the field of DR detection. In image preprocessing techniques, contrast enhancement combined with green channel extraction contributed the most in classification accuracy. In features, shape-based, texture-based and statistical features were reported as the most discriminative in DR detection. The Artificial Neural Network was proven eminent classifier compared to other machine learning classifiers. In deep learning, Convolutional Neural Network outperformed compared to other deep learning networks. Finally, to measure the classification performance, accuracy, sensitivity, and specificity metrics were mostly employed. This review presents a comprehensive summary of DR detection techniques and will be proven useful for the community of scientists working in the field of automated DR detection techniques. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Accelerating retinal fundus image classification using artificial neural networks (ANNs) and reconfigurable hardware (FPGA)
A1  - Ghani, A
A1  - See, C H
A1  - Sudhakaran, V
A1  - Ahmad, J
A1  - Abd-Alhameed, R
Y1  - 2019///
PB  - MDPI AG
JF  - Electronics (Switzerland)
VL  - 8
IS  - 12
DO  - 10.3390/electronics8121522
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076766230&doi=10.3390%2Felectronics8121522&partnerID=40&md5=621229e0edb63d297ec69fd4f9e038bf
N1  - cited By 17
N2  - Diabetic retinopathy (DR) and glaucoma are common eye diseases that affect a blood vessel in the retina and are two of the leading causes of vision loss around the world. Glaucoma is a common eye condition where the optic nerve that connects the eye to the brain becomes damaged, whereas DR is a complication of diabetes caused by high blood sugar levels damaging the back of the eye. In order to produce an accurate and early diagnosis, an extremely high number of retinal images needs to be processed. Given the required computational complexity of image processing algorithms and the need for high-performance architectures, this paper proposes and demonstrates the use of fully parallel field programmable gate arrays (FPGAs) to overcome the burden of real-time computing in conventional software architectures. The experimental results achieved through software implementation were validated on an FPGA device. The results showed a remarkable improvement in terms of computational speed and power consumption. This paper presents various preprocessing methods to analyse fundus images, which can serve as a diagnostic tool for detection of glaucoma and diabetic retinopathy. In the proposed adaptive thresholding-based preprocessing method, features were selected by calculating the area of the segmented optic disk, which was further classified using a feedforward neural network (NN). The analysis was carried out using feature extraction through existing methodologies such as adaptive thresholding, histogram and wavelet transform. Results obtained through these methods were quantified to obtain optimum performance in terms of classification accuracy. The proposed hardware implementation outperforms existing methods and offers a significant improvement in terms of computational speed and power consumption. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
ER  - 
TY  - JOUR
T1  - Unsupervised segmentation of retinal vessels from fundus Fluorescein angiogram images
A1  - Abirami, S
A1  - Swapna, T R
A1  - Pulari, S R
A1  - Chakraborty, C
Y1  - 2015///
PB  - Research India Publications
JF  - International Journal of Applied Engineering Research
VL  - 10
IS  - 69
SP  - 38
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942427216&partnerID=40&md5=54256132b223ef87ea714b4b59a40ae8
N1  - cited By 1
N2  - Diabetic retinopathy is the most common reason for visual disability particularly in type II diabetes. Diabetic maculopathy is retinopathy affecting the macular region of the eye. Fundus Fluorescein Angiograms (FFA) is one of the modalities for detection of pathologies associated with diabetic maculopathy. This paper evaluates a set of machine learning algorithms like K-Means, Fuzzy C-Means, Rough Fuzzy C-Means and Expectation Maximization for segmenting the retinal vessels of FFA image, which is a preliminary step for automating the process of detection and classification of different forms of maculopathy. The ground truth of Fundus images were taken from the DRIVE database for evaluating the segmented results. The similarity measures like Jaccard and Dice coefficients were used to compare the segmented results with the ground truth images and we found that Fuzzy C-Means algorithm gives an accuracy of 96%. When comparing with existing methods in the literature, our proposed method has less user intervention, requires less number of parameters and does not require the identification of optic disc to locate the retinal vessels. This method also does not require any preprocessing like contrast enhancement and noise removal. © Research India Publications.
ER  - 
TY  - JOUR
T1  - Exudates detection- A potential marker for diabetic retinopathy and its complications
A1  - Sumathy, B
A1  - Poornachandra, S
Y1  - 2015///
PB  - Research India Publications
JF  - International Journal of Applied Engineering Research
VL  - 10
IS  - 55
SP  - 555
EP  - 562
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942855591&partnerID=40&md5=286726dedc0b1314343e8afcbd5482d3
N1  - cited By 1
N2  - Diabetic Retinopathy is a complication of Diabetes and causes damage to the blood vessels and other organs of retina. Exudates are the primary and potential abnormal signs of Diabetic Retinopathy. In this paper , an automated exudates detection algorithm is proposed and implemented. In many situations, the exudates detected becomes the potential marker for many systemic organ diseases like cardio vascular diseases, and kidney diseases. Exudates detection could also predict and indicate the systemic organs disease and its complications. The method involves better preprocessing techniques followed by removal of healthy features namely Optic disk and blood vessel network. The exudates are detected and extracted in the next stage. The shape feature for detected exudates are found and is compared with the hand drawn ground truth retinal image. The performance measures are evaluated as Sensitivity and Specificity. The overall performance of the proposed method is 85.11% sensitivity and 99.79% specificity. The high specificity indicates that the proposed algorithm does not recognize non-exudates pixel as exudates. © Research India Publications.
ER  - 
TY  - CONF
T1  - Novel algorithm by low complexity filter on retinal vessel segmentation
A1  - Rostampour, S
Y1  - 2011///
KW  -  Algorithms; Blood vessels; Eye protection; Imaging systems; Microcirculation; Ophthalmology; Pixels
KW  -  Image segmentation
KW  - Bayesian; Classification methods; Dark colors; Digital image; Low-complexity filter; Novel algorithm; Preprocessing phase; retina; Retinal vessels; Second phase; Side effect; vessel detection
JF  - Proceedings of SPIE - The International Society for Optical Engineering
VL  - 8285
SN  - 9780819489326
DO  - 10.1117/12.913411
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054714814&doi=10.1117%2F12.913411&partnerID=40&md5=306846700ab22cbb7df2c789a26bd312
N1  - cited By 0
N2  - This article shows a new method to detect blood vessels in the retina by digital images. Retinal vessel segmentation is important for detection of side effect of diabetic disease, because diabetes can form new capillaries which are very brittle. The research has been done in two phases: preprocessing and processing. Preprocessing phase consists to apply a new filter that produces a suitable output. It shows vessels in dark color on white background and make a good difference between vessels and background. The complexity is very low and extra images are eliminated. The second phase is processing and used the method is called Bayesian. It is a built-in in supervision classification method. This method uses of mean and variance of intensity of pixels for calculate of probability. Finally Pixels of image are divided into two classes: vessels and background. Used images are related to the DRIVE database. After performing this operation, the calculation gives 95 percent of efficiency average. The method also was performed from an external sample DRIVE database which has retinopathy, and perfect result was obtained. © 2011 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).
ER  - 
TY  - CONF
T1  - Comparative analysis on supervised classification techniques for segmentation and detecting abnormal blood vessels in retinal images
A1  - Deepa, M
A1  - Mymoon Zuviriya, N
ED  - Sathiyamoorthy S. Elizabeth Caroline B., Gnana Jayanthi J
Y1  - 2014///
KW  -  Blood vessels
KW  -  Diabetic retinopathy; K-NN classifier; Moment invariant; Naive bayes; Retinal image; Supervised classification; SVM classifiers
KW  - Blood; Classification (of information); Extraction; Eye protection; Feature extraction; Image classification; Image segmentation; Ophthalmology; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Proceedings of the INternational Conference On Emerging Trends in Science Engineering and Technology: Recent Advancements on Science and Engineering Innovation, INCOSET 2012
SP  - 180
EP  - 185
SN  - 9781467351447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911164091&partnerID=40&md5=b5a2395ebd3ff13751bbbd3fee6b724d
N1  - cited By 1
N2  - The development of new vessels on the retina of people with diabetes is rare, but is likely to lead to severe visual impairment. The technique implements a supervised classification method for blood vessel detection as well as new vessels on the optic disc in digital retinal images. Blood vessel segmentation is performed through various stages: Preprocessing, Feature Extraction by using Gray-level and Moment Invariants-based, Classification and Post processing. For new vessel detection, the fourteen features are chosen based on their discrimination capability and absence of correlation with other features. Classification is performed using a Support Vector Machine. The system is trained and tested by cross-validation using 25 images with new vessels and 25 normal images without new vessels. © 2012 IEEE.
ER  - 
TY  - CONF
T1  - A Customized Preprocessing Framework for Ayurvedic Nadi Signals
A1  - Umasha, H E J
A1  - Ranaweera, R D B
A1  - Wijayakulasooriya, J V
Y1  - 2021///
KW  -  Diabetes diagnosis; Disease diagnosis; Internal organs; Nadi; Pre-processing method; Preprocessing; Pulse segmentation; Radial artery; Sources of informations; Wrist pulse
KW  -  Electromagnetic pulse
KW  - Diagnosis; Signal processing
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 IEEE 16th International Conference on Industrial and Information Systems, ICIIS 2021 - Proceedings
SP  - 221
EP  - 226
SN  - 9781665426374
DO  - 10.1109/ICIIS53135.2021.9660706
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124705111&doi=10.1109%2FICIIS53135.2021.9660706&partnerID=40&md5=9e48a941417458beb199cc1866d01ea3
N1  - cited By 0
N2  - Nadi pulse sensed at the radial artery of a human wrist is a rich source of information not just on the heart but also on many internal organs, hence being ideal for disease diagnosis. Still, it easily gets distorted by many noise artifacts from physiological movements, noise added by hardware circuitry used in the signal acquisition, and other electromagnetic interference in the surrounding. Thus, it is essential to develop a preprocessing framework that can effectively remove these artifacts prior to signal analysis. In this paper, we propose an efficient and effective five-stage preprocessing method for Nadi. Then using the features extracted from preprocessed Nadi, an experiment on diabetes diagnosis gave 88.9 % accuracy proving the proposed preprocessing method to be effective on Nadi. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - Identifying microaneurysms in retinal images using fuzzy C-means clustering
A1  - Ganesh Naga Sai Prasad, V
A1  - Khan, H
A1  - Gopinathan, E
Y1  - 2015///
PB  - Asian Research Publishing Network
JF  - ARPN Journal of Engineering and Applied Sciences
VL  - 10
IS  - 6
SP  - 2366
EP  - 2372
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928309778&partnerID=40&md5=fc8db6b37fd27cd5268ba8128b596b01
N1  - cited By 1
N2  - The identification of MAs is an important phase in the research and grading of suffering from diabetes retinopathy. Analysis of online cross-section profiles in accordance with the regional highest possible pixels of the preprocessed image in microanyrism identification in retinal pictures. The statistical measures of these features principles as the alignment of the cross-section changes represent the function set that is used in a naïve Bayes category to remove unwarranted applicants. In this document we present clustering strategy to identify the microanyrisms from the optic disk and cup in the retinal fundus pictures. Fuzzy C-Means (FCM) Clustering is used for clustering the information in which the information factors are grouped with different account level. The first and major phase is preprocessing function, in which the optic cup and hard drive of the feedback picture is being turned. Originally the optic hard drive is turned in some position and the range between the information factors is calculated and a group is established in accordance with the centroid. The centroid and information factor along with the group can be recognized in each phase then the typical set of factors is grouped together. This procedure carries on until no more centroid is discovered. The group with more information factors that do not coordinate with the unique picture is regarded as the retinal picture with microanyrism illness. The experimental outcomes determines efficient and precise discovering microanyrisms in retinal pictures with great possibilities in picture pixel spinning. © 2006-2015 Asian Research Publishing Network (ARPN).
ER  - 
TY  - JOUR
T1  - Enhanced hybrid data preprocessing technique for eliminating inconsistencies in the diabetic dataset to improve mining results
A1  - Sathya, S
A1  - Rajesh, A
Y1  - 2018///
PB  - American Scientific Publishers
JF  - Journal of Computational and Theoretical Nanoscience
VL  - 15
IS  - 6-7
SP  - 1999
EP  - 2002
DO  - 10.1166/jctn.2018.7396
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057406866&doi=10.1166%2Fjctn.2018.7396&partnerID=40&md5=fceecf336cc56217869c9e1fb9ce340d
N1  - cited By 1
N2  - Data mining plays the vital role in predicting the future in all the fields. As like other domains, many researchers have implemented the mining process in the healthcare industry. Many researchers were experimented the history of patient records with the data mining techniques to diagnose and predict the chances of getting diseases in future. Even there are many researches in the healthcare industry for disease prediction; there is no efficient tool to provide accurate decisions due to the lack of data cleaning techniques. As the data stored in the database systems were subjected to inconsistencies liknoise, errors, outdated values, missing or blank values etc., it is necessary to introduce a good data preprocessing technique to remove the mentioned impurities in the datasets, so that the mining results can be improved accurately. The PIMA diabetic dataset has been considered for experimentation in this paper. The PIMA dataset is given as input to the proposed hybrid data cleaning technique. The output obtained is further given as input to the two classification algorithms ID3 and C4.5. The results obtained were measured for both the algorithms in both the cases of original dataset and preprocessed dataset. It is found that the preprocessed dataset yields better results than the original dataset. © 2018 American Scientific Publishers. All rights reserved.
ER  - 
TY  - CONF
T1  - Implementation of Random Forest in Classification Model of Diabetes Prediction based on Drug Review Content
A1  - Karimah, S
A1  - Setiawan, E B
A1  - Kurniawan, I
Y1  - 2021///
KW  -  Bigrams; Classification models; Glucose level; High glucose; N-grams; Prediction-based; Random forests; Review contents; Term frequencyinverse document frequency (TF-IDF); Tri grams
KW  -  Decision trees
KW  - Blood; Blood vessels; Forecasting; Inverse problems; Machine learning; Random forests; Text processing
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 International Conference on Data Science and Its Applications, ICoDSA 2021
SP  - 228
EP  - 232
SN  - 9781665443036
DO  - 10.1109/ICoDSA53588.2021.9617218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123453707&doi=10.1109%2FICoDSA53588.2021.9617218&partnerID=40&md5=5752f72b17a3952f32391cff12053df8
N1  - cited By 0
N2  - Diabetes is a disease that causes abnormally high glucose levels in the blood and is considered the main factor of damage to blood vessels. This disease can affect the heart, eyes, kidneys, and nerves and cause various complications. The risks of diabetes can be reduced with proper treatment in the early stage. Early identification of diabetes can be made by implementing a machine learning model to predict or diagnose diabetes. In this study, we utilized Random Forest (RF) to develop a classification model to predict people with diabetes. Random Forest is known as an ensemble method that combines the prediction of each basic model. The model development was performed by using drug reviews content retrieved from the UCI Machine Learning repository. The data set consists of the review and confirmed patient condition. We performed preprocessing step to obtain the important feature from the text. Then the feature extraction was done by using N-gram and term frequency-inverse document frequency (TF-IDF). We utilized six N-gram methods, i.e., unigram, bigram, trigram, unigram-bigram, bigram-trigram, and unigram-big ram-trigram. The best result of the experiments is obtained from the model developed by using a unigram feature with F-1 score is 0.952. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Detecting Diabetes in Human Body using Different Machine Learning Techniques
A1  - Zaman, S M T
A1  - Paul, S K
A1  - Paul, R R
A1  - Hamid, M E
Y1  - 2021///
KW  -  Blood glucose; Chronic disease; Human bodies; Machine learning models; Machine learning techniques; Machine-learning; Modeling (decision tree); PIMA dataset; Potential risks; Preventive action
KW  -  Insulin
KW  - Classification (of information); Decision trees; Machine learning; Nearest neighbor search
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 6th International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering, IC4ME2 2021
SN  - 9781665406376
DO  - 10.1109/IC4ME253898.2021.9768501
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130546797&doi=10.1109%2FIC4ME253898.2021.9768501&partnerID=40&md5=4ad61f843416689bc9f5d736d904721e
N1  - cited By 0
N2  - One of the most widespread and chronic diseases in our world today is diabetes. Human suffers from an increased amount of blood glucose in the body caused due to inadequate insulin production. Early detection of diabetes is thus now become a must to combat and take preventive actions against the potential risks of this disease. In this paper, we present a methodology for the classification of diabetics using different machine learning models (Decision Tree, K-Nearest Neighbors, Naïve Bayes, Random Forest) to analyze diabetes patients and detect diabetes in human bodies. The proposed methodology is evaluated on Diabetes Dataset namely Pima Indians Diabetes (PIMA) Dataset. Initially, we apply the different preprocessing techniques to prepare data for the experiment. The extracted training data are passed through each model. For measuring the performance of the classifiers, we apply the measurements of sensitivity, precision, f1-score, and overall accuracy. However, in the experiment, we see that the Random Forest model is achieved the highest score with an accuracy of 86%. We hope that these classification techniques as per our study can be combined with real-time data with the help of the 'Internet of Things' to make real-time devices for health care applications. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Deep diagnosis of non-proliferative diabetic retinopathy in a mobile system [Bir mobil sistemde proliferatif olmayan diyabetik retinopatinin derin tanisi]
A1  - Dizdaroglu, B
A1  - Corbacioglu, B
Y1  - 2019///
KW  -  Diabetes mellitus; Diabetic retinopathy; Hard exudates; Intraretinal hemorrhage; Mikroaneurym; Mobile systems; Nonproliferative diabetic retinopathies; Soft exudate
KW  -  Eye protection
KW  - Biomedical engineering; Deep neural networks; Diagnosis; mHealth; Neural networks
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - TIPTEKNO 2019 - Tip Teknolojileri Kongresi
SN  - 9781728124209
DO  - 10.1109/TIPTEKNO.2019.8894946
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075608058&doi=10.1109%2FTIPTEKNO.2019.8894946&partnerID=40&md5=2d8bee10353e20aa58949e2bbb206f4f
N1  - cited By 2
N2  - In this study, a mobile based system is presented for deep diagnosis of non-proliferative diabetic retinopathy. In this system, firstly, fundus images taken sequentially via the microlens mounted on the smartphone will be sent to the server. The images will then be fused in the server and after some image preprocessing steps, the pathological regions in the fundus image will be classified. For the classification process, the deep convolutional neural network approach will be considered. The system, in its current state, performs detection of hard exudates in the moderate state of non-proliferative diabetic retinopathy in a cloud environment. In the later phases of the study, it is planned to implement a semi-automated mobile-based deep diagnostic system that performs in the cloud environment to detect other abnormalities in abnormal fundus images. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - PLS Predictive Model for In-Vivo Non-Invasive Finger Touch Blood Glucose NIR Spectrosensor
A1  - Basri, K N
A1  - Tuhaime, N A
A1  - Hisham, M H
A1  - Laili, M H
A1  - Yusof, Z M
A1  - Mazni, F S
A1  - Yusof, A R
A1  - Mustafa, N
A1  - Yusof, H M
Y1  - 2021///
KW  -  Generalized least square; Glucose monitoring; Non-communicable disease; Optimum number; Partial least square algorithms; Predictive modeling; Signal preprocessing; Variable selection
KW  -  Predictive analytics
KW  - Blood; Glucose; Infrared devices; Nanoelectronics
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2021 IEEE Regional Symposium on Micro and Nanoelectronics, RSM 2021
SP  - 88
EP  - 91
SN  - 9781665412315
DO  - 10.1109/RSM52397.2021.9511583
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115051751&doi=10.1109%2FRSM52397.2021.9511583&partnerID=40&md5=c2995155e83331b268bcc54fae9e20bf
N1  - cited By 0
N2  - Diabetes is one of the non-communicable disease that has high prevalence trends. The use of finger pricking to monitor the glucose level is painful thus non-invasive alternative is needed. Non-invasive finger touch blood glucose NIR spectrosensor was utilized for the in-vivo glucose monitoring on patients at Hospital Universiti Kebangsaan Malaysia (HUKM). The development of predictive model for the glucose monitoring was based on partial least square (PLS) algorithm. Different type of preprocesses were selected to check the performance of the model with different method of signal preprocessing. The optimum number of variable when performing variable selection is 744. The R2C and R2p acquired were 0.2492 and 0.1734 respectively. The RMSEC and RMSEP for the model were 3.0324 and 2.9901 for the combination of preprocessing of generalized least square (Gls) weighting and autoscale (As). Interval PLS (iPLS) was implemented for the wavelength extraction to enhance the predictive model. R2p for 500 variables (wavelength points) shown the best result with a value of 0.2390. RMSEP also has decreased by to 2.8221 from the previous model 2.9956. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Modeling risk prediction of diabetes - A preventive measure
A1  - Prasad, B R
A1  - Agarwal, S
ED  - Arya K.V., Kumar S
Y1  - 2015///
KW  -  Appropriate models; Classification mechanism; Classifier models; Disease diagnosis; Health care application; High dimensional data; Preventive measures; Voting mechanism
KW  -  Classification (of information)
KW  - Clustering algorithms; Computer aided diagnosis; Data mining; Decision making; Feature extraction; Forecasting; Medical problems; Risk assessment
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 9th International Conference on Industrial and Information Systems, ICIIS 2014
SN  - 9781479964994
DO  - 10.1109/ICIINFS.2014.7036646
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924250537&doi=10.1109%2FICIINFS.2014.7036646&partnerID=40&md5=35184c5a18abc2eb99df5a111fbb31d4
N1  - cited By 2
N2  - Databases in clinical scenario have tremendous amount of data regarding patients and clinical history associated. Here, data mining plays vital role in searching for patterns within huge clinical data that could provide useful basis of knowledge for efficient and effective decision-making. Classification mechanism is widely used tool of data mining employed in healthcare applications to facilitate disease diagnosis and prediction. Usually medical dataset are high dimension in nature containing many insignificant attributes or features and result poor classification with inaccuracies. Feature selection is a technique used for preprocessing the high-dimensional data to reduce data dimension and to remove redundant and irrelevant features. This paper provides a systematic data mining approach for selecting best indicators of diabetes among many attributes present in the database and gives an appropriate model to track the diabetes before its onset. It selects the most appropriate classifier model for the given dataset through voting mechanism to achieve best accuracy and eliminating any biased result. © 2014 IEEE.
ER  - 
TY  - CONF
T1  - Classification of Heart Rate data using BFO-KFCM clustering and improved extreme learning machine classifier
A1  - Kavitha, R
A1  - Christopher, T
Y1  - 2016///
KW  -  Classification (of information)
KW  -  Clustering; Electrocardiogram signal; Feature detection; IELM; KFCM
KW  - Cardiology; Diagnosis; Electrocardiography; Feature extraction; Heart; Information science; Knowledge acquisition; Learning systems; Reduction; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2016 International Conference on Computer Communication and Informatics, ICCCI 2016
SN  - 9781467366793
DO  - 10.1109/ICCCI.2016.7479978
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978028199&doi=10.1109%2FICCCI.2016.7479978&partnerID=40&md5=caa8a07615306a9f84b89b68238fdf45
N1  - cited By 2
N2  - The Electrocardiogram is a tool used to access the electrical recording and muscular function of the heart and in last few decades it is extensively used in the investigation and diagnosis of heart related diseases. It must be noted that the heart rate fluctuates not only because of cardiac demand, however is also influenced as a result of the occurrence of cardiac disease and diabetes. In addition, it has been shown that Heart Rate Variability (HRV) may well be utilized as an early indicator of cardiac disease susceptibility and the existence of diabetes. As a result, the HRV can be exercised for early clinical test of these diseases. Most existing systems make use of Support Vector Machine (SVM), owing to the generalization performance, it is not sufficient for the accurate classification of heart rate data. In order to overcome this complication, Improved Extreme Learning Machine (IELM) classifier is used, to obtain the best parameter value and best feature subset through the use of Bacterial Foraging Optimization (BFO) that feed the classifier. Here in this work, features of linear and nonlinear are extracted from the HRV signals. Following the preprocessing, feature extraction is done effectively together with feature selection with the assistance of BFO for the purpose of data reduction. Subsequently, proposed a scheme to integrate Kernel Fuzzy C-Means (KFCM) clustering and classifier to adequately enhance the accuracy result for ECG beat classification. The accuracy result for classification of heart rate data is shown in the proposed scheme. © 2016 IEEE.
ER  - 
TY  - CONF
T1  - Disease Prediction based on Retinal Images
A1  - Shivappriya, S N
A1  - Rajaguru, H
A1  - Ramya, M
A1  - Asiyabegum, U
A1  - Prasanth, D
ED  - Harikumar R. Babu C.G., Poongodi C
Y1  - 2021///
KW  -  Convolution neural network; Heart attack; Highly-correlated; Key feature; Prediction-based; Preprocessing; Retina; Retinal fundus images; Retinal image; VGG16 architecture
KW  -  Ophthalmology
KW  - Blood vessels; Diagnosis; Median filters; Network architecture
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 1st International Conference on Smart Technologies Communication and Robotics, STCR 2021
SN  - 9781665418065
DO  - 10.1109/STCR51658.2021.9588829
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119422875&doi=10.1109%2FSTCR51658.2021.9588829&partnerID=40&md5=12468713766a0a20e920145dcfc5f025
N1  - cited By 0
N2  - The paper's major goal is to create a basic framework for recognising disorders including diabetes, hypertension, and heart attacks. In the retinal fundus image shows two types of blood vessels that is arteries and veins. To diagnosis the different types of diseases, it is more important to distinguish the vessels into arteries veins, which are highly correlated with brain and heart functioning. Preprocessing steps are introduced for the segmentation vessel using median filter algorithm. After pre-processing the images are undergone with Convolution Neural Network (CNN) approach. CNN is used to recognize the key feature of the images and they are differentiated based on the key feature. When we give the input of retinal images the difference in the retinal eye was observed based on their diseases. There are numerous diseases like heart attack, diabetics etc, which kills the people. With the help of this project some of the diseases are predicted with the help of the retinal eye. Retinal eye is very important which consists of retinal nerves which are connected to the brain. The VGG 16 architecture is used to find the diseases using retinal images. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - ENHANCING THE PERFORMANCE OF DIABETES PREDICTION USING TUNING OF HYPERPARAMETERS OF CLASSIFIERS ON IMBALANCED DATASET
A1  - Gupta, S C
A1  - Singh, D K
A1  - Goel, N
Y1  - 2021///
PB  - Engg Journals Publications
JF  - Indian Journal of Computer Science and Engineering
VL  - 12
IS  - 6
SP  - 1646
EP  - 1662
DO  - 10.21817/indjcse/2021/v12i6/211206049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124567987&doi=10.21817%2Findjcse%2F2021%2Fv12i6%2F211206049&partnerID=40&md5=74646b1fc7a7ef19f6835714548d4530
N1  - cited By 0
N2  - Background: The prediction ability of a classifier is important for a diabetes prediction model. The more correct prediction a classifier make, the better performance of the model will be. Although a number of researches has been done in this area, but still there are some scope to improve the performing capability of model. In this experimental work an effort is made to do it by applying these three method-identifying appropriate preprocessing act, perform oversampling to make balanced class dataset and tune the hyperparameter of classifiers to improve its performance. Methods: In research experiment, four different prediction model is built on preprocessed, oversampled balanced class datasets which are created from PIMA diabetes dataset by different preprocessing methods. Each model uses hypertuned classifiers KNN, SVM, DT and random forest to classify samples in diabetic and non-diabetic category. The obtained results are stored and analyzed, and the best model is selected by considering F1 score of classifiers of all prediction model. Results: The results obtained from these models show that the highest F1score of classifiers of each model on dataset D1, D2, D3 and D4 are 88.52 %, 88.79 %, 93.33 % and 95.23% respectively, and it is achieved by random forest classifier for every model. Conclusion: From the analysis of the results obtained from these models it is found that the best prediction model is based on dataset D4 which is created from the removal of outliers and rows having missing values during preprocessing. © 2021, Engg Journals Publications. All rights reserved.
ER  - 
TY  - CONF
T1  - Cascaded Architecture for Classifying the Preliminary Stages of Diabetic Retinopathy
A1  - Khaled, O
A1  - El-Sahhar, M
A1  - El-Dine, M A
A1  - Talaat, Y
A1  - Hassan, Y M I
A1  - Hamdy, A
Y1  - 2020///
KW  -  Cascaded models; Detection problems; Diabetes mellitus; Diabetic retinopathy; Eye disease; Input image; Preprocessing techniques; Retinal image
KW  -  Multilayer neural networks
KW  - Classification (of information); Convolution; Convolutional neural networks; Eye protection; Large dataset; Network layers; Object detection
PB  - Association for Computing Machinery
JF  - ACM International Conference Proceeding Series
SP  - 108
EP  - 112
SN  - 9781450377218
DO  - 10.1145/3436829.3436854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099209494&doi=10.1145%2F3436829.3436854&partnerID=40&md5=16c9c07149cbc39c61068872541d44c2
N1  - cited By 1
N2  - Diabetes Mellitus is one of the modern world's most dominant diseases. This condition leads to a dangerous eye disease called Diabetic Retinopathy (DR), which eventually causes total blindness. The purpose of this research is the early detection of this condition to prevent further complications in the future. Over the past few years, Convolutional Neural Networks (CNNs) became very popular in resolving image processing and object detection problems for huge datasets. A cascaded model was proposed to detect the presence of DR and classify it into 4 stages, taking into consideration using a large dataset. Furthermore, preprocessing techniques such as normalization are applied, and finally, the input images are fed into a multi-layer Convolutional Neural Network. This method was utilized on 61,248 retinal images, which are a portion of the (EyePACS) dataset. It achieved a specificity of 96.1% for detecting the presence of the disease and 63.1% for determining its stage. © 2020 ACM.
ER  - 
TY  - JOUR
T1  - Extracting Classification Rules from Artificial Neural Network Trained with Discretized Inputs
A1  - Yedjour, D
Y1  - 2020///
KW  -  Association rules mining; Classification rules; Continuous attribute; Data preprocessing; Medical decision making; Multi-objective genetic algorithm; Network classification; Rule extraction algorithms
KW  -  Neural networks
KW  - Data handling; Decision making; Diseases; Extraction; Genetic algorithms
PB  - Springer
JF  - Neural Processing Letters
VL  - 52
IS  - 3
SP  - 2469
EP  - 2491
DO  - 10.1007/s11063-020-10357-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092385087&doi=10.1007%2Fs11063-020-10357-x&partnerID=40&md5=af7df9621bf13e17b83a91f133c1bba1
N1  - cited By 3
N2  - Rule extraction from artificial neural networks remains important task in complex diseases such as diabetes and breast cancer where the rules should be accurate and comprehensible. The quality of rules is improved by the improvement of the network classification accuracy which is done by the discretization of input attributes. In this paper, we developed a rule extraction algorithm based on multiobjective genetic algorithms and association rules mining to extract highly accurate and comprehensible classification rules from ANN’s that have been trained using the discretization of the continuous attributes. The data pre-processing provides very good improvement of the ANN accuracy and consequently leads to improve the performance of the classification rules in terms of fidelity and coverage. The results show that our algorithm is very suitable for medical decision making, so an excellent average accuracy of 94.73 has been achieved for the Pima dataset and 99.36 for the breast cancer dataset. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - CONF
T1  - A framework for classification using genetic algorithm based clustering
A1  - Gajawada, S
A1  - Toshniwal, D
Y1  - 2012///
KW  -  Classification (of information); Cluster analysis; Diagnosis; Genetic algorithms; Intelligent systems; Optimization; Systems analysis; Trees (mathematics)
KW  -  Clustering algorithms
KW  - Based clustering; Classification accuracy; Classification methods; Clustering solutions; Dendrograms; Hier-archical clustering; Hierarchical clustering algorithms; Hierarchical clustering methods; Number of clusters; Optimal clustering; Optimal level; Partitional clustering; Partitional methods; Pre-processing; Pre-processing step; Tree structures
JF  - International Conference on Intelligent Systems Design and Applications, ISDA
SP  - 752
EP  - 757
SN  - 9781467351188
DO  - 10.1109/ISDA.2012.6416631
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874346689&doi=10.1109%2FISDA.2012.6416631&partnerID=40&md5=588f832270a3f2b61e7a30ef3dc25fd6
N1  - cited By 1
N2  - Clustering has been used in literature to enhance classification accuracy. But most partitional clustering methods need the number of clusters as input and also they are sensitive to initialization. Although hierarchical clustering methods may be more effective in finding clustering structure of the dataset than partitional methods but hierarchical clustering methods give tree structure known as dendrogram which is a sequence of clustering solutions. Hence hierarchical clustering algorithms are not generally applied in the preprocessing step to classification methods. This problem can be solved by cutting the dendrogram to get single clustering solution. In this paper we propose a framework for classification which uses Optimal Clustering Genetic Algorithm (OCGA) to obtain optimal level of cutting the dendrogram. A single clustering solution is obtained by cutting the dendrogram at optimal level. The clusters obtained are used to enhance classification accuracy of the classification methods. The proposed classification methods have been applied for the diagnosis of diabetes disease. © 2012 IEEE.
ER  - 
TY  - JOUR
T1  - Health care data analysis using evolutionary algorithm
A1  - Suresh, A
A1  - Kumar, R
A1  - Varatharajan, R
Y1  - 2020///
KW  -  Assessment models; Conventional classifier; Dimensionality reduction; Healthcare industry; Medical data mining; Pre-processed data; Pre-processing step; Recursive feature selection-based support vector machine (RFS-SVM)
KW  -  Medical computing
KW  - Data mining; Feature extraction; Health care; K-means clustering; Support vector machines
PB  - Springer
JF  - Journal of Supercomputing
VL  - 76
IS  - 6
SP  - 4262
EP  - 4271
DO  - 10.1007/s11227-018-2302-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044372816&doi=10.1007%2Fs11227-018-2302-0&partnerID=40&md5=128ae7df18a3ce4e85e220c180a6a721
N1  - cited By 23
N2  - Assessment of huge amount of data is the difficult task in the health care industry. Hence, it here brings the important need of the data mining in identifying the relationship between the data attributes. In this research work, an assessment model for the health care analysis is developed with the preprocessing steps of performing data cleaning by applying normalization with outlier detection by applying the k-means clustering. Then, the preprocessed data are subjected to the dimensionality reduction process by performing the Feature Selection task. Then, the selected features are analyzed by the wrapper model named SVM-based improved recursive feature selection, and its accuracy is evaluated and compared with the other traditional classifiers such as Naïve Bayes. The analysis demonstrates that the planned perfect has accomplished a regular correctness of 98.79% of health care dataset such as Pima Indians diabetes. It demonstrates that the planned technique has achieved improved consequences. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Severity grading and early retinopathy lesion detection through hybrid inception-resnet architecture
A1  - Yasin, S
A1  - Iqbal, N
A1  - Ali, T
A1  - Draz, U
A1  - Alqahtani, A
A1  - Irfan, M
A1  - Rehman, A
A1  - Glowacz, A
A1  - Alqhtani, S
A1  - Proniewska, K
A1  - Brumercik, F
A1  - Wzorek, L
Y1  - 2021///
KW  -  Blindness; Diabetic Retinopathy; Fundus Oculi; Humans; Research Design; Retina
KW  -  Data preprocessing; Deep learning; Developing nations; Diabetic retinopathy; Fundus image; Human vision; Impaired vision; Lesion detection; Retinal image; SMART datum
KW  -  Eye protection
KW  -  blindness; diabetic retinopathy; diagnostic imaging; eye fundus; human; methodology; retina
KW  - Blood vessels; Deep learning; Grading; Image processing; Ophthalmology
PB  - MDPI
JF  - Sensors
VL  - 21
IS  - 20
DO  - 10.3390/s21206933
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117176572&doi=10.3390%2Fs21206933&partnerID=40&md5=7033ee597d41d93ff39c20e32345e3d4
N1  - cited By 3
N2  - Diabetic retinopathy (DR) is a diabetes disorder that disturbs human vision. It starts due to the damage in the light-sensitive tissues of blood vessels at the retina. In the beginning, DR may show no symptoms or only slight vision issues, but in the long run, it could be a permanent source of impaired vision, simply known as blindness in the advanced as well as in developing nations. This could be prevented if DR is identified early enough, but it can be challenging as we know the disease frequently shows rare signs until it is too late to deliver an effective cure. In our work, we recommend a framework for severity grading and early DR detection through hybrid deep learning Inception-ResNet architecture with smart data preprocessing. Our proposed method is composed of three steps. Firstly, the retinal images are preprocessed with the help of augmentation and inten-sity normalization. Secondly, the preprocessed images are given to the hybrid Inception-ResNet architecture to extract the vector image features for the categorization of different stages. Lastly, to identify DR and decide its stage (e.g., mild DR, moderate DR, severe DR, or proliferative DR), a classification step is used. The studies and trials have to reveal suitable outcomes when equated with some other previously deployed approaches. However, there are specific constraints in our study that are also discussed and we suggest methods to enhance further research in this field. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
ER  - 
TY  - JOUR
T1  - Data cleaning for classification using misclassification analysis
A1  - Jeatrakul, P
A1  - Wong, K W
A1  - Fung, C C
Y1  - 2010///
PB  - Fuji Technology Press
JF  - Journal of Advanced Computational Intelligence and Intelligent Informatics
VL  - 14
IS  - 3
SP  - 297
EP  - 302
DO  - 10.20965/jaciii.2010.p0297
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951230501&doi=10.20965%2Fjaciii.2010.p0297&partnerID=40&md5=86e70fb8f7a5d49f8bf603645d4c8b2a
N1  - cited By 49
N2  - In most classification problems, sometimes in order to achieve better results, data cleaning is used as a preprocessing technique. The purpose of data cleaning is to remove noise, inconsistent data and errors in the training data. This should enable the use of a better and representative data set to develop a reliable classification model. In most classification models, unclean data could sometime affect the classification accuracies of a model. In this paper, we investigate the use of misclassification analysis for data cleaning. In order to demonstrate our concept, we have used Artificial Neural Network (ANN) as the core computational intelligence technique. We use four benchmark data sets obtained from the University of California Irvine (UCI) machine learning repository to investigate the results from our proposed data cleaning technique. The experimental data sets used in our experiment are binary classification problems, which are German credit data, BUPA liver disorders, Johns Hopkins Ionosphere and Pima Indians Diabetes. The results show that the proposed cleaning technique could be a good alternative to provide some confidence when constructing a classification model.
ER  - 
TY  - CONF
T1  - Automatic Discrimination of Fundus DR Based on Improved Residual Dense Block Network
A1  - Zhang, F
A1  - Miao, J
A1  - Wang, W
A1  - Xiao, Z
A1  - Xu, X
Y1  - 2021///
KW  -  Convolution neural network; Deep learning; Diabetic retinopathy; Expert analysis; Fundus camera; Fundus diabetic retinopathy discrimination; Fundus image; Modified residual dense block convolution neural network; Network structures; Retinal image
KW  -  Eye protection
KW  - Convolution; Convolutional neural networks; Deep learning; Diagnosis; Hospitals; Image enhancement; Ophthalmology; Patient treatment; Telemedicine
PB  - Association for Computing Machinery
JF  - ACM International Conference Proceeding Series
SP  - 50
EP  - 56
SN  - 9781450390057
DO  - 10.1145/3468945.3468954
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116545018&doi=10.1145%2F3468945.3468954&partnerID=40&md5=a79f9de393788e169a4716dedbe67afc
N1  - cited By 0
N2  - Diabetic retinopathy is the most serious complication of diabetes. In hospital treatment or telemedicine, experts analyze and treat patients with Diabetic Retinopathy (DR) based on the retinal images captured by the fundus camera. However, large number of non-pathological fundus images occupy too much time for the ophthalmologist to diagnose, and delay the timely treatment of patients with fundus DR. Therefore, it is a very urgent task to automatically and objectively screen whether the fundus has DR. Based on deep learning, we proposes an improved residual-dense module convolutional neural network structure (Modified Residual Dense Block Convolution Neural Network, MRDB-CNN). DR fundus images and non-DR fundus images are used for model training and the overall accuracy of the network structure is assessed by test set. Experiments have proved that the module can extract the detailed features of the fundus DR. The MRDB-CNN network structure can obtain a better generalization ability and a higher-precision network classification model while avoiding the complex image preprocessing. The accuracy of DR discrimination reached 94.90%, which reaches the needs of initial screening of fundus DR in hospital treatment and telemedicine. © 2021 ACM.
ER  - 
TY  - JOUR
T1  - ResNet-34/DR: A Residual Convolutional Neural Network for the Diagnosis of Diabetic Retinopathy
A1  - Al-Moosawi, N M
A1  - Khudeyer, R S
Y1  - 2021///
KW  -  Convolutional neural network; Deep learning; Diabetic retinopathy; Early diagnosis; Resnet-34; Transfer learning; Vision loss
KW  -  Eye protection
KW  - Convolution; Convolutional neural networks; Deep learning; Diagnosis; Transfer learning
PB  - Slovene Society Informatika
JF  - Informatica (Slovenia)
VL  - 45
IS  - 7
SP  - 115
EP  - 124
DO  - 10.31449/inf.v45i7.3774
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122583677&doi=10.31449%2Finf.v45i7.3774&partnerID=40&md5=9e1e95d60405ffe6bba35487d623a531
N1  - cited By 5
N2  - Diabetic retinopathy (DR) is an eye complication associated with diabetes, resulting in blurred vision or blindness. The early diagnosis and treatment of DR can decrease the risk of vision loss dramatically. However, such diagnosis is a tedious and complicated task due to the variability of retinal changes across the stages of the diseases, and due to the high number of undiagnosed and untreated DR cases. In this paper, we develop a computationally efficient and scalable deep learning model using convolutional neural networks (CNN), for diagnosing DR automatically. Various preprocessing algorithms are utilized to improve accuracy, and a transfer learning strategy is adopted to speed up the process. Our experiment used the fundus image set available on online Kaggle datasets. As an ultimate conclusion of applicable performance metrics, our computational simulation achieved a relatively-high F1 score of 93.2% for stage-based DR classification. © 2021 Slovene Society Informatika. All rights reserved.
ER  - 
TY  - JOUR
T1  - A framework for meaningful use of clinical decision model: A diabetic nephropathy prediction modeling based on real world data
A1  - Jiang, K
A1  - Shang, Y
A1  - Wang, L
A1  - Zhang, Z
A1  - Zhou, S
A1  - Dong, J
A1  - Wu, H
Y1  - 2021///
KW  -  Classification rules; Clinical decision making; Clinical efficiency; Data preprocessing; Diabetic nephropathy; Electronic medical record; Model construction; Predictive modeling
KW  -  Predictive analytics
KW  - Classification (of information); Decision making; Decision trees; Diseases; Forecasting; Medical computing
PB  - IOS Press BV
JF  - Journal of Intelligent and Fuzzy Systems
VL  - 40
IS  - 5
SP  - 9597
EP  - 9608
DO  - 10.3233/JIFS-202030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104992416&doi=10.3233%2FJIFS-202030&partnerID=40&md5=47aff9351e8bfad62470195689e786cd
N1  - cited By 0
N2  - This study aims to propose a framework for developing a sharable predictive model of diabetic nephropathy (DN) to improve the clinical efficiency of automatic DN detection in data intensive clinical scenario. Different classifiers have been developed for early detection, while the heterogeneity of data makes meaningful use of such developed models difficult. Decision tree (DT) and random forest (RF) were adopted as training classifiers in de-identified electronic medical record dataset from 6,745 patients with diabetes. After model construction, the obtained classification rules from classifier were coded in a standard PMML file. A total of 39 clinical features from 2159 labeled patients were included as risk factors in DN prediction after data preprocessing. The mean testing accuracy of the DT classifier was 0.8, which was consistent to that of the RF classifier (0.823). The DT classifier was choose to recode as a set of operable rules in PMML file that could be transferred and shared, which indicates the proposed framework of constructing a sharable prediction model via PMML is feasible and will promote the interoperability of trained classifiers among different institutions, thus achieving meaningful use of clinical decision making. This study will be applied to multiple sites to further verify feasibility. © 2021 - IOS Press. All rights reserved.
ER  - 
TY  - CONF
T1  - A new method for detection of optical disc and macula for diabetic retinopathy
A1  - Ashe, S S G
A1  - Ali, I
ED  - Chaturvedi A. Shukla A., Deolia V K Sau P C
Y1  - 2017///
KW  -  Diabetic retinopathy; Edema; Health organizations; Macula; Optic disc; Pre-processing algorithms; Retina; World Health Organization
KW  -  Eye protection
KW  - Health; Image processing; Intelligent systems
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2nd International Conference on Communication, Control and Intelligent Systems, CCIS 2016
SP  - 136
EP  - 140
SN  - 9781509032105
DO  - 10.1109/CCIntelS.2016.7878216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017261930&doi=10.1109%2FCCIntelS.2016.7878216&partnerID=40&md5=86924dda2ba4dc97aca7549fa7bd208b
N1  - cited By 1
N2  - In recent years, the rapid growth of the silent killer, Diabetes, is a major reason of concern for health and also started a buzz among our society. On the same time, World Health Organization (WHO) is a dedicated health organization from United Nations quotes, 'By 2030 diabetes will be one of the seventh leading causes of death, whereas, the global prevalence of diabetes was estimated to be about 9% among adults aged above 18 (measured in 2014)'. Though, diabetes is not completely curable but early stage detection can helps in controlling the degree of severity of this silent killer. One of the controlling factors, which could help in early detection, is the retina of the eye, a light sensitive metabolically active region on which noninvasive observation can possible. As eye is a metabolically active organ, it gets affected at the earliest stage for the people suffering from Diabetes. Thereafter, analyzing the symptoms of retina, like, miroaneurysms, exudates, and edemas can predict the degree of severity. Since, a plan is required towards the studying these symptoms on digital image, the fundus image of the retina, and the detection of attributes, like, miroaneurysms, exudates and edemas. Before carrying out the analysis, it is very essential to properly locate the optic disc and macula which are geometrically important for the detection of Diabetic Retinopathy on digital images. In this paper, we propose intensity based preprocessing algorithm which is suitable for the detection of optic disc (OD) and Macula before the analysis of Diabetic Retinopathy. © 2016 IEEE.
ER  - 
TY  - JOUR
T1  - Eye-color and Type-2 diabetes phenotype prediction from genotype data using deep learning methods
A1  - Muneeb, M
A1  - Henschel, A
Y1  - 2021///
KW  -  Binary classification problems; Genetic mutations; Gradient boosting; Machine learning models; Machine learning techniques; Multi-class classification; Overall accuracies; Statistical techniques
KW  -  Deep Learning; Diabetes Mellitus
KW  -  Learning algorithms
KW  -  Type 2; Eye Color; Genotype; Humans; Phenotype
KW  -  eye color; genetics; genotype; human; non insulin dependent diabetes mellitus; phenotype
KW  - Color; Deep learning; Forecasting; Learning systems; Long short-term memory; Statistical methods
PB  - BioMed Central Ltd
JF  - BMC Bioinformatics
VL  - 22
IS  - 1
DO  - 10.1186/s12859-021-04077-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104496632&doi=10.1186%2Fs12859-021-04077-9&partnerID=40&md5=b80ae88d391d41f2e026e52b03bb981f
N1  - cited By 5
N2  - Background: Genotype–phenotype predictions are of great importance in genetics. These predictions can help to find genetic mutations causing variations in human beings. There are many approaches for finding the association which can be broadly categorized into two classes, statistical techniques, and machine learning. Statistical techniques are good for finding the actual SNPs causing variation where Machine Learning techniques are good where we just want to classify the people into different categories. In this article, we examined the Eye-color and Type-2 diabetes phenotype. The proposed technique is a hybrid approach consisting of some parts from statistical techniques and remaining from Machine learning. Results: The main dataset for Eye-color phenotype consists of 806 people. 404 people have Blue-Green eyes where 402 people have Brown eyes. After preprocessing we generated 8 different datasets, containing different numbers of SNPs, using the mutation difference and thresholding at individual SNP. We calculated three types of mutation at each SNP no mutation, partial mutation, and full mutation. After that data is transformed for machine learning algorithms. We used about 9 classifiers, RandomForest, Extreme Gradient boosting, ANN, LSTM, GRU, BILSTM, 1DCNN, ensembles of ANN, and ensembles of LSTM which gave the best accuracy of 0.91, 0.9286, 0.945, 0.94, 0.94, 0.92, 0.95, and 0.96% respectively. Stacked ensembles of LSTM outperformed other algorithms for 1560 SNPs with an overall accuracy of 0.96, AUC = 0.98 for brown eyes, and AUC = 0.97 for Blue-Green eyes. The main dataset for Type-2 diabetes consists of 107 people where 30 people are classified as cases and 74 people as controls. We used different linear threshold to find the optimal number of SNPs for classification. The final model gave an accuracy of 0.97%. Conclusion: Genotype–phenotype predictions are very useful especially in forensic. These predictions can help to identify SNP variant association with traits and diseases. Given more datasets, machine learning model predictions can be increased. Moreover, the non-linearity in the Machine learning model and the combination of SNPs Mutations while training the model increases the prediction. We considered binary classification problems but the proposed approach can be extended to multi-class classification. © 2021, The Author(s).
ER  - 
TY  - CONF
T1  - Retinal Hemorrhage Detection Using Splat Segmentation of Retinal Fundus Images
A1  - Kurale, N G
A1  - Vaidya, M V
Y1  - 2018///
KW  -  Contrast Enhancement; Diabetic retinopathy; Important features; Retinal blood vessels; Retinal fundus images; Retinal hemorrhages; Spatial location; Watershed transform
KW  -  Ophthalmology
KW  - Blood vessels; Classification (of information); Damage detection; Eye protection; Image segmentation
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2017 International Conference on Computing, Communication, Control and Automation, ICCUBEA 2017
SN  - 9781538640081
DO  - 10.1109/ICCUBEA.2017.8463939
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054509353&doi=10.1109%2FICCUBEA.2017.8463939&partnerID=40&md5=53c8fdd37e87901efa17d3e39d7f9269
N1  - cited By 3
N2  - Diabetic Retinopathy is a condition caused because of diabetes, which can lead to severe loss of vision. It occurs due to damage of retinal blood vessels and leaking fluids from vessels. This results into total vision loss. Detecting DR at earlier stage, helps to treat early and save from severe damage to eye. To detect hemorrhages effectively, a new algorithm is evaluated. Firstly the important factor is preprocessing like edge effect removal, contrast enhancement etc. for better result. This algorithm does partitions to the retinal image into segments which is called as splats which is of same color, intensity and spatial location. Each splat contains various information from which we can extract different features. These segments i.e splat establish a set of information which extracts appropriate boundary. We select some important features from all extracted features like different filters, area, color, texture and splat related features etc. And a trained supervised SVM classifier is used to detect the disease. © 2017 IEEE.
ER  - 
TY  - CONF
T1  - A Framework for Type-II Diabetes Prediction Using Machine Learning Approaches
A1  - Bhuiyan, M F U
A1  - Rahman, M T
A1  - Anik, M A
A1  - Khan, M
Y1  - 2021///
KW  -  Classification accuracy; Classification algorithm; Data preprocessing; Least squares fitting; Missing value estimation; PIMA indian diabetes dataset; Processing approach; Score normalization; Z-score normalization; Z-scores
KW  -  Decision trees
KW  - Classification (of information); Classifiers; Data handling; Logistic regression; Random forests; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2021 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021
SN  - 9781728185958
DO  - 10.1109/ICCCNT51525.2021.9580158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126209405&doi=10.1109%2FICCCNT51525.2021.9580158&partnerID=40&md5=5f61982ee48a1c69399e30c0b1966fbc
N1  - cited By 0
N2  - Diabetes mellitus is a life-threatening condition affecting people of any age. Physicians face a difficult challenge in detecting diabetes in the early stages. Early diabetes prediction using classification algorithms can help physicians quickly identify the condition. Classification algorithms are sensitive to data pre-processing. In this work, we propose a framework to pre-process medical data with missing values and features with varying ranges. We evaluate our proposed data pre-processing method using the PIMA Indian Diabetes dataset. We experiment using eight classification algorithms, namely, Random Forests, Decision Tree-CART, SVM, Naive Bayes, Logistic Regression, KNN, MLP, and Decision Tree-C4.5 algorithms. Experimental results show that our proposed data pre-processing approach produces better results than all the previous attempts with various data pre-processing approaches and different classification algorithms. Moreover, the Naive Bayes algorithm with the proposed data pre-processing method produces 92.27% classification accuracy, which is so far the highest classification accuracy achieved. © 2021 IEEE.
ER  - 
TY  - CONF
T1  - Decision support systems for predicting diabetes mellitus-A Review
A1  - Vijayan, V V
A1  - Anjali, C
Y1  - 2015///
KW  -  Data mining
KW  -  Discretizations; ensembling; hybridization; Medical data mining; Naive Bayes classifiers; preprocessing; variability
KW  - Artificial intelligence; Classification (of information); Classifiers; Decision support systems; Decision trees; Diagnosis; Diseases; Forecasting; Learning systems; Medical computing; Pattern recognition; Principal component analysis; Support vector machines; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Global Conference on Communication Technologies, GCCT 2015
SP  - 98
EP  - 103
SN  - 9781479985531
DO  - 10.1109/GCCT.2015.7342631
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960085897&doi=10.1109%2FGCCT.2015.7342631&partnerID=40&md5=310082526ae0b7bbb889fc789732ba95
N1  - cited By 22
N2  - Diabetes mellitus is caused due to the increased level of sugar content in the blood. This can cause series complications like kidney failure, stroke, cancer, heart disease and blindness. The early detection and diagnosis, helps to identify and avoid these complications. A number of computerized information systems were designed using different classifiers for predicting and diagnosing diabetes. Selecting proper algorithms for classification clearly increases the accuracy and efficiency of the system. The main objective of this study is to review the benefits of different preprocessing techniques for decision support systems for predicting diabetes which are based on Support Vector Machine (SVM), Naive Bayes classifier and Decision Tree. The preprocessing methods focused on this study are Principal Component Analysis and Discretization. The accuracy variation with and without preprocessing techniques are also evaluated. The tool under consideration is the Weka for this study. The dataset was taken from the University of California, Irvine (UCI) repository of machine learning. © 2015 IEEE.
ER  - 
TY  - JOUR
T1  - Efficient diagnostic system for smart diabetes
A1  - Prakash, V
A1  - Bhavani, R
A1  - Anupriya, A
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Engineering and Advanced Technology
VL  - 8
IS  - 6
SP  - 3789
EP  - 3792
DO  - 10.35940/ijeat.F9392.088619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072012973&doi=10.35940%2Fijeat.F9392.088619&partnerID=40&md5=e7510255587de9a632bf7a1641afb004
N1  - cited By 1
N2  - 5G networks, analytics of medical data and the internet of things are recent advances in big data technologies. Combining these advances with wearable computing and artificial intelligence, innovative diabetics monitoring system is implemented. In the existing system, it classifies the Diabetes 1.0 and Diabetes 2.0 methods, which show the intelligence and networking deficiencies. Thus, with personalized treatment, our goal is to design a sustainable, cost-effective, and smart diabetes diagnosis solution. Uses the machine learning algorithms in the proposed 5G smart diabetes-Naive Bayes, Logistic regression and artificial neural networks (ANN) are for the results. © BEIESP.
ER  - 
TY  - JOUR
T1  - Recent innovations in automated detection and classification of diabetic retinopathy
A1  - Aladawi, W
A1  - Jayakumari, C
A1  - Sumesh, E P
A1  - Vidhyalavanya
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Innovative Technology and Exploring Engineering
VL  - 8
IS  - 10
SP  - 1997
EP  - 2004
DO  - 10.35940/ijitee.J9306.0881019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071277303&doi=10.35940%2Fijitee.J9306.0881019&partnerID=40&md5=dce58d91544e8edb284ed11b9c1e178e
N1  - cited By 0
N2  - Due to the increasing prevalence of diabetic retinopathy worldwide, it’s an urgent need to develop smart system that help to detect disease using one of the modern technologies. Artificial intelligence is one of the popular techniques nowadays which has the ability to learn from experience and carry out human-like tasks. Large number of researches have been conducted to find out effective medical diagnosis methods for numerous diseases. Likewise, huge number of researches have been done that discuss automated detection and classification of diabetic retinopathy. This paper reviews the existing methodologies, datasets, sensitivity, specificity and classification accuracy in diabetic retinopathy. © BEIESP.
ER  - 
TY  - CONF
T1  - Effective prediction of three common diseases by combining SMOTE with Tomek links technique for imbalanced medical data
A1  - Zeng, M
A1  - Zou, B
A1  - Wei, F
A1  - Liu, X
A1  - Wang, L
ED  - Chen G., Peng J
Y1  - 2016///
KW  -  Medical computing
KW  -  Medical data; Parkinson's disease; SMOTE; Tomek links; Vertebral column
KW  - Computer aided diagnosis; Decision making; Medical problems; Neurodegenerative diseases; Pathology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of 2016 IEEE International Conference of Online Analysis and Computing Science, ICOACS 2016
SP  - 225
EP  - 228
SN  - 9781467377546
DO  - 10.1109/ICOACS.2016.7563084
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989157837&doi=10.1109%2FICOACS.2016.7563084&partnerID=40&md5=6571b903872ecd9fbe43a0326929fe6f
N1  - cited By 55
N2  - Diabetes, vertebral column pathologies and Parkinson's disease are three common diseases which have high prevalence and brought great trouble and pain to billions of patients. Computer aided diagnosis can support decision making of physicians. However, imbalanced nature of data sets hampered the mining of medical resources. In this study, we proposed a powerful preprocessing method by combining Synthetic Minority Oversampling Technique (SMOTE) with Tomek links technique and then is applied to the imbalanced medical data sets of the three diseases. By using 8 classifiers, we compared the experimental results with those of using only SMOTE technique to evaluate the effectiveness of this method. The results show that the method of SMOTE combined with Tomek links technique is much superior compared with that of using only SMOTE. The performances are evidently better, with 31, 27, 30 out of a total of 32 evaluation metrics are improved for diabetes, Parkinson's disease, and vertebral column, respectively. © 2016 IEEE.
ER  - 
TY  - CONF
T1  - Prediction and diagnosis of diabetes mellitus - A machine learning approach
A1  - Vijayan, V V
A1  - Anjali, C
Y1  - 2016///
KW  -  AdaBoost algorithm; Decision stumps; Ensembling; Hybridization; Medical data mining; Naive bayes; Preprocessing
KW  -  Data mining
KW  - Adaptive boosting; Algorithms; Artificial intelligence; Classification (of information); Classifiers; Decision support systems; Decision trees; Diagnosis; Learning systems; Medical computing; Pattern recognition; Support vector machines; Trees (mathematics)
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2015 IEEE Recent Advances in Intelligent Computational Systems, RAICS 2015
SP  - 122
EP  - 127
SN  - 9781467366700
DO  - 10.1109/RAICS.2015.7488400
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979085652&doi=10.1109%2FRAICS.2015.7488400&partnerID=40&md5=da8d5d5ee94ecb0a151b9add0263b6bb
N1  - cited By 67
N2  - Diabetes is a disease caused due of the expanded level of sugar fixation in the blood. Various computerized information systems were outlined utilizing diverse classifiers for anticipating and diagnosing diabetes. Selecting legitimate classifiers clearly expands the exactness and proficiency of the system. Here a decision support system is proposed that uses AdaBoost algorithm with Decision Stump as base classifier for classification. Additionally Support Vector Machine, Naive Bayes and Decision Tree are also implemented as base classifiers for AdaBoost algorithm for accuracy verification. The accuracy obtained for AdaBoost algorithm with decision stump as base classifier is 80.72% which is greater compared to that of Support Vector Machine, Naive Bayes and Decision Tree. © 2015 IEEE.
ER  - 
TY  - CONF
T1  - Microaneurysm detection for early diagnosis of diabetic retinopathy
A1  - Akram, M U
A1  - Tariq, A
A1  - Khan, S A
A1  - Bazar, S A
Y1  - 2013///
KW  -  Diabetic retinopathy; Feature extraction and classification; Gaussian Mixture Model; Hybrid classifier; Microaneurysms; Pre-processing step; Preprocessing; Reliable detection
KW  -  Eye protection
KW  - Automation; Feature extraction; Image retrieval; Ophthalmology
PB  - IEEE Computer Society
JF  - 2013 International Conference on Electronics, Computer and Computation, ICECCO 2013
SP  - 21
EP  - 24
SN  - 9781479933433
DO  - 10.1109/ICECCO.2013.6718218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894196151&doi=10.1109%2FICECCO.2013.6718218&partnerID=40&md5=8ffb47660a9f4e3b6c06108c035e37e9
N1  - cited By 4
N2  - Microaneurysms (MAs) are the first visible sign of diabetic retinopathy (DR), a retinal abnormality which may lead to blindness in diabetes patients. In time and precise MAs detection is very important for early diagnosis of DR and can save patient's vision. In this paper, we present an automated system for accurate and reliable detection of MAs. The proposed system consists of preprocessing, feature extraction and classification stages. The preprocessing step extracts all possible regions which may be considered as MAs from input retinal image and feature extraction stage represents each region with a number of features. A novel hybrid classifier which combines Gaussian mixture model and support vector machine in an ensemble, finally classifies each region as MA or non-MAo The proposed system uses genetic algorithm in order to optimized the weights for hybrid classifier. The evaluation of proposed system is performed using publicly available retinal image database and results are compared with existing techniques to demonstrate the validity of proposed system. © 2013 IEEE.
ER  - 
TY  - CONF
T1  - Single nucleotide polymorphism and type 2 diabetes mellitus phenotypes association using gradient boosting
A1  - Fadli, A
A1  - Hasibuan, L S
A1  - Kusuma, W A
A1  - Heryanto, R
Y1  - 2020///
KW  -  Complex disease; Diabetes mellitus; Disease treatment; Genetic information; Gradient boosting; Medical fields; Single-nucleotide polymorphisms; Type 2 diabetes mellitus
KW  -  Data fusion
KW  - Decision trees; Genes; Information systems; Information use; Mammals; Nucleotides; Personalized medicine; Polymorphism; Proteins; Websites
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 International Conference on Advanced Computer Science and Information Systems, ICACSIS 2020
SP  - 115
EP  - 120
SN  - 9781728192796
DO  - 10.1109/ICACSIS51025.2020.9263142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099751465&doi=10.1109%2FICACSIS51025.2020.9263142&partnerID=40&md5=f7d46eb62a1b3c1b080a3254be6e2390
N1  - cited By 0
N2  - Precision medicine is a medical field that aims to provide disease treatment according to an individual's genetic information, environment and lifestyle. Recently, precision medicine researchs is focusing on studying complex diseases, such as diabetes mellitus (DM). The most common form of DM is type 2 DM (T2DM). T2DM patient's genetic information can be obtained by finding an association between Single Nucleotide Polymorphism (SNP) and phenotypes of T2DM. This research aims to find SNPs which considered related to T2DM phenotypes using gradient boosting (GB) algorithm. Data were taken from Mouse Phenome Database website based on 98 protein candidates of T2DM. Preprocessing stage is conducted by deleting unused features and missing values, SNP encoding, and merging phenotypes and SNP data. Model was built using GB with decision tree base-learners and least square loss function. GB produced an average MSE value of 0.061 and MAE value of 0.171 and also obtained 30 SNPs that potentially associated with T2DM's insulin tolerance phenotype. Twenty two of 30 choosen SNPs verified to have association with T2DM phenotypes on Mouse Genome Informatics website based on SNP-protein-phenotype relationship. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - Mobile assisted diabetic retinopathy detection using deep neural network
A1  - Suriyal, S
A1  - Druzgalski, C
A1  - Gautam, K
Y1  - 2018///
KW  -  Convolutional neural network; Diabetic retinopathy; Internet connection; Microvascular; Mobile applications; Nonlinear functions; Pre-processing step; tensorflow
KW  -  Deep neural networks
KW  - Biomedical engineering; Convolution; Deep learning; Diagnosis; Eye protection; Image enhancement; mHealth; Mobile devices; Network architecture; Neural networks
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2018 Global Medical Engineering Physics Exchanges/Pan American Health Care Exchanges, GMEPE/PAHCE 2018
SP  - 1
EP  - 4
SN  - 9781538654750
DO  - 10.1109/GMEPE-PAHCE.2018.8400760
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050481398&doi=10.1109%2FGMEPE-PAHCE.2018.8400760&partnerID=40&md5=e5174f9490f9c6e84abd81c12a7f8da1
N1  - cited By 26
N2  - Diabetic retinopathy (DR) is a major microvascular complication resulting from diabetes and continues to have a serious impact on global health systems. Globally about 95 million people suffer from DR. This paper focuses on detection aspects of a mobile application developed to perform DR screening in real time. The application is powered by a tensorflow deep neural network architecture that is trained and tested on 16,798 fundus images. These images are preprocessed to remove noise and prepare them to be fed into neural network. Preprocessing steps involve averaging all the images using a 5×5 filter to improve the quality of images and then these images are resized to 256×256 pixels. After preprocessing the input dataset is fed into the neural network. The convolutional neural network model used in this project is MobileNets, which is used for mobile devices. The neural network has 28 convolutional layers and after each layer there is batchnorm and ReLU nonlinear function except at the final layer. The output from last layer is a class label either DR or no DR. The final accuracy of the model is 73.3%. This model is optimized to work on mobile devices and does not require Internet connection to run. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Developing diabetes ketoacidosis prediction using ANFIS model
A1  - Saraswati, G W
A1  - Choo, Y.-H.
A1  - Kumar, Y J
Y1  - 2018///
KW  -  Adaptive neuro fuzzy inference systems (ANFIS); Data preprocessing; Detection performance; Diabetic diagnosis; Electronic nose (e-nose); Ketoacidosis; Metal oxide gas sensors; Odour analysis
KW  -  Fuzzy inference
KW  - Acetone; Body fluids; Diagnosis; Electronic nose; Forecasting; Fuzzy neural networks; Fuzzy systems; Metals; Robotics
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceeding of 2017 International Conference on Robotics, Automation and Sciences, ICORAS 2017
VL  - 2018-March
SP  - 1
EP  - 5
SN  - 9781538619087
DO  - 10.1109/ICORAS.2017.8308066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050695577&doi=10.1109%2FICORAS.2017.8308066&partnerID=40&md5=d9d938a3cf3b8e9b61ea6606bd6f23e2
N1  - cited By 3
N2  - This paper proposes the adaptive Neuro-fuzzy Inference System (ANFIS) to construct a diabetic ketoacidosis prediction model. Diabetic ketoacidosis results in large amount of ketones can be detected in urine through distinctive odour of acetone. Hence, urine odour analysis is able to diagnose diabetic ketoacidosis facilitating dianogstic test for diabetic patient. The Electronic Nose (E-nose) system consists of four metal oxide gas sensors was used to extract urine odour. Common process of diabetic diagnosis require patient to provide fasting urine for more accurate detection. Our work has shown both fasting and non-fasting urine are able to produce good diabetic detection accuracy through urine odour analysis. A total of 40 human subjects from CITO Laboratory, Semarang Central Java, Indonesia, involving 20 diabetic patients and 20 healthy subjects were used to build the prediction model. The proposed model has achieved at least 63% average accuracy in discriminating diabetic patient from healthy subject. When data preprocessing to average the training data samples was implemented, the detection performance was increased to above 93% The findings have shown promising results of using both fasting and non-fasting data samples for diabetic prediction. This is essential towards the flexibility of diabetic dianogstic test where it does not require patient to fast, therefore can be tested at anytime anywhere. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - An Approach to Generate Software Agents for Health Data Mining
A1  - Morejón, R
A1  - Viana, M
A1  - Lucena, C
Y1  - 2017///
KW  -  Agent Oriented Software Engineering; Data mining process; Data volume; Health data; Large datasets; Mining process; Multisources; Usage scenarios
KW  -  Data mining
KW  - Artificial intelligence; Intelligent agents; Learning systems; Multi agent systems; Software agents; Software engineering
PB  - World Scientific Publishing Co. Pte Ltd
JF  - International Journal of Software Engineering and Knowledge Engineering
VL  - 27
IS  - 9-10
SP  - 1579
EP  - 1589
DO  - 10.1142/S0218194017400125
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041179811&doi=10.1142%2FS0218194017400125&partnerID=40&md5=c8a52da65fb0e097019bda95d084db6e
N1  - cited By 1
N2  - Data mining is a hot topic that attracts researchers of different areas, such as database, machine learning, and agent-oriented software engineering. As a consequence of the growth of data volume, there is an increasing need to obtain knowledge from these large datasets that are very difficult to handle and process with traditional methods. Software agents can play a significant role performing data mining processes in ways that are more efficient. For instance, they can work to perform selection, extraction, preprocessing, and integration of data as well as parallel, distributed, or multisource mining. This paper proposes a framework based on multiagent systems to apply data mining techniques to health datasets. Last but not least, the usage scenarios that we use are datasets for hypothyroidism and diabetes and we run two different mining processes in parallel in each database. © 2017 World Scientific Publishing Company.
ER  - 
TY  - JOUR
T1  - Color feature segmentation image for identification of cotton wool spots on diabetic retinopathy fundus
A1  - Pranata, F S
A1  - Na'am, J
A1  - Hidayat, R
Y1  - 2020///
PB  - Insight Society
JF  - International Journal on Advanced Science, Engineering and Information Technology
VL  - 10
IS  - 3
SP  - 974
EP  - 979
DO  - 10.18517/ijaseit.10.3.11877
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087991458&doi=10.18517%2Fijaseit.10.3.11877&partnerID=40&md5=bede366ae897136fba07332cc4c42ee9
N1  - cited By 2
N2  - Fundus is an image of the inner eye surface in the form of a colored image. This image has a lot of pixel values because it consists of three basic color components. The three colors are red, green, and blue, so they need a good technique in analyzing this image. This image can be used to diagnose diabetic retinal disease caused by diabetes mellitus. This disease can interfere with human vision because objects that cover the retina of the eye is called Cotton Wool Spot (CWS). The severity of this disease can be observed from the large area of the CWS covering the retina. This study aims to calculate the exact area ratio of CWS with the retina area. The method used in this research is Image Color Feature Segmentation (ICFS). This method has four stages, namely preprocessing, segmentation, feature extraction, and feature areas. The dataset processed in this study was sourced from the Radiology Department, General Hospital of M. Djamil Padang. The dataset consists of 16 fundus images of patients who were treated at the hospital. The results of this study can identify and calculate the percentage of retinal damage is very well. Therefore, this study can be a reference in measuring the severity of diabetic retinopathy for prevention and subsequent treatment for patients and doctors. © 2020, Insight Society.
ER  - 
TY  - JOUR
T1  - Gly-LysPred: Identification of lysine glycation sites in protein using position relative features and statistical moments via Chou's 5 step rule
A1  - Khanum, S
A1  - Ashraf, M A
A1  - Karim, A
A1  - Shoaib, B
A1  - Khan, M A
A1  - Naqvi, R A
A1  - Siddique, K
A1  - Alswaitti, M
Y1  - 2020///
KW  -  Computational model; Feature engineerings; Manual identification; Post-translational modifications; Preprocessing techniques; Self-consistency; Statistical moments; Testing technique
KW  -  Glycosylation
KW  - Amino acids; Decision trees; Testing
PB  - Tech Science Press
JF  - Computers, Materials and Continua
VL  - 66
IS  - 2
SP  - 2165
EP  - 2181
DO  - 10.32604/cmc.2020.013646
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097199313&doi=10.32604%2Fcmc.2020.013646&partnerID=40&md5=ecacd0decd67a77962508ae34a7470dc
N1  - cited By 3
N2  - Glycation is a non-enzymatic post-translational modification which assigns sugar molecule and residues to a peptide. It is a clinically important attribute to numerous age-related, metabolic, and chronic diseases such as diabetes, Alzheimer's, renal failure, etc. Identification of a non-enzymatic reaction are quite challenging in research. Manual identification in labs is a very costly and timeconsuming process. In this research, we developed an accurate, valid, and a robust model named as Gly-LysPred to differentiate the glycated sites from non-glycated sites. Comprehensive techniques using position relative features are used for feature extraction. An algorithm named as a random forest with some preprocessing techniques and feature engineering techniques was developed to train a computational model. Various types of testing techniques such as self-consistency testing, jackknife testing, and cross-validation testing are used to evaluate the model. The overall model's accuracy was accomplished through self-consistency, jackknife, and cross-validation testing 100%, 99.92%, and 99.88% with MCC 1.00, 0.99, and 0.997 respectively. In this regard, a user-friendly webserver is also urbanized to accumulate the whole procedure. These features vectorization methods suggest that they can play a critical role in other web servers which are developed to classify lysine glycation. © 2021 Tech Science Press. All rights reserved.
ER  - 
TY  - JOUR
T1  - Blood Glucose Prediction with Variance Estimation Using Recurrent Neural Networks
A1  - Martinsson, J
A1  - Schliep, A
A1  - Eliasson, B
A1  - Mogren, O
Y1  - 2020///
KW  -  Blood glucose; Blood glucose level; Blood glucose prediction; Carbohydrate sources; Diabetics patients; Glucose level; Glycemic index; Peak time; Type 1 diabetes; Variance estimation
KW  -  Forecasting
KW  - Blood; Closed loop systems; Glucose; Insulin; Mean square error; Recurrent neural networks
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Journal of Healthcare Informatics Research
VL  - 4
IS  - 1
SP  - 1
EP  - 18
DO  - 10.1007/s41666-019-00059-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087825107&doi=10.1007%2Fs41666-019-00059-y&partnerID=40&md5=ececbd44b08233e047560ac089eade09
N1  - cited By 33
N2  - Many factors affect blood glucose levels in type 1 diabetics, several of which vary largely both in magnitude and delay of the effect. Modern rapid-acting insulins generally have a peak time after 60–90 min, while carbohydrate intake can affect blood glucose levels more rapidly for high glycemic index foods, or slower for other carbohydrate sources. It is important to have good estimates of the development of glucose levels in the near future both for diabetic patients managing their insulin distribution manually, as well as for closed-loop systems making decisions about the distribution. Modern continuous glucose monitoring systems provide excellent sources of data to train machine learning models to predict future glucose levels. In this paper, we present an approach for predicting blood glucose levels for diabetics up to 1 h into the future. The approach is based on recurrent neural networks trained in an end-to-end fashion, requiring nothing but the glucose level history for the patient. Our approach obtains results that are comparable to the state of the art on the Ohio T1DM dataset for blood glucose level prediction. In addition to predicting the future glucose value, our model provides an estimate of its certainty, helping users to interpret the predicted levels. This is realized by training the recurrent neural network to parameterize a univariate Gaussian distribution over the output. The approach needs no feature engineering or data preprocessing and is computationally inexpensive. We evaluate our method using the standard root-mean-squared error (RMSE) metric, along with a blood glucose-specific metric called the surveillance error grid (SEG). We further study the properties of the distribution that is learned by the model, using experiments that determine the nature of the certainty estimate that the model is able to capture. © 2019, The Author(s).
ER  - 
TY  - JOUR
T1  - Automatic detection of non-proliferative diabetic retinopathy in retinal fundus images using convolution neural network
A1  - Saranya, P
A1  - Prabakaran, S
Y1  - 2020///
KW  -  Automatic Detection; Convolution neural network; Diabetic retinopathy; Maximum accuracies; Physical tests; Preprocessing techniques; Pupil dilation; Retinal fundus images
KW  -  Eye protection
KW  - Blood vessels; Chemical detection; Convolution; Grading; Multilayer neural networks; Ophthalmology; Vision
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Journal of Ambient Intelligence and Humanized Computing
DO  - 10.1007/s12652-020-02518-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091077199&doi=10.1007%2Fs12652-020-02518-6&partnerID=40&md5=60db15b9ed98d5d5834c677947dc9647
N1  - cited By 23
N2  - Diabetic retinopathy (DR) is one of the complications of diabetes and a leading cause of blindness in the world. The tiny blood vessels inside the retina are damaged due to diabetes and result in various vision-related problems and it may lead to complete vision loss without early detection and treatment. Diabetic retinopathy may not cause any symptoms during its earlier stage of the disease and many physical tests such as visual acuity tests, pupil dilation, etc., are required to detect diabetic retinopathy disease. So, early detection of diabetic retinopathy disease is required to avoid vision loss. This work aims to automate the detection and grading of non-proliferative Diabetic Retinopathy from retinal fundus images using Convolution Neural Networks. The model was tested on two popular datasets such as MESSIDOR and IDRiD. Before applying the Convolution Neural Network (CNN) layers, the images were pre-processed and resolution was adjusted (256 × 256). The maximum accuracy achieved is 90.89% using MESSIDOR images. The research can be carried forward by applying various preprocessing techniques before putting them through different computational layers. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Novel Data Mining Analysis Method on Risk Prediction of Type 2 Diabetes
A1  - Guo, H
A1  - Fan, Z C
A1  - Zeng, Y
Y1  - 2021///
KW  -  Data preprocessing; Diabetics patients; Human health; Model-based OPC; Performances evaluation; Prediction performance; Prediction performance evaluation; Risk predictions; SVM model; Type-2 diabetes
KW  -  Risk assessment
KW  - Classification (of information); Computer aided diagnosis; Data mining; Forecasting; Quality control; Support vector machines
PB  - Springer
JF  - Journal of Signal Processing Systems
DO  - 10.1007/s11265-021-01717-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119276174&doi=10.1007%2Fs11265-021-01717-4&partnerID=40&md5=c9a562eaa9dbc51bb73ccbb9e93f46ac
N1  - cited By 0
N2  - Diabetes is the third chronic disease threatening human health after cardiovascular and cerebrovascular diseases and malignant tumors. The latest survey shows that there are as many as 463 million diabetic patients in the world, most of which are type 2 diabetes, and present a state of high incidence. Therefore, preventing and controlling the occurrence of type 2 diabetes is of great strategic significance for protecting human health and saving medical resources. This paper uses the SVM classification technology in data mining to establish a type 2 diabetes risk prediction model based on the SVM classifier, and uses the model to predict the original data of diabetic patients in the endocrinology department of a third-class hospital in Wuhan. Finally, an evaluation tool is used to evaluate the prediction performance and quality of the prediction model. The experimental results show that the prediction model based on the SVM classifier has the advantages of high prediction accuracy, good stability, fast learning speed and good classification effect under complex clinical data sets. It has important guiding significance for assisting the clinical diagnosis and risk prediction of type 2 diabetes. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - A generalized method for the detection of vascular structure in pathological retinal images
A1  - Kaur, J
A1  - Mittal, D
Y1  - 2017///
KW  - age related macular degeneration; anatomical concepts; Article; controlled study; diabetes mellitus; diabetic retinopathy; diagnostic accuracy; eye fundus; fundus camera; human; image analysis; major clinical study; neovascularization (pathology); optic disk; priority journal; prophylaxis; retina image; sensitivity and specificity; visual impairment; visual system parameters
PB  - PWN-Polish Scientific Publishers
JF  - Biocybernetics and Biomedical Engineering
VL  - 37
IS  - 1
SP  - 184
EP  - 200
DO  - 10.1016/j.bbe.2016.09.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012082491&doi=10.1016%2Fj.bbe.2016.09.002&partnerID=40&md5=475469a92ee349bdba5a17deb0790e85
N1  - cited By 24
N2  - Variations in blood vasculature morphology of retinal fundus images is one of the dominant characteristic for the early detection and analysis of retinal abnormalities. Therefore the accurate interpretation of blood vasculature is useful for ophthalmologists to diagnose patients that suffer from retinal abnormalities. A generalized method to detect and segment blood vasculature using retinal fundus images has been proposed in this work using (i) preprocessing for quality improvement of retinal fundus images, (ii) initial segmentation of vasculature map to find vascular and non vascular structures, (iii) extraction of relevant set of geometrical based features from the vasculature map and intensity based features from original retinal fundus image that differentiate vascular and non vascular structures efficiently, (iv) supervised classification of vascular and non vascular structures using the extracted features, and (v) joining of candidate vascular structures to create connectivity. The proposed method is evaluated on clinically acquired dataset and different publically available standard datasets such as DRIVE, STARE, ARIA and HRF. The clinically acquired dataset consists of 468 retinal fundus images comprising of healthy images, images with mild, intermediate and severe pathologies. Test results of the proposed method shows average sensitivity/specificity/accuracy of 85.43/97.94/95.45 on the 785 retinal fundus images. The proposed method shows an improvement of 14.01% in sensitivity without degrading specificity and accuracy in comparison to the recently published methods. © 2017 Nałęcz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences
ER  - 
TY  - CONF
T1  - Machine learning and pattern classification in identification of indigenous retinal pathology
A1  - Jelinek, H F
A1  - Rocha, A
A1  - Carvalho, T
A1  - Goldenstein, S
A1  - Wainer, J
Y1  - 2011///
KW  -  Aldehydes
KW  -  Algorithms; Artificial Intelligence; Diabetic Retinopathy; Humans; Image Enhancement; Image Interpretation
KW  -  Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity
KW  -  Computer-Assisted; Information Storage and Retrieval; Pattern Recognition
KW  -  Diseases; Eye protection; Ophthalmology; Pathology
KW  -  algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; diabetic retinopathy; human; image enhancement; information retrieval; methodology; pathology; reproducibility; retinoscopy; sensitivity and specificity
KW  - Automated assessment; Diabetic retinopathy; Early diagnosis; Ethnic groups; Human retina; Training sets
JF  - Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS
SP  - 5951
EP  - 5954
SN  - 9781424441211
DO  - 10.1109/IEMBS.2011.6091471
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862298785&doi=10.1109%2FIEMBS.2011.6091471&partnerID=40&md5=8e3da0d5f5d24f3ec3ed41a5236afb77
N1  - cited By 21
N2  - Diabetic retinopathy (DR) is a complication of diabetes, which if untreated leads to blindness. DR early diagnosis and treatment improve outcomes. Automated assessment of single lesions associated with DR has been investigated for sometime. To improve on classification, especially across different ethnic groups, we present an approach using points-of-interest and visual dictionary that contains important features required to identify retinal pathology. Variation in images of the human retina with respect to differences in pigmentation and presence of diverse lesions can be analyzed without the necessity of preprocessing and utilizing different training sets to account for ethnic differences for instance. © 2011 IEEE.
ER  - 
TY  - JOUR
T1  - Agent-based big data mining
A1  - Alotaibi, N M
A1  - Abdullah, M
A1  - Mosli, H
Y1  - 2019///
PB  - World Academy of Research in Science and Engineering
JF  - International Journal of Advanced Trends in Computer Science and Engineering
VL  - 8
IS  - 1
SP  - 245
EP  - 252
DO  - 10.30534/ijatcse/2019/4481.12019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063725261&doi=10.30534%2Fijatcse%2F2019%2F4481.12019&partnerID=40&md5=d676afd911faa387d3a93ff35a5ef2b8
N1  - cited By 3
N2  - Big data is the term used to describe a data, which is difficult to process, manage and analyze using traditional databases or data mining algorithms. Useful knowledge can be extracted from this big data with the help of data mining. Due to the volume, variety, and velocity of data, traditional techniques of data mining may be unsuitable to work with big data. As a result, there is a basic need to create powerful and productive enormous information mining methods. Characterization is one of the information mining strategies that is capable of processing a large amount of data and is growing in popularity. It is used to map a data item into one of several predefined classes or categories. Healthcare data is one form of big data not only for its sheer volume, but also for its complexity, diversity, and speed at which it is generated and must be managed. In this paper, we present the problem of mining the big data using software agent. The main goal of this paper is to develop and implement an agent-based big data classification model that can predict the severity of diabetes disease. Results proved that using agent technology in the preprocessing stage saved the memory storage from 8.66 TB to 5 GB memory space. The transfer data time is reduced from about 12 days to about 10 minutes after preprocessing data remotely using the agent. Regarding classification accuracy, the proposed model has proven 87% accuracy and 65% reliability. © 2019, World Academy of Research in Science and Engineering. All rights reserved.
ER  - 
TY  - JOUR
T1  - Hyper parameter tuned deep learning based lenet architecture for detection and classification of diabetic retinopathy images
A1  - Yazhini, K
A1  - Loganathan, D
Y1  - 2020///
PB  - International Journal of Scientific and Technology Research
JF  - International Journal of Scientific and Technology Research
VL  - 9
IS  - 4
SP  - 958
EP  - 964
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083494417&partnerID=40&md5=7ea53b4f5e672b421cf94674b9607879
N1  - cited By 0
N2  - Diabetic retinopathy (DR) is a sickness appearing in the eye because of a rise in blood glucose level. Amongst the people under the age group of 70, half of the death is connected to diabetes. The earlier detection and medication could result in minimal loss of sight in various DR patients. When the signs of DR are detected, the seriousness of the disease needs to be validated for providing appropriate treatment. This study develops a new classification model for DR images by the use of deep learning based LetNet model. The proposed model involves a gradient descent (GD) based Hyper parameter tuned LeNet-5 model called GD-LeNet-5 model for the classification of DR images. The GD-LeNet-5 model involves a series of processes namely preprocessing, segmentation, feature extraction and finally classification. The presented model is tested using a DR dataset from Kaggle. The extensive experimental study clearly portrayed the superior outcome of the GD-LeNet-5 model with the maximum accuracy, sensitivity and specificity of 72.80%, 51.50% and 81.82% respectively. © 2020 IJSTR.
ER  - 
TY  - CONF
T1  - Impact of preprocessing for diagnosis of diabetes mellitus using artificial neural networks
A1  - Jayalskshmi, T
A1  - Santhakumaran, A
Y1  - 2010///
KW  -  Autocorrelation; Backpropagation; Classification (of information); Data flow analysis; Learning systems; Medical problems; Processing; Value engineering
KW  -  Neural networks
KW  - Area of interest; Artificial Neural Network; Classification accuracy; Data sets; Diabetes mellitus; Experimental system; Machine-learning; Medical data; Missing values; Pima Indian Diabetes; Pre-processing; Pre-processing method
JF  - ICMLC 2010 - The 2nd International Conference on Machine Learning and Computing
SP  - 109
EP  - 112
SN  - 9780769539775
DO  - 10.1109/ICMLC.2010.65
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953074797&doi=10.1109%2FICMLC.2010.65&partnerID=40&md5=c91001cb1d353b22a49c36476c896904
N1  - cited By 15
N2  - Medicine has always benefited from the technology. Artificial Neural Networks is currently the promising area of interest to solve medical problems. Diagnosis of diabetes is one of the most challenging problems in machine learning. This medical data set is seldom complete. Artificial neural networks require complete set of data for an accurate classification. The system explains how the pre-processing procedure and missing values influence the data set during the classification. The implemented system compares various missing value techniques and pre-processing techniques. Some combinations prove the real influence of these techniques. A classifier has applied to Pima Indian Diabetes dataset and the results were improved tremendously when using certain combination of preprocessing and missing value techniques. The experimental system achieves an excellent classification accuracy of 99% which is best than before. © 2010 IEEE.
ER  - 
TY  - CONF
T1  - Shape-printed nanozyme coated wet tissue paper based sensor for electrochemical sensing of 8-Hydroxy-2' -deoxyguanosine
A1  - Pandey, I
A1  - Tiwari, J D
ED  - Soni S.K. Tiwari P., Yadav O P Maurya R Singh B N
Y1  - 2020///
KW  -  Biological samples; Chronic disease; Conductive papers; Electrochemical sensing; Oxidative stress biomarkers; Selective adsorption; Selective sensing; Specific interaction
KW  -  Conductive Papers; Diseases; Monitoring; Paper Products; Penetration; Shape; Stresses; Tissue
KW  -  Wearable sensors
KW  - Biocompatibility; Biomolecules; Diseases; Graphene; Paper products; Tissue
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - International Conference on Electrical and Electronics Engineering, ICE3 2020
SP  - 522
EP  - 526
SN  - 9781728158464
DO  - 10.1109/ICE348803.2020.9122854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087720078&doi=10.1109%2FICE348803.2020.9122854&partnerID=40&md5=ddfabc4301364687821fdf9130c03cf0
N1  - cited By 1
N2  - Flexible and biocompatible wearable sensors for the management of chronic diseases are in demand. The disposable, biocompatible, flexible and lightweight conductive paper-based sensing system was fabricated for 8-hydroxydeoxyguanosine. This biomolecule can biomark the early symptoms of various clinically important diseases such as cancer, diabetes, and atherosclerosis. Wet tissue paper modified with graphene nanoparticles was used for the analysis of 8-hydroxydeoxyguanosine in healthy human and patient biological samples. Here imprinted nanozymes were used as a receptor that promotes selective sensing of 8-hydroxydeoxyguanosine. Shape printed nanozyme contains hydroxyl and carboxyl groups which promote the penetration of 8-hydroxydeoxyguanosine and its selective adsorption. Graphene and nanozyme both catalyze the selective sensing of 8-hydroxydeoxyguanosine. When 8-OHdG was added the specific interaction of 8-OHdG and nanozyme was examined by current signals. The current signal was linear with 8-OHdG concentration. The fabricated shape printed nanozyme based sensor shows a biological justifiable sensitivity with linear range of 1.0-100.00 μg mL-1 with no matrix effects. The Shape printed nanozyme modified tissue paper could be used as a wearable patch for early monitoring of oxidative stress biomarker by cancer patients itself without any preprocessing plan. © 2020 IEEE.
ER  - 
TY  - JOUR
T1  - Haemorrhages detection using geometrical techniques
A1  - Joshi, S
A1  - Karule, P T
Y1  - 2020///
KW  -  Degenerative disease; Diabetic retinopathy; Early diagnosis; Eye disease; Fundus image; Haemorrage; Hemorrhage detection; Lesion detection; Morphological segmentation; Red lesions
KW  -  Morphology
KW  -  accuracy; Article; blindness; contrast enhancement; disease exacerbation; eye axis length; eye disease; eye photography; histogram; human; image quality; image segmentation; intraocular hemorrhage; k nearest neighbor; optic nerve; priority journal; retina blood vessel; retina image; screening; sensitivity and specificity; support vector machine
KW  - Diagnosis; Extraction; Eye protection; Grading; Image analysis; Image enhancement; Ophthalmology
PB  - Taylor and Francis Ltd.
JF  - Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization
VL  - 8
IS  - 4
SP  - 436
EP  - 445
DO  - 10.1080/21681163.2020.1720823
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079150179&doi=10.1080%2F21681163.2020.1720823&partnerID=40&md5=504f68665fb4329c840ca7837f30d75a
N1  - cited By 1
N2  - Aim: With an increasing percentage of retinal pathology because of diabetes, eye screening for diabetic retinopathy (DR) is in demand throughout the world. Haemorrhages (HEs) are one of the common signs for the red lesion detection for early diagnosis of this progressive degenerative disease of the retina. The detection of HEs in early stage prevents further progression of the eye disease and reduces the risk of blindness. Methods: The proposed method is based on morphological segmentation and geometrical feature techniques for HEs extraction. This approach uses preprocessing, removal of other retinal image details, determining connected components analysis and applying a specific shape feature set which results in improving the recognition of HEs. Results: The proposed algorithm demonstrated 95.47% accuracy for a Diaretdb1 database at image level detection. Moreover, the proposed method achieved better performance results for HEs extraction when individual images were analysed and compared with the true HEs count according to the multiple expert evaluations. Conclusion: The results obtained prove the potential use of the proposed algorithm in terms of true HEs count to facilitate the DR grading criteria related to HEs. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.
ER  - 
TY  - CONF
T1  - Automated microaneurysm detection using mathematical morphology
A1  - Purwita, A A
A1  - Adityowibowo, K
A1  - Dameitry, A
A1  - Atman, M W S
Y1  - 2011///
KW  -  Algorithms; Biomedical engineering; Communication; Edge detection; Eye protection; Health risks; Image processing; Information technology
KW  -  Mathematical morphology
KW  - Adaptive histogram equalization; Canny edge detection; Diabetic retinopathy; Evaluation protocol; Ground truth; Microaneurysms; Optimal performance; postprocessing; preprocessing; Three stages
JF  - Proceedings - International Conference on Instrumentation, Communication, Information Technology and Biomedical Engineering 2011, ICICI-BME 2011
SP  - 117
EP  - 120
SN  - 9781457711657
DO  - 10.1109/ICICI-BME.2011.6108606
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855918304&doi=10.1109%2FICICI-BME.2011.6108606&partnerID=40&md5=63849bcfcdc8a3cb6e3cb30b2adbb795
N1  - cited By 14
N2  - Diabetes is one of the most rapidly increasing health threats worldwide. One of the further abnormalities is on retina (diabetic retinopathy). Early treatment can be conduct from detection of microaneurysms. The main concentration of this paper is the algorithm to detect microaneurysm with mathematical morphology. The mathematical morphology is choosen because microaneurysms tend to have typical shape. Generally, the algorithm is consist of three stages. The first is preprocessing, the second is detecting candidate microaneurysms, and the third is postprocessing handling the process of removing unused features. The performances is evaluated using the database from DIARETDB1 which provides ground truth collected from several experts and a strict evaluation protocol. The optimal performance will be satified when considering green channel obtaining, PAL size image processing, adaptive histogram equalization threshold at 0.03, canny edge detection threshold at 0.16, MAs and optimum microaneurysms size at 5 to 16 pixels. © 2011 IEEE.
ER  - 
TY  - CONF
T1  - Region Growing Based Segmentation Using Forstner Corner Detection Theory for Accurate Microaneurysms Detection in Retinal Fundus Images
A1  - Badgujar, R D
A1  - Deore, P J
Y1  - 2018///
KW  -  Corner detection; Diabetic retinopathy; Microaneurysms; Predator- preys; Region growing
KW  -  Edge detection
KW  - Computation theory; Diagnosis; Eye protection; Image denoising; Image enhancement; Image segmentation; Mathematical morphology; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2018 4th International Conference on Computing, Communication Control and Automation, ICCUBEA 2018
SN  - 9781538652572
DO  - 10.1109/ICCUBEA.2018.8697671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065204276&doi=10.1109%2FICCUBEA.2018.8697671&partnerID=40&md5=a5c4ddcae3c868b3aeb644f75be5e791
N1  - cited By 2
N2  - Several diseases have adverb effect on visual system of human visual system (eye) and diabetes is leading one among them. Prolong and uncontrolled diabetic patient is prone to Diabetic Retinopathy (DR). DR is asymptomatic disease hence requires detection in early stages to avoid big loss in vision. It shows immediate necessity of screening system to access eye images and analyze for DR detection. Microaneurysms (MAs) are primary lesion of DR, so their detection can give time for patient and ophthalmologist to prevent further vision loss. Region growing segmentation method is proposed for accurate detection of MAs. The preprocessing of retinal images uses non local means (NLM) filter and contrast limited adaptive histogram equalization (CLAHE) for noise removal and enhancement image quality. In segmentation, region growing algorithm in which the seeds for the grower are selected and positioned by means of Forstner Corner Detection theory is utilized. After segmentation, the redundant areas are removed using morphological operations (Niblack Adaptive Thresholding) and finally the Predator prey optimizer is used for optimizing the features for MA detection. © 2018 IEEE.
ER  - 
TY  - CONF
T1  - Optimization Convolutional Neural Network for Classification Diabetic Retinopathy Severity
A1  - Agustin, T
A1  - Sunyoto, A
Y1  - 2020///
KW  -  Computer aided; Data augmentation; Diabetes mellitus; Diabetic retinopathy; Diabetic retinopathy screening; Learning methods; Overfitting; Regularization technique
KW  -  Convolutional neural networks
KW  - Computer aided diagnosis; Convolution; Deep learning; Deep neural networks; Eye protection; Image recognition; Learning systems; Medical computing; Medical imaging
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 3rd International Conference on Information and Communications Technology, ICOIACT 2020
SP  - 66
EP  - 71
SN  - 9781728173566
DO  - 10.1109/ICOIACT50329.2020.9332087
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100871394&doi=10.1109%2FICOIACT50329.2020.9332087&partnerID=40&md5=dcf650bb3c986df11329bc43e38fd718
N1  - cited By 0
N2  - Computer-Aided Diabetic Retinopathy Screening makes it easier for doctors to provide a fast and accurate diagnosis to save the vision of diabetes mellitus patients. Artificial intelligence based on Deep Learning Convolutional Neural Networks is widely used in medical image analysis because it has excellent image recognition capabilities. However, the problem that often arises in the deep learning method is overfitting. To solve this problem, we propose an Ordinary Convolutional Neural Network. Using a dropout regularization technique, data augmentation, and preprocessing before performing the four-stage classification of Diabetic Retinopathy. With our proposed approach, we obtained an accuracy of 98.02%, a sensitivity of 95.33%, and a specificity of 98.53% without overfitting. © 2020 IEEE.
ER  - 
TY  - JOUR
T1  - Chronic kidney disease prediction using machine learning models
A1  - Revathy, S
A1  - Bharathi, B
A1  - Jeyanthi, P
A1  - Ramesh, M
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Engineering and Advanced Technology
VL  - 9
IS  - 1
SP  - 6364
EP  - 6367
DO  - 10.35940/ijeat.A2213.109119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074664745&doi=10.35940%2Fijeat.A2213.109119&partnerID=40&md5=7d3831ec44a1b3225d4a96c0a8c497fb
N1  - cited By 10
N2  - The field of biosciences have advanced to a larger extent and have generated large amounts of information from Electronic Health Records. This have given rise to the acute need of knowledge generation from this enormous amount of data. Data mining methods and machine learning play a major role in this aspect of biosciences. Chronic Kidney Disease(CKD) is a condition in which the kidneys are damaged and cannot filter blood as they always do. A family history of kidney diseases or failure, high blood pressure, type 2 diabetes may lead to CKD. This is a lasting damage to the kidney and chances of getting worser by time is high. The very common complications that results due to a kidney failure are heart diseases, anemia, bone diseases, high potasium and calcium. The worst case situation leads to complete kidney failure and necessitates kidney transplant to live. An early detection of CKD can improve the quality of life to a greater extent. This calls for good prediction algorithm to predict CKD at an earlier stage. Literature shows a wide range of machine learning algorithms employed for the prediction of CKD. This paper uses data preprocessing,data transformation and various classifiers to predict CKD and also proposes best Prediction framework for CKD. The results of the framework show promising results of better prediction at an early stage of CKD. © BEIESP.
ER  - 
TY  - CONF
T1  - Diabetes data analysis and prediction model discovery using RapidMiner
A1  - Han, J
A1  - Rodriguze, J C
A1  - Beheshti, M
Y1  - 2008///
KW  -  Bioinformatics; Information management; Mining
KW  -  Data mining
KW  - Biomedical datum; Data analysis; Data mining process; Data mining techniques; Data normalizations; Data pre-processing; Data sets; Numerical discretization; Prediction models; Visual data analysis
JF  - Proceedings of the 2008 2nd International Conference on Future Generation Communication and Networking, FGCN 2008 and BSBT 2008: 2008 International Conference on Bio-Science and Bio-Technology
VL  - 3
SP  - 96
EP  - 99
SN  - 9780769534312
DO  - 10.1109/FGCN.2008.226
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-62449195440&doi=10.1109%2FFGCN.2008.226&partnerID=40&md5=49ea2783f5640a4d2b56f3b023380177
N1  - cited By 71
N2  - Data mining techniques have been extensively applied in bioinformatics to analyze biomedical data. In this paper, we choose the Rapid-I's RapidMiner as our tool to analyze a Pima Indians Diabetes Data Set, which collects the information of patients with and without developing diabetes. The discussion follows the data mining process. The focus will be on the data preprocessing, including attribute identification and selection, outlier removal, data normalization and numerical discretization, visual data analysis, hidden relationships discovery, and a diabetes prediction model construction. © 2008 IEEE.
ER  - 
TY  - CONF
T1  - Weighted ensemble based automatic detection of exudates in fundus photographs
A1  - Prentasic, P
A1  - Loncaric, S
Y1  - 2014///
KW  -  Algorithms; Diabetic Retinopathy; Exudates and Transudates; Fundus Oculi; Humans; Image Interpretation
KW  -  Automatic Detection; Color fundus photograph; Diabetic retinopathy; Experimental validations; Extraction algorithms; Exudate detections; Image processing and analysis; State-of-the-art methods
KW  -  Computer-Assisted; Ophthalmoscopy; Photography; Sensitivity and Specificity; Support Vector Machine
KW  -  Eye protection
KW  -  algorithm; computer assisted diagnosis; diabetic retinopathy; exudate; eye fundus; human; ophthalmoscopy; photography; sensitivity and specificity; support vector machine
KW  - Artificial intelligence; Color; Diagnosis; Extraction; Image processing; Learning systems; Photography; Simulated annealing
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014
SP  - 138
EP  - 141
SN  - 9781424479290
DO  - 10.1109/EMBC.2014.6943548
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929463463&doi=10.1109%2FEMBC.2014.6943548&partnerID=40&md5=9ba4879355de7f21a9d61693c0c38a7f
N1  - cited By 9
N2  - Diabetic retinopathy (DR) is a visual complication of diabetes, which has become one of the leading causes of preventable blindness in the world. Exudate detection is an important problem in automatic screening systems for detection of diabetic retinopathy using color fundus photographs. In this paper, we present a method for detection of exudates in color fundus photographs, which combines several preprocessing and candidate extraction algorithms to increase the exudate detection accuracy. The first stage of the method consists of an ensemble of several exudate candidate extraction algorithms. In the learning phase, simulated annealing is used to determine weights for combining the results of the ensemble candidate extraction algorithms. The second stage of the method uses a machine learning-based classification for detection of exudate regions. The experimental validation was performed using the DRiDB color fundus image set. The validation has demonstrated that the proposed method achieved higher accuracy in comparison to state-of-the art methods. © 2014 IEEE.
ER  - 
TY  - CONF
T1  - An Improved Method for Disease Prediction Using Fuzzy Approach
A1  - Chetty, N
A1  - Vaisla, K S
A1  - Patil, N
Y1  - 2015///
KW  -  Accuracy of classifications; Cross-validation technique; Fuzzy C-means algorithms; Fuzzy c-means clustering algorithms; Fuzzy k-NN algorithm; K nearest neighbor algorithm; k-NN algorithm; Liver disorder
KW  -  Clustering algorithms
KW  - Algorithms; Copying; Data mining; Forecasting; Fuzzy clustering; Fuzzy systems; Learning algorithms; Nearest neighbor search
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2015 2nd IEEE International Conference on Advances in Computing and Communication Engineering, ICACCE 2015
SP  - 568
EP  - 572
SN  - 9781479917341
DO  - 10.1109/ICACCE.2015.67
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959046915&doi=10.1109%2FICACCE.2015.67&partnerID=40&md5=1912e359171f8f78a3ea3955fad309df
N1  - cited By 20
N2  - Data mining is a process of extracting useful information from the huge amount of data. Data Mining has great scope in the field of medicine. This article deals with the working on PIMA and Liver-disorder datasets. Many researchers have proposed the use of K-nearest neighbor (KNN) algorithm for diabetes disease prediction. Some researchers have proposed a different approach by using K-means clustering for preprocessing and then using KNN for classification. These approaches resulted in poor classification accuracy or prediction. In our work we proposed and developed two different methods first one is fuzzy c-means clustering algorithm followed by a KNN classifier and second one is fuzzy c-means clustering algorithm followed by fuzzy KNN classifier to improve the accuracy of classification. We are successful in obtaining the better results than the existing methods for the given datasets. Our second approach produced better result than the first one. Classification is carried out using ten folds cross-validation technique. © 2015 IEEE.
ER  - 
TY  - CONF
T1  - Accuracy Enhancement of Correlated Naive Bayes Method by Using Correlation Feature Selection (CFS) for Health Data Classification
A1  - Hairani, H
A1  - Innuddin, M
A1  - Rahardi, M
Y1  - 2020///
KW  -  10-fold cross-validation; Accuracy enhancement; Classification methods; Correlation features; Data attributes; Feature selection methods; Health data; Pima Indian Diabetes
KW  -  Classification (of information)
KW  - Barium compounds; Classifiers; Feature extraction; Metadata
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 3rd International Conference on Information and Communications Technology, ICOIACT 2020
SP  - 51
EP  - 55
SN  - 9781728173566
DO  - 10.1109/ICOIACT50329.2020.9332021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100909778&doi=10.1109%2FICOIACT50329.2020.9332021&partnerID=40&md5=794f0e0363b219f8a5f7f0b865075eea
N1  - cited By 2
N2  - The main problem of health datasets is having a lot of data attributes and irrelevant features, so that the computation of the classification method takes more time to solve it. The purpose of this study is to implement the CFS feature selection method to improve the accuracy of the Correlated Naive Bayes method. Therefore, there are some stages used in this study such as: collecting dataset of Pima Indian Diabetes, preprocessing data especially for transformation data, selection of CFS feature, classification, and then performance evaluation based on accuracy. Based on the test results using the 10-fold cross validation method, the best accuracy is about 69.4% compared without feature selection, it is obtained by a combination of Correlated Naive Bayes and CFS methods. Thus, the CFS feature selection method may increase the accuracy of the Correlated Naive Bayes method by 2.25%. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - Predicting Diabetes Implementing Hybrid Approach
A1  - Giri, B
A1  - Ghosh, N S
A1  - Majumdar, R
A1  - Ghosh, A
Y1  - 2020///
KW  -  Blood glucose; Classification accuracy; Hybrid approach; Hybrid forms; Medical information
KW  -  Diagnosis
KW  - Classification (of information); Glucose
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - ICRITO 2020 - IEEE 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)
SP  - 388
EP  - 391
SN  - 9781728170169
DO  - 10.1109/ICRITO48877.2020.9197971
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093112415&doi=10.1109%2FICRITO48877.2020.9197971&partnerID=40&md5=f5ab901c20bcc00ee189e78503101a7b
N1  - cited By 3
N2  - Medically stated diabetes is the outcome when glucose/sugar level is high. Blood glucose is the key category of sugar and major foundation of energy. Hybrid approach is used to extract knowledge related diabetes from medical information source as database and helps to generate clear and understandable diagnosis of the problem as well as implementing medical diagnosis. This work orients towards the progress of a hybrid form based on Pima Indian diabetic database (PIDD).Experimental results indicate that the proposed method along with preprocessing produces better result with classification accuracy of 86%. In this study different algorithms are implemented to predict whether a patient having diabetes along its severity level. © 2020 IEEE.
ER  - 
TY  - CONF
T1  - Diabetic Retinopathy Detection using Deep Convolutional Neural Network with Visualization of Guided Grad-CA
A1  - Paradisa, R H
A1  - Bustamam, A
A1  - Victor, A A
A1  - Yudantha, A R
A1  - Sarwinda, D
ED  - Ismail I.E. Hermawan I., Rasyidin M Y B Huzaifa M Muharram A T Marcheeta N Kurniawati D Yuly A R Agustin M Nalawati R E Nugrahadi D T Budiman I
Y1  - 2021///
KW  -  Automated systems; Blood sugar levels; Diabetic retinopathy; Fundus image; Guided grad-cam; Learning techniques; Moment estimation; Neural network method; Optimizers; Stochastic gradient descent
KW  -  Deep neural networks
KW  - Backpropagation; Blood vessels; Convolution; Convolutional neural networks; Eye protection; Gradient methods; Health risks; Image segmentation; Optimization; Stochastic systems
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2021 4th International Conference on Computer and Informatics Engineering: IT-Based Digital Industrial Innovation for the Welfare of Society, IC2IE 2021
SP  - 19
EP  - 24
SN  - 9781665442886
DO  - 10.1109/IC2IE53219.2021.9649326
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124286346&doi=10.1109%2FIC2IE53219.2021.9649326&partnerID=40&md5=e35e1e6e90a4eda5a153a1a2f916ec18
N1  - cited By 1
N2  - One of the complications of diabetes that represents a serious threat to world health is Diabetic Retinopathy (DR). High blood sugar levels in people with diabetes can damage the blood vessels in the retina and causing blindness. DR can be detected by examining the fundus image by an ophthalmologist. However, the limited number of ophthalmologists who can analyze fundus image is an obstacle because the number of DR sufferers continues to increase. Therefore, an automated system is needed to help doctors diagnose the disease. Researchers have developed deep learning techniques as Artificial Intelligence (AI) approach to finding DR in fundus images. In this research, we use the Deep Convolutional Neural Networks method with InceptionV3 structure and various optimizers such as the Stochastic Gradient Descent with Momentum (SGDM), Root Mean Square Propagation (RMSprop), and Adaptive Moment Estimation (Adam). The fundus image dataset previously through the augmentation and preprocessing steps to make it easier for the model to recognize the image. The InceptionV3 model with the Adam optimizer gave the best results in detecting DR lesions from the Kaggle dataset with 96% accuracy. This paper also presents a Grad-CAM guided activation map that can describe the position of the suspicious lesion to explain the results of DR detection. © 2021 IEEE.
ER  - 
TY  - JOUR
T1  - Proliferative Diabetic Retinopathy Diagnostic Investigation Using Retinal Blood Vessels Mining Technique
A1  - Ponnibala, M
A1  - Priyanka, E B
A1  - Thangavel, S
Y1  - 2021///
KW  -  Blood vessels
KW  -  Diabetic retinopathy; Mining techniques; Retinal blood vessels; Scar tissues; Thresholding
KW  - Blood; Diseases; Eye protection; Image processing; Ophthalmology
PB  - Springer
JF  - Sensing and Imaging
VL  - 22
IS  - 1
DO  - 10.1007/s11220-021-00331-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100932476&doi=10.1007%2Fs11220-021-00331-9&partnerID=40&md5=b10828a5d27bf10acdfcf7d16a72b1ca
N1  - cited By 3
N2  - In clinical field, wide assortments of utilizations can be managed utilizing image handling. Recognition and screening of retinal sicknesses was one such application in picture preparing. Diabetic retinopathy is an inconvenience of diabetes. The ailment influences veins inside the retina. The retina is a region lying at the rear of the eyeball. In the most punctual phase of the infection, the little veins, or vessels, gotten slenderer, more fragile and inevitably they spill blood. A patient's sight at this stage is still acceptable yet an ophthalmologist can identify and see the irregularities in the retina. As the sickness advances, some veins are obstructed. These trigger the retina to develop fresh blood vessels, which are unusual, delicate, and effectively drain. In the later phase of the ailment, fresh blood vessels are developed ceaselessly just as scar tissue. Eventually, retina will be isolates from an eye. Another strategy for removing the retinal veins from the shading fundus retinal picture dependent on include grouping was proposed in this undertaking, to decrease the ophthalmologists' time and vitality for checking the retinal pictures. The veins are separated from the shading fundus picture by applying the preprocessing strategies and division procedures utilizing coordinated channel and adjusted nearby entropy thresholding activity. The preprocessed picture was then utilized for highlight extraction and these highlights were utilized for order reason. At long last, arrangement procedure was utilized for diagnosing the proliferative diabetic retinopathy. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Machine Learning-Based Gynecologic Tumor Diagnosis and Its Postoperative Incisional Infection Influence Factor Analysis
A1  - Shen, Q
A1  - Wang, L
Y1  - 2021///
KW  -  Adaptive neural network models; Breast cancer diagnosis; Control groups; Gynecologic tumors; Infection rates; Machine-learning; Nursing interventions; Study Groups; Tumor diagnosis; Tumor patient
KW  -  Factor Analysis
KW  -  Female; Humans; Machine Learning; Retrospective Studies; Surgical Wound Infection
KW  -  Hospitals
KW  -  Statistical; Female; Genital Neoplasms
KW  -  anemia; Article; artificial neural network; breast cancer; cancer surgery; data mining; diabetes mellitus; diagnostic accuracy; female genital tract tumor; fever; hospitalization; human; incisional hernia; infection rate; length of stay; machine learning; natural language processing; operation duration; Parkinson disease; postoperative care; receiver operating characteristic; recurrent neural network; support vector machine; tumor diagnosis; controlled study; factor analysis; female; female genital tract tumor; machine learning; randomized controlled trial; retrospective study; surgical infection
KW  - Diagnosis; Diseases; Nursing; Recurrent neural networks; Surgery; Tumors
PB  - Hindawi Limited
JF  - Journal of Healthcare Engineering
VL  - 2021
DO  - 10.1155/2021/7956184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120887642&doi=10.1155%2F2021%2F7956184&partnerID=40&md5=86da0d0a7fb5c2c9637b9f73c830c48e
N1  - cited By 0
N2  - Various factors influencing postoperative incisional infection in gynecologic tumors were analyzed, and the value of quality nursing intervention was studied. In this study, 74 surgically treated gynecologic tumor patients were randomly selected from within the hospital as the study population and were divided into study and control groups. For this purpose, the whole-group random sampling method is utilized to compare the postoperative incisional infection rates of the two groups, analyze their influencing factors, and develop quality nursing interventions. In this paper, a breast cancer diagnosis prediction model was developed by combining the self-attentive mechanism. The preprocessing work such as data quantification and normalization was performed first which is followed by adding the preprocessed data to the self-attentive mechanism. This model has solved the problem that recurrent neural networks (RNNs) could not extract and calculate the features at the same time. Likewise, it has solved the drawback that the RNN could not consider global features at the same time when extracting the features, and then, the feature matrix extracted by the self-attentive mechanism was added to the adaptive neural network. The adaptive neural network model for breast cancer diagnosis prediction was constructed and, finally, relevant parameters of the adaptive neural network model were adjusted according to different tasks to make the model performance optimal. Experimental results showed that the postoperative incision infection rate of patients in the study group was 2.70%, which was significantly lower than that of 21.62% in the control group (P<0.05). Likewise, operation time, operation method, hospitalization time, preoperative fever, diabetes mellitus, and anemia were the main influencing factors of postoperative incision infection in women with gynecologic tumors. The time of surgery, surgical method, long hospital stay, preoperative fever, diabetes, and anemia are the main factors that lead to postoperative incisional infection in female gynecologic tumor patients. © 2021 Qian Shen and Ling Wang.
ER  - 
TY  - CONF
T1  - Automatic fundus image segmentation for diabetic retinopathy diagnosis by multiple modified U-nets and segnets
A1  - Ananda, S
A1  - Kitahara, D
A1  - Hirabayashi, A
A1  - Udaya Kumar Reddy, K R
Y1  - 2019///
KW  -  Blood sugar levels; Diabetes mellitus; Diabetic retinopathy; Individual network; Learning techniques; Retinal fundus images; Segmentation accuracy; Segmentation methods
KW  -  Image segmentation
KW  - Deep learning; Deep neural networks; Diagnosis; Eye protection
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2019
SP  - 1582
EP  - 1588
SN  - 9781728132488
DO  - 10.1109/APSIPAASC47483.2019.9023290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082386466&doi=10.1109%2FAPSIPAASC47483.2019.9023290&partnerID=40&md5=9d8b6ffe9cff36228573d7c6595ca19d
N1  - cited By 3
N2  - Diabetes mellitus leads to damage of the retina by a high blood sugar level. This disease is called diabetic retinopathy (DR), and it is one major cause of blindness among working-aged people. DR affects about 80% of patients who have had diabetes for twenty years or more. The longer a period of diabetes is, the higher the risk of developing DR is. In order to prevent the blindness caused by DR, accurate DR diagnosis from a retinal fundus image is important. Recently, deep learning techniques play significant role in the field of computer vision. When we apply deep learning to segmentation of abnormal parts in fundus images, two major problems arise. One is that the number of available data is insufficient to train a deep neural network. The other is that the sizes of the abnormal parts are quite different depending on the type of the disease, which leads to low segmentation accuracy of small diseases. These two problems make the fundus image segmentation challenging. In this paper, we propose a segmentation method using multiple deep neural networks. To train the deep neural networks from a small number of data, we use data augmentation as preprocessing and adopt the Dice coefficient with the binary cross entropy as a loss function. Moreover, to improve the segmentation accuracy of small diseases, e.g., microaneurysms, we construct one individual network for each type of the disease. In experiments, the networks are trained from IDRiD dataset and tested for MESSIDOR dataset. We compare and discuss the accuracy of the proposed method with modified U-Nets and SegNets. © 2019 IEEE.
ER  - 
TY  - CONF
T1  - An amalgam KNN to predict diabetes mellitus
A1  - Nirmaladevi, M
A1  - Alias Balamurugan, S A
A1  - Swathi, U V
Y1  - 2013///
KW  -  Classification (of information); Communication; Data mining; Nanotechnology
KW  -  Learning algorithms
KW  - Classification accuracy; Cross validation; Cross-validation technique; Diabetes mellitus; K nearest neighbor (KNN); K-means; k-NN algorithm; Medical data mining
JF  - 2013 IEEE International Conference on Emerging Trends in Computing, Communication and Nanotechnology, ICE-CCN 2013
SP  - 691
EP  - 695
SN  - 9781467350365
DO  - 10.1109/ICE-CCN.2013.6528591
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881036675&doi=10.1109%2FICE-CCN.2013.6528591&partnerID=40&md5=fd2ff0e513819e118d5d081315d2cb75
N1  - cited By 53
N2  - Medical Data mining extracts hidden patterns from medical data. This paper presents the development of an amalgam model for classifying Pima Indian diabetic database (PIDD). This amalgam model combines k-means with k-Nearest Neighbor (KNN) with multi-sep preprocessing. Many researchers have found that the KNN algorithm accomplishes very good performance in their experiments on different data sets. In this amalgam model, the quality of the data is improved by removing noisy data thereby helping to improve the accuracy and efficiency of the KNN algorithm.k-means clustering is used to identify and eliminate incorrectly classified instances. The missing values are replaced by means and medians. A fine tuned classification is done using k-Nearest Neighbor (KNN) by taking the correctly clustered instance with preprocessed subset as inputs for the KNN. The best choice of k depends upon the data. Generally, larger values of k reduce the effect of noise on the classification. A good k is selected by cross-validation technique. The aim of this paper is determining the value of k for PIDD for better classification accuracy using amalgam KNN. Experimental results signify the proposed amalgam KNN along with preprocessing produces best result for different k values. If k value is more the proposed model obtained the classification accuracy of 97.4%. Ten fold cross validation with larger k value produces better classification accuracy for PIDD. The results are also compared with simple KNN and cascaded K-MEANS and KNN for the same k values. © 2013 IEEE.
ER  - 
TY  - CONF
T1  - K-nearest neighbor based methodology for accurate diagnosis of diabetes mellitus
A1  - Panwar, M
A1  - Acharyya, A
A1  - Shafik, R A
A1  - Biswas, D
Y1  - 2017///
KW  -  Attribute checking; Classification accuracy; Diabetes mellitus; K-nearest neighbor classifier; K-nearest neighbors; Preprocessing; Preprocessing techniques; Quantitative metrics
KW  -  Diagnosis
KW  - Learning algorithms; Learning systems; Losses; Motion compensation; Nearest neighbor search; Pattern matching; Systems analysis
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2016 6th International Symposium on Embedded Computing and System Design, ISED 2016
SP  - 132
EP  - 136
SN  - 9781509025411
DO  - 10.1109/ISED.2016.7977069
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027542288&doi=10.1109%2FISED.2016.7977069&partnerID=40&md5=ce0255fbdeb06bec986a8d47227dcf5c
N1  - cited By 12
N2  - Diabetes is one of the leading causes of death, disability and economic loss throughout the world. Type 2 diabetes is more common (90-95% worldwide) type of diabetes. However, it can be prevented or delayed by taking the right care and interventions which indeed an early diagnosis. There has been much advancement in the field of various machine learning algorithms specifically for medical diagnosis. But due to partially complete medical data sets, accuracy often decreases, results in more number of misclassification that can lead t o harmful complications. An accurate prediction and diagnosis of a disease becomes a challenging research problem for many researchers. Therefore, aimed to improve the diagnosis accuracy we have proposed a new methodology, based on novel preprocessing techniques, and K-nearest neighbor classifier. The effectiveness of the proposed methodology is validated with the help of various quantitative metrics and a comparative analysis, with previously reported studies using the same UCI dataset focusing on pima-diabetes disease diagnosis. This is the first work of its kind, where 100% classification accuracy is achieved by feature reduction from eight to two that shows the out performance of the proposed methodology over existing methods. © 2016 IEEE.
ER  - 
TY  - JOUR
T1  - The prediction of mortality influential variables in an intensive care unit: a case study
A1  - Khajehali, N
A1  - Khajehali, Z
A1  - Tarokh, M J
Y1  - 2021///
KW  -  Comparison models; Data preprocessing; Diabetes mellitus; Health-care system; High blood pressures; Machine learning models; Optimal results; Pulmonary embolism
KW  -  Intensive care units
KW  - Adaptive boosting; Blood; Blood pressure; Data handling; Forecasting
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Personal and Ubiquitous Computing
DO  - 10.1007/s00779-021-01540-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101765037&doi=10.1007%2Fs00779-021-01540-5&partnerID=40&md5=060d0b9bce1de69b6da9dafca642ccbb
N1  - cited By 2
N2  - The intensive care units (ICUs) are among the most expensive and essential parts of all hospitals for extremely ill patients. This study aims to predict mortality and explore the crucial factors affecting it. Generally, in the health care systems, having a fast and precise ICU mortality prediction for patients plays a key role in care quality, resulting in reduced costs and improved survival chances of the patients. In this study, we used a medical dataset, including patients’ demographic details, underlying diseases, laboratory disorder, and LOS. Since accurate estimates are required to have optimal results, various data pre-processings as the initial steps are used here. Besides, machine learning models are employed to predict the risk of mortality ICU discharge. For AdaBoost model, these measures are considered AUC= 0.966, sensitivity (recall) = 87.88%, Kappa=0.859, F-measure = 89.23% making it, AdaBoost, accounts for the highest rate. Our model outperforms other comparison models by using various scenarios of data processing. The obtained results demonstrate that the high mortality can be caused by underlying diseases such as diabetes mellitus and high blood pressure, moderate Pulmonary Embolism Wells Score risk, platelet blood count less than 100000 (mcl), hypertension (HTN), high level of Bilirubin, smoking, and GCS level between 6 and 9. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Provissional access for improving classification accuracy on diabetes dataset
A1  - Sumathi, A
A1  - Meganathan, S
A1  - Revathi, S
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Engineering and Advanced Technology
VL  - 8
IS  - 6
SP  - 5245
EP  - 5248
DO  - 10.35940/ijeat.F9389.088619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072011985&doi=10.35940%2Fijeat.F9389.088619&partnerID=40&md5=212a93488b495396a10b98688bbec7a1
N1  - cited By 0
N2  - Data mining helps to solve many problems in the area of medical diagnosis using real-world data. However, much of the data is unrealizable as it does not have desirable features and contains a lot of gaps and errors. A complete set of data is a prerequisite for precise grouping and classification of a dataset. Preprocessing is a data mining technique that transforms the unrefined dataset into reliable and useful data. It is used for resolving the issues and changes raw data for next level processing. Discretization is a necessary step for data preprocessing task. It reduces the large chunks of numeric values to a group of well-organized values. It offers remarkable improvements in speed and accuracy in classification. This paper investigates the impact of preprocessing on the classification process. This work implements three techniques such as NaiveBayes, Logistic Regression, and SVM to classify Diabetes dataset. The experimental system is validated using discretize techniques and various classification algorithms. © BEIESP.
ER  - 
TY  - CONF
T1  - Study of machine learning algorithms for special disease prediction using principal of component analysis
A1  - Dhomse Kanchan, B
A1  - Mahale Kishor, M
Y1  - 2017///
KW  -  Algorithm approaches; Component analysis; Diabetic patient; Health organizations; Healthcare industry; Hidden information; Supervised machine learning; Vector machines
KW  -  Learning algorithms
KW  - Artificial intelligence; Cardiology; Computer aided diagnosis; Data mining; Decision making; Decision trees; Diseases; Education; Forecasting; Heart; Learning systems; Principal component analysis; Signal processing; Statistical tests; Supervised learning; Supports
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - International Conference on Global Trends in Signal Processing, Information Computing and Communication, ICGTSPICC 2016
SP  - 5
EP  - 10
SN  - 9781509004676
DO  - 10.1109/ICGTSPICC.2016.7955260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025148381&doi=10.1109%2FICGTSPICC.2016.7955260&partnerID=40&md5=8eb5637272c95d188b652e232f5970f2
N1  - cited By 58
N2  - The worldwide study on causes of death due to heart disease/syndrome has been observed that it is the major cause of death. If recent trends are allowed to continue, 23.6 million people will die from heart disease in coming 2030. The healthcare industry collects large amounts of heart disease data which unfortunately are not 'mined' to discover hidden information for effective decision making. In this paper, study of PCA has been done which finds the minimum number of attributes required to enhance the precision of various supervised machine learning algorithms. The purpose of this research is to study supervised machine learning algorithms to predict heart disease. Data mining has number of important techniques like categorization, preprocessing. Diabetic is a life threatening disease which prevent in several urbanized as well as emergent countries like India. The data categorization is diabetic patients datasets which is developed by collecting data from hospital repository consists of 1865 instances with dissimilar attributes. The examples in the dataset are two categories of blood tests, urine tests. In this research paper we discuss a variety of algorithm approaches of data mining that have been utilized for diabetic disease prediction. Data mining is a well known practice used by health organizations for classification of diseases such as diabetes and cancer in bioinformatics research. © 2016 IEEE.
ER  - 
TY  - JOUR
T1  - DR-IIXRN : Detection Algorithm of Diabetic Retinopathy Based on Deep Ensemble Learning and Attention Mechanism
A1  - Ai, Z
A1  - Huang, X
A1  - Fan, Y
A1  - Feng, J
A1  - Zeng, F
A1  - Lu, Y
Y1  - 2021///
KW  - Article; controlled study; convolutional neural network; coronavirus disease 2019; deep ensemble learning and attention; deep learning; diabetic retinopathy; diagnostic accuracy; disease severity; DR IIXRN algorithm; human; image enhancement; image preprocessing; image processing; image quality; imaging algorithm; imaging and display; inception resnet V2 network model; inception V3 network model; learning algorithm; machine learning; major clinical study; model; nasnetlarge network model; proliferative diabetic retinopathy; resnext101 network model; support vector machine; weighted voting model; xception network model
PB  - Frontiers Media S.A.
JF  - Frontiers in Neuroinformatics
VL  - 15
DO  - 10.3389/fninf.2021.778552
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122438501&doi=10.3389%2Ffninf.2021.778552&partnerID=40&md5=7c800f3b787dde5fa920f42245874fad
N1  - cited By 6
N2  - Diabetic retinopathy (DR) is one of the common chronic complications of diabetes and the most common blinding eye disease. If not treated in time, it might lead to visual impairment and even blindness in severe cases. Therefore, this article proposes an algorithm for detecting diabetic retinopathy based on deep ensemble learning and attention mechanism. First, image samples were preprocessed and enhanced to obtain high quality image data. Second, in order to improve the adaptability and accuracy of the detection algorithm, we constructed a holistic detection model DR-IIXRN, which consists of Inception V3, InceptionResNet V2, Xception, ResNeXt101, and NASNetLarge. For each base classifier, we modified the network model using transfer learning, fine-tuning, and attention mechanisms to improve its ability to detect DR. Finally, a weighted voting algorithm was used to determine which category (normal, mild, moderate, severe, or proliferative DR) the images belonged to. We also tuned the trained network model on the hospital data, and the real test samples in the hospital also confirmed the advantages of the algorithm in the detection of the diabetic retina. Experiments show that compared with the traditional single network model detection algorithm, the auc, accuracy, and recall rate of the proposed method are improved to 95, 92, and 92%, respectively, which proves the adaptability and correctness of the proposed method. Copyright © 2021 Ai, Huang, Fan, Feng, Zeng and Lu.
ER  - 
TY  - CONF
T1  - Exudate detection in retinal fundus images using combination of mathematical morphology and Renyi entropy thresholding
A1  - Qomariah, D U N
A1  - Tjandrasa, H
Y1  - 2018///
KW  -  Automatic Detection; Contrast Enhancement; Diabetic retinopathy; Exudate; Exudate detections; Measurement evaluations; Retinal fundus images; Thresholding
KW  -  Mathematical morphology
KW  - Blood vessels; Data communication systems; Eye protection; Image segmentation; Morphology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings of the 11th International Conference on Information and Communication Technology and System, ICTS 2017
VL  - 2018-January
SP  - 31
EP  - 36
SN  - 9781538628256
DO  - 10.1109/ICTS.2017.8265642
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050534246&doi=10.1109%2FICTS.2017.8265642&partnerID=40&md5=dc55042e749a1796d22a3dfb8cc013c2
N1  - cited By 9
N2  - Diabetic retinopathy (DR) is a microvascular complication of diabetes, causing abnormalities in the retina, and it is can cause blindness. Diabetic retinopathy can be detected by the appearance of hard exudates. Hard exudates are lipid formations leaking from these weakened blood vessels. Automatic detection of exudates is an early handler to diagnose diabetic retinopathy. This research proposed automatic detection of exudates using Renyi entropy thresholding and mathematical morphology. Renyi entropy thresholding has a controlling variable so that the obtained threshold value is more optimal. The proposed method using Renyi entropy thresholding and mathematical morphology has three stages: (1) preprocessing using contrast enhancement, (2) initial exudates detection based on mathematical morphology, and (3) exudates detection based on Renyi entropy thresholding. The test was performed using measurement evaluation method, sensitivity, specificity, and accuracy were 85.06%, 99.63%, and 99.54% respectively. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - Visualizing the clinical data of diabetes using data science and machine learning algorithms
A1  - Vishnu Priya, V
A1  - Abdul Gaffar, H
Y1  - 2019///
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
JF  - International Journal of Recent Technology and Engineering
VL  - 7
IS  - 6
SP  - 1960
EP  - 1963
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067932247&partnerID=40&md5=766b8666284f9fcdf52a6909ae78b4f9
N1  - cited By 0
N2  - In recent decades Machine learning and Data Science are providing best ways to analyze and solve various problems. In fact, those Machine Learning algorithms gives the best and optimized solutions. These methods are playing key role in providing efficient solutions for the health care problems like predicting the diseases in early stage, and even some automated systems run by Machine Learning Algorithms are prescribing medicines based on the patient’s symptoms. Diabetes is one among the chronic diseases from past years, which leads to the damage of patients eyes, nerves, heart and kidneys etc., In this project we are going to create a pipeline in which the data collected from the source is undergone through some preprocessing techniques and the Machine Learning Algorithms like SVM, KNN, Gradient Boasting, logistic regression and Random Forest are used to classify whether the patient is diabetic or not and the accuracy of these algorithms was measured by using some Evaluation methods like Train/Test Split. Finally, these data will be visualized by using Visualization Tools. © BEIESP.
ER  - 
TY  - CONF
T1  - Illumination Correction by Dehazing for Retinal Vessel Segmentation
A1  - Savelli, B
A1  - Bria, A
A1  - Galdran, A
A1  - Marrocco, C
A1  - Molinara, M
A1  - Campilho, A
A1  - Tortorella, F
ED  - Bamidis P.D. Konstantinidis S.Th., Rodrigues P P
Y1  - 2017///
KW  -  Color image processing
KW  -  Computer assisted diagnosis; Convolutional neural network; Dehazing; Illumination correction; Non-uniform illumination; retina; Retinal vessel segmentations; Vessel segmentation
KW  - Cameras; Computer aided diagnosis; Deep neural networks; Demulsification; Diagnosis; Eye protection; Image processing; Image segmentation; Mathematical morphology; Neural networks; Ophthalmology
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - IEEE Symposium on Computer-Based Medical Systems
VL  - 2017-June
SP  - 219
EP  - 224
SN  - 9781538617106
DO  - 10.1109/CBMS.2017.28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032509539&doi=10.1109%2FCBMS.2017.28&partnerID=40&md5=da1ccc30d0296c86e92e93389866ae35
N1  - cited By 17
N2  - Assessment of retinal vessels is fundamental for the diagnosis of many disorders such as heart diseases, diabetes and hypertension. The imaging of retina using advanced fundus camera has become a standard in computer-assisted diagnosis of opthalmic disorders. Modern cameras produce high quality color digital images, but during the acquisition process the light reflected by the retinal surface generates a luminosity and contrast variation. Irregular illumination can introduce severe distortions in the resulting images, decreasing the visibility of anatomical structures and consequently demoting the performance of the automated segmentation of these structures. In this paper, a novel approach for illumination correction of color fundus images is proposed and applied as preprocessing step for retinal vessel segmentation. Our method builds on the connection between two different phenomena, shadows and haze, and works by removing the haze from the image in the inverted intensity domain. This is shown to be equivalent to correct the nonuniform illumination in the original intensity domain. We tested the proposed method as preprocessing stage of two vessel segmentation methods, one unsupervised based on mathematical morphology, and one supervised based on deep learning Convolutional Neural Networks (CNN). Experiments were performed on the publicly available retinal image database DRIVE. Statistically significantly better vessel segmentation performance was achieved in both test cases when illumination correction was applied. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - An enhanced naïve bayes classification algorithm to predict type ii diabetes
A1  - CHOWDARY, P B K
A1  - KUMAR, R U
Y1  - 2021///
PB  - Taylor's University
JF  - Journal of Engineering Science and Technology
VL  - 16
IS  - 4
SP  - 2927
EP  - 2937
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111706522&partnerID=40&md5=b5698bb3615d43fff1b9fdf69c9ffe4f
N1  - cited By 1
N2  - Diabetes has become a dreadful and chronic disease in recent years and that increases blood sugar. The accumulation of blood sugar causes multiple complications which sometimes remains untreated and unknown. The traditional methods of identification lead to a poor diagnosis. The growth of machine learning provides a solution to predict this disease in the early stages and provide better inputs to diagnose the problem. This research is inspired by Naïve Bayes classification and handles the preprocessing of data efficiently. This paper designs a model to predict the probability of diabetes in patients with the highest precision. This paper examines various classification algorithms like Decision Trees, support vector machines (SVM). This paper initially preprocesses the Pima Indians Diabetes (PID) dataset by filling the missing values with multivariate imputation by chained equations. We later applied an enhanced Naïve Bayes classification algorithm on the dataset. Detailed comparison of the results on preprocessed and non-processed data is also presented in this research. This paper uses different measures such as accuracy, precision and F-Measure to compare various models on PID dataset. The proposed selection and enhanced Naïve Bayes classification algorithm obtained promising results. © 2021 Taylor's University. All rights reserved.
ER  - 
TY  - CONF
T1  - Decision tree discovery for the diagnosis of type II diabetes
A1  - AlJarullah, A A
Y1  - 2011///
KW  -  Data mining
KW  -  Decision trees; Diagnosis; Information technology; Innovation; Mathematical models; Medical computing; Medical education; Plant extracts
KW  - Data preprocessing; Data sets; Decision tree method; Handling missing values; Medical database; Numerical discretization; Prediction model; Second phase; Tree discovery; Type II
JF  - 2011 International Conference on Innovations in Information Technology, IIT 2011
SP  - 303
EP  - 307
SN  - 9781457703140
DO  - 10.1109/INNOVATIONS.2011.5893838
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959965420&doi=10.1109%2FINNOVATIONS.2011.5893838&partnerID=40&md5=443fbfa11bd4b472be3caee139b5aa6e
N1  - cited By 80
N2  - The discovery of knowledge from medical databases is important in order to make effective medical diagnosis. The aim of data mining is to extract knowledge from information stored in database and generate clear and understandable description of patterns. In this study, decision tree method was used to predict patients with developing diabetes. The dataset used is the Pima Indians Diabetes Data Set, which collects the information of patients with and without developing diabetes. The study goes through two phases. The first phase is data preprocessing including attribute identification and selection, handling missing values, and numerical discretization. The second phase is a diabetes prediction model construction using the decision tree method. Weka software was used throughout all the phases of this study. © 2011 IEEE.
ER  - 
TY  - CONF
T1  - A novel classification method for diagnosis of diabetes mellitus using artificial neural networks
A1  - Jayalakshmi, T
A1  - Santhakumaran, A
Y1  - 2010///
KW  -  Data storage equipment; Diagnosis; Medical problems; Pattern recognition; Processing; Signal processing; Value engineering
KW  -  Neural networks
KW  - Artificial Neural Network; Classification accuracy; Classification methods; Data sets; Diabetes mellitus; Experimental system; Medical data; Medical diagnosis; Missing values; Pima Indian Diabetes; Pre-processing method; Preprocessing techniques; Real-world problem
JF  - DSDE 2010 - International Conference on Data Storage and Data Engineering
SP  - 159
EP  - 163
SN  - 9780769539584
DO  - 10.1109/DSDE.2010.58
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952657106&doi=10.1109%2FDSDE.2010.58&partnerID=40&md5=d73854eee2262a28c3ef4951cccaf4ff
N1  - cited By 77
N2  - Many real world problems can be solved with Artificial Neural Networks in the areas of pattern recognition, signal processing and medical diagnosis. Most of the medical data set is seldom complete. Artificial Neural Networks require complete set of data for an accurate classification. This paper dwells on the various missing value techniques to improve the classification accuracy. The proposed system also investigates the impact on preprocessing during the classification. A classifier was applied to Pima Indian Diabetes Dataset and the results were improved tremendously when using certain combination of preprocessing techniques. The experimental system achieves an excellent classification accuracy of 99% which is best than before. © 2010 IEEE.
ER  - 
TY  - CONF
T1  - Monitor blood glucose levels via breath analysis system and sparse representation approach
A1  - Guo, D
A1  - Zhang, D
A1  - Li, N
Y1  - 2010///
KW  -  Acetone; Blood; Sensors
KW  -  Glucose
KW  - Abnormal concentrations; Blood glucose level; Breath analysis; Classification approach; Collection methods; Human breath; Signal preprocessing; Sparse representation; System structures
JF  - Proceedings of IEEE Sensors
SP  - 1238
EP  - 1241
SN  - 9781424481682
DO  - 10.1109/ICSENS.2010.5690611
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951876314&doi=10.1109%2FICSENS.2010.5690611&partnerID=40&md5=0bb5499f1d5eabbf849940dad7a308aa
N1  - cited By 6
N2  - It has been reported that the abnormal concentration of acetone in exhaled air is an indicator of diabetes and the concentration rises progressively with the blood glucose level of patients. Therefore, the acetone in human breath can be used to monitor the development of diabetes. In this paper, we introduce a breath analysis system to measure acetone in human breath, and therefore to evaluate the blood glucose levels of diabetics. The system structure, breath collection method, and signal preprocessing method are introduced. To enhance the system performance, we use a novel classification approach, i.e., Sparse Representation based Classification (SRC), to classify diabetics' breath samples into different blood glucose levels. Experimental results show that coupling with SRC, the system is able to classify these levels with satisfactory accuracy. ©2010 IEEE.
ER  - 
TY  - JOUR
T1  - A Hybrid Proposed Fundus Image Enhancement Framework for Diabetic Retinopathy
A1  - Qureshi, I
A1  - Ma, J
A1  - Shaheed, K
Y1  - 2019///
KW  -  Color modeling; Contrast Enhancement; Diabetic retinopathy; Histogram-based enhancement procedures; Nonlinear contrast enhancement
KW  -  Image enhancement
KW  - Angiography; Color; Color computer graphics; Eye protection; Grading; Graphic methods; Image segmentation; Luminance; Ophthalmology; Signal to noise ratio
PB  - MDPI AG
JF  - Algorithms
VL  - 12
IS  - 1
DO  - 10.3390/a12010014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060451764&doi=10.3390%2Fa12010014&partnerID=40&md5=4ea9f95a1ca127d3b5b8fab52231fa0d
N1  - cited By 29
N2  - Diabetic retinopathy (DR) is a complication of diabetes and is known as visual impairment, and is diagnosed in various ethnicities of the working-age population worldwide. Fundus angiography is a widely applicable modality used by ophthalmologists and computerized applications to detect DR-based clinical features such as microaneurysms (MAs), hemorrhages (HEMs), and exudates (EXs) for early screening of DR. Fundus images are usually acquired using funduscopic cameras in varied light conditions and angles. Therefore, these images are prone to non-uniform illumination, poor contrast, transmission error, low brightness, and noise problems. This paper presents a novel and real-time mechanism of fundus image enhancement used for early grading of diabetic retinopathy, macular degeneration, retinal neoplasms, and choroid disruptions. The proposed system is based on two folds: (i) An RGB fundus image is initially taken and converted into a color appearance module (called lightness and denoted as J) of the CIECAM02 color space model to obtain image information in grayscale with bright light. Afterwards, in step (ii), the achieved J component is processed using a nonlinear contrast enhancement approach to improve the textural and color features of the fundus image without any further extraction steps. To test and evaluate the strength of the proposed technique, several performance and quality parameters-namely peak signal-to-noise ratio (PSNR), contrast-to-noise ratio (CNR), entropy (content information), histograms (intensity variation), and a structure similarity index measure (SSIM)-were applied to 1240 fundus images comprised of two publicly available datasets, DRIVE and MESSIDOR. It was determined from the experiments that the proposed enhancement procedure outperformed histogram-based approaches in terms of contrast, sharpness of fundus features, and brightness. This further revealed that it can be a suitable preprocessing tool for segmentation and classification of DR-related features algorithms. © 2018 by the authors.
ER  - 
TY  - CONF
T1  - Preemptive Diagnosis of Diabetes Mellitus Using Machine Learning
A1  - Alassaf, R A
A1  - Alsulaim, K A
A1  - Alroomi, N Y
A1  - Alsharif, N S
A1  - Aljubeir, M F
A1  - Olatunji, S O
A1  - Alahmadi, A Y
A1  - Imran, M
A1  - Alzahrani, R A
A1  - Alturayeif, N S
Y1  - 2018///
KW  -  Chronic disease; Diabetes mellitus; Health complications; Intelligent method; K-nearest neighbors; Relevant features; Testing accuracy; ve Bayes
KW  -  Diagnosis
KW  - Barium compounds; Deterioration; Diseases; Motion compensation; Nearest neighbor search; Neural networks; Support vector machines
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 21st Saudi Computer Society National Computer Conference, NCC 2018
SN  - 9781538641095
DO  - 10.1109/NCG.2018.8593201
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061478764&doi=10.1109%2FNCG.2018.8593201&partnerID=40&md5=a51b1da91a7e8cfb956e3b5a74f67a58
N1  - cited By 6
N2  - Diabetes Mellitus (DM) is one of the most prevalent chronic diseases in the world with around 150 million patients. Patients with chronic diseases are highly susceptible to deterioration in their physical and mental health; consequently, hindering their independence, restricting their daily activities imposing a large financial burden on them and the government. If not discovered early, chronic diseases may lead to serious health complications or in extreme cases, death. Diagnostic solutions have been explored using intelligent methods, however, different ethnic groups have variant factors leading to the development of a disease. Therefore, the proposed system aims to preemptively diagnose DM in a region never explored before. Data are retrieved from King Fahd University Hospital (KFUH) in Khobar, Saudi Arabia. Data undergoes preprocessing to identify relevant features and prepare for identification/classification process. Experimental results show that ANN outperformed SVM, Naïve Bayes, and K-Nearest Neighbor with the testing accuracy of 77.5%. © 2018 IEEE.
ER  - 
TY  - JOUR
T1  - Diabetes Type 2: Poincaré data preprocessing for quantum machine learning
A1  - Sierra-Sosa, D
A1  - Arcila-Moreno, J D
A1  - Garcia-Zapirain, B
A1  - Elmaghraby, A
Y1  - 2021///
KW  -  Data preprocessing; Geometrical transformation; Hybrid approach; Quantum machines; Quantum state preparation; Selection techniques; Stokes parameters; Type 2 diabetes mellitus
KW  -  Quantum theory
KW  - Classification (of information); Machine learning; Mathematical transformations; Pipelines; Variational techniques
PB  - Tech Science Press
JF  - Computers, Materials and Continua
VL  - 67
IS  - 2
SP  - 1849
EP  - 1861
DO  - 10.32604/cmc.2021.013196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102487895&doi=10.32604%2Fcmc.2021.013196&partnerID=40&md5=1b6e30346b2ee4e25d7d52a332ae2aac
N1  - cited By 3
N2  - Quantum Machine Learning (QML) techniques have been recently attracting massive interest. However reported applications usually employ synthetic or well-known datasets. One of these techniques based on using a hybrid approach combining quantum and classic devices is the Variational Quantum Classifier (VQC), which development seems promising.Albeit being largely studied, VQC implementations for "real-world" datasets are still challenging on Noisy Intermediate Scale Quantum devices (NISQ). In this paper we propose a preprocessing pipeline based on Stokes parameters for data mapping. This pipeline enhances the prediction rates when applying VQC techniques, improving the feasibility of solving classification problems using NISQ devices. By including feature selection techniques and geometrical transformations, enhanced quantum state preparation is achieved.Also, a representation based on the Stokes parameters in the Poincare Sphere is possible for visualizing the data.Our results showthat by using the proposed techniques we improve the classification score for the incidence of acute comorbid diseases in Type 2 Diabetes Mellitus patients. We used the implemented version of VQC available on IBM s framework Qiskit, and obtained with two and three qubits an accuracy of 70% and 72% respectively. © 2021 Tech Science Press. All rights reserved.
ER  - 
TY  - JOUR
T1  - Diabetic complication prediction using a similarity-enhanced latent Dirichlet allocation model
A1  - Ding, S
A1  - Li, Z
A1  - Liu, X
A1  - Huang, H
A1  - Yang, S
Y1  - 2019///
KW  -  K nearest neighbor (KNN); Latent Dirichlet allocation; Logistic regressions; Multi-label classifications; Prediction accuracy; Similarity indices; Support vector machine (SVMs); Topic minings
KW  -  Medical computing
KW  - Classification (of information); Data mining; Decision trees; Deep neural networks; Forecasting; Health risks; Medical imaging; Nearest neighbor search; Statistics; Support vector machines
PB  - Elsevier Inc.
JF  - Information Sciences
VL  - 499
SP  - 12
EP  - 24
DO  - 10.1016/j.ins.2019.05.037
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066245316&doi=10.1016%2Fj.ins.2019.05.037&partnerID=40&md5=055e3c0c7b416f83d90822531fbf7040
N1  - cited By 16
N2  - Diabetes and its complications have been recognized worldwide as a major public health threat. Predicting diabetic complications is regarded as a highly effective technique for increasing the survival rate of diabetic patients. While many studies currently use medical images and structured medical records, very limited efforts have been dedicated to applying data mining techniques for unstructured textual medical records, such as admission and discharge records. Moreover, the similarities among medical records that are overlooked by existing approaches could potentially improve the accuracy of prediction models. In this paper, we propose an approach for diabetic complication prediction based on a similarity-enhanced latent Dirichlet allocation (seLDA) model. Specifically, we first estimate the similarity between textual medical records after data preprocessing, and then we perform seLDA-based diabetic complication topic mining based on similarity constraints. Finally, we construct a prediction model by solving a multilabel classification problem with support vector machines (SVMs). The experimental results show that our approach outperforms the conventional LDA-based approach in similarity indices by 22.49%. Additionally, our approach shows significant improvements in prediction accuracy over four other representative seLDA-based approaches, including random forests (RF), k-nearest neighbors (KNN), logistic regression (LR) and deep neural networks (DNNs). © 2019
ER  - 
TY  - CONF
T1  - Comparison selection of attributes in preprocessing data for diagnosis of diabetes
A1  - Maulana, F
A1  - Endah, S N
Y1  - 2017///
KW  -  Attribute selection; Backward elimination; K fold cross validations; K nearest neighbor algorithm; Preprocessing Data; Selection of attributes; Stepwise regression; Technological advances
KW  -  Data handling
KW  - Diagnosis; Genetic algorithms; Information management; Medical problems; Motion compensation; Nearest neighbor search; Pattern recognition
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 2017 1st International Conference on Informatics and Computational Sciences, ICICoS 2017
VL  - 2018-January
SP  - 141
EP  - 146
SN  - 9781538609033
DO  - 10.1109/ICICOS.2017.8276352
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046750051&doi=10.1109%2FICICOS.2017.8276352&partnerID=40&md5=079add9e6e80cfa183a616d219528042
N1  - cited By 3
N2  - Diabetes is the sixth leading cause of death for all ages in Indonesia according to Indonesia Health Profile 2008. The high mortality rate caused by diabetes causes the need for early detection so that it can be done quickly. In line with technological advances, Diagnosis of diabetes can be done with a system where the system can help detect diabetes accurately based on medical record data of diabetes. The problems that often arise in medical record data processing is the data that amounts to many, the attribute is not relevant, missing value, and vulnerable to outliers so that required efficient data preprocessing so that the resulting accuracy becomes more leverage and computation time becomes more minimal. This study examines the technique of efficient data preprocessing by comparing Backward Elimination (BE), Forward Selection (FS) and Stepwise Regression (SR) algorithms in performing attribute selection, while for diabetic data classification using K-Nearest Neighbor Algorithm. The test was conducted using K-Fold Cross Validation strategy with fold 10 value from medical record data of information management of diabetes disease of Pusat Pertamina Hospital as many as 1260 Data. The results showed that attribute selection in the preprocessing stages of data can improve accuracy in diagnosing diabetes. The highest accuracy is obtained from Stepwise Regression Algorithm on preprocessing data. © 2017 IEEE.
ER  - 
TY  - JOUR
T1  - Automated detection and classification of fundus diabetic retinopathy images using synergic deep learning model
A1  - Shankar, K
A1  - Sait, A R W
A1  - Gupta, D
A1  - Lakshmanaprabu, S K
A1  - Khanna, A
A1  - Pandey, H M
Y1  - 2020///
KW  -  Automated detection and classification; Diabetic retinopathy; Glucose level; Histogram based segmentation; Learning models; Messidor dataset; Research papers; Warning signs
KW  -  Deep learning
KW  - Classification (of information); Eye protection; Image classification; Image segmentation; Learning systems
PB  - Elsevier B.V.
JF  - Pattern Recognition Letters
VL  - 133
SP  - 210
EP  - 216
DO  - 10.1016/j.patrec.2020.02.026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081571516&doi=10.1016%2Fj.patrec.2020.02.026&partnerID=40&md5=99f9bc9bcd14224a843575fa03d6ad26
N1  - cited By 128
N2  - In recent days, the incidence of Diabetic Retinopathy (DR)has become high, affecting the eyes because of drastic increase in the glucose level in blood. Globally, almost half of the people under the age of 70 gets severely affected by diabetes. In the absence of earlier recognition and proper medication, the DR patients tend to lose their vision. When the warning signs are tracked down, the severity level of the disease has to be validated so to take decisions regarding appropriate treatment further. The current research paper focuses on the concept of classification of DR fundus images on the basis of severity level using a deep learning model. This paper proposes a deep learning-based automated detection and classification model for fundus DR images. The proposed method involves various processes namely preprocessing, segmentation and classification. The methods begins with preprocessing stage in which unnecessary noise that exists in the edges is removed. Next, histogram-based segmentation takes place to extract the useful regions from the image. Then, Synergic Deep Learning (SDL) model was applied to classify the DR fundus images to various severity levels. The justification for the presented SDL model was carried out on Messidor DR dataset. The experimentation results indicated that the presented SDL model offers better classification over the existing models. © 2020 Elsevier B.V.
ER  - 
TY  - JOUR
T1  - A predictive risk level classification of diabetic patients using deep learning modified neural network
A1  - alias Balamurugan, S
A1  - Salomi, M
Y1  - 2021///
KW  -  Deep learning modified neural network; Distributed file systems; Hadoop distributed file system; Improved K-mean clustering algorithm; K-mean clustering; K-means clustering algorithms; K-means++ clustering; Modified neural networks; Naive baye; Naive bayes
KW  -  K-means clustering
KW  - Barium compounds; Data mining; Deep learning; File organization; Forecasting; Hospital data processing; Learning algorithms; Learning systems; Risk analysis; Risk assessment
PB  - Springer Science and Business Media Deutschland GmbH
JF  - Journal of Ambient Intelligence and Humanized Computing
VL  - 12
IS  - 7
SP  - 7703
EP  - 7713
DO  - 10.1007/s12652-020-02490-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090301584&doi=10.1007%2Fs12652-020-02490-1&partnerID=40&md5=d055f8e2f7acc54e7975dcdcbb77626c
N1  - cited By 1
N2  - In health care firm, data mining (DM) has an effectual role in predicting the diseases. Today, diabetes is the chief global health issue. Several algorithms are introduced for predicting the diabetes disease and its accuracy estimation. Yet, there is no effectual algorithm for providing the severity of diabetes in respect of ratio which interprets the impact of diabetes on different organs of the human body. To overcome such drawbacks, predictive and risk level classification of diabetes patients using DLMNN and Naïve Bayes (NB) classification methods is system model. This system model system comprises 2 phases namely, phase-1: diabetic disease prediction model, and phase-2: risk analysis. In phase-1, the patient data are taken as of the dataset. Then, from this patient dataset repeated data are removed using HDFS Map Reduce (). Next, as the preprocessing stage, the missing attributes are replaced by averaging the considered data. After that, from the preprocessed data the disease is predicted using DLMNN classification method which results in obtaining the diabetic patient data. Then, the diabetic patient data are sent to phase-2. In phase 2, the missing attributes are replaced using the same average method. Next, the patient data is sorted centered on age utilizing recursive K-means clustering algorithm. Finally, the clustered patient data is classified using the NB classifier algorithm. Experiential results contrasted the system model modified deep learning algorithm with the existing IKMC algorithm in rapports of precision, accuracy, F-measure, and recall. The outcomes confirmed that the system model diabetes prediction and analysis model shows better results on considering the existent methods. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - An Effective Approach for Detecting Diabetes using Deep Learning Techniques based on Convolutional LSTM Networks
A1  - Chowdary, P B K
A1  - Kumar R.U., Dr.
Y1  - 2021///
KW  -  Automated systems; Clinical data; Convolutional long short-term memory; Detection methods; Diabetes detection; Diabetes prediction; Effective approaches; Learning techniques; Physical data; Pre-processing
KW  -  Long short-term memory
KW  - Automation; Brain; Classification (of information); Convolution; Convolutional neural networks; Forecasting; Learning algorithms
PB  - Science and Information Organization
JF  - International Journal of Advanced Computer Science and Applications
VL  - 12
IS  - 4
SP  - 519
EP  - 525
DO  - 10.14569/IJACSA.2021.0120466
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105795258&doi=10.14569%2FIJACSA.2021.0120466&partnerID=40&md5=6a5790c55c85f169a9aa18aca882d081
N1  - cited By 2
N2  - The most common disorder affecting millions of population worldwide due to insufficient release of insulin by pancreas is diabetes. Early detection or precaution of diabetes is necessary, otherwise leads to many complicated problems. Predicting diabetes at early stages with appropriate treatment, individuals can maintain a happy life. If the conventional diabetes detection method is tedious, the identification of diabetes from clinical and physical data requires an automated system. This paper proposes an approach to enhance diabetes prediction using deep learning techniques. Based on the Convolutional Long Short-term Memory (CLSTM), we developed a diabetes classification model and compared with the existing methods on the Pima Indians Diabetes Database (PIDD). We assessed the findings of various classification approaches in this study. The proposed approach is further improved by an efficient preprocessing mechanism called multivariate imputation by chained equations. The outcomes are promising compared to existing machine learning approaches and other research models. © 2021
ER  - 
TY  - JOUR
T1  - A novel machine learning framework for diagnosing the type 2 diabetics using temporal fuzzy ant miner decision tree classifier with temporal weighted genetic algorithm
A1  - Bhuvaneswari, G
A1  - Manikandan, G
Y1  - 2018///
KW  -  Ant miners; Cross validation; Decision tree classifiers; Detection accuracy; Diabetic diagnosis; Fuzzy decision trees; TFAMT; TWGA
KW  -  Data mining
KW  - Behavioral research; Decision making; Decision trees; Diagnosis; Fuzzy inference; Fuzzy rules; Genetic algorithms; Image enhancement; Learning systems; Miners; Trees (mathematics)
PB  - Springer-Verlag Wien
JF  - Computing
VL  - 100
IS  - 8
SP  - 759
EP  - 772
DO  - 10.1007/s00607-018-0599-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044174417&doi=10.1007%2Fs00607-018-0599-4&partnerID=40&md5=400235e12e2904e27032936b3a8eea9e
N1  - cited By 12
N2  - Diabetic is becoming a very serious disease today for the most of people all over the world due to the unhealthy food habits. For predicting the diabetes, we introduce a new diabetic diagnosis system which combines a newly proposed temporal feature selection and temporal fuzzy ant miner tree (TFAMT) classifier for effective decision making in type-2 diabetes analysis. Moreover, a new temporal weighted genetic algorithm is proposed in this work for enhancing the detection accuracy by preprocessing the text and image data. Moreover, intelligent fuzzy rules are extracted from the weighted temporal capabilities with ant miner fuzzy decision tree classifier, and then fuzzy rule extractor is used to reduce the variety of functions in the extracted regulations. We empirically evaluated the effectiveness of the proposed TFAMT–TWGA model using the UCI Repository dataset and the collected retinopathy image dataset. The outcomes are analyzed and as compared with other exiting works. Furthermore, the detection accuracy is proven by way of using the ten-fold cross validation. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature.
ER  - 
TY  - JOUR
T1  - Multi-level deep supervised networks for retinal vessel segmentation
A1  - Mo, J
A1  - Zhang, L
Y1  - 2017///
KW  -  Diagnostic Techniques
KW  -  Ophthalmological; Humans; Neural Networks (Computer); Retinal Diseases; Retinal Vessels
KW  - algorithm; Article; artificial neural network; back propagation; classifier; disease classification; false negative result; image processing; image segmentation; medical education; priority journal; retina blood vessel; retina image; vascularization; diagnostic imaging; human; retina blood vessel; retina disease; visual system examination
PB  - Springer Verlag
JF  - International Journal of Computer Assisted Radiology and Surgery
VL  - 12
IS  - 12
SP  - 2181
EP  - 2193
DO  - 10.1007/s11548-017-1619-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020130055&doi=10.1007%2Fs11548-017-1619-0&partnerID=40&md5=671dbf65be3d90024e15d7e347c79b3d
N1  - cited By 116
N2  - Purpose: Changes in the appearance of retinal blood vessels are an important indicator for various ophthalmologic and cardiovascular diseases, including diabetes, hypertension, arteriosclerosis, and choroidal neovascularization. Vessel segmentation from retinal images is very challenging because of low blood vessel contrast, intricate vessel topology, and the presence of pathologies such as microaneurysms and hemorrhages. To overcome these challenges, we propose a neural network-based method for vessel segmentation. Methods: A deep supervised fully convolutional network is developed by leveraging multi-level hierarchical features of the deep networks. To improve the discriminative capability of features in lower layers of the deep network and guide the gradient back propagation to overcome gradient vanishing, deep supervision with auxiliary classifiers is incorporated in some intermediate layers of the network. Moreover, the transferred knowledge learned from other domains is used to alleviate the issue of insufficient medical training data. The proposed approach does not rely on hand-crafted features and needs no problem-specific preprocessing or postprocessing, which reduces the impact of subjective factors. Results: We evaluate the proposed method on three publicly available databases, the DRIVE, STARE, and CHASE_DB1 databases. Extensive experiments demonstrate that our approach achieves better or comparable performance to state-of-the-art methods with a much faster processing speed, making it suitable for real-world clinical applications. The results of cross-training experiments demonstrate its robustness with respect to the training set. Conclusions: The proposed approach segments retinal vessels accurately with a much faster processing speed and can be easily applied to other biomedical segmentation tasks. © 2017, CARS.
ER  - 
TY  - JOUR
T1  - A comprehensive study of machine learning methods on diabetic retinopathy classification
A1  - Gurcan, O F
A1  - Beyca, O F
A1  - Dogan, O
Y1  - 2021///
KW  -  Classification system; Cost-effective solutions; Diagnostic procedure; Dimensionality reduction algorithms; Low-resource settings; Machine learning methods; Performance comparison; World Health Organization
KW  -  Learning systems
KW  - Automation; Classification (of information); Convolutional neural networks; Cost effectiveness; Deep neural networks; Diagnosis; Dimensionality reduction; Extraction; Eye protection; Feature extraction; Health risks; Transfer learning
PB  - Atlantis Press
JF  - International Journal of Computational Intelligence Systems
VL  - 14
IS  - 1
SP  - 1132
EP  - 1141
DO  - 10.2991/IJCIS.D.210316.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104639972&doi=10.2991%2FIJCIS.D.210316.001&partnerID=40&md5=df7d2f4cf832412685c9b5bb67c4abae
N1  - cited By 4
N2  - Diabetes is one of the emerging threats to public health all over the world. According to projections by the World Health Organization, diabetes will be the seventh foremost cause of death in 2030 (WHO, Diabetes, 2020. https://www.afro.who.int/healthtopics/diabetes). Diabetic retinopathy (DR) results from long-lasting diabetes and is the fifth leading cause of visual impairment, worldwide. Early diagnosis and treatment processes are critical to overcoming this disease. The diagnostic procedure is challenging, especially in low-resource settings, or time-consuming, depending on the ophthalmologist’s experience. Recently, automated systems now address DR classification tasks. This study proposes an automated DR classification system based on preprocessing, feature extraction, and classification steps using deep convolutional neural network (CNN) and machine learning methods. Features are extracted from a pretrained model by the transfer learning approach. DR images are classified by several machine learning methods. XGBoost outperforms other methods. Dimensionality reduction algorithms are applied to obtain a lowerdimensional representation of extracted features. The proposed model is trained and evaluated on a publicly available dataset. Grid search and calibration are used in the analysis. This study provides researchers with performance comparisons of different machine learning methods. The proposed model offers a robust solution for detecting DR with a small number of images. We used a transfer learning approach, which differs from other studies in the literature, during the feature extraction. It provides a data-driven, cost-effective solution, which includes comprehensive preprocessing and fine-tuning processes. © 2021 The Authors. Published by Atlantis Press B.V.
ER  - 
TY  - JOUR
T1  - Diabetes Mellitus Prediction and Severity Level Estimation Using OWDANN Algorithm
A1  - R, A
A1  - R, N
Y1  - 2021///
KW  -  Algorithms; Diabetes Mellitus; Humans
KW  - algorithm; diabetes mellitus; human
PB  - NLM (Medline)
JF  - Computational intelligence and neuroscience
VL  - 2021
SP  - 5573179
EP  - 5573179
DO  - 10.1155/2021/5573179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115282746&doi=10.1155%2F2021%2F5573179&partnerID=40&md5=cc3667896894997a725288815ea7b351
N1  - cited By 4
N2  - Today, diabetes is one of the most prevalent, chronic, and deadly diseases in the world owing to some complications. If accurate early diagnosis is feasible, the risk factor and incidence of diabetes may be greatly decreased. Diabetes prediction is stable and reliable, since there are only minimal labelling evidence and outliers found in the datasets of diabetes. Numerous works coped with diabetes disease prediction and provided the solution. But the existing methods proffered low accuracy detection and consumed more training time. So, this paper proposed an OWDANN algorithm for diabetes mellitus disease prediction and severity level estimation. The proposed system mainly consists of two phases, namely, disease prediction and severity level estimation phase. In the disease prediction phase, the preprocessing is performed for the Pima dataset. Then, the features are extracted from the preprocessed data, and finally, the classification step is performed by using OWDANN. In the severity level estimation phase, the diabetes positive dataset is preprocessed first. Then, the features are extracted, and lastly, the severity level is predicted using GDHC. The extensive experimental results showed that the proposed system outperforms with 98.97% accuracy, 94.98% sensitivity, 95.62% specificity, 97.02% precision, 93.84% recall, 9404% f-measure, 0.094% FDR, and 0.023% FPR compared with the state-of-the-art methods. Copyright © 2021 Annamalai R and Nedunchelian R.
ER  - 
TY  - CONF
T1  - Effect of biosignal preprocessing and recording length on clinical decision making for cardiac autonomic neuropathy
A1  - Jelinek, H J
A1  - Alothman, T
A1  - Cornforth, D J
A1  - Khalaf, K
A1  - Khandoker, A H
Y1  - 2014///
KW  - Biosignals; Clinical decision making; Controlling parameters; ECG recording; Heart rate variability; RR intervals; Time and frequency domains; Treatment outcomes
PB  - IEEE Computer Society
JF  - 2014 8th Conference of the European Study Group on Cardiovascular Oscillations, ESGCO 2014
SP  - 3
EP  - 4
SN  - 9781479939695
DO  - 10.1109/ESGCO.2014.6847490
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904464000&doi=10.1109%2FESGCO.2014.6847490&partnerID=40&md5=670abb977f4d67e5bc11f4622f17ae4e
N1  - cited By 9
N2  - Early identification of cardiac autonomic neuropathy (CAN) leads to better treatment outcomes. Heart rate variability (HRV) analysis allows identification of CAN but is sensitive to the length of recording and the presence of artifacts and ectopics, requiring preprocessing and consideration of length of recording. RR intervals from 10-second and 5-minute ECG recordings from patients with no CAN, early CAN and definite CAN were preprocessed using adaptive filtering with the controlling parameter c set at 0.2, 0.5 and 0.8, and time and frequency domain HRV analysis applied. Early CAN and definite CAN required different setting of c with respect to the length of recording. The 5-minute recording with c=0.2 provided the best results using RMSSD for normal versus eCAN (p=0.0007) and for eCAN versus dCAN (0.019). Clinically, this has potential use in general practice for screening patients at risk, newly diagnosed with diabetes, or for follow-up during the course of diabetes. © 2014 IEEE.
ER  - 
TY  - CONF
T1  - Near-infrared data pre-processing for glucose level prediction in blood
A1  - Abd Rahim, I M
A1  - Rahim, H A
A1  - Ghazali, R
Y1  - 2020///
KW  -  Data distribution; Data preprocessing; Multisensory systems; Near infra red; Non-linear predictions; Sensor implementation; Wavelength regions; Wavelength selection
KW  -  Data handling
KW  - Blood; Forecasting; Glucose; Infrared devices; Systems engineering
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2020 IEEE 10th International Conference on System Engineering and Technology, ICSET 2020 - Proceedings
SP  - 73
EP  - 78
SN  - 9781728199108
DO  - 10.1109/ICSET51301.2020.9265391
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098284549&doi=10.1109%2FICSET51301.2020.9265391&partnerID=40&md5=abf75810546e4e8284ea5dd72958879b
N1  - cited By 0
N2  - Estimated, 347 million people suffered from diabetes in 2004, and around 3.4 million patients died from consequences of high blood sugar. There are various techniques investigated by researchers in measuring the glucose level in human blood non-invasively, including ultrasonic sensor implementation, multisensory systems, absorbance of transmittance, bio-impedance, voltage intensity, and thermography. The implementation of near infrared (NIR) in predicting the glucose level in blood had been investigated in and the process of data pre-processing is presented in this paper. The selection of the wavelength region by previous researchers has been a debate as the suitable wavelength chose vary from one researcher to another. Despite the wavelength selection problem, the other fragment that needed a close attention is the process of enhancing the NIR data obtained from the experiment. The data pre-processing techniques used in this paper are the data filtering, data sampling, interval correction, wavelength selection and data distribution phases. The processed data then fed as an input to the linear and nonlinear prediction system implemented in this study. © 2020 IEEE
ER  - 
TY  - JOUR
T1  - Inter-Patient Classification with Encoded Peripheral Pulse Series and Multi-Task Fusion CNN: Application in Type 2 Diabetes
A1  - Ouyang, C
A1  - Gan, Z
A1  - Zhen, J
A1  - Guan, Y
A1  - Zhu, X
A1  - Zhou, P
Y1  - 2021///
KW  -  Aged; Algorithms; Diabetes Mellitus
KW  -  Classification accuracy; Diabetes mellitus; Feature recognition; Oral glucose tolerance tests; Processing modules; Recurrence plot; State-of-the-art algorithms; Transition fields
KW  -  Computer
KW  -  Multilayer neural networks
KW  -  Type 2; Humans; Middle Aged; Neural Networks
KW  -  algorithm; Article; computer model; controlled study; convolutional neural network; event related potential; feature extraction; female; gene expression; glucose tolerance test; Gramian angular field; human; information science; machine learning; major clinical study; male; Markov decision process; Markov transition field; non insulin dependent diabetes mellitus; oral glucose tolerance test; phenotype; physician; radial artery; receiver operating characteristic; recurrence plots; time series analysis; wrist; aged; middle aged; non insulin dependent diabetes mellitus
KW  - Blood; Convolutional neural networks; Glucose
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - IEEE Journal of Biomedical and Health Informatics
VL  - 25
IS  - 8
SP  - 3130
EP  - 3140
DO  - 10.1109/JBHI.2021.3061114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101795598&doi=10.1109%2FJBHI.2021.3061114&partnerID=40&md5=f9b510f6fb3eb78a7cf1e4de33543e75
N1  - cited By 1
N2  - Diabetes mellitus, a chronic disease associated with elevated accumulation of glucose in the blood, is generally diagnosed through an invasive blood test such as oral glucose tolerance test (OGTT). An effective method is proposed to test type 2 diabetes using peripheral pulse waves, which can be measured fast, simply and inexpensively by a force sensor on the wrist over the radial artery. A self-designed pulse waves collection platform includes a wristband, force sensor, cuff, air tubes, and processing module. A dataset was acquired clinically for more than one year by practitioners. A group of 127 healthy candidates and 85 patients with type 2 diabetes, all between the ages of 45 and 70, underwent assessments in both OGTT and pulse data collection at wrist arteries. After preprocessing, pulse series were encoded as images using the Gramian angular field (GAF), Markov transition field (MTF), and recurrence plots (RPs). A four-layer multi-task fusion convolutional neural network (CNN) was developed for feature recognition, the network was well-trained within 30 minutes based on our server. Compared to single-task CNN, multi-task fusion CNN was proved better in classification accuracy for nine of twelve settings with empirically selected parameters. The results show that the best accuracy reached 90.6% using an RP with threshold $ε$ of 6000, which is competitive to that using state-of-the-art algorithms in diabetes classification. © 2013 IEEE.
ER  - 
TY  - JOUR
T1  - Preprocessing Handling to Enhance Detection of Type 2 Diabetes Mellitus based on Random Forest
A1  - Ramadhan, N G
A1  - Adiwijaya
A1  - Romadhony, A
Y1  - 2021///
KW  -  Data augmentation; Data preprocessing; Diabetes mellitus; Imbalanced data; Missing values; Non-communicable disease; Random forest methods; Random forests; Type 2 diabetes mellitus; Type-2 diabetes
KW  -  Decision trees
KW  - Logistic regression; Random forests
PB  - Science and Information Organization
JF  - International Journal of Advanced Computer Science and Applications
VL  - 12
IS  - 7
SP  - 223
EP  - 228
DO  - 10.14569/IJACSA.2021.0120726
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112209875&doi=10.14569%2FIJACSA.2021.0120726&partnerID=40&md5=287dcb4aa801c41bca9120b5e99ae86c
N1  - cited By 1
N2  - Diabetes is a non-communicable disease that has a death rate of 70% in the world. Majority of diabetes cases, 90–95%, are of diabetes cases are type 2 diabetes which is caused by an unhealthy lifestyle. Type 2 diabetes can be detected earlier by using examination that contains diabetes-related parameters. However, the dataset does not always contain complete information, the distribution between positive and negative classes is mostly imbalanced, and some parameters have low importance to the decision class. To overcome the problems, this study needs to carry out preprocessing to improve detection precision and recall. In this paper, propose an approach on dataset preprocessing, which is applied to diabetes prediction. The preprocessing approach consists of the following process: missing value process, imbalanced data process, feature importance process, and data augmentation process. The data preprocessing process uses the median for missing value, random oversampling for imbalanced data, the Gini score in the random forest for feature importance, and posterior distribution for data augmentation. This research used random forest and logistic regression as classification algorithms. The experimental results show that the classification increased by 20% precision and 24% recall by applying proposed method and random forest method compared to without proposed method and random forest method. © 2021. All Rights Reserved.
ER  - 
